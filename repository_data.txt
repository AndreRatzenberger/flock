# Repository Analysis

## Repository Statistics

- **Extensions analyzed**: .md, .py
- **Number of files analyzed**: 151
- **Total lines of code (approx)**: 26362

## Project Files

### 1. docs\about\changelog.md

- **File ID**: file_0
- **Type**: File
- **Line Count**: 8
- **Description**: File at docs\about\changelog.md

**Content**:
```
---
hide: # Optional: Hide table of contents on simple pages
  - toc
---

# Changelog

*Documentation in progress...*

```

---

### 2. docs\about\contributing.md

- **File ID**: file_1
- **Type**: File
- **Line Count**: 8
- **Description**: File at docs\about\contributing.md

**Content**:
```
---
hide: # Optional: Hide table of contents on simple pages
  - toc
---

# Contributing

*Documentation in progress...*

```

---

### 3. docs\components\evaluators.md

- **File ID**: file_2
- **Type**: File
- **Line Count**: 8
- **Description**: File at docs\components\evaluators.md

**Content**:
```
---
hide: # Optional: Hide table of contents on simple pages
  - toc
---

# Evaluators

*Documentation in progress...*

```

---

### 4. docs\components\index.md

- **File ID**: file_3
- **Type**: File
- **Line Count**: 8
- **Description**: File at docs\components\index.md

**Content**:
```
---
hide: # Optional: Hide table of contents on simple pages
  - toc
---

# Overview

*Documentation in progress...*

```

---

### 5. docs\components\modules.md

- **File ID**: file_4
- **Type**: File
- **Line Count**: 8
- **Description**: File at docs\components\modules.md

**Content**:
```
---
hide: # Optional: Hide table of contents on simple pages
  - toc
---

# Modules

*Documentation in progress...*

```

---

### 6. docs\components\tools.md

- **File ID**: file_5
- **Type**: File
- **Line Count**: 8
- **Description**: File at docs\components\tools.md

**Content**:
```
---
hide: # Optional: Hide table of contents on simple pages
  - toc
---

# Tools

*Documentation in progress...*

```

---

### 7. docs\core-concepts\agents.md

- **File ID**: file_6
- **Type**: File
- **Line Count**: 8
- **Description**: File at docs\core-concepts\agents.md

**Content**:
```
---
hide: # Optional: Hide table of contents on simple pages
  - toc
---

# Agents

*Documentation in progress...*

```

---

### 8. docs\core-concepts\declarative.md

- **File ID**: file_7
- **Type**: File
- **Line Count**: 8
- **Description**: File at docs\core-concepts\declarative.md

**Content**:
```
---
hide: # Optional: Hide table of contents on simple pages
  - toc
---

# Declarative Approach

*Documentation in progress...*

```

---

### 9. docs\core-concepts\index.md

- **File ID**: file_8
- **Type**: File
- **Line Count**: 8
- **Description**: File at docs\core-concepts\index.md

**Content**:
```
---
hide: # Optional: Hide table of contents on simple pages
  - toc
---

# Overview

*Documentation in progress...*

```

---

### 10. docs\core-concepts\workflows.md

- **File ID**: file_9
- **Type**: File
- **Line Count**: 8
- **Description**: File at docs\core-concepts\workflows.md

**Content**:
```
---
hide: # Optional: Hide table of contents on simple pages
  - toc
---

# Workflows

*Documentation in progress...*

```

---

### 11. docs\create_doc_boilerplate.py

- **File ID**: file_10
- **Type**: Code File
- **Line Count**: 131
- **Description**: File at docs\create_doc_boilerplate.py
- **Dependencies**: None
- **Used By**: None

**Content**:
```
import os
from pathlib import Path

import yaml

# The mkdocs navigation structure
NAV_STRUCTURE = """
nav:
  - Home: index.md
  
  - Getting Started:
    - Quick Start: getting-started/quickstart.md
    - Installation: getting-started/installation.md
    - Basic Concepts: getting-started/concepts.md
    - Configuration: getting-started/configuration.md
    
  - Core Concepts:
    - Agents: core-concepts/agents.md
    - Type System: core-concepts/type-system.md
    - Workflows: core-concepts/workflows.md
    - Declarative Programming: core-concepts/declarative.md
    - Error Handling: core-concepts/error-handling.md
    
  - Features:
    - Agent Definition: features/agent-definition.md
    - Type Safety: features/type-safety.md
    - Pydantic Integration: features/pydantic.md
    - Agent Chaining: features/agent-chaining.md
    - Lifecycle Hooks: features/lifecycle-hooks.md
    
  - Integrations:
    - Temporal: integrations/temporal.md
    - DSPy: integrations/dspy.md
    - LiteLLM: integrations/litellm.md
    - Tavily: integrations/tavily.md
    
  - Advanced Usage:
    - Custom Agents: advanced/custom-agents.md
    - Complex Workflows: advanced/complex-workflows.md
    - Testing: advanced/testing.md
    - Performance Optimization: advanced/performance.md
    
  - Deployment:
    - Production Setup: deployment/production-setup.md
    - Monitoring: deployment/monitoring.md
    - Scalability: deployment/scalability.md
    - Security: deployment/security.md
    
  - Tutorials:
    - Basic Blog Generator: tutorials/blog-generator.md
    - Multi-Agent Systems: tutorials/multi-agent.md
    - Custom Tool Integration: tutorials/custom-tools.md
    - Error Recovery: tutorials/error-recovery.md
    
  - API Reference:
    - FlockAgent: api/flockagent.md
    - Flock Core: api/flock-core.md
    - Types: api/types.md
    - Utilities: api/utilities.md
    
  - Contributing:
    - Development Setup: contributing/development.md
    - Code Style: contributing/code-style.md
    - Testing Guide: contributing/testing.md
    - Documentation Guide: contributing/documentation.md
    
  - Architecture:
    - Overview: architecture/overview.md
    - Components: architecture/components.md
    - Design Decisions: architecture/design-decisions.md
    
  - Examples:
    - Hello Flock: examples/hello-flock.md
    - Type System Usage: examples/type-system.md
    - Pydantic Models: examples/pydantic.md
    - Chain Gang: examples/chain-gang.md
"""


def create_markdown_file(file_path: Path, title: str) -> None:
    """Create a markdown file with a title and placeholder content."""
    content = f"""# {title}

Documentation in progress...
"""
    file_path.write_text(content)


def extract_paths_from_nav(
    nav_dict: dict, paths: list, current_path: str = ""
) -> None:
    """Recursively extract all markdown file paths from the nav structure."""
    for item in nav_dict:
        if isinstance(item, dict):
            for key, value in item.items():
                if isinstance(value, str):
                    paths.append(value)
                elif isinstance(value, list):
                    extract_paths_from_nav(value, paths, current_path)


def main():
    # Parse the YAML structure
    nav_data = yaml.safe_load(NAV_STRUCTURE)

    # Extract all markdown file paths
    markdown_paths = []
    extract_paths_from_nav(nav_data["nav"], markdown_paths)

    # Create docs directory if it doesn't exist
    docs_dir = Path("docs")
    docs_dir.mkdir(exist_ok=True)

    # Create all necessary directories and markdown files
    for md_path in markdown_paths:
        # Convert path to Path object relative to docs directory
        full_path = docs_dir / md_path

        # Create parent directories if they don't exist
        full_path.parent.mkdir(parents=True, exist_ok=True)

        # Generate title from the filename
        title = os.path.splitext(full_path.name)[0].replace("-", " ").title()

        # Create the markdown file
        create_markdown_file(full_path, title)
        print(f"Created: {full_path}")


if __name__ == "__main__":
    main()

```

---

### 12. docs\deployment\index.md

- **File ID**: file_11
- **Type**: File
- **Line Count**: 8
- **Description**: File at docs\deployment\index.md

**Content**:
```
---
hide: # Optional: Hide table of contents on simple pages
  - toc
---

# Overview

*Documentation in progress...*

```

---

### 13. docs\deployment\temporal.md

- **File ID**: file_12
- **Type**: File
- **Line Count**: 8
- **Description**: File at docs\deployment\temporal.md

**Content**:
```
---
hide: # Optional: Hide table of contents on simple pages
  - toc
---

# Temporal Integration

*Documentation in progress...*

```

---

### 14. docs\getting-started\configuration.md

- **File ID**: file_13
- **Type**: File
- **Line Count**: 113
- **Description**: File at docs\getting-started\configuration.md

**Content**:
```
---
hide:
  - toc
---

# Configuration ⚙️

Flock offers several ways to configure its behavior, from setting up LLM providers to controlling logging and execution modes.

## Environment Variables (.env file)

The primary and recommended way to configure Flock and its integrations is through **environment variables**. Flock uses the `python-decouple` library to read these variables, prioritizing:

1.  Environment variables set directly in your system.
2.  Values defined in a `.env` file located in your project's root directory.

**Create a `.env` file in your project root:**

```dotenv
# .env - Example Configuration

# --- LLM Provider API Keys (Required) ---
# Add keys for the providers you use (OpenAI, Anthropic, Gemini, Azure, etc.)
# Refer to litellm documentation for specific variable names
OPENAI_API_KEY="your-openai-api-key"
# ANTHROPIC_API_KEY="your-anthropic-api-key"
# GOOGLE_API_KEY="your-google-api-key" # For Gemini

# --- Default Flock Settings ---
DEFAULT_MODEL="openai/gpt-4o" # Default LLM used if not specified by Agent/Flock

# --- Tool-Specific Keys (Optional) ---
TAVILY_API_KEY="your-tavily-search-key"
GITHUB_PAT="your-github-personal-access-token" # For GitHub tools
GITHUB_REPO="your-username/your-repo-name"      # For GitHub tools
GITHUB_USERNAME="your-github-username"        # For GitHub tools
AZURE_SEARCH_ENDPOINT="your-azure-search-url"   # For Azure Search tools
AZURE_SEARCH_API_KEY="your-azure-search-key"    # For Azure Search tools
# AZURE_SEARCH_INDEX_NAME="your-default-index"  # For Azure Search tools

# --- Temporal Integration (Optional) ---
# TEMPORAL_SERVER_URL="localhost:7233" # Default if Temporal is enabled

# --- Logging & Debugging ---
# LOCAL_DEBUG="True" # Set to True to force local execution even if enable_temporal=True in code (DEPRECATED - use enable_temporal=False instead)
LOG_LEVEL="INFO" # Logging level (DEBUG, INFO, WARNING, ERROR)
LOGGING_DIR="logs" # Directory to store log files

# --- Telemetry (OpenTelemetry/Jaeger - Optional) ---
# OTEL_SERVICE_NAME="flock-service"
# JAEGER_ENDPOINT="http://localhost:14268/api/traces" # Thrift HTTP endpoint
# JAEGER_TRANSPORT="http" # Or "grpc" (e.g., "localhost:14250")
# OTEL_SQL_DATABASE_NAME="flock_events.db"
# OTEL_FILE_NAME="flock_events.jsonl"
# OTEL_ENABLE_SQL="True"
# OTEL_ENABLE_FILE="True"
# OTEL_ENABLE_JAEGER="False" # Set to True to enable Jaeger exporting

# --- CLI Settings (Managed by `flock settings`) ---
# SHOW_SECRETS="False"
# VARS_PER_PAGE="20"
```


**LLM Keys**: Essential for your agents to function. Use the variable names specified by litellm.

**Tool Keys**: Required only if you use specific tools (Tavily, GitHub, Azure Search).

**DEFAULT_MODEL**: Sets the fallback LLM identifier if an agent doesn't specify its own.

**TEMPORAL_SERVER_URL**: Needed if you enable Temporal for distributed execution.

**Logging/Telemetry**: Control log verbosity, output directories, and OpenTelemetry settings.

## Instance Configuration (Code)

You can also override or set configurations directly when creating Flock or FlockAgent instances in your Python code:


### Configure Flock instance
```python
from flock.core import Flock, FlockAgent, FlockFactory

my_flock = Flock(
    name="ConfiguredFlock",
    model="anthropic/claude-3-sonnet-20240229", # Override default model
    enable_temporal=False, # Force local execution
    enable_logging=True    # Enable logging for this flock instance
)
```

### Configure Agent instance
```python
specific_agent = FlockFactory.create_default_agent(
    name="SpecificAgent",
    model="openai/gpt-3.5-turbo", # Use a different model for this agent
    use_cache=False,              # Disable caching for this agent
    temperature=0.7,              # Set LLM temperature
    # ... other agent-specific configs
)

my_flock.add_agent(specific_agent)
```


Settings provided during instantiation take precedence over environment variables for that specific instance.

## CLI Settings

When starting Flock as CLI-Tool (typing `flock` in your terminal) it offers a Settings editor.
These settings are typically stored in ~/.flock/.env. If this file exists flock will ignore local .env files and only use the global .env.

This behavior can be changed by the `LOCAL_ENV_OVERWRITES_GLOBAL_ENV` flag.

```

---

### 15. docs\getting-started\index.md

- **File ID**: file_14
- **Type**: File
- **Line Count**: 50
- **Description**: File at docs\getting-started\index.md

**Content**:
```
---
hide:
  - toc
---

# Getting Started: Your First Flight with Flock! 🚀

Welcome aboard! You're about to take your first steps into the world of **Flock**, where building powerful AI agents is less about wrestling with endless prompts and more about declaring **what** you want to achieve.

Ready to ditch the prompt-palaver and build robust, scalable agent systems with surprising ease? You're in the right place! This section is your launchpad, guiding you from zero to running your first Flock agent.

## What You'll Find Here

This section covers the essentials to get you up and running:

*   **[Quick Start](quickstart.md):** Jump right in and build your very first Flock agent in minutes. See how simple the declarative approach can be.
*   **[Installation](installation.md):** Get Flock set up on your machine, along with any necessary dependencies.
*   **[Basic Concepts](concepts.md):** Understand the fundamental ideas behind Flock – Agents, the Declarative approach, and the core components you'll interact with. *(Coming Soon)*
*   **[Configuration](configuration.md):** Learn how to configure Flock for your specific needs, including setting up LLM providers.

## Prerequisites

Before you dive in, make sure you have:

*   🐍 Python 3.10 or higher installed.
*   📦 `pip` or `uv` (highly recommended!) for installing packages.

Download `uv`: **[https://docs.astral.sh/uv/getting-started/installation/](https://docs.astral.sh/uv/getting-started/installation/)**

*   🔑 An API key for your preferred Large Language Model (LLM) provider (like OpenAI, Anthropic, Gemini, etc.). Flock uses `litellm` to connect, so many are supported!

Checkout which models are supported and how to use them! **[https://docs.litellm.ai/docs/providers](https://docs.litellm.ai/docs/providers)**

For example you want to use an Anthropic model with flock:

- Click on "Anthropic"

- Find the name of the environment variable which litellm will check: `ANTHROPIC_API_KEY`

- Find how litellm expects the model name to be defined: `model="anthropic/claude-3-5-sonnet-20240620"`


Based on these information all you need to do for flock using any anthropic model is adding the key with the correct name into your .env and build your flock with `flock = Flock(model="anthropic/...")`


## Where to Begin?

We recommend starting with the **[Quick Start](quickstart.md)** guide for a hands-on introduction. If you prefer to set up everything first, head over to the **[Installation](installation.md)** page.

Let's get your first agent airborne! 🐦‍⬛💨
```

---

### 16. docs\getting-started\installation.md

- **File ID**: file_15
- **Type**: File
- **Line Count**: 103
- **Description**: File at docs\getting-started\installation.md

**Content**:
```
---
hide:
  - toc
---

# Installation 🐣

Getting Flock onto your system is easy using modern Python package managers like `uv` or the standard `pip`. We recommend using `uv` for faster installations.

## Using `uv` (Recommended)

`uv` is a fast Python package installer and resolver.

1.  **Install `uv`** (if you haven't already):
    ```bash
    # On macOS and Linux
    curl -LsSf https://astral.sh/uv/install.sh | sh

    # On Windows
    powershell -c "irm https://astral.sh/uv/install.ps1 | iex"

    # Or via pip
    pip install uv
    ```

2.  **Install Flock Core:**
    ```bash
    uv pip install flock-core
    ```
    This installs the essential `flock-core` package.

3.  **Install with Extras (Optional):**
    If you need additional tools or features:
    ```bash
    # For common tools (web search, markdownify, etc.)
    uv pip install flock-core[tools]

    # To install everything, including tools and future extras
    uv pip install flock-core[all]
    ```

## Using `pip`

If you prefer using `pip`:

1.  **Install Flock Core:**
    ```bash
    pip install flock-core
    ```

2.  **Install with Extras (Optional):**
    ```bash
    # For common tools
    pip install flock-core[tools]

    # To install everything
    pip install flock-core[all]
    ```

!!! note "Available Extras"
    *   `tools`: Installs `tavily-python` and `markdownify`.
    *   `all`: Installs all optional dependencies defined in `tools`, plus `docling`.

## Setting Up Your Environment (API Keys)

Flock agents typically interact with Large Language Models (LLMs). You'll need to configure API keys for the services you intend to use. Flock uses `litellm` under the hood, which supports numerous providers (OpenAI, Anthropic, Gemini, Azure, etc.).

The recommended way to manage your API keys is through **environment variables**. You can set them directly in your shell, or use a `.env` file in your project's root directory.

1.  **Create a `.env` file:** In the main folder of your project, create a file named `.env`.
2.  **Add your keys:** Add the necessary API keys for your chosen LLM provider(s). Refer to the `litellm` documentation for the correct environment variable names.

    Example `.env` file:
    ```dotenv
    # .env
    OPENAI_API_KEY="your-openai-api-key"
    ANTHROPIC_API_KEY="your-anthropic-api-key"
    TAVILY_API_KEY="your-tavily-api-key" # For Tavily search tool
    # Add Azure, GitHub, etc., keys if needed by specific tools
    # AZURE_SEARCH_ENDPOINT="your-azure-search-endpoint"
    # AZURE_SEARCH_API_KEY="your-azure-search-api-key"
    # GITHUB_PAT="your-github-pat"
    ```

Flock (via `litellm` and `python-decouple`) will automatically load these variables when needed.

!!! warning "Security"
    Never commit your `.env` file containing secret keys to version control (like Git). Add `.env` to your `.gitignore` file.

## Verifying Installation

You can quickly verify that Flock is installed by opening a Python interpreter and importing it:

```python
import flock.core
print("Flock imported successfully!")
```

If this runs without errors, you're all set!

Next Steps

With Flock installed, you're ready for the Quick Start guide to build and run your first agent!
```

---

### 17. docs\getting-started\quickstart.md

- **File ID**: file_16
- **Type**: File
- **Line Count**: 124
- **Description**: File at docs\getting-started\quickstart.md

**Content**:
```
---
hide:
  - toc
---

# Quick Start: Your First Agent in 5 Minutes! ⏱️

Ready for liftoff? This guide will get you running your first Flock agent faster than a hummingbird's wings! We'll create a simple agent that takes a topic and generates a fun presentation title and some slide headers.

## Prerequisites

Make sure you've completed the **[Installation](installation.md)** steps, especially setting up your LLM API keys in your environment (e.g., in a `.env` file).

## The "Hello Flock!" Code

Create a new Python file (e.g., `hello_flock.py`) and paste the following code:

```python
# hello_flock.py
from flock.core import Flock, FlockFactory

# --------------------------------
# 1. Choose Your LLM
# --------------------------------
# Flock uses litellm, so you can specify many different providers.
# Make sure the corresponding API key is set in your environment!
# Examples: "openai/gpt-4o", "anthropic/claude-3-sonnet-20240229", "gemini/gemini-1.5-pro"
MODEL = "openai/gpt-4o" # <-- Replace with your preferred model if needed

# --------------------------------
# 2. Create Your Flock
# --------------------------------
# The Flock is the main orchestrator.
flock = Flock(
    name="presentation_helper",
    description="My first Flock!",
    model=MODEL # Set the default model for agents in this Flock
)

# --------------------------------
# 3. Define Your Agent Declaratively
# --------------------------------
# No complex prompts needed! Just declare what goes in and what comes out.
# We'll use FlockFactory for a quick setup.
presentation_agent = FlockFactory.create_default_agent(
    name="my_presentation_agent",
    description="Creates a fun presentation outline about a given topic",
    input="topic: str", # The agent expects a string named 'topic'
    output="fun_title: str, fun_slide_headers: list[str]" # It will output a string 'fun_title' and a list of strings 'fun_slide_headers'
)

# --------------------------------
# 4. Add the Agent to the Flock
# --------------------------------
flock.add_agent(presentation_agent)

# --------------------------------
# 5. Run the Flock!
# --------------------------------
# Tell the Flock which agent to start with and provide the input.
print(f"Running agent '{presentation_agent.name}'...")
result = flock.run(
    start_agent=presentation_agent, # Or use the name: "my_presentation_agent"
    input={"topic": "Why Llamas Make Great Urban Pets"} # The input data
)

# --------------------------------
# 6. Admire the Results!
# --------------------------------
# The result is a clean Python object (a Box object, behaves like a dict/object)
print("\n--- Agent Output ---")
print(f"✨ Title: {result.fun_title}")
print(f"📊 Headers: {result.fun_slide_headers}")
print("--------------------")
```

## Running the Code

Save the file and run it from your terminal:

`python hello_flock.py`


You should see output similar to this (the exact content will vary based on the LLM's response):

```bash
Running agent 'my_presentation_agent'...

--- Agent Output ---
✨ Title: Concrete Jungles & Camelids: Why Your Next Roommate Should Be A Llama
📊 Headers: ['Llamas: Nature's Low-Maintenance Loft-Dwellers', "Traffic Jam? No Prob-llama!", 'The Ultimate Urban Eco-Warrior (They Mow with Their Mouths!)', 'Spit Happens: Debunking Llama Myths', 'From Andes to Apartments: Integrating Your Llama Lifestyle']
--------------------
```

## What Just Happened?

Let's break down the magic:

**Choose Your LLM**: You specified which LLM to use via MODEL. litellm handles the connection using the API key from your environment.

**Create Your Flock**: The Flock object acts as the container and orchestrator for your agents.

**Define Your Agent Declaratively**: This is the core of Flock! Instead of writing a long prompt, you used FlockFactory.create_default_agent and simply declared:

input="topic: str": The agent needs one input called topic, which should be a string.

output="fun_title: str, fun_slide_headers: list[str]": The agent should produce two outputs: fun_title (a string) and fun_slide_headers (a list of strings).
Flock's default evaluator takes care of constructing the necessary instructions for the LLM based on these declarations and the agent's description.

**Add the Agent**: You registered your presentation_agent with the flock.

**Run the Flock**: flock.run() kicked off the process, starting with your specified agent and input.

**Admire the Results**: Flock executed the agent (calling the LLM behind the scenes) and returned the output as a convenient Box object, allowing you to access the results using dot notation (like result.fun_title).

Congratulations! You've successfully run your first Flock agent without writing a single complex prompt.

## Next Steps

Explore adding Tools to give your agents superpowers (like web search or code execution).

Learn how to Chain Agents together for more complex workflows.

Dive deeper into the Core Concepts that make Flock tick.
```

---

### 18. docs\guides\chaining-agents.md

- **File ID**: file_17
- **Type**: File
- **Line Count**: 8
- **Description**: File at docs\guides\chaining-agents.md

**Content**:
```
---
hide: # Optional: Hide table of contents on simple pages
  - toc
---

# Chaining Agents

*Documentation in progress...*

```

---

### 19. docs\guides\index.md

- **File ID**: file_18
- **Type**: File
- **Line Count**: 8
- **Description**: File at docs\guides\index.md

**Content**:
```
---
hide: # Optional: Hide table of contents on simple pages
  - toc
---

# Overview

*Documentation in progress...*

```

---

### 20. docs\index.md

- **File ID**: file_19
- **Type**: File
- **Line Count**: 128
- **Description**: File at docs\index.md

**Content**:
```
---
hide: # Optional: Hide table of contents on simple pages
  - toc
---


# 🚀 Welcome to Flock! Take Flight with Declarative AI Agents 🚀

<!-- Optional: Add banner back later if desired -->


<p align="center">
<!-- Badges can be added back if relevant/maintained -->
<img alt="Flock Banner" src="assets/images/flock.png">
<img alt="Dynamic TOML Badge" src="https://img.shields.io/badge/dynamic/toml?url=https%3A%2F%2Fraw.githubusercontent.com%2Fwhiteducksoftware%2Fflock%2Frefs%2Fheads%2Fmaster%2Fpyproject.toml&query=%24.project.version&style=for-the-badge&logo=pypi&label=pip%20version">
<a href="https://www.linkedin.com/company/whiteduck" target="_blank"><img alt="LinkedIn" src="https://img.shields.io/badge/linkedin-%230077B5.svg?style=for-the-badge&logo=linkedin&logoColor=white&label=whiteduck"></a>
<a href="https://bsky.app/profile/whiteduck-gmbh.bsky.social" target="_blank"><img alt="Bluesky" src="https://img.shields.io/badge/bluesky-Follow-blue?style=for-the-badge&logo=bluesky&logoColor=%23fff&color=%23333&labelColor=%230285FF&label=whiteduck-gmbh"></a>
</p>

---

Tired of wrestling with paragraphs of prompt text just to get your AI agent to do *one specific thing*? 😫 Enter **Flock**, the agent framework that lets you ditch the prompt-palaver and focus on **what** you want your agents to achieve!

Flock takes a **declarative approach**. Think of it like ordering a pizza 🍕: you specify the toppings (your agent's `input` and `output`), not the 30 steps the chef needs to follow. Flock, powered by modern LLMs and a clever architecture, figures out the "how" for you.

Built with real-world deployment in mind, Flock integrates seamlessly with tools like **Temporal** for building robust, fault-tolerant, and scalable agent systems right out of the box.

## ✨ Why Join the Flock?

| Traditional Agent Frameworks 😟 | Flock Framework 🐤🐧🐓🦆 |
| :-------------------------------- | :------------------------------------------ |
| 🤯 **Prompt Nightmare**             | ✅ **Declarative Simplicity**               |
| *Long, brittle, hard-to-tune prompts* | *Clear input/output specs (with types!)*  |
| 💥 **Fragile & Unpredictable**      | ⚡ **Robust & Production-Ready**           |
| *Single errors crash the system*  | *Fault-tolerant via Temporal integration* |
| 🧩 **Monolithic & Rigid**          | 🔧 **Modular & Flexible**                   |
| *Hard to extend or modify logic*  | *Pluggable Evaluators, Modules, Routers*  |
| ⛓️ **Basic Chaining**              | 🚀 **Advanced Orchestration**               |
| *Often just linear workflows*     | *Dynamic routing, parallel/batch runs*    |

## 💡 Core Ideas

Flock's power comes from a few key concepts:

1.  **Declarative Agents:** Define agents by *what* they do (inputs/outputs), not *how*. Flock uses **Evaluators** to handle the underlying logic (be it LLM calls, rules, or custom code).
2.  **Modular Components:** Extend agent capabilities with pluggable **Modules** (for memory, metrics, output formatting, etc.) that hook into the agent's lifecycle without touching its core definition.
3.  **Intelligent Workflows:** Chain agents explicitly or use **Routers** (like LLM-based or Agent-based) for dynamic decision-making on the next step.
4.  **Reliable Execution:** Run locally for debugging or switch seamlessly to **Temporal** for production-grade fault tolerance, retries, and state management.
5.  **Type Safety:** Leverage Python type hints and Pydantic for clear contracts, validation, and easier integration.

## 🐥 Hello Flock! - A Quick Taste

Building your first agent is refreshingly simple:

```python
import os
from flock.core import Flock, FlockFactory 


# --------------------------------
# Define the model
# --------------------------------
# Flock uses litellm to talk to LLMs
# Please consult the litellm documentation for valid IDs:
# https://docs.litellm.ai/docs/providers
MODEL = "openai/gpt-4o"


# --------------------------------
# Create the flock and context
# --------------------------------
# The flock is the place where all the agents are at home
flock = Flock(name="hello_flock", description="This is your first flock!", model=MODEL)

# --------------------------------
# Create an agent
# --------------------------------
# The Flock doesn't believe in prompts (see the docs for more info)
# The Flock just declares what agents get in and what agents produce
# my_presentation_agent takes in a topic and outputs a
# funny_title, fun_slide_headers and fun_slide_summaries
presentation_agent = FlockFactory.create_default_agent(
    name="my_presentation_agent",
    input="topic",
    output="fun_title, fun_slide_headers, fun_slide_summaries"
)
flock.add_agent(presentation_agent)


# --------------------------------
# Run the flock
# --------------------------------
# Tell the flock who the starting agent is and what input to give it
flock.run(
    start_agent=presentation_agent, 
    input={"topic": "A presentation about robot kittens"}
)

```


That's it! Flock handled turning your simple declaration into the necessary LLM interaction and gave you back a clean, typed Python object.

🗺️ Ready to Explore?

- Dive deeper into the Flock ecosystem:

- Getting Started: Your first flight – installation and basic usage.

- Core Concepts: Understand the foundations – Agents, Declarative logic, Workflows.

- Key Components: Learn about the building blocks – Evaluators, Modules, Tools.

- Guides: Practical walkthroughs for common tasks like chaining agents.

- Deployment: Take your Flock to production with Temporal.

🤝 Join the Flock Community!

- Flock is actively developed and welcomes contributions.

- Check out the code on GitHub

- Report issues or suggest features.

- Help us improve the documentation!

Let's build amazing, reliable AI agent systems together! 🚀

```

---

### 21. docs\interacting-with-flock\cli-tool.md

- **File ID**: file_20
- **Type**: File
- **Line Count**: 40
- **Description**: File at docs\interacting-with-flock\cli-tool.md

**Content**:
```
---
hide:
  - toc
---

# 🛠️ Flock CLI Tool

Beyond integrating Flock into your Python code or exposing it via an API, Flock provides a dedicated command-line interface (CLI) tool, accessed simply by running `flock` in your terminal (once installed).

This tool is primarily designed for **managing and interacting with persistent Flock configurations** stored in `.flock.yaml` files.

## Key Features

* **Loading Flocks:** Load complete Flock configurations, including agents, types, and components, from `.flock.yaml` files.
* **Creating Flocks:** A guided wizard (`flock` -> "Create a new Flock") helps you scaffold a new `.flock.yaml` file with basic settings and initial agents.
* **Executing Flocks:** Run workflows (single or batch) defined within a loaded `.flock.yaml` file directly from the command line.
* **Registry Management:** View, add, remove, and automatically scan for components (agents, types, callables/tools) in the global Flock registry. This is crucial for ensuring correct serialization and deserialization.
* **Settings Management:** View and edit global Flock settings, primarily environment variables stored in `~/.flock/.env`, including API keys and configuration flags. Manage different `.env` profiles.
* **Theme Builder:** Interactively preview and configure output themes for agents using rich tables.
* **(Future):** Potential deployment commands (e.g., to Docker, Kubernetes).

## How to Use

1. **Installation:** Ensure `flock-core` (or the relevant package) is installed in your environment.
2. **Run:** Simply type `flock` in your terminal.
3. **Navigate:** Use the interactive menu (powered by `questionary`) to select actions like "Load a *.flock file", "Create a new Flock", "Registry management", "Settings", etc.

*(Placeholder: Screenshot of the main `flock` CLI menu)*

## Distinction from Other Interaction Methods

* **vs. Programmatic API:** The CLI tool operates on *serialized* Flock states (`.flock.yaml`), while the programmatic API works with `Flock` *objects* in Python memory. The CLI is for management; the API is for integration.
* **vs. Interactive Script CLI:** The `flock` tool is a standalone application for managing configurations. The `flock.start_cli()` method discussed in [Interactive Script CLI](interactive-cli.md) is used *within* a Python script for debugging the *current*, in-memory `Flock` instance.

Use the Flock CLI tool when you need to:

* Save and load reusable Flock configurations.
* Execute predefined workflows without writing Python scripts each time.
* Manage the global registry of components, types, and tools shared across your projects.
* Configure global settings like API keys or CLI behavior.

```

---

### 22. docs\interacting-with-flock\index.md

- **File ID**: file_21
- **Type**: File
- **Line Count**: 20
- **Description**: File at docs\interacting-with-flock\index.md

**Content**:
```
---
hide:
  - toc
---

# Interacting with Flock: Beyond the Build 🚀

You've designed your Flock, defined your agents, and maybe even chained them together. Now what? How do you actually *use* your sophisticated AI system?

Flock is designed for flexibility, offering multiple ways to interact with your agents and workflows, catering to different needs from development and debugging to integration and deployment.

This section explores the primary methods for interacting with a configured `Flock` instance:

*   **[🐍 Programmatic API (Python)](programmatic.md):** The most direct way. Integrate Flock directly into your Python applications, trigger runs, and process results using simple method calls like `flock.run()`, `flock.run_async()`, and `flock.run_batch()`. Ideal for core development and integration within Python projects.
*   **[🌐 REST API](rest-api.md):** Expose your Flock as a web service. Start a FastAPI server (`flock.start_api()`) to allow remote execution via HTTP requests. Perfect for integrating Flock with web frontends, other microservices, or external systems.
*   **[🖥️ Web UI (FastHTML)](web-ui.md):** A simple, automatically generated web interface for interactive testing and demonstrations. Launch it alongside the REST API (`flock.start_api(create_ui=True)`) for a quick way to select agents, provide inputs, and view results in your browser.
*   **[🛠️ Flock CLI Tool](cli-tool.md):** Manage your Flock configurations using the main `flock` command-line tool. Load `.flock.yaml` files, execute runs, manage the registry, and configure settings without writing Python code. Best for system administration and managing saved Flock states.
*   **[⌨️ Interactive Script CLI](interactive-cli.md):** Debug agent interactions directly within your terminal while running a Python script using `flock.start_cli()`. Useful for stepping through agent handoffs and inspecting context during development.

Choose the interaction method that best suits your current task, whether it's embedding AI logic deep within an application, providing an API for others, quickly demoing capabilities, or managing persistent configurations.
```

---

### 23. docs\interacting-with-flock\interactive-cli.md

- **File ID**: file_22
- **Type**: File
- **Line Count**: 79
- **Description**: File at docs\interacting-with-flock\interactive-cli.md

**Content**:
```
---
hide:
  - toc
---

# ⌨️ Interactive Script CLI (`flock.start_cli()`)

While the main [Flock CLI Tool](cli-tool.md) is used for managing saved configurations, Flock also offers an **interactive command-line interface directly within your running Python script** using the `flock.start_cli()` method.

This mode is specifically designed for **development and debugging** purposes, allowing you to step through agent interactions and inspect the context of your *current, in-memory* `Flock` instance.

## Starting the Interactive CLI

You typically call `start_cli()` at a point in your script where you have a fully configured `Flock` object ready.

```python
from flock.core import Flock, FlockFactory
from flock.routers.default import DefaultRouter, DefaultRouterConfig # Assuming DefaultRouter is used

# --- Configure your Flock and Agents ---
flock = Flock(name="Debug Flock", model="openai/gpt-4o")

agent_a = FlockFactory.create_default_agent(
    name="AgentA",
    input="start_query: str",
    output="intermediate_result: str",
    wait_for_input=True # Useful in interactive mode
)

agent_b = FlockFactory.create_default_agent(
    name="AgentB",
    input="intermediate_result: str", # Takes output from AgentA
    output="final_answer: str",
    wait_for_input=True # Useful in interactive mode
)

# Setup handoff (simple chain)
agent_a.handoff_router = DefaultRouter(config=DefaultRouterConfig(hand_off=agent_b.name))

flock.add_agent(agent_a)
flock.add_agent(agent_b)

# --- Start the Interactive CLI ---
if __name__ == "__main__":
    print("Starting Flock Interactive CLI...")
    print("Type your initial query for AgentA, or 'quit' to exit.")
    # This call enters an interactive loop in the terminal
    flock.start_cli(start_agent=agent_a) # Specify the starting agent

    print("Interactive CLI finished.")
```

## Using the Interactive CLI

When you run the script above, instead of executing the full workflow automatically, `flock.start_cli()` will present an interactive prompt in your terminal:

1. **Initial Input:** It will typically prompt for the input required by the start_agent you specified.

2. **Agent Execution:** After you provide input, the starting agent (AgentA in the example) executes.

3. **Output Display:** The agent's output is displayed (often using the configured OutputModule, potentially with rich tables).

4. **Handoff & Next Input:** If the agent has a handoff_router, Flock determines the next agent (AgentB). If the next agent is configured with wait_for_input=True (or if interaction is needed), the CLI might pause or show the next agent's expected input. Often, data flows automatically via context matching the handoff mode.

5. **Loop:** The process repeats for the next agent.

6. **Commands:** You can typically type commands like `quit`, `exit`, or potentially others (depending on implementation details) to control the flow or inspect state.

(Placeholder: Screenshot of the terminal during an interactive flock.start_cli() session)

## Use Cases

- **Debugging Agent Chains:** Observe the output of each agent in a sequence.
- **Inspecting Context:** (If enhanced) See how the FlockContext changes after each step.
- **Testing Routers:** Verify that your LLMRouter or AgentRouter selects the expected next agent based on intermediate results.
- **Manual Control:** Intervene in the workflow or provide specific inputs at different stages.

**Important:** `flock.start_cli()` operates on the Flock object currently in your Python script's memory. It does not load or save .flock.yaml files like the main flock management tool. It's purely a development aid within a script.


```

---

### 24. docs\interacting-with-flock\programmatic.md

- **File ID**: file_23
- **Type**: File
- **Line Count**: 185
- **Description**: File at docs\interacting-with-flock\programmatic.md

**Content**:
```
---
hide:
  - toc
---

# 🐍 Programmatic API (Python)

The most fundamental way to interact with Flock is directly within your Python code. This approach gives you the most control over execution flow, input/output handling, and integration with other Python libraries.

Once you have a configured `Flock` instance, you can trigger agent executions using several key methods:

## 1. Single Synchronous Run: `flock.run()`

This is the simplest way to execute a workflow starting from a specific agent. It runs synchronously, meaning your script will wait until the entire workflow (including any agent handoffs) completes before proceeding.

```python
from flock.core import Flock, FlockFactory

# Assume 'flock' is your configured Flock instance
# Assume 'my_agent' is a FlockAgent instance added to the flock

input_data = {"topic": "Benefits of Declarative AI"}

# Run starting with 'my_agent'
try:
    # result is a Box object (dot-accessible dict) by default
    result = flock.run(start_agent=my_agent, input=input_data)

    print("Workflow Completed!")
    print(f"Title: {result.generated_title}") # Access results easily
    print(f"Points: {result.key_points}")

except Exception as e:
    print(f"An error occurred: {e}")
```

**Use Case:** 

Simple scripts, direct integration where blocking execution is acceptable, easier debugging for linear flows.

**Returns:** 

A Box object (dot-accessible dictionary) containing the final result of the last agent executed in the chain, or a standard dictionary if box_result=False.

## 2. Single Asynchronous Run: flock.run_async()

For applications using asyncio, this method allows you to run a workflow without blocking the main event loop.

```python
import asyncio
from flock.core import Flock, FlockFactory

# Assume 'flock' and 'my_agent' are configured

async def main():
    input_data = {"topic": "Async AI Workflows"}
    try:
        result = await flock.run_async(start_agent=my_agent.name, input=input_data) # Can use agent name
        print("Async Workflow Completed!")
        print(f"Result: {result}") # result is a Box object by default
    except Exception as e:
        print(f"An error occurred: {e}")

# asyncio.run(main())
```

**Use Case:** 

Integrating Flock into asynchronous applications (like web servers), running multiple Flock workflows concurrently.

**Returns:** 

Same as flock.run(), but returns an awaitable coroutine.

## 3. Batch Processing: flock.run_batch() / flock.run_batch_async()

Process multiple input items efficiently, either sequentially or in parallel. This is ideal for tasks like generating variations, processing datasets, or evaluating agent performance.

```python
from flock.core import Flock, FlockFactory

# Assume 'flock' and 'summarizer_agent' are configured
# summarizer_agent takes 'text_to_summarize', outputs 'summary'

batch_data = [
    {"text_to_summarize": "Text block 1..."},
    {"text_to_summarize": "Another long text block..."},
    {"text_to_summarize": "Final piece of text..."}
]

static_data = {"max_summary_length": 100} # Input common to all items

try:
    # Run in parallel locally, show progress bar
    results = flock.run_batch(
        start_agent=summarizer_agent,
        batch_inputs=batch_data,
        static_inputs=static_data,
        parallel=True,
        max_workers=4,
        silent_mode=True, # Shows progress bar
        return_errors=True # Return Exceptions instead of raising
    )

    print(f"Batch processing completed for {len(results)} items.")
    for i, res in enumerate(results):
        if isinstance(res, Exception):
            print(f"Item {i+1} failed: {res}")
        else:
            print(f"Item {i+1} Summary: {res.summary}") # Access results

except Exception as e:
    print(f"Batch processing error: {e}")

# Async version: await flock.run_batch_async(...)
```

**Use Case:** 

Processing large datasets, generating multiple outputs from varying inputs, performance testing.

**Input:** 

Can be a list of dictionaries, a pandas DataFrame, or a path to a CSV file.

**Features:** 

Parallel execution (local threads or Temporal), input mapping (for DataFrames/CSVs), static inputs, error handling options, CSV output.

**Returns:**

 A list containing the result (Box or dict) or Exception object for each input item, in the original order.

## 4. Agent Evaluation: flock.evaluate() / flock.evaluate_async()

Evaluate an agent's performance against a dataset using specified metrics.

```python
from flock.core import Flock, FlockFactory

# Assume 'flock' and 'qa_agent' are configured
# qa_agent takes 'question', outputs 'answer'

dataset_path = "path/to/qa_dataset.csv" # CSV with 'question' and 'true_answer' columns
input_mapping = {"question": "question"} # Map CSV 'question' col to agent 'question' input
answer_mapping = {"answer": "true_answer"} # Map agent 'answer' output to CSV 'true_answer' col
metrics_to_run = ["exact_match", "semantic_similarity"] # Use built-in metrics

try:
    results_df = flock.evaluate(
        dataset=dataset_path,
        start_agent=qa_agent,
        input_mapping=input_mapping,
        answer_mapping=answer_mapping,
        metrics=metrics_to_run,
        output_file="evaluation_results.csv", # Save detailed results
        return_dataframe=True,
        silent_mode=True
    )
    print("Evaluation Complete!")
    print("Average Scores:")
    print(results_df[["metric_exact_match", "metric_semantic_similarity"]].mean())

except Exception as e:
    print(f"Evaluation error: {e}")
```

**Use Case:** 

Assessing agent accuracy, comparing different models or prompts, regression testing.

**Input:** 

Hugging Face dataset ID, path to CSV, list of dictionaries, pandas DataFrame, or HF Dataset object.

**Features:** 

Input/Answer mapping, various built-in metrics (exact match, fuzzy, ROUGE, semantic similarity), custom metric functions, LLM-as-judge, parallel execution, detailed output saving.

**Returns:** 

A pandas DataFrame (if return_dataframe=True) or a list of dictionaries containing inputs, expected answers, agent output, calculated metrics, and errors for each dataset item.

Choose the programmatic method that best fits your execution needs within your Python application.


```

---

### 25. docs\interacting-with-flock\rest-api.md

- **File ID**: file_24
- **Type**: File
- **Line Count**: 140
- **Description**: File at docs\interacting-with-flock\rest-api.md

**Content**:
```
---
hide:
  - toc
---

# 🌐 REST API

Flock allows you to easily expose your configured agents and workflows as a RESTful API, making them accessible to other services, web frontends, or remote clients. This is achieved using FastAPI under the hood.

## Starting the API Server

To start the API server, simply call the `start_api()` method on your configured `Flock` instance within your Python script:

```python
from flock.core import Flock, FlockFactory

# --- Configure your Flock and Agents ---
flock = Flock(name="My API Flock", model="openai/gpt-4o")

analyzer_agent = FlockFactory.create_default_agent(
    name="text_analyzer",
    input="text: str",
    output="sentiment: str, keywords: list[str]"
)
flock.add_agent(analyzer_agent)
# ... add more agents ...

# --- Start the API Server ---
if __name__ == "__main__":
    # This will start a Uvicorn server (blocking call)
    flock.start_api(
        host="127.0.0.1", # Host to bind to (0.0.0.0 for external access)
        port=8344,       # Port to listen on
        server_name="My Flock Service API" # Optional title for docs
        # create_ui=False # Set to True to also enable the Web UI
    )
```

Running this script will start a web server listening on http://127.0.0.1:8344.

## Key API Endpoints

The server automatically provides the following endpoints:

### POST /run/flock: 

Executes a single agent workflow run.

**Request Body (JSON)**: FlockAPIRequest

```json
{
  "agent_name": "text_analyzer",
  "inputs": {
    "text": "Flock makes building agents easy and fun!"
  },
  "async_run": false
}
```

- **agent_name** (string, required): The name of the agent to start the workflow with.
- **inputs** (object, optional): Input data for the starting agent.
- **async_run** (boolean, optional, default: false): If true, the API returns immediately with a run_id and processes the request in the background. If false, the API waits for the run to complete before responding.

**Response Body (JSON)**: FlockAPIResponse

- If async_run=true or run is in progress: Contains run_id and status ("starting" or "running").
- If async_run=false and run completes: Contains run_id, status ("completed" or "failed"), result (the output data), started_at, completed_at, and potentially error.

### POST /run/batch: 

Executes a batch processing job.

**Request Body (JSON)**: FlockBatchRequest (See API reference or flock.core.api.models for full details). Allows specifying batch_inputs (list of dicts or CSV path string), agent_name, static_inputs, parallel execution options, etc.

**Response Body (JSON)**: FlockBatchResponse. Batch runs are always asynchronous. Returns batch_id and status ("starting"). Use the /batch/{batch_id} endpoint to track progress and get results.

### GET /run/{run_id}: 

Retrieves the status and result of a specific workflow run initiated via POST /run/flock.

**Response Body (JSON)**: FlockAPIResponse

### GET /batch/{batch_id}: 

Retrieves the status, progress, and results of a specific batch run initiated via POST /run/batch.

**Response Body (JSON)**: FlockBatchResponse (Includes total_items, completed_items, progress_percentage, and results list).

### GET /agents: 

Lists all agents registered within the running Flock instance.

**Response Body (JSON)**:

```json
{
  "agents": [
    {"name": "agent_name_1", "description": "Description 1"},
    {"name": "agent_name_2", "description": "Description 2"}
  ]
}
```

## Example Interaction (curl)

```bash
# Start a synchronous run
curl -X POST http://127.0.0.1:8344/run/flock -H "Content-Type: application/json" -d '{
  "agent_name": "text_analyzer",
  "inputs": {
    "text": "This is great!"
  }
}'

# Start an asynchronous run
curl -X POST http://127.0.0.1:8344/run/flock -H "Content-Type: application/json" -d '{
  "agent_name": "text_analyzer",
  "inputs": {
    "text": "This will run in the background."
  },
  "async_run": true
}'
# Response will include a run_id, e.g.: {"run_id": "...", "status": "running", ...}

# Check the status of the asynchronous run (replace {run_id} with the actual ID)
curl http://127.0.0.1:8344/run/{run_id}
```

## API Documentation

When the API server is running, you can access interactive documentation generated by FastAPI:

- **Swagger UI**: http://<host>:<port>/docs
- **ReDoc**: http://<host>:<port>/redoc

These interfaces allow you to explore endpoints, view request/response schemas, and even try out API calls directly from your browser.




```

---

### 26. docs\interacting-with-flock\web-ui.md

- **File ID**: file_25
- **Type**: File
- **Line Count**: 75
- **Description**: File at docs\interacting-with-flock\web-ui.md

**Content**:
```
---
hide:
  - toc
---

# 🖥️ Web UI (FastHTML)

For quick interactive testing, demonstrations, or simple internal tools, Flock can generate a basic web user interface using [FastHTML](https://fastht.ml/). This UI allows you to select agents, provide inputs through a form, and view the results directly in your browser.

## Starting the Server with UI

To enable the Web UI, simply set the `create_ui=True` flag when starting the API server:

```python
from flock.core import Flock, FlockFactory

# --- Configure your Flock and Agents ---
flock = Flock(name="My UI Flock", model="openai/gpt-4o")

greeting_agent = FlockFactory.create_default_agent(
    name="greeter",
    input="name: str | Person's name",
    output="greeting: str | A friendly greeting"
)
flock.add_agent(greeting_agent)
# ... add more agents ...

# --- Start the API Server with UI Enabled ---
if __name__ == "__main__":
    flock.start_api(
        host="127.0.0.1",
        port=8344,
        server_name="My Flock Service UI",
        create_ui=True # Enable the UI
    )
```

Now, when you run this script, not only will the REST API be available, but you can also access the Web UI by navigating your browser to:

http://127.0.0.1:8344/ui/

(Note: The API root / will typically redirect to /ui/ when the UI is enabled).

## Using the Web UI

The generated UI is straightforward:

1. **Select Starting Agent**: A dropdown menu lists all agents registered in your Flock instance. Choose the agent you want to execute.

2. **Agent Inputs**: Once you select an agent, the UI will dynamically generate input fields based on the agent's input specification string. It attempts to create appropriate HTML elements (text inputs, number inputs, checkboxes for booleans, textareas for complex types like lists/dicts). Descriptions provided in the input spec (using |) will often be used as placeholders or hints.

3. **Run Flock**: Fill in the required input fields. Click the "Run Flock" button.

4. **Result Area**: The UI will make a request to the backend API. Once the agent run completes, the formatted result will be displayed in the "Result" section below the form.

(Placeholder: Screenshot of the agent selection dropdown and dynamic input fields)

(Placeholder: Screenshot of the result area showing a formatted output table)

## How it Works (Briefly)

The UI uses:

- **FastHTML**: For building the server-side Python code that generates the HTML structure.
- **HTMX**: For handling dynamic updates (like loading agent inputs) and form submissions without full page reloads.
- **Pico.css**: For basic styling.

It communicates with the same backend REST API endpoints used for programmatic interaction, providing a visual layer over the API.

The Web UI is excellent for:

- Quickly testing different agents and inputs.
- Demonstrating Flock capabilities without writing client code.
- Simple internal tools where a basic web interface is sufficient.


```

---

### 27. docs\reference\flock_agent.md

- **File ID**: file_26
- **Type**: File
- **Line Count**: 8
- **Description**: File at docs\reference\flock_agent.md

**Content**:
```
---
hide: # Optional: Hide table of contents on simple pages
  - toc
---

# FlockAgent

*Documentation in progress...*

```

---

### 28. docs\reference\index.md

- **File ID**: file_27
- **Type**: File
- **Line Count**: 8
- **Description**: File at docs\reference\index.md

**Content**:
```
---
hide: # Optional: Hide table of contents on simple pages
  - toc
---

# Overview

*Documentation in progress...*

```

---

### 29. docs\release-notes\magpie.md

- **File ID**: file_28
- **Type**: File
- **Line Count**: 0
- **Description**: File at docs\release-notes\magpie.md

**Content**:
```

```

---

### 30. scripts\code_collector.py

- **File ID**: file_29
- **Type**: Code File
- **Line Count**: 704
- **Description**: Code Repository Analyzer...
- **Dependencies**: None
- **Used By**: None

**Content**:
```
#!/usr/bin/env python3
"""Code Repository Analyzer

This script generates a comprehensive Markdown document of a code repository,
optimized for LLM consumption and understanding.
"""

import ast
import datetime
import glob
import os
from typing import Any


def find_files(folder: str, extension: str) -> list[str]:
    """Find all files with the specified extension in the folder and subfolders."""
    pattern = os.path.join(folder, f"**/*{extension}")
    return sorted(glob.glob(pattern, recursive=True))


def get_file_metadata(file_path: str) -> dict[str, Any]:
    """Extract metadata from a file."""
    metadata = {
        "path": file_path,
        "size_bytes": 0,
        "line_count": 0,
        "last_modified": "Unknown",
        "created": "Unknown",
    }

    try:
        stats = os.stat(file_path)
        metadata["size_bytes"] = stats.st_size
        metadata["last_modified"] = datetime.datetime.fromtimestamp(
            stats.st_mtime
        ).strftime("%Y-%m-%d %H:%M:%S")
        metadata["created"] = datetime.datetime.fromtimestamp(
            stats.st_ctime
        ).strftime("%Y-%m-%d %H:%M:%S")

        with open(file_path, encoding="utf-8") as f:
            content = f.read()
            metadata["line_count"] = len(content.splitlines())
    except Exception as e:
        print(f"Warning: Could not get complete metadata for {file_path}: {e}")

    return metadata


def extract_python_components(file_path: str) -> dict[str, Any]:
    """Extract classes, functions, and imports from Python files."""
    components = {
        "classes": [],
        "functions": [],
        "imports": [],
        "docstring": None,
    }

    try:
        with open(file_path, encoding="utf-8") as f:
            content = f.read()

        tree = ast.parse(content)

        # Extract module docstring
        if ast.get_docstring(tree):
            components["docstring"] = ast.get_docstring(tree)

        # Helper to determine if a function is top-level or a method
        def is_top_level_function(node):
            # Check if the function is defined inside a class
            for parent_node in ast.walk(tree):
                if isinstance(parent_node, ast.ClassDef):
                    for child in parent_node.body:
                        if (
                            child is node
                        ):  # This is a direct reference comparison
                            return False
            return True

        # Extract top-level classes and functions
        for node in ast.iter_child_nodes(tree):
            if isinstance(node, ast.ClassDef):
                class_info = {
                    "name": node.name,
                    "docstring": ast.get_docstring(node),
                    "methods": [
                        m.name
                        for m in node.body
                        if isinstance(m, ast.FunctionDef)
                    ],
                }
                components["classes"].append(class_info)
            elif isinstance(node, ast.FunctionDef):
                func_info = {
                    "name": node.name,
                    "docstring": ast.get_docstring(node),
                    "args": [
                        arg.arg for arg in node.args.args if hasattr(arg, "arg")
                    ],
                }
                components["functions"].append(func_info)

        # Extract all imports
        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for name in node.names:
                    components["imports"].append(name.name)
            elif isinstance(node, ast.ImportFrom):
                module = node.module or ""
                for name in node.names:
                    components["imports"].append(f"{module}.{name.name}")

    except Exception as e:
        print(f"Warning: Could not parse Python components in {file_path}: {e}")

    return components


def analyze_code_dependencies(files: list[str]) -> dict[str, set[str]]:
    """Analyze dependencies between Python files based on imports."""
    dependencies = {file: set() for file in files}

    # Create a mapping from module names to file paths
    module_map = {}
    package_map = {}

    for file_path in files:
        if not file_path.endswith(".py"):
            continue

        # Handle both absolute and relative paths
        abs_path = os.path.abspath(file_path)

        # Map file paths to potential module names
        # First, try to extract package structure
        parts = []
        current = abs_path

        # Build the full module path
        while current:
            parent = os.path.dirname(current)

            # If we've reached the top or left the project directory, stop
            if parent == current or not os.path.exists(
                os.path.join(parent, "__init__.py")
            ):
                break

            parts.insert(0, os.path.basename(current))
            current = parent

        # Use the file name (without .py) as the last part
        base_name = os.path.basename(file_path)
        if base_name.endswith(".py"):
            if base_name != "__init__.py":
                module_name = os.path.splitext(base_name)[0]
                parts.append(module_name)

            full_module_name = ".".join(parts) if parts else None
            if full_module_name:
                module_map[full_module_name] = file_path

            # Also map short names for common imports
            if module_name := os.path.splitext(base_name)[0]:
                # Don't overwrite existing mappings with short names
                if module_name not in module_map:
                    module_map[module_name] = file_path

            # Map package names
            for i in range(len(parts)):
                package_name = ".".join(parts[: i + 1])
                package_map[package_name] = os.path.dirname(file_path)

    # Now analyze imports in each file
    for file_path in files:
        if not file_path.endswith(".py"):
            continue

        try:
            with open(file_path, encoding="utf-8") as f:
                code = f.read()

            tree = ast.parse(code)

            for node in ast.walk(tree):
                # Handle direct imports: import x, import x.y
                if isinstance(node, ast.Import):
                    for name in node.names:
                        # Check for the full module path
                        module_path = name.name
                        if module_path in module_map:
                            dependencies[file_path].add(module_map[module_path])

                        # Check for package imports
                        parts = module_path.split(".")
                        for i in range(len(parts), 0, -1):
                            prefix = ".".join(parts[:i])
                            if prefix in module_map:
                                dependencies[file_path].add(module_map[prefix])
                                break

                # Handle from imports: from x import y, from x.y import z
                elif isinstance(node, ast.ImportFrom):
                    if node.module:  # from x import y
                        # See if the module is in our map
                        if node.module in module_map:
                            dependencies[file_path].add(module_map[node.module])

                        # Check for package imports
                        for prefix in get_module_prefixes(node.module):
                            if prefix in module_map:
                                dependencies[file_path].add(module_map[prefix])

                    # Handle relative imports: from . import x, from .. import y
                    if node.level > 0:  # Relative import
                        # Get the directory of the current file
                        dir_path = os.path.dirname(file_path)

                        # Go up levels according to the number of dots
                        for _ in range(node.level - 1):
                            dir_path = os.path.dirname(dir_path)

                        # Try to find matching imports
                        for name in node.names:
                            if node.module:
                                target_module = f"{node.module}.{name.name}"
                            else:
                                target_module = name.name

                            # Check for the module within the relative directory
                            rel_path = os.path.join(
                                dir_path, target_module.replace(".", os.sep)
                            )

                            # Try with .py extension first
                            py_path = f"{rel_path}.py"
                            if os.path.exists(py_path) and py_path in files:
                                dependencies[file_path].add(py_path)

                            # Try as directory with __init__.py
                            init_path = os.path.join(rel_path, "__init__.py")
                            if os.path.exists(init_path) and init_path in files:
                                dependencies[file_path].add(init_path)

        except Exception as e:
            print(f"Warning: Could not analyze imports in {file_path}: {e}")

    return dependencies


def get_module_prefixes(module_name: str) -> list[str]:
    """Generate all possible module prefixes for a given module name.
    For example, 'a.b.c' would return ['a.b.c', 'a.b', 'a']
    """
    parts = module_name.split(".")
    return [".".join(parts[:i]) for i in range(len(parts), 0, -1)]


def generate_folder_tree(folder: str, included_files: list[str]) -> str:
    """Generate an ASCII folder tree representation, only showing directories and files that are included."""
    tree_output = []
    included_paths = set(included_files)

    # Get all directories containing included files
    included_dirs = set()
    for file_path in included_paths:
        dir_path = os.path.dirname(file_path)
        while dir_path and dir_path != folder:
            included_dirs.add(dir_path)
            dir_path = os.path.dirname(dir_path)

    def _generate_tree(dir_path: str, prefix: str = "", is_last: bool = True):
        # Get the directory name
        dir_name = os.path.basename(dir_path) or dir_path

        # Add the current directory to the output
        tree_output.append(
            f"{prefix}{'└── ' if is_last else '├── '}{dir_name}/"
        )

        # Update prefix for children
        new_prefix = f"{prefix}{'    ' if is_last else '│   '}"

        # Get relevant entries in the directory
        try:
            entries = os.listdir(dir_path)
            relevant_dirs = []
            relevant_files = []

            for entry in entries:
                entry_path = os.path.join(dir_path, entry)
                if os.path.isdir(entry_path):
                    # Include directory if it or any of its subdirectories contain included files
                    if (
                        any(
                            f.startswith(entry_path + os.sep)
                            for f in included_paths
                        )
                        or entry_path in included_dirs
                    ):
                        relevant_dirs.append(entry)
                elif entry_path in included_paths:
                    # Only include the specific files we're interested in
                    relevant_files.append(entry)

            # Sort entries for consistent output
            relevant_dirs.sort()
            relevant_files.sort()

            # Process relevant subdirectories
            for i, entry in enumerate(relevant_dirs):
                entry_path = os.path.join(dir_path, entry)
                is_last_dir = i == len(relevant_dirs) - 1
                is_last_item = is_last_dir and len(relevant_files) == 0
                _generate_tree(entry_path, new_prefix, is_last_item)

            # Process relevant files
            for i, entry in enumerate(relevant_files):
                is_last_file = i == len(relevant_files) - 1
                tree_output.append(
                    f"{new_prefix}{'└── ' if is_last_file else '├── '}{entry}"
                )

        except (PermissionError, FileNotFoundError):
            return

    # Start the recursion from the root folder
    _generate_tree(folder)

    return "\n".join(tree_output)


def get_common_patterns(files: list[str]) -> dict[str, list[str]]:
    """Identify common design patterns in the codebase."""
    patterns = {
        "singleton": [],
        "factory": [],
        "observer": [],
        "decorator": [],
        "mvc_components": {"models": [], "views": [], "controllers": []},
    }

    for file_path in files:
        if not file_path.endswith(".py"):
            continue

        try:
            with open(file_path, encoding="utf-8") as f:
                content = f.read().lower()

            # Check for singleton pattern
            if "instance = none" in content and "__new__" in content:
                patterns["singleton"].append(file_path)

            # Check for factory pattern
            if "factory" in os.path.basename(file_path).lower() or (
                "def create" in content and "return" in content
            ):
                patterns["factory"].append(file_path)

            # Check for observer pattern
            if ("observer" in content or "listener" in content) and (
                "notify" in content or "update" in content
            ):
                patterns["observer"].append(file_path)

            # Check for decorator pattern
            if "decorator" in os.path.basename(file_path).lower() or (
                "def wrapper" in content and "return wrapper" in content
            ):
                patterns["decorator"].append(file_path)

            # Check for MVC components
            if "model" in os.path.basename(file_path).lower():
                patterns["mvc_components"]["models"].append(file_path)
            elif "view" in os.path.basename(file_path).lower():
                patterns["mvc_components"]["views"].append(file_path)
            elif (
                "controller" in os.path.basename(file_path).lower()
                or "handler" in os.path.basename(file_path).lower()
            ):
                patterns["mvc_components"]["controllers"].append(file_path)

        except Exception:
            continue

    # Remove empty categories
    for key in list(patterns.keys()):
        if isinstance(patterns[key], list) and not patterns[key]:
            patterns.pop(key)
        elif isinstance(patterns[key], dict):
            empty = True
            for subkey in patterns[key]:
                if patterns[key][subkey]:
                    empty = False
                    break
            if empty:
                patterns.pop(key)

    return patterns


def find_key_files(
    files: list[str], dependencies: dict[str, set[str]]
) -> list[str]:
    """Identify key files based on dependencies and naming conventions."""
    # Initialize scores for each file
    scores = {file: 0 for file in files}

    # Track how many files depend on each file (dependents)
    dependent_count = {file: 0 for file in files}
    for file, deps in dependencies.items():
        for dep in deps:
            if dep in dependent_count:
                dependent_count[dep] += 1

    # Score by number of files that depend on this file
    for file, count in dependent_count.items():
        scores[file] += count * 2

    # Score by file naming heuristics
    for file in files:
        base_name = os.path.basename(file).lower()

        # Core files
        if any(
            core_name in base_name
            for core_name in ["main", "app", "core", "init", "cli"]
        ):
            scores[file] += 5

        # Configuration and settings
        if any(
            config_name in base_name
            for config_name in ["config", "settings", "constants"]
        ):
            scores[file] += 3

        # Base classes and abstract components
        if any(
            base_name in base_name
            for base_name in ["base", "abstract", "interface", "factory"]
        ):
            scores[file] += 2

        # Utilities and helpers
        if any(
            util_name in base_name
            for util_name in ["util", "helper", "common", "tools"]
        ):
            scores[file] += 1

        # Score directories by importance
        dir_path = os.path.dirname(file)
        if "core" in dir_path.lower():
            scores[file] += 2
        if "main" in dir_path.lower():
            scores[file] += 1

        # Score by file size and complexity
        try:
            metadata = get_file_metadata(file)
            line_count = metadata["line_count"]
            scores[file] += min(line_count / 50, 3)  # Cap at 3 points for size

            # Additional points for very significant files
            if line_count > 200:
                scores[file] += 1
        except Exception:
            pass

        # Score by extension - Python files are often more important
        if file.endswith(".py"):
            scores[file] += 1

        # Examples and documentation are important but not as much as core files
        if "example" in file.lower() or "demo" in file.lower():
            scores[file] += 0.5

    # Sort by score in descending order
    key_files = sorted(files, key=lambda f: scores[f], reverse=True)

    # Debugging info
    print(f"Top 5 key files with scores:")
    for file in key_files[:5]:
        print(f"  {file}: {scores[file]:.1f} points")

    # Return top 25% of files or at least 5 files (if available)
    num_key_files = max(min(len(files) // 4, 20), min(5, len(files)))
    return key_files[:num_key_files]


def generate_markdown_string(
    files: list[str],
    extension: str,
    folder: str,
    key_files: list[str],
    dependencies: dict[str, set[str]],
    patterns: dict[str, list[str]],
) -> str:
    """Generate a comprehensive markdown document about the codebase as a string."""
    md_content = []

    # Write header
    md_content.append(f"# Code Repository Analysis\n")
    md_content.append(f"Generated on {datetime.datetime.now()}\n\n")

    # Write repository summary
    md_content.append("## Repository Summary\n\n")
    md_content.append(f"- **Extension analyzed**: `{extension}`\n")
    md_content.append(f"- **Number of files**: {len(files)}\n")
    md_content.append(f"- **Root folder**: `{folder}`\n")

    total_lines = sum(get_file_metadata(f)["line_count"] for f in files)
    md_content.append(f"- **Total lines of code**: {total_lines}\n\n")

    # Generate and write folder tree
    md_content.append("## Project Structure\n\n")
    md_content.append("```\n")
    md_content.append(generate_folder_tree(folder, files))
    md_content.append("\n```\n\n")

    # Write key files section
    md_content.append("## Key Files\n\n")
    md_content.append(
        "These files appear to be central to the codebase based on dependencies and naming conventions:\n\n"
    )

    for file in key_files:
        rel_path = os.path.relpath(file, folder)
        md_content.append(f"### {rel_path}\n\n")

        metadata = get_file_metadata(file)
        md_content.append(f"- **Lines**: {metadata['line_count']}\n")
        md_content.append(f"- **Last modified**: {metadata['last_modified']}\n")

        # Add dependency info
        dependent_files = [
            os.path.relpath(f, folder)
            for f in dependencies
            if file in dependencies[f]
        ]
        if dependent_files:
            md_content.append(f"- **Used by**: {len(dependent_files)} files\n")

        # For Python files, add component analysis
        if file.endswith(".py"):
            components = extract_python_components(file)

            if components["docstring"]:
                md_content.append(
                    f"\n**Description**: {components['docstring'].strip()}\n"
                )

            if components["classes"]:
                md_content.append("\n**Classes**:\n")
                for cls in components["classes"]:
                    md_content.append(
                        f"- `{cls['name']}`: {len(cls['methods'])} methods\n"
                    )

            if components["functions"]:
                md_content.append("\n**Functions**:\n")
                for func in components["functions"]:
                    md_content.append(
                        f"- `{func['name']}({', '.join(func['args'])})`\n"
                    )

        md_content.append("\n**Content**:\n")
        md_content.append(f"```{extension.lstrip('.')}\n")

        # Read and write file content
        try:
            with open(file, encoding="utf-8") as code_file:
                content = code_file.read()
                md_content.append(content)
                if not content.endswith("\n"):
                    md_content.append("\n")
        except Exception as e:
            md_content.append(f"Error reading file: {e!s}\n")

        md_content.append("```\n\n")

    # Write design patterns section if any were detected
    if patterns:
        md_content.append("## Design Patterns\n\n")
        md_content.append(
            "The following design patterns appear to be used in this codebase:\n\n"
        )

        for pattern, files_list in patterns.items():
            if isinstance(files_list, list) and files_list:
                md_content.append(f"### {pattern.title()} Pattern\n\n")
                for f in files_list:
                    md_content.append(f"- `{os.path.relpath(f, folder)}`\n")
                md_content.append("\n")
            elif isinstance(files_list, dict):
                md_content.append(
                    f"### {pattern.replace('_', ' ').title()}\n\n"
                )
                for subpattern, subfiles in files_list.items():
                    if subfiles:
                        md_content.append(f"**{subpattern.title()}**:\n")
                        for f in subfiles:
                            md_content.append(
                                f"- `{os.path.relpath(f, folder)}`\n"
                            )
                        md_content.append("\n")

    # Write all other files section
    md_content.append("## All Files\n\n")

    for file in files:
        if file in key_files:
            continue  # Skip files already detailed in key files section

        rel_path = os.path.relpath(file, folder)
        md_content.append(f"### {rel_path}\n\n")

        metadata = get_file_metadata(file)
        md_content.append(f"- **Lines**: {metadata['line_count']}\n")
        md_content.append(
            f"- **Last modified**: {metadata['last_modified']}\n\n"
        )

        md_content.append("```" + extension.lstrip(".") + "\n")

        # Read and write file content
        try:
            with open(file, encoding="utf-8") as code_file:
                content = code_file.read()
                md_content.append(content)
                if not content.endswith("\n"):
                    md_content.append("\n")
        except Exception as e:
            md_content.append(f"Error reading file: {e!s}\n")

        md_content.append("```\n\n")

    return "".join(md_content)


def collect_code(extension: str = ".py", folder: str = ".") -> str:
    """Main function to analyze code repository and generate markdown string.

    Args:
        extension: File extension to analyze
        folder: Root folder to analyze

    Returns:
        A string containing the markdown analysis
    """
    print(f"Analyzing {extension} files from {folder}...")

    # Find all matching files
    files = find_files(folder, extension)
    print(f"Found {len(files)} files")

    # Get dependencies
    dependencies = analyze_code_dependencies(files)

    # Find key files
    key_files = find_key_files(files, dependencies)

    # Get design patterns
    patterns = get_common_patterns(files)

    # Generate markdown content
    markdown_content = generate_markdown_string(
        files, extension, folder, key_files, dependencies, patterns
    )
    print(f"Repository analysis complete.")

    return markdown_content


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(
        description="Analyze code repository for LLM consumption"
    )
    parser.add_argument(
        "-c", "--extension", default=".py", help="File extension to analyze"
    )
    parser.add_argument("-f", "--folder", default=".", help="Folder to analyze")
    parser.add_argument(
        "-o",
        "--output",
        default="repository_analysis.md",
        help="Output markdown file",
    )

    args = parser.parse_args()

    # Generate the markdown content
    markdown_content = collect_code(args.extension, args.folder)

    # Write the content to the output file
    with open(args.output, "w", encoding="utf-8") as output_file:
        output_file.write(markdown_content)

    print(f"Output written to '{args.output}'")

```

---

### 31. scripts\create_docs.py

- **File ID**: file_30
- **Type**: Code File
- **Line Count**: 135
- **Description**: File at scripts\create_docs.py
- **Dependencies**: None
- **Used By**: None

**Content**:
```
from pathlib import Path
from typing import Any

import yaml  # <--- Keep standard import

# --- Configuration ---
MKDOCS_YAML_PATH = "mkdocs.yml"
DOCS_DIR = Path("docs")
PLACEHOLDER_CONTENT = """---
hide: # Optional: Hide table of contents on simple pages
  - toc
---

# {title}

*Documentation in progress...*
"""
# --- End Configuration ---


def create_markdown_file(file_path: Path, title: str) -> None:
    """Creates a markdown file with a title and placeholder content."""
    file_path.parent.mkdir(parents=True, exist_ok=True)
    content = PLACEHOLDER_CONTENT.format(title=title)
    if not file_path.exists():
        try:
            file_path.write_text(content, encoding="utf-8")
            print(f"Created: {file_path}")
        except OSError as e:
            print(f"Error creating file {file_path}: {e}")
    else:
        print(f"Skipped (already exists): {file_path}")


def process_nav_item(item: Any, base_dir: Path) -> None:
    """Recursively processes items from the MkDocs nav structure."""
    if isinstance(item, str):
        if ":" in item:
            title, path_str = item.split(":", 1)
            title = title.strip()
            path_str = path_str.strip()
        else:
            path_str = item.strip()
            title = (
                Path(path_str).stem.replace("-", " ").replace("_", " ").title()
            )
            if Path(path_str).name == "index.md":
                parent_name = Path(path_str).parent.name
                if parent_name and parent_name != ".":
                    title = (
                        parent_name.replace("-", " ").replace("_", " ").title()
                        + " Overview"
                    )
                else:
                    title = "Home"

        if path_str.endswith(".md"):
            file_path = base_dir / path_str
            create_markdown_file(file_path, title)

    elif isinstance(item, dict):
        for key, value in item.items():
            if isinstance(value, str) and value.endswith(".md"):
                file_path = base_dir / value
                create_markdown_file(file_path, key)
            elif isinstance(value, list):
                for sub_item in value:
                    process_nav_item(sub_item, base_dir)
            else:
                print(
                    f"Warning: Unexpected value type in nav for key '{key}': {type(value)}"
                )

    elif isinstance(item, list):
        for sub_item in item:
            process_nav_item(sub_item, base_dir)

    else:
        print(f"Warning: Unexpected item type in nav: {type(item)}")


def main():
    """Loads mkdocs.yml, parses the nav structure, and creates placeholder files."""
    DOCS_DIR.mkdir(exist_ok=True)

    try:
        with open(MKDOCS_YAML_PATH, encoding="utf-8") as f:
            # --- CHANGE HERE: Use yaml.Loader instead of yaml.safe_load ---
            config = yaml.load(f, Loader=yaml.Loader)
            # --- END CHANGE ---
    except FileNotFoundError:
        print(f"Error: {MKDOCS_YAML_PATH} not found.")
        return
    except yaml.YAMLError as e:
        print(f"Error parsing {MKDOCS_YAML_PATH}: {e}")
        return

    nav_structure = config.get("nav")
    if not nav_structure or not isinstance(nav_structure, list):
        print("Error: 'nav' structure not found or invalid in mkdocs.yml.")
        return

    print(f"Processing navigation structure from {MKDOCS_YAML_PATH}...")
    for item in nav_structure:
        process_nav_item(item, DOCS_DIR)

    # --- Create essential assets directory and placeholder files ---
    assets_dir = DOCS_DIR / "assets" / "images"
    assets_dir.mkdir(parents=True, exist_ok=True)
    placeholder_logo = assets_dir / "flock_logo_small.png"
    placeholder_favicon = assets_dir / "favicon.png"
    if not placeholder_logo.exists():
        placeholder_logo.touch()
        print(f"Created placeholder: {placeholder_logo}")
    if not placeholder_favicon.exists():
        placeholder_favicon.touch()
        print(f"Created placeholder: {placeholder_favicon}")

    css_dir = DOCS_DIR / "stylesheets"
    css_dir.mkdir(exist_ok=True)
    placeholder_css = css_dir / "extra.css"
    if not placeholder_css.exists():
        placeholder_css.touch()
        print(f"Created placeholder: {placeholder_css}")
    # --- End assets creation ---

    print("\nBoilerplate documentation structure created.")
    print(
        f"You can now start filling in the content in the '{DOCS_DIR}' directory."
    )
    print("Run 'mkdocs serve' to view the documentation locally.")


if __name__ == "__main__":
    main()

```

---

### 32. src\flock\__init__.py

- **File ID**: file_31
- **Type**: Code File
- **Line Count**: 138
- **Description**: Flock package initialization.
- **Dependencies**: None
- **Used By**: None

**Content**:
```
"""Flock package initialization."""

from rich.panel import Panel

from flock.cli.config import init_config_file, load_config_file
from flock.cli.constants import (
    CLI_CFG_FILE,
    CLI_EXIT,
    CLI_NOTES,
    CLI_REGISTRY_MANAGEMENT,
    CLI_THEME_BUILDER,
)
from flock.cli.load_release_notes import load_release_notes
from flock.cli.settings import settings_editor
from flock.core.logging.formatters.theme_builder import theme_builder


def main():
    """Main function."""
    import questionary
    from rich.console import Console

    from flock.cli.constants import (
        CLI_CREATE_AGENT,
        CLI_CREATE_FLOCK,
        CLI_LOAD_AGENT,
        CLI_LOAD_EXAMPLE,
        CLI_LOAD_FLOCK,
        CLI_SETTINGS,
        CLI_START_WEB_SERVER,
    )
    from flock.cli.load_flock import load_flock
    from flock.core.util.cli_helper import init_console

    console = Console()

    # Show a welcome message on first run with the new tool serialization format
    import os

    cfg_file = os.path.expanduser(f"~/.flock/{CLI_CFG_FILE}")
    if not os.path.exists(cfg_file):
        # Create the directory if it doesn't exist
        os.makedirs(os.path.dirname(cfg_file), exist_ok=True)

        init_config_file()
    else:
        # Load the config file
        load_config_file()

    feature_flag_file = os.path.expanduser("~/.flock/tool_serialization_notice")
    if not os.path.exists(feature_flag_file):
        # Create the directory if it doesn't exist
        os.makedirs(os.path.dirname(feature_flag_file), exist_ok=True)

        # Show the notice about the new tool serialization
        console.print(
            Panel(
                "[bold green]Flock 0.4.0b 'Magpie'- Flock CLI Management Console BETA[/]\n\n"
                "Flock now offers a tool for managing your flock:\n"
                "- Serialization and deserialization of Flock instances\n"
                "- Execution of Flock instances\n"
                "- Managing components, types, and agents via the registry management\n"
                "- Starting a web server to interact with your flock\n"
                "plannes featues: deployment on docker, kubernetes, and more!"
            ),
            justify="center",
        )
        console.line()

        # Create the flag file to prevent showing this notice again
        with open(feature_flag_file, "w") as f:
            f.write("Tool serialization notice shown")

        input("Press Enter to continue to the main menu...")
        console.clear()

    while True:
        init_console()

        # console.print("Flock Management Console\n", style="bold green")
        console.print(
            Panel("[bold green]Flock Management Console[/]"), justify="center"
        )
        console.line()

        result = questionary.select(
            "What do you want to do?",
            choices=[
                questionary.Separator(line=" "),
                # CLI_CREATE_AGENT,
                CLI_CREATE_FLOCK,
                # CLI_LOAD_AGENT,
                CLI_LOAD_FLOCK,
                # CLI_LOAD_EXAMPLE,
                questionary.Separator(),
                CLI_REGISTRY_MANAGEMENT,
                questionary.Separator(),
                CLI_THEME_BUILDER,
                CLI_SETTINGS,
                questionary.Separator(),
                CLI_NOTES,
                CLI_EXIT,
            ],
        ).ask()

        if result == CLI_LOAD_FLOCK:
            load_flock()
        elif result == CLI_CREATE_FLOCK:
            # This will be implemented in a separate create_flock.py
            from flock.cli.create_flock import create_flock

            create_flock()
        elif result == CLI_THEME_BUILDER:
            theme_builder()
        elif result == CLI_REGISTRY_MANAGEMENT:
            # Import registry management when needed
            from flock.cli.registry_management import manage_registry

            manage_registry()
        elif result == CLI_SETTINGS:
            settings_editor()
        elif result == CLI_START_WEB_SERVER:
            # Simple web server without a loaded Flock - could create a new one
            console.print(
                "[yellow]Web server without loaded Flock not yet implemented.[/]"
            )
            input("\nPress Enter to continue...")
        elif result == CLI_NOTES:
            load_release_notes()
        elif result == CLI_EXIT:
            break
        input("\nPress Enter to continue...\n\n")

        console.clear()


if __name__ == "__main__":
    main()

```

---

### 33. src\flock\cli\config.py

- **File ID**: file_32
- **Type**: Code File
- **Line Count**: 8
- **Description**: File at src\flock\cli\config.py
- **Dependencies**: None
- **Used By**: None

**Content**:
```
def init_config_file():
    """Initialize the config file."""
    pass


def load_config_file():
    """Load the config file."""
    pass

```

---

### 34. src\flock\cli\constants.py

- **File ID**: file_33
- **Type**: Code File
- **Line Count**: 36
- **Description**: Constants for the CLI module.
- **Dependencies**: None
- **Used By**: None

**Content**:
```
"""Constants for the CLI module."""

CLI_CFG_FILE = ".env"
CLI_DEFAULT_ENV_VARS = {
    "FLICK_API_KEY": "flock-api-key",
    "FLICK_API_URL": "https://api.flock.com",
    "FLICK_API_VERSION": "v1",
    "FLICK_API_KEY": "flock-api-key",
    "FLICK_API_URL": "https://api.flock.com",
    "FLICK_API_VERSION": "v1",
}
CLI_DEFAULT_FOLDER = ".flock"

CLI_CREATE_AGENT = "Create an agent"
CLI_CREATE_FLOCK = "Create a new Flock"
CLI_LOAD_AGENT = "Load an agent"
CLI_LOAD_FLOCK = "Load a *.flock file"
CLI_THEME_BUILDER = "Theme builder"
CLI_LOAD_EXAMPLE = "Load a example"
CLI_SETTINGS = "Settings"
CLI_NOTES = "'Hummingbird' release notes"
CLI_START_WEB_SERVER = "Start web server"
CLI_REGISTRY_MANAGEMENT = "Registry management"
CLI_EXIT = "Exit"
CLI_CHOICES = [
    CLI_CREATE_AGENT,
    CLI_CREATE_FLOCK,
    CLI_LOAD_AGENT,
    CLI_LOAD_FLOCK,
    CLI_LOAD_EXAMPLE,
    CLI_THEME_BUILDER,
    CLI_REGISTRY_MANAGEMENT,
    CLI_SETTINGS,
    CLI_START_WEB_SERVER,
    CLI_EXIT,
]

```

---

### 35. src\flock\cli\create_agent.py

- **File ID**: file_34
- **Type**: Code File
- **Line Count**: 1
- **Description**: File at src\flock\cli\create_agent.py
- **Dependencies**: None
- **Used By**: None

**Content**:
```
# TODO

```

---

### 36. src\flock\cli\create_flock.py

- **File ID**: file_35
- **Type**: Code File
- **Line Count**: 287
- **Description**: Create a new Flock through a guided wizard....
- **Dependencies**: None
- **Used By**: None

**Content**:
```
"""Create a new Flock through a guided wizard.

This module provides a wizard-like interface for creating new Flock instances,
with options for basic configuration and initial agent creation.
"""

from datetime import datetime
from pathlib import Path

import questionary
from rich.console import Console
from rich.panel import Panel

from flock.cli.constants import CLI_DEFAULT_FOLDER
from flock.cli.loaded_flock_cli import start_loaded_flock_cli
from flock.core.flock import Flock
from flock.core.flock_factory import FlockFactory
from flock.core.logging.logging import get_logger
from flock.core.util.cli_helper import init_console

# Create console instance
console = Console()
logger = get_logger("cli.create_flock")


def create_flock():
    """Create a new Flock through a guided wizard."""
    init_console()
    console.print(Panel("[bold green]Create New Flock[/]"), justify="center")
    console.line()

    # Step 1: Basic Flock Configuration
    console.print("[bold]Step 1: Basic Flock Configuration[/]")
    console.line()

    flock_name = questionary.text(
        "Enter a name for this Flock:",
        default="",
    ).ask()

    # Get description
    description = questionary.text(
        "Enter a description for this Flock (optional):",
        default="",
    ).ask()

    # Default model selection
    default_models = [
        "openai/gpt-4o",
        "openai/gpt-3.5-turbo",
        "anthropic/claude-3-opus-20240229",
        "anthropic/claude-3-sonnet-20240229",
        "gemini/gemini-1.5-pro",
        "Other (specify)",
    ]

    model_choice = questionary.select(
        "Select a default model:",
        choices=default_models,
    ).ask()

    if model_choice == "Other (specify)":
        model = questionary.text(
            "Enter the model identifier:",
            default="openai/gpt-4o",
        ).ask()
    else:
        model = model_choice

    # Execution options
    # enable_temporal = questionary.confirm(
    #     "Enable Temporal for distributed execution?",
    #     default=False,
    # ).ask()
    enable_temporal = False

    # Logging configuration
    enable_logging = questionary.confirm(
        "Enable logging?",
        default=True,
    ).ask()

    # Create the Flock instance
    flock = Flock(
        name=flock_name,
        model=model,
        description=description,
        enable_temporal=enable_temporal,
        enable_logging=enable_logging,
    )

    console.print("\n[green]✓[/] Flock created successfully!")
    console.line()

    # Step 2: Create Initial Agent (optional)
    create_agent = questionary.confirm(
        "Would you like to create an initial agent?",
        default=True,
    ).ask()

    if create_agent:
        _create_initial_agent(flock)

    # Step 3: Save Options
    console.print("\n[bold]Step 3: Save Options[/]")
    console.line()

    save_choice = questionary.select(
        "What would you like to do with this Flock?",
        choices=[
            "Save to YAML file",
            "Continue in CLI without saving",
            "Execute immediately",
            "Cancel and discard",
        ],
    ).ask()

    if save_choice == "Save to YAML file":
        _save_flock_to_yaml(flock)

        # Ask if user wants to continue working with this Flock
        continue_with_flock = questionary.confirm(
            "Would you like to continue working with this Flock in the CLI?",
            default=True,
        ).ask()

        if continue_with_flock:
            start_loaded_flock_cli(flock, server_name="New Flock")

    elif save_choice == "Continue in CLI without saving":
        start_loaded_flock_cli(flock, server_name="New Flock")

    elif save_choice == "Execute immediately":
        from flock.cli.execute_flock import execute_flock

        try:
            execute_flock(flock)
        except ImportError:
            console.print(
                "[yellow]Execute functionality not yet implemented.[/]"
            )
            input("\nPress Enter to continue...")
            start_loaded_flock_cli(flock, server_name="New Flock")


def _create_initial_agent(flock):
    """Create an initial agent for the Flock.

    Args:
        flock: The Flock instance to add the agent to
    """
    console.print("\n[bold]Step 2: Create Initial Agent[/]")
    console.line()

    # Get agent name
    name = questionary.text(
        "Enter a name for the agent:",
        default="my_agent",
    ).ask()

    # Get agent description
    description = questionary.text(
        "Enter a description for the agent (optional):",
        default="",
    ).ask()

    # Get input specification
    input_spec = questionary.text(
        "Enter input specification (e.g., 'query: str | The search query'):",
        default="query",
    ).ask()

    # Get output specification
    output_spec = questionary.text(
        "Enter output specification (e.g., 'result: str | The generated result'):",
        default="result",
    ).ask()

    # Additional options
    use_cache = questionary.confirm(
        "Enable caching for this agent?",
        default=True,
    ).ask()

    enable_rich_tables = questionary.confirm(
        "Enable rich table output for this agent?",
        default=True,
    ).ask()

    # Create the agent
    agent = FlockFactory.create_default_agent(
        name=name,
        description=description,
        input=input_spec,
        output=output_spec,
        use_cache=use_cache,
        enable_rich_tables=enable_rich_tables,
    )

    # Add the agent to the Flock
    flock.add_agent(agent)
    console.print(f"\n[green]✓[/] Agent '{name}' created and added to Flock!")


def _save_flock_to_yaml(flock):
    """Save the Flock to a YAML file.

    Args:
        flock: The Flock instance to save
    """
    # Get file path
    # default = flock.name + current date in 04_04_2025 format
    default_name = f"{flock.name}_{datetime.now().strftime('%m_%d_%Y')}"
    file_path = questionary.text(
        "Enter file path to save Flock:",
        default=default_name,
    ).ask()

    # Ensure the file has the correct extension
    if not file_path.endswith((".yaml", ".yml")):
        file_path += ".flock.yaml"

    file_path = CLI_DEFAULT_FOLDER + "/" + file_path

    # Create directory if it doesn't exist
    save_path = Path(file_path)
    save_path.parent.mkdir(parents=True, exist_ok=True)

    # Ask about path_type
    path_type_choice = questionary.select(
        "How should file paths be formatted?",
        choices=[
            "absolute (full paths, best for local use)",
            "relative (relative paths, better for sharing)",
        ],
        default="absolute (full paths, best for local use)",
    ).ask()

    # Extract just the first word
    path_type = path_type_choice.split()[0]

    console.print(
        f"[bold]Path type selected: [green]{path_type}[/green][/bold]"
    )

    try:
        # Check if the flock has tools to provide a helpful message
        has_tools = False
        for agent in flock.agents.values():
            if agent.tools and len(agent.tools) > 0:
                has_tools = True
                break

        # Save the Flock to YAML with proper tool serialization
        logger.info(f"Saving Flock to {file_path}")
        flock.to_yaml_file(file_path, path_type=path_type)
        console.print(
            f"\n[green]✓[/] Flock saved to {file_path} with {path_type} paths"
        )

        # Provide helpful information about tool serialization
        if has_tools:
            console.print("\n[bold blue]Tools Information:[/]")
            console.print(
                "This Flock contains tools that have been serialized as callable references."
            )
            console.print(
                "When loading this Flock on another system, ensure that:"
            )
            console.print(
                "  - The tools/functions are registered in the Flock registry"
            )
            console.print(
                "  - The containing modules are available in the Python path"
            )
    except Exception as e:
        logger.error(f"Error saving Flock: {e}", exc_info=True)
        console.print(f"\n[bold red]Error saving Flock:[/] {e!s}")

        # Provide guidance on potential issues with tool serialization
        if "callable" in str(e).lower() or "registry" in str(e).lower():
            console.print(
                "\n[yellow]This error might be related to tool serialization.[/]"
            )
            console.print(
                "[yellow]Check if all tools are properly registered in the Flock registry.[/]"
            )

```

---

### 37. src\flock\cli\execute_flock.py

- **File ID**: file_36
- **Type**: Code File
- **Line Count**: 610
- **Description**: Execute a Flock instance with a selected agent....
- **Dependencies**: None
- **Used By**: None

**Content**:
```
"""Execute a Flock instance with a selected agent.

This module provides functionality to execute a Flock instance with
a selected agent and input configuration, including batch processing.
"""

import json
import os

import questionary
from rich.console import Console
from rich.panel import Panel

from flock.core.flock import Flock
from flock.core.util.cli_helper import init_console

# Create console instance
console = Console()

# Try importing pandas for DataFrame support
try:
    import pandas as pd

    PANDAS_AVAILABLE = True
except ImportError:
    pd = None
    PANDAS_AVAILABLE = False


def execute_flock(flock: Flock):
    """Execute a Flock instance.

    Args:
        flock: The Flock instance to execute
    """
    if not flock:
        console.print("[bold red]Error: No Flock instance provided.[/]")
        return

    agent_names = list(flock._agents.keys())

    if not agent_names:
        console.print("[yellow]No agents in this Flock to execute.[/]")
        return

    init_console()
    console.print(Panel("[bold green]Execute Flock[/]"), justify="center")

    # Step 1: Select start agent
    console.print("\n[bold]Step 1: Select Start Agent[/]")

    start_agent_name = questionary.select(
        "Select an agent to start with:",
        choices=agent_names,
    ).ask()

    if not start_agent_name:
        return

    start_agent = flock._agents[start_agent_name]

    # Step 2: Configure input
    console.print("\n[bold]Step 2: Configure Input[/]")

    # Parse input schema
    input_fields = _parse_input_schema(start_agent.input)

    # If we couldn't parse any fields, ask for generic input
    if not input_fields:
        raw_input = questionary.text(
            "Enter input (JSON format):",
            default="{}",
        ).ask()

        try:
            input_data = json.loads(raw_input)
        except json.JSONDecodeError:
            console.print("[bold red]Error: Invalid JSON input.[/]")
            return
    else:
        # Otherwise, ask for each field
        input_data = {}

        for field, info in input_fields.items():
            field_type = info.get("type", "str")
            description = info.get("description", "")
            prompt = f"Enter value for '{field}'"

            if description:
                prompt += f" ({description})"

            prompt += ":"

            value = questionary.text(prompt).ask()

            # Convert value to appropriate type
            if field_type == "int":
                try:
                    value = int(value)
                except ValueError:
                    console.print(
                        f"[yellow]Warning: Could not convert value to int, using as string.[/]"
                    )

            input_data[field] = value

    # Step 3: Run Options
    console.print("\n[bold]Step 3: Run Options[/]")

    # Logging options
    enable_logging = questionary.confirm(
        "Enable detailed logging?",
        default=False,
    ).ask()

    # Preview input
    console.print("\n[bold]Input Preview:[/]")
    console.print(json.dumps(input_data, indent=2))

    # Confirm execution
    confirm = questionary.confirm(
        "Execute Flock with this configuration?",
        default=True,
    ).ask()

    if not confirm:
        return

    # Execute the Flock
    console.print("\n[bold]Executing Flock...[/]")

    try:
        # Handle logging settings
        if enable_logging:
            # Enable logging through the logging configuration method
            flock._configure_logging(True)

        # Run the Flock
        result = flock.run(
            start_agent=start_agent_name,
            input=input_data,
        )

        # Display result
        console.print("\n[bold green]Execution Complete![/]")

        if result and enable_logging:
            console.print("\n[bold]Result:[/]")
            if isinstance(result, dict):
                # Display as formatted JSON
                console.print(json.dumps(result, indent=2))
            else:
                # Display as plain text
                console.print(str(result))

    except Exception as e:
        console.print(f"\n[bold red]Error during execution:[/] {e!s}")


def _parse_input_schema(input_schema: str) -> dict[str, dict[str, str]]:
    """Parse the input schema string into a field dictionary.

    Args:
        input_schema: The input schema string (e.g., "query: str | The search query")

    Returns:
        A dictionary mapping field names to field info (type, description)
    """
    if not input_schema:
        return {}

    fields = {}

    try:
        # Split by comma for multiple fields
        for field_def in input_schema.split(","):
            field_def = field_def.strip()

            # Check for type hint with colon
            if ":" in field_def:
                field_name, rest = field_def.split(":", 1)
                field_name = field_name.strip()
                rest = rest.strip()

                # Check for description with pipe
                if "|" in rest:
                    field_type, description = rest.split("|", 1)
                    fields[field_name] = {
                        "type": field_type.strip(),
                        "description": description.strip(),
                    }
                else:
                    fields[field_name] = {"type": rest.strip()}
            else:
                # Just a field name without type hint
                if "|" in field_def:
                    field_name, description = field_def.split("|", 1)
                    fields[field_name.strip()] = {
                        "description": description.strip()
                    }
                else:
                    fields[field_def.strip()] = {}

    except Exception as e:
        console.print(
            f"[yellow]Warning: Could not parse input schema: {e!s}[/]"
        )
        return {}

    return fields


def execute_flock_batch(flock: Flock):
    """Execute a Flock instance in batch mode.

    Args:
        flock: The Flock instance to execute
    """
    if not flock:
        console.print("[bold red]Error: No Flock instance provided.[/]")
        return

    agent_names = list(flock._agents.keys())

    if not agent_names:
        console.print("[yellow]No agents in this Flock to execute.[/]")
        return

    init_console()
    console.print(
        Panel("[bold green]Execute Flock - Batch Mode[/]"), justify="center"
    )

    # Step 1: Select start agent
    console.print("\n[bold]Step 1: Select Start Agent[/]")

    start_agent_name = questionary.select(
        "Select an agent to start with:",
        choices=agent_names,
    ).ask()

    if not start_agent_name:
        return

    start_agent = flock._agents[start_agent_name]

    # Step 2: Configure batch input source
    console.print("\n[bold]Step 2: Select Batch Input Source[/]")

    if not PANDAS_AVAILABLE:
        console.print(
            "[yellow]Warning: pandas not available. CSV input/output functionality will be limited.[/]"
        )

    input_source_choices = ["Enter batch items manually"]

    if PANDAS_AVAILABLE:
        input_source_choices.insert(0, "Load from CSV file")

    input_source = questionary.select(
        "How would you like to provide batch inputs?",
        choices=input_source_choices,
    ).ask()

    if not input_source:
        return

    batch_inputs = []
    input_mapping = {}

    if input_source == "Load from CSV file" and PANDAS_AVAILABLE:
        # Ask for CSV file path
        csv_path = questionary.path(
            "Enter path to CSV file:",
        ).ask()

        if not csv_path:
            return

        try:
            # Validate path exists
            if not os.path.exists(csv_path):
                console.print(
                    f"[bold red]Error: File '{csv_path}' does not exist.[/]"
                )
                return

            # Preview CSV
            df = pd.read_csv(csv_path)
            console.print("\n[bold]CSV Preview (first 5 rows):[/]")
            console.print(df.head().to_string())

            # Configure column mapping
            console.print("\n[bold]Configure Column Mapping:[/]")

            # Parse input schema (if available)
            input_fields = _parse_input_schema(start_agent.input)

            # If we have input fields, map CSV columns to them
            if input_fields:
                columns = df.columns.tolist()

                for field in input_fields.keys():
                    field_desc = input_fields[field].get("description", "")
                    prompt = f"Select column for '{field}'"

                    if field_desc:
                        prompt += f" ({field_desc})"

                    selected_col = questionary.select(
                        prompt,
                        choices=["(Skip this field)"] + columns,
                    ).ask()

                    if selected_col and selected_col != "(Skip this field)":
                        input_mapping[selected_col] = field
            else:
                # No schema, ask user to map columns manually
                columns = df.columns.tolist()

                for col in columns:
                    mapping = questionary.text(
                        f"Map column '{col}' to input field (leave empty to ignore):",
                    ).ask()

                    if mapping:
                        input_mapping[col] = mapping

            if not input_mapping:
                console.print("[yellow]Warning: No column mapping defined.[/]")
                if not questionary.confirm(
                    "Continue without mapping?", default=False
                ).ask():
                    return

            # Use the CSV file path directly
            batch_inputs = csv_path

        except Exception as e:
            console.print(f"[bold red]Error loading CSV: {e}[/]")
            return

    elif input_source == "Enter batch items manually":
        # Parse input schema
        input_fields = _parse_input_schema(start_agent.input)

        if not input_fields:
            console.print(
                "[yellow]No input schema available. Using JSON input.[/]"
            )

            while True:
                raw_input = questionary.text(
                    "Enter batch item as JSON (empty to finish):",
                    default="{}",
                ).ask()

                if not raw_input:
                    break

                try:
                    item_data = json.loads(raw_input)
                    batch_inputs.append(item_data)
                    console.print(f"[green]Added item {len(batch_inputs)}[/]")
                except json.JSONDecodeError:
                    console.print("[bold red]Error: Invalid JSON input.[/]")

        else:
            # We have input fields, ask for each field for each item
            item_count = 1

            while True:
                console.print(f"\n[bold]Batch Item {item_count}[/]")

                item_data = {}
                for field, info in input_fields.items():
                    field_type = info.get("type", "str")
                    description = info.get("description", "")
                    prompt = f"Enter value for '{field}'"

                    if description:
                        prompt += f" ({description})"

                    prompt += " (empty to skip):"

                    value = questionary.text(prompt).ask()

                    if not value:
                        continue

                    # Convert value to appropriate type
                    if field_type == "int":
                        try:
                            value = int(value)
                        except ValueError:
                            console.print(
                                f"[yellow]Warning: Could not convert value to int, using as string.[/]"
                            )

                    item_data[field] = value

                if item_data:
                    batch_inputs.append(item_data)
                    console.print(f"[green]Added item {len(batch_inputs)}[/]")

                if not questionary.confirm(
                    "Add another batch item?",
                    default=len(batch_inputs)
                    < 2,  # Default to yes if we have less than 2 items
                ).ask():
                    break

                item_count += 1

    if isinstance(batch_inputs, list) and not batch_inputs:
        console.print("[yellow]No batch items defined. Exiting.[/]")
        return

    # Step 3: Configure static inputs (if needed)
    static_inputs = {}

    if questionary.confirm(
        "Would you like to add static inputs (common to all batch items)?",
        default=False,
    ).ask():
        console.print("\n[bold]Configure Static Inputs[/]")

        raw_static = questionary.text(
            "Enter static inputs as JSON:",
            default="{}",
        ).ask()

        try:
            static_inputs = json.loads(raw_static)
        except json.JSONDecodeError:
            console.print(
                "[bold red]Error: Invalid JSON for static inputs. Proceeding without static inputs.[/]"
            )
            static_inputs = {}

    # Step 4: Configure batch execution options
    console.print("\n[bold]Step 4: Configure Batch Execution Options[/]")

    # Determine if we should use Temporal
    use_temporal = False
    # if questionary.confirm(
    #     f"Override Temporal setting? (Current: {flock.enable_temporal})",
    #     default=False,
    # ).ask():
    #     use_temporal = questionary.confirm(
    #         "Use Temporal for batch execution?",
    #         default=flock.enable_temporal,
    #     ).ask()

    # Configure parallelism
    parallel = True
    max_workers = 5

    if not flock.enable_temporal if use_temporal is None else not use_temporal:
        parallel = questionary.confirm(
            "Run batch items in parallel?",
            default=True,
        ).ask()

        if parallel:
            max_workers_input = questionary.text(
                "Maximum number of parallel workers:",
                default="5",
            ).ask()

            try:
                max_workers = int(max_workers_input)
            except ValueError:
                console.print(
                    "[yellow]Invalid worker count. Using default (5).[/]"
                )
                max_workers = 5

    # Configure output options
    silent_mode = questionary.confirm(
        "Use silent mode with progress bar? (Recommended for large batches)",
        default=True,
    ).ask()

    write_to_csv = None
    if (
        PANDAS_AVAILABLE
        and questionary.confirm(
            "Write results to CSV file?",
            default=True,
        ).ask()
    ):
        write_to_csv = questionary.text(
            "CSV output path:",
            default="batch_results.csv",
        ).ask()

        hide_columns = questionary.text(
            "Hide columns (comma-separated - leave blank for hiding no columns):",
            default="",
        ).ask()

        hide_columns = hide_columns.split(",") if hide_columns else []

        delimiter = questionary.text(
            "Delimiter (default is comma):",
            default=",",
        ).ask()

    # Logging options
    enable_logging = questionary.confirm(
        "Enable detailed logging?",
        default=False,
    ).ask()

    # Preview configuration
    console.print("\n[bold]Batch Configuration Preview:[/]")
    console.print(f"Agent: {start_agent_name}")

    if isinstance(batch_inputs, str):
        console.print(f"Input Source: CSV file ({batch_inputs})")
        console.print(f"Column Mapping: {input_mapping}")
    else:
        console.print(f"Input Source: Manual entry ({len(batch_inputs)} items)")

    if static_inputs:
        console.print(f"Static Inputs: {json.dumps(static_inputs, indent=2)}")

    # temporal_status = (
    #     "Default" if use_temporal is None else ("Yes" if use_temporal else "No")
    # )
    # console.print(f"Use Temporal: {temporal_status}")

    if not (flock.enable_temporal if use_temporal is None else use_temporal):
        console.print(f"Parallel Execution: {parallel}")
        if parallel:
            console.print(f"Max Workers: {max_workers}")

    console.print(f"Silent Mode: {silent_mode}")

    if write_to_csv:
        console.print(f"Write Results to: {write_to_csv}")

    # Confirm execution
    confirm = questionary.confirm(
        "Execute batch with this configuration?",
        default=True,
    ).ask()

    if not confirm:
        return

    # Execute the batch
    console.print("\n[bold]Executing Batch...[/]")

    try:
        # Handle logging settings
        if enable_logging:
            flock._configure_logging(True)

        # Run the batch
        results = flock.run_batch(
            start_agent=start_agent_name,
            batch_inputs=batch_inputs,
            input_mapping=input_mapping or None,
            static_inputs=static_inputs or None,
            parallel=parallel,
            max_workers=max_workers,
            use_temporal=use_temporal,
            box_results=True,
            return_errors=True,
            silent_mode=silent_mode,
            write_to_csv=write_to_csv,
            hide_columns=hide_columns,
            delimiter=delimiter,
        )

        # Display results summary
        console.print("\n[bold green]Batch Execution Complete![/]")

        success_count = sum(1 for r in results if not isinstance(r, Exception))
        error_count = sum(1 for r in results if isinstance(r, Exception))

        console.print(f"Total Items: {len(results)}")
        console.print(f"Successful: {success_count}")

        if error_count > 0:
            console.print(f"[bold red]Errors: {error_count}[/]")

        # Ask if user wants to see detailed results
        if questionary.confirm(
            "View detailed results?",
            default=False,
        ).ask():
            for i, result in enumerate(results):
                console.print(f"\n[bold]Item {i + 1}:[/]")
                if isinstance(result, Exception):
                    console.print(f"[bold red]Error: {result}[/]")
                else:
                    # Display as formatted JSON
                    try:
                        console.print(json.dumps(result, indent=2))
                    except:
                        console.print(str(result))

        if write_to_csv:
            console.print(f"\n[green]Results written to: {write_to_csv}[/]")

    except Exception as e:
        console.print(f"\n[bold red]Error during batch execution:[/] {e!s}")

```

---

### 38. src\flock\cli\load_agent.py

- **File ID**: file_37
- **Type**: Code File
- **Line Count**: 1
- **Description**: File at src\flock\cli\load_agent.py
- **Dependencies**: None
- **Used By**: None

**Content**:
```
# TODO

```

---

### 39. src\flock\cli\load_examples.py

- **File ID**: file_38
- **Type**: Code File
- **Line Count**: 1
- **Description**: File at src\flock\cli\load_examples.py
- **Dependencies**: None
- **Used By**: None

**Content**:
```
# TODO

```

---

### 40. src\flock\cli\load_flock.py

- **File ID**: file_39
- **Type**: Code File
- **Line Count**: 192
- **Description**: Load a Flock from a file.
- **Dependencies**: None
- **Used By**: None

**Content**:
```
"""Load a Flock from a file."""

from pathlib import Path

import questionary
from rich.console import Console
from rich.markdown import Markdown

from flock.cli.loaded_flock_cli import start_loaded_flock_cli
from flock.core.flock import Flock
from flock.core.logging.logging import get_logger

logger = get_logger("cli.load_flock")


def filter(file_path) -> bool:
    """Filter function for file selection."""
    path = Path(file_path)
    if path.is_dir():
        return True
    return path.is_file() and (
        path.suffix == ".flock"
        or path.suffix == ".yaml"
        or path.suffix == ".yml"
    )


def load_flock():
    """Load a Flock from a file."""
    console = Console()

    console.print(
        "\nPlease select a *.flock, *.yaml, or *.yml file\n", style="bold green"
    )

    result = questionary.path("", file_filter=filter).ask()

    if not result:
        return

    selected_file = Path(result)
    if selected_file.is_file():
        console.print(f"Selected file: {selected_file}", style="bold green")

        try:
            # Try loading with detailed error handling
            try:
                logger.info(f"Attempting to load Flock from: {result}")
                flock = Flock.load_from_file(result)
            except ImportError as e:
                # Handle missing module path errors
                if "No module named" in str(e):
                    console.print(
                        f"[yellow]Warning: Module import failed: {e}[/]"
                    )
                    console.print(
                        "[yellow]Trying file path fallback mechanism...[/]"
                    )
                    # Re-try loading with fallback
                    flock = Flock.load_from_file(result)
                else:
                    raise  # Re-raise if it's not a missing module error
            except KeyError as e:
                # This could be caused by missing tool references
                if "__callable_ref__" in str(e):
                    console.print(
                        f"[yellow]Warning: Tool reference error: {e}[/]"
                    )
                    console.print(
                        "[yellow]This may be due to missing tool registrations. Attempting to scan for tools...[/]"
                    )
                    # Scan for tools and retry
                    from flock.cli.registry_management import (
                        auto_registration_scanner,
                    )

                    auto_registration_scanner()
                    # Try loading again
                    flock = Flock.load_from_file(result)
                else:
                    raise  # Re-raise if it's not a tool reference error

            console.line()
            console.print(
                Markdown("# Flock loaded successfully"), style="bold green"
            )
            console.line()

            # Instead of just running the Flock, start our enhanced CLI
            start_loaded_flock_cli(
                flock, server_name=f"Flock - {selected_file.name}"
            )

        except Exception as e:
            console.print(f"Error loading Flock: {e!s}", style="bold red")
            logger.error(f"Failed to load Flock: {e}", exc_info=True)

            # Add more detailed error information for specific errors
            if "No module named" in str(e):
                console.print(
                    "\n[yellow]This error might be due to missing module paths.[/]"
                )
                console.print(
                    "[yellow]Component references may need to be updated with file paths.[/]"
                )

                # Show the option to scan the directory for components
                fix_paths = questionary.confirm(
                    "Would you like to scan directories for components to fix missing imports?",
                    default=True,
                ).ask()

                if fix_paths:
                    from flock.cli.registry_management import (
                        auto_registration_scanner,
                    )

                    auto_registration_scanner()

                    # Try loading again
                    console.print(
                        "\n[yellow]Attempting to load Flock again...[/]"
                    )
                    try:
                        flock = Flock.load_from_file(result)
                        console.line()
                        console.print(
                            Markdown(
                                "# Flock loaded successfully after component scan"
                            ),
                            style="bold green",
                        )
                        console.line()

                        start_loaded_flock_cli(
                            flock, server_name=f"Flock - {selected_file.name}"
                        )
                        return
                    except Exception as e2:
                        console.print(
                            f"Error loading Flock after scan: {e2!s}",
                            style="bold red",
                        )

            # Handle tool reference issues
            elif "__callable_ref__" in str(e) or "callable" in str(e).lower():
                console.print(
                    "\n[yellow]This error might be due to missing tool registrations.[/]"
                )

                # Show the option to scan the directory for tools
                fix_tools = questionary.confirm(
                    "Would you like to scan directories for tools to fix missing references?",
                    default=True,
                ).ask()

                if fix_tools:
                    from flock.cli.registry_management import (
                        auto_registration_scanner,
                    )

                    console.print(
                        "\n[yellow]Scanning for tools and callables...[/]"
                    )
                    auto_registration_scanner()

                    # Try loading again
                    console.print(
                        "\n[yellow]Attempting to load Flock again...[/]"
                    )
                    try:
                        flock = Flock.load_from_file(result)
                        console.line()
                        console.print(
                            Markdown(
                                "# Flock loaded successfully after tool scan"
                            ),
                            style="bold green",
                        )
                        console.line()

                        start_loaded_flock_cli(
                            flock, server_name=f"Flock - {selected_file.name}"
                        )
                        return
                    except Exception as e2:
                        console.print(
                            f"Error loading Flock after tool scan: {e2!s}",
                            style="bold red",
                        )

            input("\nPress Enter to continue...")

```

---

### 41. src\flock\cli\load_release_notes.py

- **File ID**: file_40
- **Type**: Code File
- **Line Count**: 23
- **Description**: File at src\flock\cli\load_release_notes.py
- **Dependencies**: None
- **Used By**: None

**Content**:
```
from pathlib import Path

from flock.core.util.cli_helper import display_hummingbird


def load_release_notes():
    """Load release notes."""
    from rich.console import Console
    from rich.markdown import Markdown

    from flock.core.util.cli_helper import init_console

    console = Console()
    file_path = Path(__file__).parent / "assets" / "release_notes.md"

    init_console()
    console.print(Markdown("# *'Hummingbird'* Release Notes"))
    display_hummingbird()
    with open(file_path) as file:
        release_notes = file.read()

    
    console.print(Markdown(release_notes))

```

---

### 42. src\flock\cli\loaded_flock_cli.py

- **File ID**: file_41
- **Type**: Code File
- **Line Count**: 234
- **Description**: CLI interface for working with a loaded Flock instance....
- **Dependencies**: None
- **Used By**: None

**Content**:
```
"""CLI interface for working with a loaded Flock instance.

This module provides a CLI interface for a Flock instance that has already been loaded,
allowing users to execute, edit, or manage agents from the existing configuration.
"""

import questionary
from rich.console import Console
from rich.panel import Panel

from flock.cli.constants import (
    CLI_REGISTRY_MANAGEMENT,
    CLI_SETTINGS,
)
from flock.core.api import runner
from flock.core.flock import Flock
from flock.core.util.cli_helper import init_console

# Import future modules we'll create
# These will be implemented later
try:
    from flock.cli.yaml_editor import yaml_editor

    yaml_editor_available = True
except ImportError:
    yaml_editor_available = False

try:
    from flock.cli.manage_agents import manage_agents

    manage_agents_available = True
except ImportError:
    manage_agents_available = False

try:
    from flock.cli.execute_flock import execute_flock, execute_flock_batch

    execute_flock_available = True
except ImportError:
    execute_flock_available = False

try:
    from flock.cli.view_results import view_results

    view_results_available = True
except ImportError:
    view_results_available = False

# Create console instance
console = Console()


def start_loaded_flock_cli(
    flock: Flock,
    server_name: str = "Flock CLI",
    show_results: bool = False,
    edit_mode: bool = False,
) -> None:
    """Start a CLI interface with a loaded Flock instance.

    Args:
        flock: The loaded Flock instance
        server_name: Optional name for the CLI interface
        show_results: Whether to initially show results of previous runs
        edit_mode: Whether to open directly in edit mode
    """
    if not flock:
        console.print("[bold red]Error: No Flock instance provided.[/]")
        return

    agent_names = list(flock._agents.keys())

    # Directly go to specific modes if requested
    if edit_mode and yaml_editor_available:
        yaml_editor(flock)
        return

    if show_results and view_results_available:
        view_results(flock)
        return

    # Main CLI loop
    while True:
        # Initialize console for each loop iteration
        init_console()

        # Display header with Flock information
        console.print(Panel(f"[bold green]{server_name}[/]"), justify="center")
        console.print(
            f"Flock loaded with [bold cyan]{len(agent_names)}[/] agents: {', '.join(agent_names)}"
        )
        console.line()

        # Main menu choices
        choices = [
            questionary.Separator(line=" "),
            "Execute Flock",
            "Execute Flock - Batch Mode",
            "Start Web Server",
            "Start Web Server with UI",
            "Manage Agents",
            "View Results of Past Runs",
        ]

        # Add YAML Editor option if available
        if yaml_editor_available:
            choices.append("Edit YAML Configurations")

        # Add remaining options
        choices.extend([questionary.Separator(), CLI_REGISTRY_MANAGEMENT])
        choices.extend(
            [
                questionary.Separator(),
                CLI_SETTINGS,
                questionary.Separator(),
                "Exit",
            ]
        )

        # Display menu and get choice
        choice = questionary.select(
            "What would you like to do?",
            choices=choices,
        ).ask()

        # Handle menu selection
        if choice == "Execute Flock":
            if execute_flock_available:
                execute_flock(flock)
            else:
                console.print(
                    "[yellow]Execute Flock functionality not yet implemented.[/]"
                )
                input("\nPress Enter to continue...")

        elif choice == "Execute Flock - Batch Mode":
            if execute_flock_available:
                execute_flock_batch(flock)
            else:
                console.print(
                    "[yellow]Batch execution functionality not yet implemented.[/]"
                )
                input("\nPress Enter to continue...")

        elif choice == "Start Web Server":
            _start_web_server(flock, create_ui=False)

        elif choice == "Start Web Server with UI":
            _start_web_server(flock, create_ui=True)

        elif choice == "Manage Agents":
            if manage_agents_available:
                manage_agents(flock)
            else:
                console.print(
                    "[yellow]Manage Agents functionality not yet implemented.[/]"
                )
                input("\nPress Enter to continue...")

        elif choice == "View Results of Past Runs":
            if view_results_available:
                view_results(flock)
            else:
                console.print(
                    "[yellow]View Results functionality not yet implemented.[/]"
                )
                input("\nPress Enter to continue...")

        elif choice == CLI_REGISTRY_MANAGEMENT:
            from flock.cli.registry_management import manage_registry

            manage_registry()

        elif choice == "Edit YAML Configurations" and yaml_editor_available:
            yaml_editor(flock)

        elif choice == CLI_SETTINGS:
            from flock.cli.settings import settings_editor

            settings_editor()

        elif choice == "Exit":
            break

        # Pause after each action unless we're exiting
        if choice != "Exit" and not choice.startswith("Start Web Server"):
            input("\nPress Enter to continue...")


def _start_web_server(flock: Flock, create_ui: bool = False) -> None:
    """Start a web server with the loaded Flock instance.

    Args:
        flock: The loaded Flock instance
        create_ui: Whether to create a UI for the web server
    """
    host = "127.0.0.1"
    port = 8344
    server_name = "Flock API"

    # Get configuration from user
    console.print("\n[bold]Web Server Configuration[/]")

    host_input = questionary.text(
        "Host (default: 127.0.0.1):", default=host
    ).ask()
    if host_input:
        host = host_input

    port_input = questionary.text(
        "Port (default: 8344):", default=str(port)
    ).ask()
    if port_input and port_input.isdigit():
        port = int(port_input)

    server_name_input = questionary.text(
        "Server name (default: FlockName API):", default=flock.name + " API"
    ).ask()
    if server_name_input:
        server_name = server_name_input

    # Start the web server
    console.print(
        f"\nStarting web server on {host}:{port} {'with UI' if create_ui else 'without UI'}..."
    )

    # Use the Flock's start_api method
    runner.start_flock_api(
        flock=flock,
        host=host,
        port=port,
        server_name=server_name,
        create_ui=create_ui,
    )

```

---

### 43. src\flock\cli\manage_agents.py

- **File ID**: file_42
- **Type**: Code File
- **Line Count**: 444
- **Description**: Agent management functionality for the Flock CLI....
- **Dependencies**: None
- **Used By**: None

**Content**:
```
"""Agent management functionality for the Flock CLI.

This module provides a CLI interface for managing agents within a Flock system,
including listing, adding, editing, and removing agents.
"""

import questionary
from rich.console import Console
from rich.panel import Panel
from rich.table import Table

from flock.core.flock import Flock
from flock.core.flock_agent import FlockAgent
from flock.core.flock_factory import FlockFactory
from flock.core.util.cli_helper import init_console

# Create console instance
console = Console()


def manage_agents(flock: Flock):
    """Agent management entry point.

    Args:
        flock: The Flock instance containing agents to manage
    """
    if not flock:
        console.print("[bold red]Error: No Flock instance provided.[/]")
        return

    while True:
        init_console()
        console.print(Panel("[bold green]Agent Manager[/]"), justify="center")

        agent_names = list(flock._agents.keys())
        console.print(f"Flock contains [bold cyan]{len(agent_names)}[/] agents")

        if agent_names:
            console.print(f"Agents: {', '.join(agent_names)}")
        else:
            console.print("[yellow]No agents in this Flock yet.[/]")

        console.line()

        # Main menu
        choice = questionary.select(
            "What would you like to do?",
            choices=[
                questionary.Separator(line=" "),
                "List All Agents",
                "Add New Agent",
                "Edit Agent",
                "Remove Agent",
                "Export Agent to YAML",
                "Import Agent from YAML",
                questionary.Separator(),
                "Back to Main Menu",
            ],
        ).ask()

        if choice == "List All Agents":
            _list_agents(flock)
        elif choice == "Add New Agent":
            _add_agent(flock)
        elif choice == "Edit Agent":
            _edit_agent(flock)
        elif choice == "Remove Agent":
            _remove_agent(flock)
        elif choice == "Export Agent to YAML":
            _export_agent(flock)
        elif choice == "Import Agent from YAML":
            _import_agent(flock)
        elif choice == "Back to Main Menu":
            break

        if choice != "Back to Main Menu":
            input("\nPress Enter to continue...")


def _list_agents(flock: Flock):
    """List all agents in the Flock.

    Args:
        flock: The Flock instance
    """
    agent_names = list(flock._agents.keys())

    if not agent_names:
        console.print("[yellow]No agents in this Flock.[/]")
        return

    # Create table for agents
    table = Table(title="Agents in Flock")
    table.add_column("Name", style="cyan")
    table.add_column("Model", style="green")
    table.add_column("Description", style="yellow")
    table.add_column("Input", style="magenta")
    table.add_column("Output", style="magenta")

    for name in agent_names:
        agent = flock._agents[name]

        # Format model nicely
        model = agent.model or flock.model or "Default"

        # Format description (truncate if too long)
        description = str(agent.description)
        if len(description) > 30:
            description = description[:27] + "..."

        # Format input/output (truncate if too long)
        input_str = str(agent.input)
        if len(input_str) > 30:
            input_str = input_str[:27] + "..."

        output_str = str(agent.output)
        if len(output_str) > 30:
            output_str = output_str[:27] + "..."

        table.add_row(
            name,
            model,
            description,
            input_str,
            output_str,
        )

    console.print(table)

    # Option to view detailed info for a specific agent
    if len(agent_names) > 0:
        view_details = questionary.confirm(
            "View detailed information for an agent?",
            default=False,
        ).ask()

        if view_details:
            agent_to_view = questionary.select(
                "Select an agent to view:",
                choices=agent_names,
            ).ask()

            if agent_to_view:
                _view_agent_details(flock._agents[agent_to_view])


def _view_agent_details(agent: FlockAgent):
    """Display detailed information about an agent.

    Args:
        agent: The agent to display details for
    """
    init_console()
    console.print(
        Panel(f"[bold green]Agent Details: {agent.name}[/]"), justify="center"
    )

    # Create a panel for each section
    basic_info = Table(show_header=False, box=None)
    basic_info.add_column("Property", style="cyan")
    basic_info.add_column("Value", style="green")

    basic_info.add_row("Name", agent.name)
    basic_info.add_row("Model", str(agent.model or "Default"))
    basic_info.add_row("Description", str(agent.description))
    basic_info.add_row("Input", str(agent.input))
    basic_info.add_row("Output", str(agent.output))
    basic_info.add_row("Write to File", str(agent.write_to_file))
    basic_info.add_row("Wait for input", str(agent.wait_for_input))

    console.print(Panel(basic_info, title="Basic Information"))

    # Evaluator info
    evaluator_info = (
        f"Type: {type(agent.evaluator).__name__ if agent.evaluator else 'None'}"
    )
    console.print(Panel(evaluator_info, title="Evaluator"))

    # Router info
    router_info = f"Type: {type(agent.handoff_router).__name__ if agent.handoff_router else 'None'}"
    console.print(Panel(router_info, title="Router"))

    # Tools
    if agent.tools:
        tool_names = [t.__name__ for t in agent.tools]
        tools_info = ", ".join(tool_names)
    else:
        tools_info = "None"

    console.print(Panel(tools_info, title="Tools"))

    # Modules
    if agent.modules:
        module_table = Table(show_header=True)
        module_table.add_column("Name", style="cyan")
        module_table.add_column("Type", style="green")
        module_table.add_column("Enabled", style="yellow")

        for name, module in agent.modules.items():
            module_table.add_row(
                name,
                type(module).__name__,
                "Yes" if module.config.enabled else "No",
            )

        console.print(Panel(module_table, title="Modules"))
    else:
        console.print(Panel("None", title="Modules"))


def _add_agent(flock: Flock):
    """Add a new agent to the Flock.

    Args:
        flock: The Flock instance to add the agent to
    """
    console.print("\n[bold]Add New Agent[/]")
    console.line()

    # Get agent name
    name = questionary.text(
        "Enter a name for the agent:",
        default="my_agent",
    ).ask()

    # Check for name conflicts
    if name in flock._agents:
        console.print(
            f"[bold red]Error: An agent named '{name}' already exists.[/]"
        )
        return

    # Get agent description
    description = questionary.text(
        "Enter a description for the agent (optional):",
        default="",
    ).ask()

    # Get input specification
    input_spec = questionary.text(
        "Enter input specification (e.g., 'query: str | The search query'):",
        default="query",
    ).ask()

    # Get output specification
    output_spec = questionary.text(
        "Enter output specification (e.g., 'result: str | The generated result'):",
        default="result",
    ).ask()

    # Model selection
    use_flock_model = questionary.confirm(
        f"Use Flock's default model ({flock.model or 'None'})? Select 'n' to specify a different model.",
        default=True,
    ).ask()

    if use_flock_model:
        model = None  # Use Flock's default
    else:
        default_models = [
            "openai/gpt-4o",
            "openai/gpt-3.5-turbo",
            "anthropic/claude-3-opus-20240229",
            "anthropic/claude-3-sonnet-20240229",
            "gemini/gemini-1.5-pro",
            "Other (specify)",
        ]

        model_choice = questionary.select(
            "Select a model:",
            choices=default_models,
        ).ask()

        if model_choice == "Other (specify)":
            model = questionary.text(
                "Enter the model identifier:",
                default="openai/gpt-4o",
            ).ask()
        else:
            model = model_choice

    # Additional options
    use_cache = questionary.confirm(
        "Enable caching for this agent?",
        default=True,
    ).ask()

    enable_rich_tables = questionary.confirm(
        "Enable rich table output for this agent?",
        default=True,
    ).ask()

    # Create the agent
    agent = FlockFactory.create_default_agent(
        name=name,
        description=description,
        model=model,
        input=input_spec,
        output=output_spec,
        use_cache=use_cache,
        enable_rich_tables=enable_rich_tables,
    )

    # Add the agent to the Flock
    flock.add_agent(agent)
    console.print(f"\n[green]✓[/] Agent '{name}' created and added to Flock!")


def _edit_agent(flock: Flock):
    """Edit an existing agent in the Flock.

    Args:
        flock: The Flock instance containing the agent to edit
    """
    agent_names = list(flock._agents.keys())

    if not agent_names:
        console.print("[yellow]No agents in this Flock to edit.[/]")
        return

    # Select agent to edit
    agent_name = questionary.select(
        "Select an agent to edit:",
        choices=agent_names,
    ).ask()

    if not agent_name:
        return

    agent = flock._agents[agent_name]

    # Choose edit method
    edit_choice = questionary.select(
        "How would you like to edit this agent?",
        choices=[
            "Use Abstract Editor (Field by Field)",
            "Edit YAML Directly",
            "Cancel",
        ],
    ).ask()

    if edit_choice == "Use Abstract Editor (Field by Field)":
        # Not fully implemented yet
        console.print(
            "[yellow]Abstract editor not fully implemented. Opening YAML editor instead.[/]"
        )
        from flock.cli.yaml_editor import yaml_editor

        updated_agent = yaml_editor(agent)
        if updated_agent and isinstance(updated_agent, FlockAgent):
            flock._agents[agent_name] = updated_agent

    elif edit_choice == "Edit YAML Directly":
        from flock.cli.yaml_editor import _edit_yaml_directly

        updated_agent = _edit_yaml_directly(agent)
        if updated_agent and isinstance(updated_agent, FlockAgent):
            flock._agents[agent_name] = updated_agent
            console.print(f"\n[green]✓[/] Agent '{agent_name}' updated!")


def _remove_agent(flock: Flock):
    """Remove an agent from the Flock.

    Args:
        flock: The Flock instance containing the agent to remove
    """
    agent_names = list(flock._agents.keys())

    if not agent_names:
        console.print("[yellow]No agents in this Flock to remove.[/]")
        return

    # Select agent to remove
    agent_name = questionary.select(
        "Select an agent to remove:",
        choices=agent_names,
    ).ask()

    if not agent_name:
        return

    # Confirm deletion
    confirm = questionary.confirm(
        f"Are you sure you want to remove agent '{agent_name}'?",
        default=False,
    ).ask()

    if confirm:
        del flock._agents[agent_name]
        console.print(f"\n[green]✓[/] Agent '{agent_name}' removed from Flock!")


def _export_agent(flock: Flock):
    """Export an agent to a YAML file.

    Args:
        flock: The Flock instance containing the agent to export
    """
    agent_names = list(flock._agents.keys())

    if not agent_names:
        console.print("[yellow]No agents in this Flock to export.[/]")
        return

    # Select agent to export
    agent_name = questionary.select(
        "Select an agent to export:",
        choices=agent_names,
    ).ask()

    if not agent_name:
        return

    agent = flock._agents[agent_name]

    # Get file path
    file_path = questionary.text(
        "Enter file path to save agent:",
        default=f"{agent_name}.agent.yaml",
    ).ask()

    # Ensure the file has the correct extension
    if not file_path.endswith((".yaml", ".yml")):
        file_path += ".yaml"

    try:
        # Save the agent to YAML
        agent.to_yaml_file(file_path)
        console.print(
            f"\n[green]✓[/] Agent '{agent_name}' exported to {file_path}"
        )
    except Exception as e:
        console.print(f"\n[bold red]Error exporting agent:[/] {e!s}")


def _import_agent(flock: Flock):
    """Import an agent from a YAML file.

    Args:
        flock: The Flock instance to import the agent into
    """
    console.print("[yellow]Import functionality not yet implemented.[/]")
    # TODO: Implement agent import from YAML file

```

---

### 44. src\flock\cli\registry_management.py

- **File ID**: file_43
- **Type**: Code File
- **Line Count**: 889
- **Description**: Registry Management Module for the Flock CLI.
- **Dependencies**: None
- **Used By**: None

**Content**:
```
"""Registry Management Module for the Flock CLI."""

import datetime
import importlib
import inspect
import os
from dataclasses import is_dataclass
from pathlib import Path
from typing import Any

import questionary
from rich.console import Console
from rich.panel import Panel
from rich.progress import BarColumn, Progress, SpinnerColumn, TextColumn
from rich.table import Table

from flock.core.flock_registry import (
    get_registry,
)
from flock.core.logging.logging import get_logger

logger = get_logger("registry_cli")
console = Console()

# Constants for registry item types
REGISTRY_CATEGORIES = ["Agent", "Callable", "Type", "Component"]
REGISTRY_ACTIONS = [
    "View Registry Contents",
    "Add Item to Registry",
    "Remove Item from Registry",
    "Auto-Registration Scanner",
    "Export Registry",
    "Back to Main Menu",
]


def manage_registry() -> None:
    """Main function for managing the Flock Registry from the CLI."""
    while True:
        console.clear()
        console.print(
            Panel("[bold blue]Flock Registry Management[/]"), justify="center"
        )
        console.line()

        # Show registry stats
        display_registry_stats()

        action = questionary.select(
            "What would you like to do?",
            choices=REGISTRY_ACTIONS,
        ).ask()

        if action == "View Registry Contents":
            view_registry_contents()
        elif action == "Add Item to Registry":
            add_item_to_registry()
        elif action == "Remove Item from Registry":
            remove_item_from_registry()
        elif action == "Auto-Registration Scanner":
            auto_registration_scanner()
        elif action == "Export Registry":
            export_registry()
        elif action == "Back to Main Menu":
            break

        input("\nPress Enter to continue...")


def display_registry_stats() -> None:
    """Display statistics about the current registry contents."""
    registry = get_registry()

    table = Table(title="Registry Statistics")
    table.add_column("Category", style="cyan")
    table.add_column("Count", style="green")

    table.add_row("Agents", str(len(registry._agents)))
    table.add_row("Callables", str(len(registry._callables)))
    table.add_row("Types", str(len(registry._types)))
    table.add_row("Components", str(len(registry._components)))

    console.print(table)


def view_registry_contents(
    category: str | None = None, search_pattern: str | None = None
) -> None:
    """Display registry contents with filtering options."""
    registry = get_registry()

    if category is None:
        category = questionary.select(
            "Select a category to view:",
            choices=REGISTRY_CATEGORIES + ["All Categories"],
        ).ask()

    if search_pattern is None:
        search_pattern = questionary.text(
            "Enter search pattern (leave empty to show all):"
        ).ask()

    console.clear()

    if category == "All Categories" or category == "Agent":
        display_registry_section("Agents", registry._agents, search_pattern)

    if category == "All Categories" or category == "Callable":
        display_registry_section(
            "Callables", registry._callables, search_pattern
        )

    if category == "All Categories" or category == "Type":
        display_registry_section("Types", registry._types, search_pattern)

    if category == "All Categories" or category == "Component":
        display_registry_section(
            "Components", registry._components, search_pattern
        )


def display_registry_section(
    title: str, items: dict[str, Any], search_pattern: str
) -> None:
    """Display a section of registry items in a table."""
    filtered_items = {
        k: v
        for k, v in items.items()
        if not search_pattern or search_pattern.lower() in k.lower()
    }

    if not filtered_items:
        console.print(
            f"[yellow]No {title.lower()} found matching the search pattern.[/]"
        )
        return

    table = Table(title=f"Registered {title}")
    table.add_column("Name/Path", style="cyan")
    table.add_column("Type", style="green")

    # Add file path column for components
    if title == "Components":
        table.add_column("File Path", style="yellow")

    for name, item in filtered_items.items():
        item_type = type(item).__name__

        if title == "Components":
            # Try to get the file path for component classes
            file_path = (
                inspect.getfile(item) if inspect.isclass(item) else "N/A"
            )
            table.add_row(name, item_type, file_path)
        else:
            table.add_row(name, item_type)

    console.print(table)
    console.print(f"Total: {len(filtered_items)} {title.lower()}")


def add_item_to_registry() -> None:
    """Add an item to the registry manually."""
    registry = get_registry()

    item_type = questionary.select(
        "What type of item do you want to add?",
        choices=["agent", "callable", "type", "component"],
    ).ask()

    # For component types, offer file path option
    use_file_path = False
    if item_type == "component":
        path_type = questionary.select(
            "How do you want to specify the component?",
            choices=["Module Path", "File Path"],
        ).ask()
        use_file_path = path_type == "File Path"

    if use_file_path:
        file_path = questionary.path(
            "Enter the file path to the component:", only_directories=False
        ).ask()

        if not file_path or not os.path.exists(file_path):
            console.print(f"[red]Error: File {file_path} does not exist[/]")
            return False

        module_name = questionary.text(
            "Enter the component class name in the file:"
        ).ask()

        try:
            # Use dynamic import to load the module from file path
            import importlib.util

            spec = importlib.util.spec_from_file_location(
                "temp_module", file_path
            )
            module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(module)

            if not hasattr(module, module_name):
                console.print(
                    f"[red]Error: {module_name} not found in {file_path}[/]"
                )
                return False

            item = getattr(module, module_name)
        except Exception as e:
            console.print(f"[red]Error importing from file: {e!s}[/]")
            return False
    else:
        module_path = questionary.text(
            "Enter the module path (e.g., 'your_module.submodule'):"
        ).ask()

        item_name = questionary.text(
            "Enter the item name within the module:"
        ).ask()

        try:
            # Attempt to import the module
            module = importlib.import_module(module_path)

            # Get the item from the module
            if not hasattr(module, item_name):
                console.print(
                    f"[red]Error: {item_name} not found in {module_path}[/]"
                )
                return False

            item = getattr(module, item_name)
        except Exception as e:
            console.print(f"[red]Error importing module: {e!s}[/]")
            return False

    alias = questionary.text(
        "Enter an alias (optional, press Enter to skip):"
    ).ask()

    if not alias:
        alias = None

    # Register the item based on its type
    try:
        if item_type == "agent":
            registry.register_agent(item)
            console.print(
                f"[green]Successfully registered agent: {item_name}[/]"
            )
        elif item_type == "callable":
            result = registry.register_callable(item, alias)
            console.print(
                f"[green]Successfully registered callable: {result}[/]"
            )
        elif item_type == "type":
            result = registry.register_type(item, alias)
            console.print(f"[green]Successfully registered type: {result}[/]")
        elif item_type == "component":
            result = registry.register_component(item, alias)
            # Store the file path information if we loaded from a file
            if use_file_path and hasattr(registry, "_component_file_paths"):
                # Check if the registry has component file paths attribute
                # This will be added to registry in our update
                registry._component_file_paths[result] = file_path
            console.print(
                f"[green]Successfully registered component: {result}[/]"
            )
    except Exception as e:
        console.print(f"[red]Error registering item: {e!s}[/]")
        return False

    return True


def remove_item_from_registry() -> None:
    """Remove an item from the registry."""
    registry = get_registry()

    item_type = questionary.select(
        "What type of item do you want to remove?",
        choices=["agent", "callable", "type", "component"],
    ).ask()

    # Get the appropriate dictionary based on item type
    if item_type == "agent":
        items = registry._agents
    elif item_type == "callable":
        items = registry._callables
    elif item_type == "type":
        items = registry._types
    elif item_type == "component":
        items = registry._components

    if not items:
        console.print(f"[yellow]No {item_type}s registered.[/]")
        return False

    # Create a list of items for selection
    item_names = list(items.keys())
    item_name = questionary.select(
        f"Select the {item_type} to remove:",
        choices=item_names + ["Cancel"],
    ).ask()

    if item_name == "Cancel":
        return False

    # Ask for confirmation
    confirm = questionary.confirm(
        f"Are you sure you want to remove {item_name}?",
        default=False,
    ).ask()

    if not confirm:
        console.print("[yellow]Operation cancelled.[/]")
        return False

    # Remove the item
    try:
        if item_type == "agent":
            del registry._agents[item_name]
        elif item_type == "callable":
            del registry._callables[item_name]
        elif item_type == "type":
            del registry._types[item_name]
        elif item_type == "component":
            del registry._components[item_name]

        console.print(
            f"[green]Successfully removed {item_type}: {item_name}[/]"
        )
        return True

    except Exception as e:
        console.print(f"[red]Error: {e!s}[/]")
        return False


def auto_registration_scanner() -> None:
    """Launch the auto-registration scanner interface."""
    console.clear()
    console.print(
        Panel("[bold blue]Auto-Registration Scanner[/]"), justify="center"
    )
    console.line()

    console.print(
        "This utility will scan Python files for components, types, callables (tools), and agents that can be registered."
    )
    console.print(
        "[yellow]Note: Registration is required for proper serialization and deserialization of your Flock.[/]"
    )
    console.line()

    # Target directory selection
    def path_filter(path):
        """Filter paths for selection."""
        if os.path.isdir(path):
            return True
        return path.endswith(".py")

    target_path = questionary.path(
        "Select directory to scan:", file_filter=path_filter
    ).ask()

    if not target_path or not os.path.exists(target_path):
        console.print("[red]Invalid path selected. Aborting.[/]")
        return

    is_recursive = questionary.confirm(
        "Scan recursively (include subdirectories)?", default=True
    ).ask()

    auto_register = questionary.confirm(
        "Automatically register items found during scan?", default=True
    ).ask()

    # Special callout for tools/callables
    console.print(
        "[bold blue]Tool Registration:[/] This scanner will look for functions that can be used as tools."
    )
    console.print(
        "These will be registered as callables and can be properly serialized in your Flock YAML."
    )
    console.line()

    with Progress(
        SpinnerColumn(),
        TextColumn("[bold blue]{task.description}"),
        BarColumn(),
        TextColumn("[bold green]{task.completed}/{task.total}"),
        console=console,
    ) as progress:
        task_id = progress.add_task(
            "Scanning for registry items...", total=None
        )

        # Perform the scan
        results = scan_for_registry_items(
            target_path, recursive=is_recursive, auto_register=auto_register
        )

        # Mark task as complete
        progress.update(task_id, completed=1, total=1)
        console.line()

    # Display results
    console.print("[bold green]Scan Complete![/]")
    console.line()

    total_found = sum(len(items) for items in results.values())
    total_categories = sum(1 for items in results.values() if items)

    console.print(
        f"Found {total_found} items across {total_categories} categories."
    )

    # Enhanced report section
    table = Table(title="Scan Results")
    table.add_column("Category", style="cyan")
    table.add_column("Count", style="green")
    table.add_column("Example Items", style="blue")

    for category, items in results.items():
        if items:
            examples = ", ".join(items[:3])
            if len(items) > 3:
                examples += ", ..."
            table.add_row(category, str(len(items)), examples)
        else:
            table.add_row(category, "0", "")

    console.print(table)
    console.line()

    # Callout for tools and future serialization
    if results.get("callables"):
        console.print(
            "[bold green]Note:[/] Found callable functions that can be used as tools."
        )
        console.print(
            "These functions will now be properly serialized as callable references in your Flock YAML."
        )
        console.print(
            "When sharing Flocks, ensure these callables are registered on the target system."
        )
        console.line()

    # Show details options
    if total_found > 0:
        view_details = questionary.confirm(
            "Would you like to view detailed results?", default=True
        ).ask()

        if view_details:
            view_registry_contents()  # Show the registry contents after scan


def scan_for_registry_items(
    target_path: str, recursive: bool = True, auto_register: bool = False
) -> dict[str, list[str]]:
    """Scan directory for potential registry items and optionally register them."""
    results = {
        "Agents": [],
        "Callables": [],
        "Types": [],
        "Components": [],
        "Potential Items": [],
    }

    registry = get_registry()
    path = Path(target_path)

    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        BarColumn(),
        TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
    ) as progress:
        scan_task = progress.add_task(f"Scanning {target_path}...", total=100)

        # If path is a file, scan it directly
        if path.is_file() and path.suffix == ".py":
            module_path = get_module_path_from_file(path)
            if module_path:
                scan_python_file(path, module_path, results, auto_register)
            progress.update(scan_task, completed=100)

        # If path is a directory, scan all Python files
        elif path.is_dir():
            python_files = []
            if recursive:
                for root, _, files in os.walk(path):
                    python_files.extend(
                        [
                            Path(os.path.join(root, f))
                            for f in files
                            if f.endswith(".py")
                        ]
                    )
            else:
                python_files = [p for p in path.glob("*.py")]

            total_files = len(python_files)
            for i, file_path in enumerate(python_files):
                module_path = get_module_path_from_file(file_path)
                if module_path:
                    scan_python_file(
                        file_path, module_path, results, auto_register
                    )
                progress.update(
                    scan_task, completed=(i + 1) / total_files * 100
                )

    return results


def get_module_path_from_file(file_path: Path) -> str | None:
    """Convert a file path to a module path for import."""
    try:
        # Get absolute path
        abs_path = file_path.resolve()

        # Check if it's a Python file
        if abs_path.suffix != ".py":
            return None

        # Get the directory containing the file
        file_dir = abs_path.parent

        # Find the nearest parent directory with __init__.py
        # to determine the package root
        package_root = None
        current_dir = file_dir
        while current_dir != current_dir.parent:
            if (current_dir / "__init__.py").exists():
                if package_root is None:
                    package_root = current_dir
            else:
                # We've reached a directory without __init__.py
                # If we found a package root earlier, use that
                if package_root is not None:
                    break
            current_dir = current_dir.parent

        # If no package root was found, this file can't be imported as a module
        if package_root is None:
            return None

        # Calculate the module path
        rel_path = abs_path.relative_to(package_root.parent)
        module_path = str(rel_path.with_suffix("")).replace(os.sep, ".")

        return module_path

    except Exception as e:
        logger.error(f"Error determining module path: {e}")
        return None


def scan_python_file(
    file_path: Path,
    module_path: str,
    results: dict[str, list[str]],
    auto_register: bool,
) -> None:
    """Scan a Python file for registry-eligible items."""
    try:
        # Try to import the module
        module = importlib.import_module(module_path)

        # Scan for classes and functions
        for name, obj in inspect.getmembers(module):
            if name.startswith("_"):
                continue

            # Check for registry decorator presence
            is_registry_item = False

            # Check for classes
            if inspect.isclass(obj):
                # Check if it has a FlockAgent as a base class
                if is_flock_agent(obj):
                    if auto_register:
                        get_registry().register_agent(obj)
                    results["Agents"].append(f"{module_path}.{name}")
                    is_registry_item = True

                # Check for components
                elif has_component_base(obj):
                    if auto_register:
                        get_registry().register_component(obj)
                    results["Components"].append(f"{module_path}.{name}")
                    is_registry_item = True

                # Check for Pydantic models or dataclasses
                elif is_potential_type(obj):
                    if auto_register:
                        get_registry().register_type(obj)
                    results["Types"].append(f"{module_path}.{name}")
                    is_registry_item = True

                # If not already identified but seems like a potential candidate
                elif not is_registry_item and is_potential_registry_candidate(
                    obj
                ):
                    results["Potential Items"].append(
                        f"{module_path}.{name} (class)"
                    )

            # Check for functions (potential callables/tools)
            elif inspect.isfunction(obj) and obj.__module__ == module.__name__:
                if auto_register:
                    get_registry().register_callable(obj)
                results["Callables"].append(f"{module_path}.{name}")
                is_registry_item = True

    except (ImportError, AttributeError) as e:
        logger.warning(f"Could not import {module_path}: {e}")
    except Exception as e:
        logger.error(f"Error scanning {file_path}: {e}")


def is_flock_agent(cls: type) -> bool:
    """Check if a class is a FlockAgent or a subclass of FlockAgent."""
    try:
        from flock.core.flock_agent import FlockAgent

        return issubclass(cls, FlockAgent)
    except (ImportError, TypeError):
        # If FlockAgent can't be imported or cls is not a class
        return False


def has_component_base(cls: type) -> bool:
    """Check if a class has a base class that looks like a Flock component."""
    try:
        # Common Flock component base classes
        component_bases = ["FlockModule", "FlockEvaluator", "FlockRouter"]
        bases = [base.__name__ for base in cls.__mro__]
        return any(base in bases for base in component_bases)
    except (AttributeError, TypeError):
        return False


def is_potential_type(cls: type) -> bool:
    """Check if a class is a Pydantic model or dataclass."""
    try:
        from pydantic import BaseModel

        return issubclass(cls, BaseModel) or is_dataclass(cls)
    except (ImportError, TypeError):
        return False


def is_potential_registry_candidate(obj: Any) -> bool:
    """Check if an object seems like it could be registry-eligible."""
    # This is a heuristic function to identify potential registry candidates
    if inspect.isclass(obj):
        # Classes with "Flock" in their name
        if "Flock" in obj.__name__:
            return True

        # Classes with docstrings mentioning certain keywords
        if obj.__doc__ and any(
            kw in obj.__doc__.lower()
            for kw in [
                "agent",
                "flock",
                "tool",
                "module",
                "evaluator",
                "router",
            ]
        ):
            return True

    elif inspect.isfunction(obj):
        # Functions with docstrings mentioning certain keywords
        if obj.__doc__ and any(
            kw in obj.__doc__.lower() for kw in ["tool", "agent", "flock"]
        ):
            return True

    return False


def export_registry() -> None:
    """Export registry contents to a file."""
    registry = get_registry()

    # Select what to export
    export_items = questionary.checkbox(
        "Select what to export:",
        choices=[
            questionary.Choice("Agents", checked=True),
            questionary.Choice("Callables (Tools)", checked=True),
            questionary.Choice("Types", checked=True),
            questionary.Choice("Components", checked=True),
            questionary.Choice("File Paths", checked=True),
        ],
    ).ask()

    if not export_items:
        console.print("[yellow]No items selected for export.[/]")
        return

    # Select export format
    export_format = questionary.select(
        "Select export format:",
        choices=["YAML", "JSON", "Python"],
    ).ask()

    # Select path type for serialization
    path_type = questionary.select(
        "How should file paths be formatted?",
        choices=[
            "absolute (full paths, best for local use)",
            "relative (relative paths, better for sharing)",
        ],
        default="absolute (full paths, best for local use)",
    ).ask()

    # Extract just the first word
    path_type = path_type.split()[0]

    console.print(
        f"\n[bold]Path type selected: [green]{path_type}[/green][/bold]"
    )
    if path_type == "relative":
        console.print(
            "Relative paths are recommended when sharing Flocks between systems.\n"
            "They'll be converted to paths relative to the current directory."
        )
    else:
        console.print(
            "Absolute paths work best for local usage but may not work correctly\n"
            "when sharing with others or moving files."
        )
    console.line()

    # Get file path for export
    file_path = questionary.path(
        "Enter file path for export:",
        default=f"flock_registry_export.{export_format.lower()}",
    ).ask()

    if not file_path:
        return

    # Prepare export data
    export_data = {}

    if "Agents" in export_items:
        export_data["agents"] = list(registry._agents.keys())

    if "Callables (Tools)" in export_items:
        export_data["callables"] = list(registry._callables.keys())

        # Add serialization format information for tools
        callable_details = {}
        for callable_name in registry._callables.keys():
            callable_obj = registry._callables[callable_name]
            file_path_value = (
                inspect.getfile(callable_obj)
                if callable_obj and inspect.isfunction(callable_obj)
                else "Unknown"
            )

            # Convert to relative path if needed
            if path_type == "relative" and file_path_value != "Unknown":
                try:
                    file_path_value = os.path.relpath(file_path_value)
                except ValueError:
                    # Keep as absolute if can't make relative
                    pass

            callable_details[callable_name] = {
                "module": callable_obj.__module__,
                "file": file_path_value,
                "type": "function"
                if inspect.isfunction(callable_obj)
                else "other_callable",
            }
        export_data["callable_details"] = callable_details

    if "Types" in export_items:
        export_data["types"] = list(registry._types.keys())

    if "Components" in export_items:
        export_data["components"] = list(registry._components.keys())

        # Include file paths if selected
        if "File Paths" in export_items and hasattr(
            registry, "_component_file_paths"
        ):
            export_data["component_file_paths"] = {}
            for component_name in registry._components.keys():
                # Get the file path if available
                if component_name in registry._component_file_paths:
                    file_path_value = registry._component_file_paths[
                        component_name
                    ]

                    # Convert to relative path if needed
                    if path_type == "relative" and file_path_value:
                        try:
                            file_path_value = os.path.relpath(file_path_value)
                        except ValueError:
                            # Keep as absolute if can't make relative
                            pass

                    export_data["component_file_paths"][component_name] = (
                        file_path_value
                    )

    # Add metadata about serialization format
    export_data["metadata"] = {
        "export_date": datetime.datetime.now().isoformat(),
        "flock_version": "0.3.41",  # Update with actual version
        "serialization_format": {
            "tools": "Callable reference names",
            "components": "Module and class names",
            "types": "Module and class names",
        },
        "path_type": path_type,
    }

    # Add serialization settings as a top-level element
    export_data["serialization_settings"] = {"path_type": path_type}

    try:
        # Export the data
        if export_format == "YAML":
            import yaml

            with open(file_path, "w") as f:
                yaml.dump(export_data, f, default_flow_style=False)
        elif export_format == "JSON":
            import json

            with open(file_path, "w") as f:
                json.dump(export_data, f, indent=2)
        elif export_format == "Python":
            with open(file_path, "w") as f:
                f.write("# Flock Registry Export\n")
                f.write(f"# Generated on {datetime.datetime.now()}\n\n")
                f.write("registry_data = ")
                f.write(repr(export_data))
                f.write("\n")

        console.print(f"[green]Registry exported to {file_path}[/]")
        console.print(f"[green]Paths formatted as: {path_type}[/]")

        # Print information about tool serialization if tools were exported
        if "Callables (Tools)" in export_items and registry._callables:
            console.print("\n[bold blue]Tool Serialization Information:[/]")
            console.print(
                "Tools in Flock are now serialized as callable references rather than dictionaries."
            )
            console.print(
                "This makes YAML files more readable and simplifies tool management."
            )
            console.print("When loading a Flock with tools:")
            console.print("  1. Tools must be registered in the registry")
            console.print("  2. The tools' modules must be importable")
            console.print(
                "  3. Tool functions have the same signature across systems"
            )

            # Show example of how a tool would appear in YAML
            if registry._callables:
                console.print("\n[bold green]Example tool in YAML:[/]")
                example_callable = next(iter(registry._callables.keys()))
                console.print(
                    f"  - {example_callable}  # Function name reference"
                )
                console.print("instead of the old format:")
                console.print(f"  - __callable_ref__: {example_callable}")

    except Exception as e:
        console.print(f"[red]Error exporting registry: {e}[/]")
        logger.error(f"Failed to export registry: {e}", exc_info=True)


if __name__ == "__main__":
    manage_registry()

```

---

### 45. src\flock\cli\runner.py

- **File ID**: file_44
- **Type**: Code File
- **Line Count**: 41
- **Description**: Provides functionality to start the Flock CLI.
- **Dependencies**: None
- **Used By**: None

**Content**:
```
# src/flock/cli/runner.py
"""Provides functionality to start the Flock CLI."""

from typing import TYPE_CHECKING

from flock.core.logging.logging import get_logger

if TYPE_CHECKING:
    from flock.core.flock import Flock

logger = get_logger("cli.runner")


def start_flock_cli(
    flock: "Flock",
    server_name: str = "Flock CLI",
    show_results: bool = False,
    edit_mode: bool = False,
) -> None:
    """Start a CLI interface for the given Flock instance."""
    try:
        # Import CLI function locally
        from flock.cli.loaded_flock_cli import start_loaded_flock_cli
    except ImportError:
        logger.error(
            "CLI components not found. Cannot start CLI. "
            "Ensure the CLI modules are properly installed/available."
        )
        return

    logger.info(
        f"Starting CLI interface for loaded Flock instance '{flock.name}' ({len(flock.agents)} agents)"
    )

    # Pass the Flock instance to the CLI entry point
    start_loaded_flock_cli(
        flock=flock,
        server_name=server_name,
        show_results=show_results,
        edit_mode=edit_mode,
    )

```

---

### 46. src\flock\cli\settings.py

- **File ID**: file_45
- **Type**: Code File
- **Line Count**: 857
- **Description**: Settings editor for the Flock CLI.
- **Dependencies**: None
- **Used By**: None

**Content**:
```
"""Settings editor for the Flock CLI.

This module provides functionality to view, edit, add, and delete
environment variables in the .env file.
"""

import os
import shutil
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import math

import questionary
from rich.console import Console
from rich.panel import Panel
from rich.table import Table
from rich.text import Text

from flock.core.util.cli_helper import init_console

# Constants
ENV_FILE = ".env"
ENV_TEMPLATE_FILE = ".env_template"
ENV_PROFILE_PREFIX = ".env_"
DEFAULT_PROFILE_COMMENT = "# Profile: {profile_name}"
SHOW_SECRETS_KEY = "SHOW_SECRETS"
VARS_PER_PAGE_KEY = "VARS_PER_PAGE"
DEFAULT_VARS_PER_PAGE = 20

console = Console()


def settings_editor():
    """Main entry point for the settings editor."""
    while True:
        init_console()
        console.print(Panel("[bold green]Environment Settings Editor[/]"), justify="center")
        
        # Get current profile name
        current_profile = get_current_profile()
        if current_profile:
            console.print(f"Current Profile: [bold cyan]{current_profile}[/]")
        else:
            console.print("No profile detected")

        console.line()
            
        choice = questionary.select(
            "What would you like to do?",
            choices=[
                questionary.Separator(line=" "),
                "View all environment variables",
                "Edit an environment variable",
                "Add a new environment variable",
                "Delete an environment variable",
                questionary.Separator(),
                "Manage environment profiles",
                questionary.Separator(),
                "Toggle show secrets",
                "Change variables per page",
                questionary.Separator(),
                "Back to main menu",
            ],
        ).ask()
        
        if choice == "View all environment variables":
            view_env_variables()
        elif choice == "Edit an environment variable":
            edit_env_variable()
        elif choice == "Add a new environment variable":
            add_env_variable()
        elif choice == "Delete an environment variable":
            delete_env_variable()
        elif choice == "Manage environment profiles":
            manage_profiles()
        elif choice == "Toggle show secrets":
            toggle_show_secrets()
        elif choice == "Change variables per page":
            change_vars_per_page()
        elif choice == "Back to main menu":
            break
        
        if choice != "Back to main menu":
            input("\nPress Enter to continue...")


def view_env_variables(page: int = 1, page_size: Optional[int] = None):
    """View all environment variables with pagination.
    
    Args:
        page: Page number to display
        page_size: Number of variables per page (if None, use the setting in .env)
    """
    env_vars = load_env_file()
    
    # If page_size is not specified, get it from settings
    if page_size is None:
        page_size = get_vars_per_page_setting(env_vars)
    
    # Calculate pagination
    total_vars = len(env_vars)
    total_pages = math.ceil(total_vars / page_size) if total_vars > 0 else 1
    
    # Validate page number
    page = min(max(1, page), total_pages)
    
    start_idx = (page - 1) * page_size
    end_idx = min(start_idx + page_size, total_vars)
    
    # Get current page variables
    current_page_vars = list(env_vars.items())[start_idx:end_idx]
    
    # Check if secrets should be shown
    show_secrets = get_show_secrets_setting(env_vars)
    
    # Create table
    table = Table(title=f"Environment Variables (Page {page}/{total_pages}, {page_size} per page)")
    table.add_column("Name", style="cyan")
    table.add_column("Value", style="green")
    
    # Show secrets status
    secrets_status = "[green]ON[/]" if show_secrets else "[red]OFF[/]"
    init_console()
    console.print(f"Show Secrets: {secrets_status}")
    
    for key, value in current_page_vars:
        # Skip lines that are comments or empty
        if key.startswith('#') or not key:
            continue
            
        # Mask sensitive values if show_secrets is False
        if is_sensitive(key) and not show_secrets:
            masked_value = mask_sensitive_value(value)
            table.add_row(key, masked_value)
        else:
            table.add_row(key, value)
    
    console.print(table)
    
    # Pagination controls with more intuitive shortcuts
    console.print("\nNavigation: ", end="")
    if page > 1:
        console.print("[bold]Previous (p)[/] | ", end="")
    if page < total_pages:
        console.print("[bold]Next (n)[/] | ", end="")
    if show_secrets:
        console.print("[bold]Hide secrets (h)[/] | ", end="")
    else:
        console.print("[bold]Show secrets (s)[/] | ", end="")
    console.print("[bold]Change variables per page (v)[/] | ", end="")
    console.print("[bold]Back (b)[/]")
    
    # Handle navigation
    while True:
        key = input("Enter option: ").lower()
        if key == 'p' and page > 1:
            view_env_variables(page - 1, page_size)
            break
        elif key == 'n' and page < total_pages:
            view_env_variables(page + 1, page_size)
            break
        elif key == 's' and not show_secrets:
            # Confirm showing secrets
            confirm = questionary.confirm("Are you sure you want to show sensitive values?").ask()
            if confirm:
                set_show_secrets_setting(True)
                view_env_variables(page, page_size)
            break
        elif key == 'h' and show_secrets:
            set_show_secrets_setting(False)
            view_env_variables(page, page_size)
            break
        elif key == 'v':
            new_page_size = change_vars_per_page()
            if new_page_size:
                view_env_variables(1, new_page_size)  # Reset to first page with new page size
            break
        elif key == 'b':
            break


def change_vars_per_page():
    """Change the number of variables displayed per page.
    
    Returns:
        The new page size or None if cancelled
    """
    env_vars = load_env_file()
    current_setting = get_vars_per_page_setting(env_vars)
    
    console.print(f"Current variables per page: [cyan]{current_setting}[/]")
    
    # Predefined options plus custom option
    page_size_options = ["10", "20", "30", "50", "Custom", "Cancel"]
    
    choice = questionary.select(
        "Select number of variables per page:",
        choices=page_size_options,
    ).ask()
    
    if choice == "Cancel":
        return None
    
    if choice == "Custom":
        while True:
            try:
                custom_size = questionary.text(
                    "Enter custom page size (5-100):",
                    default=str(current_setting)
                ).ask()
                
                if not custom_size:
                    return None
                
                new_size = int(custom_size)
                if 5 <= new_size <= 100:
                    break
                else:
                    console.print("[yellow]Page size must be between 5 and 100.[/]")
            except ValueError:
                console.print("[yellow]Please enter a valid number.[/]")
    else:
        new_size = int(choice)
    
    # Save the setting
    set_vars_per_page_setting(new_size)
    console.print(f"[green]Variables per page set to {new_size}.[/]")
    
    return new_size


def get_vars_per_page_setting(env_vars: Dict[str, str] = None) -> int:
    """Get the current variables per page setting.
    
    Args:
        env_vars: Optional dictionary of environment variables
        
    Returns:
        Number of variables per page
    """
    if env_vars is None:
        env_vars = load_env_file()
    
    if VARS_PER_PAGE_KEY in env_vars:
        try:
            page_size = int(env_vars[VARS_PER_PAGE_KEY])
            # Ensure the value is within reasonable bounds
            if 5 <= page_size <= 100:
                return page_size
        except ValueError:
            pass
    
    return DEFAULT_VARS_PER_PAGE


def set_vars_per_page_setting(page_size: int):
    """Set the variables per page setting.
    
    Args:
        page_size: Number of variables to display per page
    """
    env_vars = load_env_file()
    env_vars[VARS_PER_PAGE_KEY] = str(page_size)
    save_env_file(env_vars)


def toggle_show_secrets():
    """Toggle the show secrets setting."""
    env_vars = load_env_file()
    current_setting = get_show_secrets_setting(env_vars)
    
    if current_setting:
        console.print("Currently showing sensitive values. Do you want to hide them?")
        confirm = questionary.confirm("Hide sensitive values?").ask()
        if confirm:
            set_show_secrets_setting(False)
            console.print("[green]Sensitive values will now be masked.[/]")
    else:
        console.print("[yellow]Warning:[/] Showing sensitive values can expose sensitive information.")
        confirm = questionary.confirm("Are you sure you want to show sensitive values?").ask()
        if confirm:
            set_show_secrets_setting(True)
            console.print("[green]Sensitive values will now be shown.[/]")


def get_show_secrets_setting(env_vars: Dict[str, str] = None) -> bool:
    """Get the current show secrets setting.
    
    Args:
        env_vars: Optional dictionary of environment variables
        
    Returns:
        True if secrets should be shown, False otherwise
    """
    if env_vars is None:
        env_vars = load_env_file()
    
    if SHOW_SECRETS_KEY in env_vars:
        return env_vars[SHOW_SECRETS_KEY].lower() == 'true'
    
    return False


def set_show_secrets_setting(show_secrets: bool):
    """Set the show secrets setting.
    
    Args:
        show_secrets: Whether to show secrets
    """
    env_vars = load_env_file()
    env_vars[SHOW_SECRETS_KEY] = str(show_secrets)
    save_env_file(env_vars)


def edit_env_variable():
    """Edit an environment variable."""
    # Get list of variables
    env_vars = load_env_file()
    
    if not env_vars:
        console.print("[yellow]No environment variables found to edit.[/]")
        return
    
    # Filter out comments
    variables = [k for k in env_vars.keys() if not k.startswith('#') and k]
    
    # Display variables with selection
    init_console()
    console.print("Select a variable to edit:")
    
    # Let user select a variable to edit
    var_name = questionary.select(
        "Select a variable to edit:",
        choices=variables + ["Cancel"],
    ).ask()
    
    if var_name == "Cancel":
        return
    
    current_value = env_vars[var_name]
    is_sensitive_var = is_sensitive(var_name)
    
    if is_sensitive_var:
        console.print(f"[yellow]Warning:[/] You are editing a sensitive value: {var_name}")
        confirm = questionary.confirm("Are you sure you want to continue?").ask()
        if not confirm:
            return
    
    # Show current value (masked if sensitive and show_secrets is False)
    show_secrets = get_show_secrets_setting(env_vars)
    if is_sensitive_var and not show_secrets:
        console.print(f"Current value: {mask_sensitive_value(current_value)}")
    else:
        console.print(f"Current value: {current_value}")
    
    # Get new value with hint
    console.print("[italic]Enter new value (or leave empty to cancel)[/]")
    new_value = questionary.text("Enter new value:", default=current_value).ask()
    
    if new_value is None:
        console.print("[yellow]Edit cancelled.[/]")
        return
    
    if new_value == "":
        # Confirm if user wants to set an empty value or cancel
        confirm = questionary.confirm("Do you want to set an empty value? Select No to cancel.", default=False).ask()
        if not confirm:
            console.print("[yellow]Edit cancelled.[/]")
            return
    
    if new_value == current_value:
        console.print("[yellow]No changes made.[/]")
        return
    
    # Update the value
    env_vars[var_name] = new_value
    save_env_file(env_vars)
    console.print(f"[green]Updated {var_name} successfully.[/]")


def add_env_variable():
    """Add a new environment variable."""
    env_vars = load_env_file()
    
    console.print("[italic]Enter variable name (or leave empty to go back)[/]")
    
    # Get variable name
    while True:
        var_name = questionary.text("Enter variable name:").ask()
        
        if not var_name:
            # Ask if user wants to go back
            go_back = questionary.confirm("Do you want to go back to the settings menu?", default=True).ask()
            if go_back:
                return
            else:
                console.print("[italic]Please enter a variable name (or leave empty to go back)[/]")
                continue
            
        if var_name in env_vars and not var_name.startswith('#'):
            console.print(f"[yellow]Variable {var_name} already exists. Please use edit instead.[/]")
            continue
            
        break
    
    # Get variable value
    var_value = questionary.text("Enter variable value:").ask()
    
    # Add to env_vars
    env_vars[var_name] = var_value
    save_env_file(env_vars)
    console.print(f"[green]Added {var_name} successfully.[/]")


def delete_env_variable():
    """Delete an environment variable."""
    # Get list of variables
    env_vars = load_env_file()
    
    if not env_vars:
        console.print("[yellow]No environment variables found to delete.[/]")
        return
    
    # Filter out comments
    variables = [k for k in env_vars.keys() if not k.startswith('#') and k]
    
    # Display variables with selection
    init_console()
    console.print("Select a variable to delete:")
    
    # Let user select a variable to delete with hint
    var_name = questionary.select(
        "Select a variable to delete:",
        choices=variables + ["Cancel"],
    ).ask()
    
    if var_name == "Cancel":
        return
    
    # Confirm deletion
    confirm = questionary.confirm(f"Are you sure you want to delete {var_name}?").ask()
    if not confirm:
        console.print("[yellow]Deletion cancelled.[/]")
        return
    
    # Delete the variable
    del env_vars[var_name]
    save_env_file(env_vars)
    console.print(f"[green]Deleted {var_name} successfully.[/]")


def manage_profiles():
    """Manage environment profiles."""
    init_console()
    console.print(Panel("[bold green]Environment Profile Management[/]"), justify="center")
    
    # Get current profile and available profiles
    current_profile = get_current_profile()
    available_profiles = get_available_profiles()
    
    if current_profile:
        console.print(f"Current Profile: [bold cyan]{current_profile}[/]")
    
    if not available_profiles:
        console.print("[yellow]No profiles found.[/]")
    else:
        console.print("Available Profiles:")
        for profile in available_profiles:
            if profile == current_profile:
                console.print(f"  [bold cyan]{profile} (active)[/]")
            else:
                console.print(f"  {profile}")

    console.line()
    
    # Profile management options
    choice = questionary.select(
        "What would you like to do?",
        choices=[
            questionary.Separator(line=" "),
            "Switch to a different profile",
            "Create a new profile",
            "Rename a profile",
            "Delete a profile",
            "Back to settings menu",
        ],
    ).ask()
    
    if choice == "Switch to a different profile":
        switch_profile()
    elif choice == "Create a new profile":
        create_profile()
    elif choice == "Rename a profile":
        rename_profile()
    elif choice == "Delete a profile":
        delete_profile()


def switch_profile():
    """Switch to a different environment profile."""
    available_profiles = get_available_profiles()
    current_profile = get_current_profile()
    
    if not available_profiles:
        console.print("[yellow]No profiles available to switch to.[/]")
        return
    
    # Remove current profile from the list to avoid switching to the same profile
    selectable_profiles = [p for p in available_profiles if p != current_profile]
    
    if not selectable_profiles:
        console.print("[yellow]No other profiles available to switch to.[/]")
        return
    
    target_profile = questionary.select(
        "Select a profile to switch to:",
        choices=selectable_profiles + ["Cancel"],
    ).ask()
    
    if target_profile == "Cancel":
        return
    
    # Confirm switch
    confirm = questionary.confirm(f"Are you sure you want to switch to the {target_profile} profile?").ask()
    if not confirm:
        return
    
    # Backup current .env file
    backup_env_file()
    
    # Copy selected profile to .env
    source_file = f"{ENV_PROFILE_PREFIX}{target_profile}"
    if os.path.exists(source_file):
        shutil.copy2(source_file, ENV_FILE)
        console.print(f"[green]Switched to {target_profile} profile successfully.[/]")
    else:
        console.print(f"[red]Error: Could not find profile file {source_file}.[/]")


def create_profile():
    """Create a new environment profile."""
    profile_name = questionary.text("Enter new profile name:").ask()
    
    if not profile_name:
        console.print("[yellow]Profile name cannot be empty.[/]")
        return
    
    # Check if profile already exists
    target_file = f"{ENV_PROFILE_PREFIX}{profile_name}"
    if os.path.exists(target_file):
        console.print(f"[yellow]Profile {profile_name} already exists.[/]")
        return
    
    # Determine source file - use current .env or template
    source_choices = ["Current environment (.env)", ".env_template"]
    if os.path.exists(ENV_TEMPLATE_FILE):
        source_choices.append(ENV_TEMPLATE_FILE)
    
    source_choice = questionary.select(
        "Create profile based on:",
        choices=source_choices + ["Cancel"],
    ).ask()
    
    if source_choice == "Cancel":
        return
    
    source_file = ENV_FILE if source_choice == "Current environment (.env)" else ENV_TEMPLATE_FILE
    
    if not os.path.exists(source_file):
        console.print(f"[red]Error: Source file {source_file} not found.[/]")
        return
    
    # Create new profile file
    try:
        # Copy source file
        shutil.copy2(source_file, target_file)
        
        # Add profile header if it doesn't exist
        with open(target_file, 'r') as file:
            content = file.read()
        
        if not content.startswith("# Profile:"):
            with open(target_file, 'w') as file:
                profile_header = DEFAULT_PROFILE_COMMENT.format(profile_name=profile_name)
                file.write(f"{profile_header}\n{content}")
        
        console.print(f"[green]Created {profile_name} profile successfully.[/]")
    except Exception as e:
        console.print(f"[red]Error creating profile: {str(e)}[/]")


def rename_profile():
    """Rename an existing profile."""
    available_profiles = get_available_profiles()
    current_profile = get_current_profile()
    
    if not available_profiles:
        console.print("[yellow]No profiles available to rename.[/]")
        return
    
    # Let user select a profile to rename
    profile_to_rename = questionary.select(
        "Select a profile to rename:",
        choices=available_profiles + ["Cancel"],
    ).ask()
    
    if profile_to_rename == "Cancel":
        return
    
    # Get new name
    new_name = questionary.text("Enter new profile name:").ask()
    
    if not new_name:
        console.print("[yellow]New profile name cannot be empty.[/]")
        return
    
    if new_name in available_profiles:
        console.print(f"[yellow]Profile {new_name} already exists.[/]")
        return
    
    # Rename profile file
    source_file = f"{ENV_PROFILE_PREFIX}{profile_to_rename}"
    target_file = f"{ENV_PROFILE_PREFIX}{new_name}"
    
    try:
        # Read content of the source file
        with open(source_file, 'r') as file:
            content = file.readlines()
        
        # Update profile header if it exists
        if content and content[0].startswith("# Profile:"):
            content[0] = DEFAULT_PROFILE_COMMENT.format(profile_name=new_name) + "\n"
        
        # Write to new file
        with open(target_file, 'w') as file:
            file.writelines(content)
        
        # Remove old file
        os.remove(source_file)
        
        # If this was the current profile, update .env as well
        if profile_to_rename == current_profile:
            with open(ENV_FILE, 'r') as file:
                content = file.readlines()
            
            if content and content[0].startswith("# Profile:"):
                content[0] = DEFAULT_PROFILE_COMMENT.format(profile_name=new_name) + "\n"
            
            with open(ENV_FILE, 'w') as file:
                file.writelines(content)
        
        console.print(f"[green]Renamed {profile_to_rename} to {new_name} successfully.[/]")
    except Exception as e:
        console.print(f"[red]Error renaming profile: {str(e)}[/]")


def delete_profile():
    """Delete an existing profile."""
    available_profiles = get_available_profiles()
    current_profile = get_current_profile()
    
    if not available_profiles:
        console.print("[yellow]No profiles available to delete.[/]")
        return
    
    # Let user select a profile to delete
    profile_to_delete = questionary.select(
        "Select a profile to delete:",
        choices=available_profiles + ["Cancel"],
    ).ask()
    
    if profile_to_delete == "Cancel":
        return
    
    # Confirm deletion
    confirm = questionary.confirm(
        f"Are you sure you want to delete the {profile_to_delete} profile? This cannot be undone."
    ).ask()
    
    if not confirm:
        return
    
    # Delete profile file
    profile_file = f"{ENV_PROFILE_PREFIX}{profile_to_delete}"
    
    try:
        os.remove(profile_file)
        
        # Warn if deleting the current profile
        if profile_to_delete == current_profile:
            console.print(
                f"[yellow]Warning: You deleted the currently active profile. "
                f"The .env file still contains those settings but is no longer marked as a profile.[/]"
            )
            
            # Remove profile header from .env
            with open(ENV_FILE, 'r') as file:
                content = file.readlines()
            
            if content and content[0].startswith("# Profile:"):
                content = content[1:]
                with open(ENV_FILE, 'w') as file:
                    file.writelines(content)
        
        console.print(f"[green]Deleted {profile_to_delete} profile successfully.[/]")
    except Exception as e:
        console.print(f"[red]Error deleting profile: {str(e)}[/]")


def is_sensitive(key: str) -> bool:
    """Check if a variable is considered sensitive.
    
    Args:
        key: The variable name
        
    Returns:
        True if sensitive, False otherwise
    """
    sensitive_patterns = ['key', 'token', 'secret', 'password', 'api', 'pat']
    key_lower = key.lower()
    return any(pattern in key_lower for pattern in sensitive_patterns)


def mask_sensitive_value(value: str) -> str:
    """Mask a sensitive value.
    
    Args:
        value: The sensitive value
        
    Returns:
        Masked value
    """
    if not value:
        return value
    
    if len(value) <= 4:
        return "••••"
    
    # Show first 2 and last 2 characters
    return value[:2] + "•" * (len(value) - 4) + value[-2:]


def get_current_profile() -> Optional[str]:
    """Get the name of the current active profile.
    
    Returns:
        Profile name or None if no profile is active
    """
    if not os.path.exists(ENV_FILE):
        return None
    
    try:
        with open(ENV_FILE, 'r') as file:
            first_line = file.readline().strip()
            
        if first_line.startswith("# Profile:"):
            return first_line.replace("# Profile:", "").strip()
    except Exception:
        pass
    
    return None


def get_available_profiles() -> List[str]:
    """Get a list of available profiles.
    
    Returns:
        List of profile names
    """
    profiles = []
    
    for file in os.listdir():
        if file.startswith(ENV_PROFILE_PREFIX):
            profile_name = file[len(ENV_PROFILE_PREFIX):]
            profiles.append(profile_name)
    
    return profiles


def backup_env_file():
    """Create a backup of the current .env file."""
    if not os.path.exists(ENV_FILE):
        return
    
    backup_file = f"{ENV_FILE}.bak"
    shutil.copy2(ENV_FILE, backup_file)


def load_env_file() -> Dict[str, str]:
    """Load the .env file into a dictionary.
    
    Returns:
        Dictionary of environment variables
    """
    env_vars = {}
    
    if not os.path.exists(ENV_FILE):
        console.print(f"[yellow]Warning: {ENV_FILE} file not found.[/]")
        return env_vars
    
    try:
        with open(ENV_FILE, 'r') as file:
            lines = file.readlines()
            
        # Process each line
        for line in lines:
            line = line.strip()
            
            # Skip empty lines
            if not line:
                env_vars[""] = ""
                continue
            
            # Handle comments
            if line.startswith('#'):
                env_vars[line] = ""
                continue
            
            # Handle regular variables
            if '=' in line:
                key, value = line.split('=', 1)
                env_vars[key] = value
            else:
                # Handle lines without equals sign
                env_vars[line] = ""
                
    except Exception as e:
        console.print(f"[red]Error loading .env file: {str(e)}[/]")
    
    return env_vars


def save_env_file(env_vars: Dict[str, str]):
    """Save environment variables back to the .env file.
    
    Args:
        env_vars: Dictionary of environment variables
    """
    # Create backup
    backup_env_file()
    
    try:
        with open(ENV_FILE, 'w') as file:
            for key, value in env_vars.items():
                if key.startswith('#'):
                    # Write comments as is
                    file.write(f"{key}\n")
                elif not key:
                    # Write empty lines
                    file.write("\n")
                else:
                    # Write regular variables
                    file.write(f"{key}={value}\n")
                    
        console.print("[green]Settings saved successfully.[/]")
    except Exception as e:
        console.print(f"[red]Error saving .env file: {str(e)}[/]")

```

---

### 47. src\flock\cli\utils.py

- **File ID**: file_46
- **Type**: Code File
- **Line Count**: 135
- **Description**: File at src\flock\cli\utils.py
- **Dependencies**: None
- **Used By**: None

**Content**:
```
# src/pilot_rules/collector/utils.py
import datetime
from pathlib import Path
from typing import Any

from rich import box
from rich.console import Console
from rich.panel import Panel
from rich.progress import BarColumn, Progress, SpinnerColumn, TextColumn
from rich.table import Table

# Create a shared console instance for consistent styling
console = Console()


def get_file_metadata(file_path: str) -> dict[str, Any]:
    """Extract metadata from a file."""
    metadata = {
        "path": file_path,
        "size_bytes": 0,
        "line_count": 0,
        "last_modified": "Unknown",
        "created": "Unknown",
    }

    try:
        p = Path(file_path)
        stats = p.stat()
        metadata["size_bytes"] = stats.st_size
        metadata["last_modified"] = datetime.datetime.fromtimestamp(
            stats.st_mtime
        ).strftime("%Y-%m-%d %H:%M:%S")
        # ctime is platform dependent (creation on Windows, metadata change on Unix)
        # Use mtime as a reliable fallback for "created" if ctime is older than mtime
        ctime = stats.st_ctime
        mtime = stats.st_mtime
        best_ctime = ctime if ctime <= mtime else mtime  # Heuristic
        metadata["created"] = datetime.datetime.fromtimestamp(
            best_ctime
        ).strftime("%Y-%m-%d %H:%M:%S")

        try:
            # Attempt to read as text, fallback for binary or encoding issues
            with p.open("r", encoding="utf-8", errors="ignore") as f:
                content = f.read()
                metadata["line_count"] = len(content.splitlines())
        except (OSError, UnicodeDecodeError) as read_err:
            # Handle cases where reading might fail (binary file, permissions etc.)
            console.print(
                f"[yellow]⚠ Warning:[/yellow] Could not read content/count lines for [cyan]{file_path}[/cyan]: [red]{read_err}[/red]"
            )
            metadata["line_count"] = 0  # Indicate unreadable or binary

    except Exception as e:
        console.print(
            f"[yellow]⚠ Warning:[/yellow] Could not get complete metadata for [cyan]{file_path}[/cyan]: [red]{e}[/red]"
        )

    return metadata


# --- Rich Formatting Utilities ---


def print_header(title: str, style: str = "blue") -> None:
    """Print a styled header with a panel."""
    console.rule()
    console.print(
        Panel.fit(f"[bold {style}]{title}[/bold {style}]", border_style=style)
    )


def print_subheader(title: str, style: str = "cyan") -> None:
    """Print a styled subheader."""
    console.print(f"[bold {style}]== {title} ==[/bold {style}]")


def print_success(message: str) -> None:
    """Print a success message."""
    console.print(f"[bold green]✓[/bold green] {message}")


def print_error(message: str, exit_code: int | None = None) -> None:
    """Print an error message and optionally exit."""
    console.print(f"[bold red]✗ ERROR:[/bold red] {message}")
    if exit_code is not None:
        exit(exit_code)


def print_warning(message: str) -> None:
    """Print a warning message."""
    console.print(f"[yellow]⚠ Warning:[/yellow] {message}")


def create_progress() -> Progress:
    """Create a standardized progress bar."""
    return Progress(
        SpinnerColumn(),
        TextColumn("[bold blue]{task.description}"),
        BarColumn(complete_style="green", finished_style="green"),
        TextColumn("[bold]{task.completed}/{task.total}"),
        console=console,
    )


def create_task_table(title: str) -> Table:
    """Create a standardized table for displaying task information."""
    table = Table(
        title=title, show_header=True, header_style="bold cyan", box=box.ROUNDED
    )
    return table


def print_file_stats(files: list[str], title: str = "File Statistics") -> None:
    """Print statistics about a list of files."""
    if not files:
        console.print("[yellow]No files found to display statistics.[/yellow]")
        return

    table = Table(title=title, show_header=True, header_style="bold magenta")
    table.add_column("Statistic", style="cyan")
    table.add_column("Value", style="green")

    extensions = {Path(f).suffix.lower() for f in files if Path(f).suffix}
    total_size = sum(get_file_metadata(f).get("size_bytes", 0) for f in files)
    total_lines = sum(get_file_metadata(f).get("line_count", 0) for f in files)

    table.add_row("Total Files", str(len(files)))
    table.add_row("Total Size", f"{total_size / 1024:.2f} KB")
    table.add_row("Total Lines", str(total_lines))
    table.add_row(
        "Extensions", ", ".join(sorted(extensions)) if extensions else "None"
    )

    console.print(table)

```

---

### 48. src\flock\cli\view_results.py

- **File ID**: file_47
- **Type**: Code File
- **Line Count**: 29
- **Description**: View execution results and history.
- **Dependencies**: None
- **Used By**: None

**Content**:
```
"""View execution results and history.

This module provides functionality to view the results of previous Flock executions.
"""

from rich.console import Console
from rich.panel import Panel

from flock.core.flock import Flock
from flock.core.util.cli_helper import init_console

# Create console instance
console = Console()


def view_results(flock: Flock):
    """View execution results for a Flock instance.

    Args:
        flock: The Flock instance to view results for
    """
    init_console()
    console.print(Panel("[bold green]View Results[/]"), justify="center")
    console.print(
        "[yellow]Results history functionality not yet implemented.[/]"
    )
    console.print(
        "This feature will allow viewing and filtering past execution results."
    )

```

---

### 49. src\flock\cli\yaml_editor.py

- **File ID**: file_48
- **Type**: Code File
- **Line Count**: 396
- **Description**: YAML Editor for Flock CLI.
- **Dependencies**: None
- **Used By**: None

**Content**:
```
"""YAML Editor for Flock CLI.

This module provides functionality to view, edit, and validate YAML configurations
for Flock and FlockAgent instances.
"""

import os
import subprocess
import tempfile
from pathlib import Path

import questionary
import yaml
from rich.console import Console
from rich.panel import Panel
from rich.syntax import Syntax
from rich.table import Table

from flock.core.flock import Flock
from flock.core.flock_agent import FlockAgent
from flock.core.util.cli_helper import init_console

# Create console instance
console = Console()


def yaml_editor(flock_or_agent: Flock | FlockAgent | None = None):
    """YAML Editor main entry point.

    Args:
        flock_or_agent: Optional Flock or FlockAgent instance to edit
    """
    init_console()
    console.print(Panel("[bold green]YAML Editor[/]"), justify="center")

    if flock_or_agent is None:
        # If no object provided, provide options to load from file
        _yaml_file_browser()
        return

    while True:
        init_console()
        console.print(Panel("[bold green]YAML Editor[/]"), justify="center")

        # Determine object type
        if isinstance(flock_or_agent, Flock):
            obj_type = "Flock"
            console.print(
                f"Editing [bold cyan]Flock[/] with {len(flock_or_agent._agents)} agents"
            )
        elif isinstance(flock_or_agent, FlockAgent):
            obj_type = "FlockAgent"
            console.print(
                f"Editing [bold cyan]FlockAgent[/]: {flock_or_agent.name}"
            )
        else:
            console.print("[bold red]Error: Unknown object type[/]")
            input("\nPress Enter to continue...")
            return

        console.line()

        choice = questionary.select(
            "What would you like to do?",
            choices=[
                questionary.Separator(line=" "),
                "View Current YAML",
                "Edit YAML Directly",
                "Abstract Editor (Visual)",
                "Validate YAML",
                "Save to File",
                questionary.Separator(),
                "Back to Main Menu",
            ],
        ).ask()

        if choice == "View Current YAML":
            _view_yaml(flock_or_agent)
        elif choice == "Edit YAML Directly":
            flock_or_agent = _edit_yaml_directly(flock_or_agent)
        elif choice == "Abstract Editor (Visual)":
            flock_or_agent = _abstract_editor(flock_or_agent)
        elif choice == "Validate YAML":
            _validate_yaml(flock_or_agent)
        elif choice == "Save to File":
            _save_to_file(flock_or_agent)
        elif choice == "Back to Main Menu":
            break

        if choice != "Back to Main Menu":
            input("\nPress Enter to continue...")


def _yaml_file_browser():
    """Browser for YAML files to load."""
    console.print("\n[bold]YAML File Browser[/]")
    console.line()

    current_dir = os.getcwd()
    console.print(f"Current directory: [cyan]{current_dir}[/]")

    # List .yaml/.yml files in current directory
    yaml_files = list(Path(current_dir).glob("*.yaml")) + list(
        Path(current_dir).glob("*.yml")
    )

    if not yaml_files:
        console.print("[yellow]No YAML files found in current directory.[/]")
        input("\nPress Enter to continue...")
        return

    # Display files
    table = Table(title="YAML Files")
    table.add_column("Filename", style="cyan")
    table.add_column("Size", style="green")
    table.add_column("Last Modified", style="yellow")

    for file in yaml_files:
        table.add_row(
            file.name, f"{file.stat().st_size} bytes", f"{file.stat().st_mtime}"
        )

    console.print(table)

    # TODO: Add file selection and loading


def _view_yaml(obj: Flock | FlockAgent):
    """View the YAML representation of an object.

    Args:
        obj: The object to view as YAML
    """
    yaml_str = obj.to_yaml()

    # Add file path information header if it's a Flock with component file paths
    if isinstance(obj, Flock) and hasattr(obj, "_component_file_paths"):
        has_file_paths = bool(getattr(obj, "_component_file_paths", {}))
        if has_file_paths:
            console.print(
                "[bold yellow]Note: This Flock contains components with file paths[/]"
            )

    # Display with syntax highlighting
    syntax = Syntax(
        yaml_str,
        "yaml",
        theme="monokai",
        line_numbers=True,
        code_width=100,
        word_wrap=True,
    )

    init_console()
    console.print(Panel("[bold green]YAML View[/]"), justify="center")
    console.print(syntax)

    # Show file path information if available
    if isinstance(obj, Flock):
        # Get registry for checking file paths
        try:
            from flock.core.flock_registry import get_registry

            registry = get_registry()

            if (
                hasattr(registry, "_component_file_paths")
                and registry._component_file_paths
            ):
                # Get component names in this Flock
                components = set()
                for agent in obj._agents.values():
                    if hasattr(agent, "module") and agent.module:
                        module_path = getattr(agent.module, "module_path", None)
                        if module_path:
                            components.add(module_path)

                # Show file paths for components in this Flock
                file_paths = []
                for component_name in components:
                    if component_name in registry._component_file_paths:
                        file_paths.append(
                            (
                                component_name,
                                registry._component_file_paths[component_name],
                            )
                        )

                if file_paths:
                    console.print("\n[bold cyan]Component File Paths:[/]")
                    table = Table()
                    table.add_column("Component", style="green")
                    table.add_column("File Path", style="yellow")

                    for component_name, file_path in file_paths:
                        table.add_row(component_name, file_path)

                    console.print(table)
        except ImportError:
            pass  # Skip if registry is not available


def _edit_yaml_directly(obj: Flock | FlockAgent) -> Flock | FlockAgent:
    """Edit the YAML representation directly using an external editor.

    Args:
        obj: The object to edit

    Returns:
        The updated object
    """
    # Convert to YAML
    yaml_str = obj.to_yaml()

    # Get file path information if it's a Flock
    component_file_paths = {}
    if isinstance(obj, Flock):
        try:
            from flock.core.flock_registry import get_registry

            registry = get_registry()

            if hasattr(registry, "_component_file_paths"):
                # Save the file paths to restore later
                component_file_paths = registry._component_file_paths.copy()
        except ImportError:
            pass

    # Create a temporary file
    with tempfile.NamedTemporaryFile(
        suffix=".yaml", mode="w+", delete=False
    ) as tmp:
        tmp.write(yaml_str)
        tmp_path = tmp.name

    try:
        # Determine which editor to use
        editor = os.environ.get(
            "EDITOR", "notepad" if os.name == "nt" else "nano"
        )

        # Open the editor
        console.print(
            f"\nOpening {editor} to edit YAML. Save and exit when done."
        )
        subprocess.call([editor, tmp_path])

        # Read updated YAML
        with open(tmp_path) as f:
            updated_yaml = f.read()

        # Parse back to object
        try:
            if isinstance(obj, Flock):
                updated_obj = Flock.from_yaml(updated_yaml)

                # Restore file path information
                if component_file_paths:
                    from flock.core.flock_registry import get_registry

                    registry = get_registry()

                    if not hasattr(registry, "_component_file_paths"):
                        registry._component_file_paths = {}

                    # Merge the updated registry with the saved file paths
                    for (
                        component_name,
                        file_path,
                    ) in component_file_paths.items():
                        if component_name in registry._components:
                            registry._component_file_paths[component_name] = (
                                file_path
                            )

                console.print("\n[green]✓[/] YAML parsed successfully!")
                return updated_obj
            elif isinstance(obj, FlockAgent):
                updated_obj = FlockAgent.from_yaml(updated_yaml)
                console.print("\n[green]✓[/] YAML parsed successfully!")
                return updated_obj
        except Exception as e:
            console.print(f"\n[bold red]Error parsing YAML:[/] {e!s}")
            console.print("\nKeeping original object.")
            return obj

    finally:
        # Clean up the temporary file
        try:
            os.unlink(tmp_path)
        except Exception:
            pass

    return obj


def _abstract_editor(obj: Flock | FlockAgent) -> Flock | FlockAgent:
    """Edit object using an abstract form-based editor.

    Args:
        obj: The object to edit

    Returns:
        The updated object
    """
    console.print("\n[yellow]Abstract visual editor not yet implemented.[/]")
    console.print("Will provide a form-based editor for each field.")

    # For now, just return the original object
    return obj


def _validate_yaml(obj: Flock | FlockAgent):
    """Validate the YAML representation of an object.

    Args:
        obj: The object to validate
    """
    try:
        yaml_str = obj.to_yaml()

        # Attempt to parse with PyYAML
        yaml.safe_load(yaml_str)

        # Attempt to deserialize back to object
        if isinstance(obj, Flock):
            Flock.from_yaml(yaml_str)
        elif isinstance(obj, FlockAgent):
            FlockAgent.from_yaml(yaml_str)

        console.print("\n[green]✓[/] YAML validation successful!")
    except Exception as e:
        console.print(f"\n[bold red]YAML validation failed:[/] {e!s}")


def _save_to_file(obj: Flock | FlockAgent):
    """Save object to a YAML file.

    Args:
        obj: The object to save
    """
    # Determine default filename based on object type
    if isinstance(obj, Flock):
        default_name = "my_flock.flock.yaml"
    elif isinstance(obj, FlockAgent):
        default_name = f"{obj.name}.agent.yaml"
    else:
        default_name = "unknown.yaml"

    # Get file path
    file_path = questionary.text(
        "Enter file path to save YAML:",
        default=default_name,
    ).ask()

    # Ensure the file has the correct extension
    if not file_path.endswith((".yaml", ".yml")):
        file_path += ".yaml"

    # Create directory if it doesn't exist
    save_path = Path(file_path)
    save_path.parent.mkdir(parents=True, exist_ok=True)

    # For Flock instances, ask about path_type
    path_type = "absolute"  # Default
    if isinstance(obj, Flock):
        path_type_choice = questionary.select(
            "How should file paths be formatted?",
            choices=[
                "absolute (full paths, best for local use)",
                "relative (relative paths, better for sharing)",
            ],
            default="absolute (full paths, best for local use)",
        ).ask()

        # Extract just the first word
        path_type = path_type_choice.split()[0]

        console.print(
            f"[bold]Path type selected: [green]{path_type}[/green][/bold]"
        )

    try:
        # Save to file with path_type for Flock instances
        if isinstance(obj, Flock):
            obj.to_yaml_file(file_path, path_type=path_type)
            console.print(
                f"\n[green]✓[/] Saved to {file_path} with {path_type} paths"
            )
        else:
            # For FlockAgent or other types, use the original method
            with open(file_path, "w") as f:
                f.write(obj.to_yaml())
            console.print(f"\n[green]✓[/] Saved to {file_path}")
    except Exception as e:
        console.print(f"\n[bold red]Error saving file:[/] {e!s}")

```

---

### 50. src\flock\config.py

- **File ID**: file_49
- **Type**: Code File
- **Line Count**: 51
- **Description**: File at src\flock\config.py
- **Dependencies**: None
- **Used By**: None

**Content**:
```
# flock/config.py
import os

from decouple import config

from flock.core.logging.telemetry import TelemetryConfig

cfg_file = os.path.expanduser(f"~/.flock/flock.cfg")


# -- Connection and External Service Configurations --
TEMPORAL_SERVER_URL = config("TEMPORAL_SERVER_URL", "localhost:7233")
DEFAULT_MODEL = config("DEFAULT_MODEL", "openai/gpt-4o")


# API Keys and related settings
TAVILY_API_KEY = config("TAVILY_API_KEY", "")
GITHUB_PAT = config("GITHUB_PAT", "")
GITHUB_REPO = config("GITHUB_REPO", "")
GITHUB_USERNAME = config("GITHUB_USERNAME", "")

# -- Debugging and Logging Configurations --
LOCAL_DEBUG = config("LOCAL_DEBUG", True)
LOG_LEVEL = config("LOG_LEVEL", "DEBUG")
LOGGING_DIR = config("LOGGING_DIR", "logs")

OTEL_SERVICE_NAME = config("OTL_SERVICE_NAME", "otel-flock")
JAEGER_ENDPOINT = config(
    "JAEGER_ENDPOINT", "http://localhost:14268/api/traces"
)  # Default gRPC endpoint for Jaeger
JAEGER_TRANSPORT = config(
    "JAEGER_TRANSPORT", "http"
).lower()  # Options: "grpc" or "http"
OTEL_SQL_DATABASE_NAME = config("OTEL_SQL_DATABASE", "flock_events.db")
OTEL_FILE_NAME = config("OTEL_FILE_NAME", "flock_events.jsonl")
OTEL_ENABLE_SQL: bool = config("OTEL_ENABLE_SQL", True) == "True"
OTEL_ENABLE_FILE: bool = config("OTEL_ENABLE_FILE", True) == "True"
OTEL_ENABLE_JAEGER: bool = config("OTEL_ENABLE_JAEGER", False) == "True"


TELEMETRY = TelemetryConfig(
    OTEL_SERVICE_NAME,
    JAEGER_ENDPOINT,
    JAEGER_TRANSPORT,
    LOGGING_DIR,
    OTEL_FILE_NAME,
    OTEL_SQL_DATABASE_NAME,
    OTEL_ENABLE_JAEGER,
    OTEL_ENABLE_FILE,
    OTEL_ENABLE_SQL,
)

```

---

### 51. src\flock\core\__init__.py

- **File ID**: file_50
- **Type**: Code File
- **Line Count**: 33
- **Description**: This module contains the core classes of the flock package.
- **Dependencies**: None
- **Used By**: None

**Content**:
```
"""This module contains the core classes of the flock package."""

from flock.core.context.context import FlockContext
from flock.core.flock import Flock
from flock.core.flock_agent import FlockAgent
from flock.core.flock_evaluator import FlockEvaluator, FlockEvaluatorConfig
from flock.core.flock_factory import FlockFactory
from flock.core.flock_module import FlockModule, FlockModuleConfig
from flock.core.flock_registry import (
    FlockRegistry,
    flock_callable,
    flock_component,
    flock_tool,
    flock_type,
    get_registry,
)

__all__ = [
    "Flock",
    "FlockAgent",
    "FlockContext",
    "FlockEvaluator",
    "FlockEvaluatorConfig",
    "FlockFactory",
    "FlockModule",
    "FlockModuleConfig",
    "FlockRegistry",
    "flock_callable",
    "flock_component",
    "flock_tool",
    "flock_type",
    "get_registry",
]

```

---

### 52. src\flock\core\api\__init__.py

- **File ID**: file_51
- **Type**: Code File
- **Line Count**: 11
- **Description**: Flock API Server components.
- **Dependencies**:
  - file_53
  - file_54
- **Used By**: None

**Content**:
```
# src/flock/core/api/__init__.py
"""Flock API Server components."""

from .main import FlockAPI
from .models import FlockAPIRequest, FlockAPIResponse

__all__ = [
    "FlockAPI",
    "FlockAPIRequest",
    "FlockAPIResponse",
]

```

---

### 53. src\flock\core\api\endpoints.py

- **File ID**: file_52
- **Type**: Code File
- **Line Count**: 322
- **Description**: FastAPI endpoints for the Flock API.
- **Dependencies**:
  - file_53
  - file_55
  - file_54
- **Used By**:
  - file_53

**Content**:
```
# src/flock/core/api/endpoints.py
"""FastAPI endpoints for the Flock API."""

import html  # For escaping
import uuid
from typing import TYPE_CHECKING  # Added Any for type hinting clarity

from fastapi import (
    APIRouter,
    BackgroundTasks,
    HTTPException,
    Request as FastAPIRequest,
)

# Import HTMLResponse for the UI form endpoint
from fastapi.responses import HTMLResponse

from flock.core.logging.logging import get_logger

# Import models and UI utils
from .models import (
    FlockAPIRequest,
    FlockAPIResponse,
    FlockBatchRequest,
    FlockBatchResponse,
)

# Import UI utils - assuming they are now in ui/utils.py

# Use TYPE_CHECKING to avoid circular imports for type hints
if TYPE_CHECKING:
    from flock.core.flock import Flock

    from .main import FlockAPI
    from .run_store import RunStore

logger = get_logger("api.endpoints")


# Factory function to create the router with dependencies
def create_api_router(flock_api: "FlockAPI") -> APIRouter:
    """Creates the APIRouter and defines endpoints, injecting dependencies."""
    router = APIRouter()
    # Get dependencies from the main FlockAPI instance passed in
    run_store: RunStore = flock_api.run_store
    flock_instance: Flock = flock_api.flock

    # --- API Endpoints ---
    @router.post("/run/flock", response_model=FlockAPIResponse, tags=["API"])
    async def run_flock_json(
        request: FlockAPIRequest, background_tasks: BackgroundTasks
    ):
        """Run a flock workflow starting with the specified agent (expects JSON)."""
        run_id = None
        try:
            run_id = str(uuid.uuid4())
            run_store.create_run(run_id)  # Use RunStore
            response = run_store.get_run(
                run_id
            )  # Get initial response from store

            processed_inputs = request.inputs if request.inputs else {}
            logger.info(
                f"API request: run flock '{request.agent_name}' (run_id: {run_id})",
                inputs=processed_inputs,
            )

            if request.async_run:
                logger.debug(
                    f"Running flock '{request.agent_name}' asynchronously (run_id: {run_id})"
                )
                # Call the helper method on the passed FlockAPI instance
                background_tasks.add_task(
                    flock_api._run_flock,
                    run_id,
                    request.agent_name,
                    processed_inputs,
                )
                run_store.update_run_status(run_id, "running")
                response.status = "running"  # Update local response copy too
            else:
                logger.debug(
                    f"Running flock '{request.agent_name}' synchronously (run_id: {run_id})"
                )
                # Call the helper method on the passed FlockAPI instance
                await flock_api._run_flock(
                    run_id, request.agent_name, processed_inputs
                )
                response = run_store.get_run(
                    run_id
                )  # Fetch updated status/result

            return response
        except ValueError as ve:
            logger.error(f"Value error starting run: {ve}")
            if run_id:
                run_store.update_run_status(run_id, "failed", str(ve))
            raise HTTPException(status_code=400, detail=str(ve))
        except Exception as e:
            error_msg = f"Internal server error: {type(e).__name__}"
            logger.error(f"Error starting run: {e!s}", exc_info=True)
            if run_id:
                run_store.update_run_status(run_id, "failed", error_msg)
            raise HTTPException(status_code=500, detail=error_msg)

    @router.post("/run/batch", response_model=FlockBatchResponse, tags=["API"])
    async def run_batch_json(
        request: FlockBatchRequest, background_tasks: BackgroundTasks
    ):
        """Run a batch of inputs through the flock workflow (expects JSON)."""
        batch_id = None
        try:
            # Validate agent exists
            if request.agent_name not in flock_instance.agents:
                raise ValueError(f"Agent '{request.agent_name}' not found")

            # Validate batch inputs
            if (
                isinstance(request.batch_inputs, list)
                and not request.batch_inputs
            ):
                raise ValueError("Batch inputs list cannot be empty")

            batch_id = str(uuid.uuid4())
            run_store.create_batch(batch_id)  # Use RunStore
            response = run_store.get_batch(
                batch_id
            )  # Get initial response from store

            # Log batch size for monitoring
            batch_size = (
                len(request.batch_inputs)
                if isinstance(request.batch_inputs, list)
                else "CSV/DataFrame"
            )
            logger.info(
                f"API request: run batch with '{request.agent_name}' (batch_id: {batch_id})",
                batch_size=batch_size,
            )

            # Always run batch processing asynchronously
            logger.debug(
                f"Running batch with '{request.agent_name}' asynchronously (batch_id: {batch_id})"
            )
            # Call the helper method on the passed FlockAPI instance
            background_tasks.add_task(
                flock_api._run_batch,
                batch_id,
                request,
            )
            run_store.update_batch_status(batch_id, "running")
            response.status = "running"  # Update local response copy too

            return response
        except ValueError as ve:
            error_msg = f"Value error starting batch: {ve}"
            logger.error(error_msg)
            if batch_id:
                run_store.update_batch_status(batch_id, "failed", str(ve))
            raise HTTPException(status_code=400, detail=str(ve))
        except Exception as e:
            error_msg = f"Internal server error: {type(e).__name__}: {e!s}"
            logger.error(error_msg, exc_info=True)
            if batch_id:
                run_store.update_batch_status(batch_id, "failed", error_msg)
            raise HTTPException(status_code=500, detail=error_msg)

    @router.get("/run/{run_id}", response_model=FlockAPIResponse, tags=["API"])
    async def get_run_status(run_id: str):
        """Get the status of a specific run."""
        logger.debug(f"API request: get status for run_id: {run_id}")
        run_data = run_store.get_run(run_id)
        if not run_data:
            logger.warning(f"Run ID not found: {run_id}")
            raise HTTPException(status_code=404, detail="Run not found")
        return run_data

    @router.get(
        "/batch/{batch_id}", response_model=FlockBatchResponse, tags=["API"]
    )
    async def get_batch_status(batch_id: str):
        """Get the status of a specific batch run.

        Returns details including:
        - Total number of items in the batch
        - Number of completed items
        - Percentage of completion
        - Any partial results available (for running batches)
        - Complete results (for completed batches)
        """
        logger.debug(f"API request: get status for batch_id: {batch_id}")
        batch_data = run_store.get_batch(batch_id)
        if not batch_data:
            logger.warning(f"Batch ID not found: {batch_id}")
            raise HTTPException(status_code=404, detail="Batch not found")

        # Add useful info for client display
        extra_info = {
            "status": batch_data.status,
            "completed_items": batch_data.completed_items,
            "total_items": batch_data.total_items,
            "progress_percentage": round(batch_data.progress_percentage, 1),
            "has_partial_results": len(batch_data.results) > 0
            and batch_data.status == "running",
            "has_error": batch_data.error is not None,
        }
        logger.debug(f"Returning batch status: {extra_info}")

        return batch_data

    @router.get("/agents", tags=["API"])
    async def list_agents():
        """List all available agents."""
        logger.debug("API request: list agents")
        # Access flock instance via factory closure
        agents_list = [
            {"name": agent.name, "description": agent.description or agent.name}
            for agent in flock_instance.agents.values()
        ]
        return {"agents": agents_list}

    # --- UI Form Endpoint ---
    @router.post("/ui/run-agent-form", response_class=HTMLResponse, tags=["UI"])
    async def run_flock_form(fastapi_req: FastAPIRequest):
        """Endpoint to handle form submissions from the UI."""
        run_id = None
        try:
            form_data = await fastapi_req.form()
            agent_name = form_data.get("agent_name")
            if not agent_name:
                logger.warning("UI form submission missing agent_name")
                return HTMLResponse(
                    '<div id="result-content" class="error-message">Error: Agent name not provided.</div>',
                    status_code=400,
                )

            logger.info(f"UI Form submission for agent: {agent_name}")
            form_inputs = {}
            # Access flock instance via factory closure
            agent_def = flock_instance.agents.get(agent_name)
            # Use helper from flock_api instance for parsing
            defined_input_fields = (
                flock_api._parse_input_spec(agent_def.input or "")
                if agent_def
                else []
            )

            for key, value in form_data.items():
                if key.startswith("inputs."):
                    form_inputs[key[len("inputs.") :]] = value
            for field in defined_input_fields:  # Handle checkboxes
                if (
                    field["html_type"] == "checkbox"
                    and field["name"] not in form_inputs
                ):
                    form_inputs[field["name"]] = False
                elif (
                    field["html_type"] == "checkbox"
                    and field["name"] in form_inputs
                ):
                    form_inputs[field["name"]] = True

            logger.debug(f"Parsed form inputs for UI run: {form_inputs}")
            run_id = str(uuid.uuid4())
            run_store.create_run(run_id)
            logger.debug(
                f"Running flock '{agent_name}' synchronously from UI (run_id: {run_id})"
            )

            # Call helper method on flock_api instance
            await flock_api._run_flock(run_id, agent_name, form_inputs)

            final_status = run_store.get_run(run_id)
            if final_status and final_status.status == "completed":
                # Use helper from flock_api instance for formatting
                formatted_html = flock_api._format_result_to_html(
                    final_status.result
                )
                logger.info(f"UI run completed successfully (run_id: {run_id})")
                return HTMLResponse(
                    f"<div id='result-content'>{formatted_html}</div>"
                )  # Wrap in target div
            elif final_status and final_status.status == "failed":
                logger.error(
                    f"UI run failed (run_id: {run_id}): {final_status.error}"
                )
                error_msg = html.escape(final_status.error or "Unknown error")
                return HTMLResponse(
                    f"<div id='result-content' class='error-message'>Run Failed: {error_msg}</div>",
                    status_code=500,
                )
            else:
                status_str = (
                    final_status.status if final_status else "Not Found"
                )
                logger.warning(
                    f"UI run {run_id} ended in unexpected state: {status_str}"
                )
                return HTMLResponse(
                    f"<div id='result-content' class='error-message'>Run ended unexpectedly. Status: {status_str}</div>",
                    status_code=500,
                )

        except ValueError as ve:
            logger.error(f"Value error processing UI form run: {ve}")
            if run_id:
                run_store.update_run_status(run_id, "failed", str(ve))
            return HTMLResponse(
                f"<div id='result-content' class='error-message'>Error: {html.escape(str(ve))}</div>",
                status_code=400,
            )
        except Exception as e:
            error_msg = f"Internal server error: {type(e).__name__}"
            logger.error(f"Error processing UI form run: {e!s}", exc_info=True)
            if run_id:
                run_store.update_run_status(run_id, "failed", error_msg)
            return HTMLResponse(
                f"<div id='result-content' class='error-message'>{html.escape(error_msg)}</div>",
                status_code=500,
            )

    return router

```

---

### 54. src\flock\core\api\main.py

- **File ID**: file_53
- **Type**: Code File
- **Line Count**: 451
- **Description**: Main Flock API server class and setup.
- **Dependencies**:
  - file_58
  - file_55
  - file_52
  - file_59
- **Used By**:
  - file_52
  - file_51

**Content**:
```
# src/flock/core/api/main.py
"""Main Flock API server class and setup."""

from typing import Any

import uvicorn
from fastapi import FastAPI
from fastapi.responses import RedirectResponse

# Flock core imports
from flock.core.api.models import FlockBatchRequest
from flock.core.flock import Flock
from flock.core.logging.logging import get_logger

from .endpoints import create_api_router

# Import components from the api package
from .run_store import RunStore
from .ui.routes import FASTHTML_AVAILABLE, create_ui_app
from .ui.utils import format_result_to_html, parse_input_spec  # Import UI utils

logger = get_logger("api.main")


class FlockAPI:
    """Coordinates the Flock API server, including endpoints and UI."""

    def __init__(self, flock: Flock):
        self.flock = flock
        self.app = FastAPI(title="Flock API")
        self.run_store = RunStore()  # Create the run store instance
        self._setup_routes()

    def _setup_routes(self):
        """Includes API routers."""
        # Create and include the API router, passing self
        api_router = create_api_router(self)
        self.app.include_router(api_router)

        # Root redirect (if UI is enabled later) will be added in start()

    # --- Core Execution Helper Methods ---
    # These remain here as they need access to self.flock and self.run_store

    async def _run_agent(
        self, run_id: str, agent_name: str, inputs: dict[str, Any]
    ):
        """Executes a single agent run (internal helper)."""
        try:
            if agent_name not in self.flock.agents:
                raise ValueError(f"Agent '{agent_name}' not found")
            agent = self.flock.agents[agent_name]
            # Type conversion (remains important)
            typed_inputs = self._type_convert_inputs(agent_name, inputs)

            logger.debug(
                f"Executing single agent '{agent_name}' (run_id: {run_id})",
                inputs=typed_inputs,
            )
            result = await agent.run_async(typed_inputs)
            logger.info(
                f"Single agent '{agent_name}' completed (run_id: {run_id})"
            )

            # Use RunStore to update
            self.run_store.update_run_result(run_id, result)

        except Exception as e:
            logger.error(
                f"Error in single agent run {run_id} ('{agent_name}'): {e!s}",
                exc_info=True,
            )
            # Update store status
            self.run_store.update_run_status(run_id, "failed", str(e))
            raise  # Re-raise for the endpoint handler

    async def _run_flock(
        self, run_id: str, agent_name: str, inputs: dict[str, Any]
    ):
        """Executes a flock workflow run (internal helper)."""
        try:
            if agent_name not in self.flock.agents:
                raise ValueError(f"Starting agent '{agent_name}' not found")

            # Type conversion
            typed_inputs = self._type_convert_inputs(agent_name, inputs)

            logger.debug(
                f"Executing flock workflow starting with '{agent_name}' (run_id: {run_id})",
                inputs=typed_inputs,
            )
            result = await self.flock.run_async(
                start_agent=agent_name, input=typed_inputs
            )
            # Result is potentially a Box object

            # Use RunStore to update
            self.run_store.update_run_result(run_id, result)

            # Log using the local result variable
            final_agent_name = (
                result.get("agent_name", "N/A") if result is not None else "N/A"
            )
            logger.info(
                f"Flock workflow completed (run_id: {run_id})",
                final_agent=final_agent_name,
            )

        except Exception as e:
            logger.error(
                f"Error in flock run {run_id} (started with '{agent_name}'): {e!s}",
                exc_info=True,
            )
            # Update store status
            self.run_store.update_run_status(run_id, "failed", str(e))
            raise  # Re-raise for the endpoint handler

    async def _run_batch(self, batch_id: str, request: "FlockBatchRequest"):
        """Executes a batch of runs (internal helper)."""
        try:
            if request.agent_name not in self.flock.agents:
                raise ValueError(f"Agent '{request.agent_name}' not found")

            logger.debug(
                f"Executing batch run starting with '{request.agent_name}' (batch_id: {batch_id})",
                batch_size=len(request.batch_inputs)
                if isinstance(request.batch_inputs, list)
                else "CSV",
            )

            # Import the thread pool executor here to avoid circular imports
            import asyncio
            import threading
            from concurrent.futures import ThreadPoolExecutor

            # Define a synchronous function to run the batch processing
            def run_batch_sync():
                # Use a new event loop for the batch processing
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                try:
                    # Set the total number of batch items if possible
                    batch_size = (
                        len(request.batch_inputs)
                        if isinstance(request.batch_inputs, list)
                        else 0
                    )
                    if batch_size > 0:
                        # Directly call the store method - no need for asyncio here
                        # since we're already in a separate thread
                        self.run_store.set_batch_total_items(
                            batch_id, batch_size
                        )

                    # Custom progress tracking wrapper
                    class ProgressTracker:
                        def __init__(self, store, batch_id, total_size):
                            self.store = store
                            self.batch_id = batch_id
                            self.current_count = 0
                            self.total_size = total_size
                            self._lock = threading.Lock()
                            self.partial_results = []

                        def increment(self, result=None):
                            with self._lock:
                                self.current_count += 1
                                if result is not None:
                                    # Store partial result
                                    self.partial_results.append(result)

                                # Directly call the store method - no need for asyncio here
                                # since we're already in a separate thread
                                try:
                                    self.store.update_batch_progress(
                                        self.batch_id,
                                        self.current_count,
                                        self.partial_results,
                                    )
                                except Exception as e:
                                    logger.error(
                                        f"Error updating progress: {e}"
                                    )
                                return self.current_count

                    # Create a progress tracker
                    progress_tracker = ProgressTracker(
                        self.run_store, batch_id, batch_size
                    )

                    # Define a custom worker that reports progress
                    async def progress_aware_worker(index, item_inputs):
                        try:
                            result = await self.flock.run_async(
                                start_agent=request.agent_name,
                                input=item_inputs,
                                box_result=request.box_results,
                            )
                            # Report progress after each item
                            progress_tracker.increment(result)
                            return result
                        except Exception as e:
                            logger.error(
                                f"Error processing batch item {index}: {e}"
                            )
                            progress_tracker.increment(
                                e if request.return_errors else None
                            )
                            if request.return_errors:
                                return e
                            return None

                    # Process the batch items with progress tracking
                    batch_inputs = request.batch_inputs
                    if isinstance(batch_inputs, list):
                        # Process list of inputs with progress tracking
                        tasks = []
                        for i, item_inputs in enumerate(batch_inputs):
                            # Combine with static inputs if provided
                            full_inputs = {
                                **(request.static_inputs or {}),
                                **item_inputs,
                            }
                            tasks.append(progress_aware_worker(i, full_inputs))

                        # Run all tasks
                        if request.parallel and request.max_workers > 1:
                            # Run in parallel with semaphore for max_workers
                            semaphore = asyncio.Semaphore(request.max_workers)

                            async def bounded_worker(i, inputs):
                                async with semaphore:
                                    return await progress_aware_worker(
                                        i, inputs
                                    )

                            bounded_tasks = []
                            for i, item_inputs in enumerate(batch_inputs):
                                full_inputs = {
                                    **(request.static_inputs or {}),
                                    **item_inputs,
                                }
                                bounded_tasks.append(
                                    bounded_worker(i, full_inputs)
                                )

                            results = loop.run_until_complete(
                                asyncio.gather(*bounded_tasks)
                            )
                        else:
                            # Run sequentially
                            results = []
                            for i, item_inputs in enumerate(batch_inputs):
                                full_inputs = {
                                    **(request.static_inputs or {}),
                                    **item_inputs,
                                }
                                result = loop.run_until_complete(
                                    progress_aware_worker(i, full_inputs)
                                )
                                results.append(result)
                    else:
                        # Let the original run_batch_async handle DataFrame or CSV
                        results = loop.run_until_complete(
                            self.flock.run_batch_async(
                                start_agent=request.agent_name,
                                batch_inputs=request.batch_inputs,
                                input_mapping=request.input_mapping,
                                static_inputs=request.static_inputs,
                                parallel=request.parallel,
                                max_workers=request.max_workers,
                                use_temporal=request.use_temporal,
                                box_results=request.box_results,
                                return_errors=request.return_errors,
                                silent_mode=request.silent_mode,
                                write_to_csv=request.write_to_csv,
                            )
                        )

                    # Update progress one last time with final count
                    if results:
                        progress_tracker.current_count = len(results)
                        self.run_store.update_batch_progress(
                            batch_id,
                            len(results),
                            results,  # Include all results as partial results
                        )

                    # Update store with results from this thread
                    self.run_store.update_batch_result(batch_id, results)

                    logger.info(
                        f"Batch run completed (batch_id: {batch_id})",
                        num_results=len(results),
                    )
                    return results
                except Exception as e:
                    logger.error(
                        f"Error in batch run {batch_id} (started with '{request.agent_name}'): {e!s}",
                        exc_info=True,
                    )
                    # Update store status
                    self.run_store.update_batch_status(
                        batch_id, "failed", str(e)
                    )
                    return None
                finally:
                    loop.close()

            # Run the batch processing in a thread pool
            try:
                loop = asyncio.get_running_loop()
                with ThreadPoolExecutor() as pool:
                    await loop.run_in_executor(pool, run_batch_sync)
            except Exception as e:
                error_msg = f"Error running batch in thread pool: {e!s}"
                logger.error(error_msg, exc_info=True)
                self.run_store.update_batch_status(
                    batch_id, "failed", error_msg
                )

        except Exception as e:
            logger.error(
                f"Error setting up batch run {batch_id} (started with '{request.agent_name}'): {e!s}",
                exc_info=True,
            )
            # Update store status
            self.run_store.update_batch_status(batch_id, "failed", str(e))
            raise  # Re-raise for the endpoint handler

    # --- UI Helper Methods (kept here as they are called by endpoints via self) ---

    def _parse_input_spec(self, input_spec: str) -> list[dict[str, str]]:
        """Parses an agent input string into a list of field definitions."""
        # Use the implementation moved to ui.utils
        return parse_input_spec(input_spec)

    def _format_result_to_html(self, data: Any) -> str:
        """Recursively formats a Python object into an HTML string."""
        # Use the implementation moved to ui.utils
        return format_result_to_html(data)

    def _type_convert_inputs(
        self, agent_name: str, inputs: dict[str, Any]
    ) -> dict[str, Any]:
        """Converts input values (esp. from forms) to expected Python types."""
        typed_inputs = {}
        agent_def = self.flock.agents.get(agent_name)
        if not agent_def or not agent_def.input:
            return inputs  # Return original if no spec

        parsed_fields = self._parse_input_spec(agent_def.input)
        field_types = {f["name"]: f["type"] for f in parsed_fields}

        for k, v in inputs.items():
            target_type = field_types.get(k)
            if target_type and target_type.startswith("bool"):
                typed_inputs[k] = (
                    str(v).lower() in ["true", "on", "1", "yes"]
                    if isinstance(v, str)
                    else bool(v)
                )
            elif target_type and target_type.startswith("int"):
                try:
                    typed_inputs[k] = int(v)
                except (ValueError, TypeError):
                    logger.warning(
                        f"Could not convert input '{k}' value '{v}' to int for agent '{agent_name}'"
                    )
                    typed_inputs[k] = v
            elif target_type and target_type.startswith("float"):
                try:
                    typed_inputs[k] = float(v)
                except (ValueError, TypeError):
                    logger.warning(
                        f"Could not convert input '{k}' value '{v}' to float for agent '{agent_name}'"
                    )
                    typed_inputs[k] = v
            # TODO: Add list/dict parsing (e.g., json.loads) if needed
            else:
                typed_inputs[k] = v  # Assume string or already correct type
        return typed_inputs

    # --- Server Start/Stop ---

    def start(
        self,
        host: str = "0.0.0.0",
        port: int = 8344,
        server_name: str = "Flock API",
        create_ui: bool = False,
    ):
        """Start the API server, optionally creating and mounting a FastHTML UI."""
        if create_ui:
            if not FASTHTML_AVAILABLE:
                logger.error(
                    "FastHTML not installed. Cannot create UI. Running API only."
                )
            else:
                logger.info("Attempting to create and mount FastHTML UI at /ui")
                try:
                    # Pass self (FlockAPI instance) to the UI creation function
                    # It needs access to self.flock and self._parse_input_spec
                    fh_app = create_ui_app(
                        self,
                        api_host=host,
                        api_port=port,
                        server_name=server_name,
                    )
                    self.app.mount("/ui", fh_app, name="ui")
                    logger.info("FastHTML UI mounted successfully.")

                    # Add root redirect only if UI was successfully mounted
                    @self.app.get(
                        "/",
                        include_in_schema=False,
                        response_class=RedirectResponse,
                    )
                    async def root_redirect():
                        logger.debug("Redirecting / to /ui/")
                        return "/ui/"

                except ImportError as e:
                    logger.error(
                        f"Could not create UI due to import error: {e}. Running API only."
                    )
                except Exception as e:
                    logger.error(
                        f"An error occurred setting up the UI: {e}. Running API only.",
                        exc_info=True,
                    )

        logger.info(f"Starting API server on http://{host}:{port}")
        if (
            create_ui
            and FASTHTML_AVAILABLE
            and any(
                m.path == "/ui" for m in self.app.routes if hasattr(m, "path")
            )
        ):
            logger.info(f"UI available at http://{host}:{port}/ui/")

        uvicorn.run(self.app, host=host, port=port)

    async def stop(self):
        """Stop the API server."""
        logger.info("Stopping API server (cleanup if necessary)")
        pass  # Add cleanup logic if needed


# --- End of file ---

```

---

### 55. src\flock\core\api\models.py

- **File ID**: file_54
- **Type**: Code File
- **Line Count**: 97
- **Description**: Pydantic models for the Flock API.
- **Dependencies**: None
- **Used By**:
  - file_55
  - file_52
  - file_51

**Content**:
```
# src/flock/core/api/models.py
"""Pydantic models for the Flock API."""

from datetime import datetime
from typing import Any

from pydantic import BaseModel, Field


class FlockAPIRequest(BaseModel):
    """Request model for running an agent via JSON API."""

    agent_name: str = Field(..., description="Name of the agent to run")
    inputs: dict[str, Any] = Field(
        default_factory=dict, description="Input data for the agent"
    )
    async_run: bool = Field(
        default=False, description="Whether to run asynchronously"
    )


class FlockAPIResponse(BaseModel):
    """Response model for API run requests."""

    run_id: str = Field(..., description="Unique ID for this run")
    status: str = Field(..., description="Status of the run")
    result: dict[str, Any] | None = Field(
        None, description="Run result if completed"
    )
    started_at: datetime = Field(..., description="When the run started")
    completed_at: datetime | None = Field(
        None, description="When the run completed"
    )
    error: str | None = Field(None, description="Error message if failed")


class FlockBatchRequest(BaseModel):
    """Request model for batch processing via JSON API."""

    agent_name: str = Field(..., description="Name of the agent to run")
    batch_inputs: list[dict[str, Any]] | str = Field(
        ..., description="List of input dictionaries or path to CSV file"
    )
    input_mapping: dict[str, str] | None = Field(
        None, description="Maps DataFrame/CSV column names to agent input keys"
    )
    static_inputs: dict[str, Any] | None = Field(
        None, description="Inputs constant across all batch runs"
    )
    parallel: bool = Field(
        default=True, description="Whether to run jobs in parallel"
    )
    max_workers: int = Field(
        default=5, description="Max concurrent workers for parallel runs"
    )
    use_temporal: bool | None = Field(
        None, description="Override Flock's enable_temporal setting"
    )
    box_results: bool = Field(
        default=True, description="Wrap results in Box objects"
    )
    return_errors: bool = Field(
        default=False, description="Return Exception objects for failed runs"
    )
    silent_mode: bool = Field(
        default=True, description="Suppress output and show progress bar"
    )
    write_to_csv: str | None = Field(
        None, description="Path to save results as CSV file"
    )


class FlockBatchResponse(BaseModel):
    """Response model for batch processing requests."""

    batch_id: str = Field(..., description="Unique ID for this batch run")
    status: str = Field(..., description="Status of the batch run")
    results: list[Any] = Field(
        default_factory=list,
        description="List of results from batch processing",
    )
    started_at: datetime = Field(..., description="When the batch run started")
    completed_at: datetime | None = Field(
        None, description="When the batch run completed"
    )
    error: str | None = Field(None, description="Error message if failed")

    # Additional fields for batch progress tracking
    total_items: int = Field(
        0, description="Total number of items in the batch"
    )
    completed_items: int = Field(
        0, description="Number of completed items in the batch"
    )
    progress_percentage: float = Field(
        0.0, description="Percentage of completion (0-100)"
    )

```

---

### 56. src\flock\core\api\run_store.py

- **File ID**: file_55
- **Type**: Code File
- **Line Count**: 224
- **Description**: Manages the state of active and completed Flock runs.
- **Dependencies**:
  - file_54
- **Used By**:
  - file_53
  - file_52

**Content**:
```
# src/flock/core/api/run_store.py
"""Manages the state of active and completed Flock runs."""

import threading
from datetime import datetime
from typing import Any

from flock.core.logging.logging import get_logger

from .models import (  # Import from the models file
    FlockAPIResponse,
    FlockBatchResponse,
)

logger = get_logger("api.run_store")


class RunStore:
    """Stores and manages the state of Flock runs."""

    def __init__(self):
        self._runs: dict[str, FlockAPIResponse] = {}
        self._batches: dict[str, FlockBatchResponse] = {}
        self._lock = threading.Lock()  # Basic lock for thread safety

    def create_run(self, run_id: str) -> FlockAPIResponse:
        """Creates a new run record with 'starting' status."""
        with self._lock:
            if run_id in self._runs:
                logger.warning(f"Run ID {run_id} already exists. Overwriting.")
            response = FlockAPIResponse(
                run_id=run_id, status="starting", started_at=datetime.now()
            )
            self._runs[run_id] = response
            logger.debug(f"Created run record for run_id: {run_id}")
            return response

    def get_run(self, run_id: str) -> FlockAPIResponse | None:
        """Gets the status of a run."""
        with self._lock:
            return self._runs.get(run_id)

    def update_run_status(
        self, run_id: str, status: str, error: str | None = None
    ):
        """Updates the status and potentially error of a run."""
        with self._lock:
            if run_id in self._runs:
                self._runs[run_id].status = status
                if error:
                    self._runs[run_id].error = error
                if status in ["completed", "failed"]:
                    self._runs[run_id].completed_at = datetime.now()
                logger.debug(f"Updated status for run_id {run_id} to {status}")
            else:
                logger.warning(
                    f"Attempted to update status for non-existent run_id: {run_id}"
                )

    def update_run_result(self, run_id: str, result: dict):
        """Updates the result of a completed run."""
        with self._lock:
            if run_id in self._runs:
                # Ensure result is serializable (e.g., convert Box)
                final_result = (
                    dict(result) if hasattr(result, "to_dict") else result
                )
                self._runs[run_id].result = final_result
                self._runs[run_id].status = "completed"
                self._runs[run_id].completed_at = datetime.now()
                logger.debug(f"Updated result for completed run_id: {run_id}")
            else:
                logger.warning(
                    f"Attempted to update result for non-existent run_id: {run_id}"
                )

    def create_batch(self, batch_id: str) -> FlockBatchResponse:
        """Creates a new batch record with 'starting' status."""
        with self._lock:
            if batch_id in self._batches:
                logger.warning(
                    f"Batch ID {batch_id} already exists. Overwriting."
                )
            response = FlockBatchResponse(
                batch_id=batch_id,
                status="starting",
                results=[],
                started_at=datetime.now(),
                total_items=0,
                completed_items=0,
                progress_percentage=0.0,
            )
            self._batches[batch_id] = response
            logger.debug(f"Created batch record for batch_id: {batch_id}")
            return response

    def get_batch(self, batch_id: str) -> FlockBatchResponse | None:
        """Gets the status of a batch run."""
        with self._lock:
            return self._batches.get(batch_id)

    def update_batch_status(
        self, batch_id: str, status: str, error: str | None = None
    ):
        """Updates the status and potentially error of a batch run."""
        with self._lock:
            if batch_id in self._batches:
                self._batches[batch_id].status = status
                if error:
                    self._batches[batch_id].error = error
                if status in ["completed", "failed"]:
                    self._batches[batch_id].completed_at = datetime.now()
                    # When completed, ensure progress is 100%
                    if (
                        status == "completed"
                        and self._batches[batch_id].total_items > 0
                    ):
                        self._batches[batch_id].completed_items = self._batches[
                            batch_id
                        ].total_items
                        self._batches[batch_id].progress_percentage = 100.0
                logger.debug(
                    f"Updated status for batch_id {batch_id} to {status}"
                )
            else:
                logger.warning(
                    f"Attempted to update status for non-existent batch_id: {batch_id}"
                )

    def update_batch_result(self, batch_id: str, results: list[Any]):
        """Updates the results of a completed batch run."""
        with self._lock:
            if batch_id in self._batches:
                # Ensure results are serializable
                final_results = [
                    dict(r) if hasattr(r, "to_dict") else r for r in results
                ]
                self._batches[batch_id].results = final_results
                self._batches[batch_id].status = "completed"
                self._batches[batch_id].completed_at = datetime.now()

                # Update progress tracking
                self._batches[batch_id].completed_items = len(final_results)
                self._batches[batch_id].total_items = len(final_results)
                self._batches[batch_id].progress_percentage = 100.0

                logger.debug(
                    f"Updated results for completed batch_id: {batch_id}"
                )
            else:
                logger.warning(
                    f"Attempted to update results for non-existent batch_id: {batch_id}"
                )

    def set_batch_total_items(self, batch_id: str, total_items: int):
        """Sets the total number of items in a batch."""
        try:
            with self._lock:
                if batch_id in self._batches:
                    self._batches[batch_id].total_items = total_items
                    # Recalculate percentage
                    if total_items > 0:
                        self._batches[batch_id].progress_percentage = (
                            self._batches[batch_id].completed_items
                            / total_items
                            * 100.0
                        )
                    logger.debug(
                        f"Set total_items for batch_id {batch_id} to {total_items}"
                    )
                else:
                    logger.warning(
                        f"Attempted to set total_items for non-existent batch_id: {batch_id}"
                    )
        except Exception as e:
            logger.error(f"Error setting batch total items: {e}", exc_info=True)

    def update_batch_progress(
        self,
        batch_id: str,
        completed_items: int,
        partial_results: list[Any] = None,
    ):
        """Updates the progress of a batch run and optionally adds partial results.

        Args:
            batch_id: The ID of the batch to update
            completed_items: The number of items that have been completed
            partial_results: Optional list of results for completed items to add to the batch
        """
        try:
            with self._lock:
                if batch_id in self._batches:
                    self._batches[batch_id].completed_items = completed_items

                    # Calculate percentage if we have a total
                    if self._batches[batch_id].total_items > 0:
                        self._batches[batch_id].progress_percentage = (
                            completed_items
                            / self._batches[batch_id].total_items
                            * 100.0
                        )

                    # Add partial results if provided
                    if partial_results:
                        # Ensure results are serializable
                        final_results = [
                            dict(r) if hasattr(r, "to_dict") else r
                            for r in partial_results
                        ]
                        self._batches[batch_id].results = final_results

                    logger.debug(
                        f"Updated progress for batch_id {batch_id}: {completed_items}/{self._batches[batch_id].total_items} "
                        f"({self._batches[batch_id].progress_percentage:.1f}%)"
                    )
                else:
                    logger.warning(
                        f"Attempted to update progress for non-existent batch_id: {batch_id}"
                    )
        except Exception as e:
            logger.error(f"Error updating batch progress: {e}", exc_info=True)

    # Add methods for cleanup, persistence, etc. later

```

---

### 57. src\flock\core\api\runner.py

- **File ID**: file_56
- **Type**: Code File
- **Line Count**: 38
- **Description**: Provides functionality to start the Flock API server.
- **Dependencies**: None
- **Used By**: None

**Content**:
```
# src/flock/api/runner.py
"""Provides functionality to start the Flock API server."""

from typing import TYPE_CHECKING

from flock.core.logging.logging import get_logger

if TYPE_CHECKING:
    from flock.core.flock import Flock

logger = get_logger("api.runner")


def start_flock_api(
    flock: "Flock",
    host: str = "127.0.0.1",
    port: int = 8344,
    server_name: str = "Flock API",
    create_ui: bool = False,
) -> None:
    """Start a REST API server for the given Flock instance."""
    try:
        # Import API class locally to avoid making it a hard dependency for core flock
        from flock.core.api import FlockAPI
    except ImportError:
        logger.error(
            "API components not found. Cannot start API. "
            "Ensure 'fastapi' and 'uvicorn' are installed."
        )
        return

    logger.info(
        f"Preparing to start API server for Flock '{flock.name}' on {host}:{port} {'with UI' if create_ui else 'without UI'}"
    )
    api_instance = FlockAPI(flock)  # Pass the Flock instance to the API
    api_instance.start(
        host=host, port=port, server_name=server_name, create_ui=create_ui
    )

```

---

### 58. src\flock\core\api\ui\__init__.py

- **File ID**: file_57
- **Type**: Code File
- **Line Count**: 0
- **Description**: File at src\flock\core\api\ui\__init__.py
- **Dependencies**: None
- **Used By**: None

**Content**:
```

```

---

### 59. src\flock\core\api\ui\routes.py

- **File ID**: file_58
- **Type**: Code File
- **Line Count**: 271
- **Description**: FastHTML UI routes for the Flock API.
- **Dependencies**: None
- **Used By**:
  - file_53

**Content**:
```
# src/flock/core/api/ui/routes.py
"""FastHTML UI routes for the Flock API."""

from typing import TYPE_CHECKING

# --- Conditional FastHTML Imports ---
try:
    import httpx
    from fasthtml.common import *

    # Import Form explicitly with an alias to avoid collisions
    from fasthtml.common import Form as FHForm

    FASTHTML_AVAILABLE = True
except ImportError:
    FASTHTML_AVAILABLE = False

    # Define necessary dummies if not available
    class Request:
        pass

    class Titled:
        pass

    class Div:
        pass

    class H1:
        pass

    class P:
        pass

    class H2:
        pass

    class Pre:
        pass

    class Code:
        pass

    class Label:
        pass

    class Select:
        pass

    class Option:
        pass

    class FHForm:
        pass  # Dummy alias if not available

    class Button:
        pass

    class Span:
        pass

    class Script:
        pass

    class Style:
        pass

    class Hidden:
        pass

    class Textarea:
        pass

    class Input:
        pass

    def fast_app():
        return None, None

    def picolink():
        return None
# ------------------------------------

# Use TYPE_CHECKING to avoid circular import errors for type hints
if TYPE_CHECKING:
    from flock.core.api.main import FlockAPI

# Import logger and utils needed by UI routes
from flock.core.logging.logging import get_logger

logger = get_logger("api.ui")


def create_ui_app(
    flock_api_instance: "FlockAPI",
    api_host: str,
    api_port: int,
    server_name: str,
) -> Any:
    """Creates and configures the FastHTML application and its routes."""
    if not FASTHTML_AVAILABLE:
        raise ImportError("FastHTML is not installed. Cannot create UI.")
    logger.debug("Creating FastHTML application instance for UI")

    # Use the passed FlockAPI instance to access necessary data/methods
    flock_instance = flock_api_instance.flock
    parse_input_spec_func = (
        flock_api_instance._parse_input_spec
    )  # Get reference to parser

    fh_app, fh_rt = fast_app(
        hdrs=(
            Script(src="https://unpkg.com/htmx.org@1.9.10/dist/htmx.min.js"),
            picolink,  # Pass directly
            Style("""
            body { padding: 20px; max-width: 800px; margin: auto; font-family: sans-serif; }
            label { display: block; margin-top: 1rem; font-weight: bold;}
            input, select, textarea { width: 100%; margin-top: 0.25rem; padding: 0.5rem; border: 1px solid #ccc; border-radius: 4px; box-sizing: border-box; }
            input[type=checkbox] { width: auto; margin-right: 0.5rem; vertical-align: middle; }
            label[for^=input_] { font-weight: normal; display: inline; margin-top: 0;} /* Style for checkbox labels */
            button[type=submit] { margin-top: 1.5rem; padding: 0.75rem 1.5rem; background-color: #007bff; color: white; border: none; border-radius: 4px; cursor: pointer; font-size: 1rem;}
            button[type=submit]:hover { background-color: #0056b3; }
            #result-area { margin-top: 2rem; background-color: #f8f9fa; padding: 15px; border: 1px solid #dee2e6; border-radius: 5px; white-space: pre-wrap; word-wrap: break-word; font-family: monospace; }
            .htmx-indicator { display: none; margin-left: 10px; font-style: italic; color: #6c757d; }
            .htmx-request .htmx-indicator { display: inline; }
            .htmx-request.htmx-indicator { display: inline; }
            .error-message { color: #721c24; margin-top: 10px; font-weight: bold; background-color: #f8d7da; border: 1px solid #f5c6cb; padding: 10px; border-radius: 5px;}
        """),
        )
    )

    @fh_rt("/get-agent-inputs")
    def get_agent_inputs(request: Request):
        """Endpoint called by HTMX to get agent input fields."""
        agent_name = request.query_params.get("agent_name")
        logger.debug(f"UI requesting inputs for agent: {agent_name}")
        if not agent_name:
            return Div("Please select an agent.", cls="error-message")

        # Access agents via the passed FlockAPI instance
        agent_def = flock_instance.agents.get(agent_name)
        if not agent_def:
            logger.warning(f"Agent '{agent_name}' not found for UI.")
            return Div(f"Agent '{agent_name}' not found.", cls="error-message")

        # Use the parsing function from the FlockAPI instance
        input_fields = parse_input_spec_func(agent_def.input or "")
        logger.debug(f"Parsed input fields for {agent_name}: {input_fields}")

        inputs_html = []
        for field in input_fields:
            field_id = f"input_{field['name']}"
            label_text = f"{field['name']}"
            if field["type"] != "bool":
                label_text += f" ({field['type']})"
            label = Label(label_text, fr=field_id)
            input_attrs = dict(
                id=field_id,
                name=f"inputs.{field['name']}",
                type=field["html_type"],
            )
            if field.get("step"):
                input_attrs["step"] = field["step"]
            if field.get("desc"):
                input_attrs["placeholder"] = field["desc"]
            if field.get("rows"):
                input_attrs["rows"] = field["rows"]

            if field["html_type"] == "textarea":
                input_el = Textarea(**input_attrs)
            elif field["html_type"] == "checkbox":
                input_el = Div(
                    Input(**input_attrs, value="true"),
                    Label(f" Enable?", fr=field_id),
                )
            else:
                input_el = Input(**input_attrs)

            inputs_html.append(
                Div(label, input_el, style="margin-bottom: 1rem;")
            )

        inputs_html.append(
            Hidden(
                id="selected_agent_name", name="agent_name", value=agent_name
            )
        )
        return (
            Div(*inputs_html)
            if inputs_html
            else P("This agent requires no input.")
        )

    @fh_rt("/")
    async def ui_root(request: Request):
        """Serves the main UI page."""
        logger.info("Serving main UI page /ui/")
        agents_list = []
        error_msg = None
        api_url = f"http://{api_host}:{api_port}/agents"
        try:
            async with httpx.AsyncClient() as client:
                logger.debug(f"UI fetching agents from {api_url}")
                response = await client.get(api_url)
                response.raise_for_status()
                agent_data = response.json()
                agents_list = agent_data.get("agents", [])
                logger.debug(f"Fetched {len(agents_list)} agents for UI")
        except Exception as e:
            error_msg = f"UI Error: Could not fetch agent list from API at {api_url}. Details: {e}"
            logger.error(error_msg, exc_info=True)

        options = [
            Option("-- Select Agent --", value="", selected=True, disabled=True)
        ] + [
            Option(
                f"{agent['name']}: {agent['description']}", value=agent["name"]
            )
            for agent in agents_list
        ]

        # Use FHForm alias here
        content = Div(
            H2(f"Agent Runner"),
            P(
                "Select an agent, provide the required inputs, and click 'Run Flock'."
            ),
            Label("Select Starting Agent:", fr="agent_select"),
            Select(
                *options,
                id="agent_select",
                name="agent_name",
                hx_get="/ui/get-agent-inputs",
                hx_trigger="change",
                hx_target="#agent-inputs-container",
                hx_indicator="#loading-indicator",
            ),
            FHForm(
                Div(id="agent-inputs-container", style="margin-top: 1rem;"),
                Button("Run Flock", type="submit"),
                Span(
                    " Processing...",
                    id="loading-indicator",
                    cls="htmx-indicator",
                ),
                hx_post="/ui/run-agent-form",  # Target the dedicated form endpoint
                hx_target="#result-area",
                hx_swap="innerHTML",
                hx_indicator="#loading-indicator",
            ),
            H2("Result"),
            Div(
                Pre(
                    Code(
                        "Result will appear here...",
                        id="result-content",
                        class_="language-json",
                    )
                ),
                id="result-area",
                style="min-height: 100px;",
            ),
        )

        if error_msg:
            content = Div(
                H1("Flock UI - Error"), P(error_msg, cls="error-message")
            )

        return Titled(f"{server_name}", content)

    return fh_app

```

---

### 60. src\flock\core\api\ui\utils.py

- **File ID**: file_59
- **Type**: Code File
- **Line Count**: 119
- **Description**: Utility functions for the Flock FastHTML UI.
- **Dependencies**: None
- **Used By**:
  - file_53

**Content**:
```
# src/flock/core/api/ui/utils.py
"""Utility functions for the Flock FastHTML UI."""

import html
from typing import Any

from flock.core.logging.logging import get_logger
from flock.core.util.input_resolver import (
    split_top_level,  # Assuming this is the correct location
)

logger = get_logger("api.ui.utils")


def parse_input_spec(input_spec: str) -> list[dict[str, str]]:
    """Parses an agent input string into a list of field definitions."""
    fields = []
    if not input_spec:
        return fields
    try:
        parts = split_top_level(input_spec)
    except NameError:
        logger.error("split_top_level utility function not found!")
        return fields  # Or raise?

    for part in parts:
        part = part.strip()
        if not part:
            continue
        field_info = {
            "name": "",
            "type": "str",
            "desc": "",
            "html_type": "text",
        }
        name_type_part, *desc_part = part.split("|", 1)
        if desc_part:
            field_info["desc"] = desc_part[0].strip()
        name_part, *type_part = name_type_part.split(":", 1)
        field_info["name"] = name_part.strip()
        if type_part:
            field_info["type"] = type_part[0].strip().lower()

        step = None
        field_type_norm = field_info["type"]
        if field_type_norm.startswith("int"):
            field_info["html_type"] = "number"
        elif field_type_norm.startswith("float"):
            field_info["html_type"] = "number"
            step = "any"
        elif field_type_norm.startswith("bool"):
            field_info["html_type"] = "checkbox"
        elif "list" in field_type_norm or "dict" in field_type_norm:
            field_info["html_type"] = "textarea"
            field_info["rows"] = 3

        if step:
            field_info["step"] = step
        if field_info["name"]:
            fields.append(field_info)
        else:
            logger.warning(
                f"Could not parse field name from input spec part: '{part}'"
            )
    return fields


def format_result_to_html(
    data: Any, level: int = 0, max_level: int = 5, max_str_len: int = 999999
) -> str:
    """Recursively formats a Python object (dict, list, Box, etc.) into an HTML string."""
    if hasattr(data, "to_dict") and callable(data.to_dict):
        data = data.to_dict()
    if level > max_level:
        return html.escape(f"[Max recursion depth {max_level} reached]")

    if isinstance(data, dict):
        if not data:
            return "<i>(empty dictionary)</i>"
        table_html = '<table style="width: 100%; border-collapse: collapse; margin-bottom: 10px; border: 1px solid #dee2e6;">'
        table_html += '<thead style="background-color: #e9ecef;"><tr><th style="text-align: left; padding: 8px; border-bottom: 2px solid #dee2e6;">Key</th><th style="text-align: left; padding: 8px; border-bottom: 2px solid #dee2e6;">Value</th></tr></thead>'
        table_html += "<tbody>"
        for key, value in data.items():
            escaped_key = html.escape(str(key))
            formatted_value = format_result_to_html(
                value, level + 1, max_level, max_str_len
            )  # Recursive call
            table_html += f'<tr><td style="vertical-align: top; padding: 8px; border-top: 1px solid #dee2e6;"><strong>{escaped_key}</strong></td><td style="padding: 8px; border-top: 1px solid #dee2e6;">{formatted_value}</td></tr>'
        table_html += "</tbody></table>"
        return table_html
    elif isinstance(data, (list, tuple)):
        if not data:
            return "<i>(empty list)</i>"
        list_html = '<dl style="margin-left: 20px; padding-left: 0; margin-bottom: 10px;">'
        for i, item in enumerate(data):
            formatted_item = format_result_to_html(
                item, level + 1, max_level, max_str_len
            )  # Recursive call
            list_html += f'<dt style="font-weight: bold; margin-top: 5px;">Item {i + 1}:</dt><dd style="margin-left: 20px; margin-bottom: 5px;">{formatted_item}</dd>'
        list_html += "</dl>"
        return list_html
    else:
        str_value = str(data)
        escaped_value = html.escape(str_value)
        if len(str_value) > max_str_len:
            escaped_value = (
                html.escape(str_value[:max_str_len])
                + f"... <i style='color: #6c757d;'>({len(str_value) - max_str_len} more chars)</i>"
            )

        style = ""
        if isinstance(data, bool):
            style = "color: #d63384; font-weight: bold;"
        elif isinstance(data, (int, float)):
            style = "color: #0d6efd;"
        elif data is None:
            style = "color: #6c757d; font-style: italic;"
            escaped_value = "None"
        return f'<code style="{style}">{escaped_value}</code>'

```

---

### 61. src\flock\core\context\context.py

- **File ID**: file_60
- **Type**: Code File
- **Line Count**: 185
- **Description**: File at src\flock\core\context\context.py
- **Dependencies**: None
- **Used By**: None

**Content**:
```
import uuid
from dataclasses import asdict
from datetime import datetime
from typing import Any, Literal

from opentelemetry import trace
from pydantic import BaseModel, Field

from flock.core.context.context_vars import FLOCK_LAST_AGENT, FLOCK_LAST_RESULT
from flock.core.logging.logging import get_logger
from flock.core.serialization.serializable import Serializable

logger = get_logger("context")
tracer = trace.get_tracer(__name__)


class AgentRunRecord(BaseModel):
    id: str = Field(default="")
    agent: str = Field(default="")
    data: dict[str, Any] = Field(default_factory=dict)
    timestamp: str = Field(default="")
    hand_off: dict | None = Field(default_factory=dict)
    called_from: str = Field(default="")


class AgentDefinition(BaseModel):
    agent_type: str = Field(default="")
    agent_name: str = Field(default="")
    agent_data: dict = Field(default_factory=dict)
    serializer: Literal["json", "cloudpickle", "msgpack"] = Field(
        default="cloudpickle"
    )


class FlockContext(Serializable, BaseModel):
    state: dict[str, Any] = Field(default_factory=dict)
    history: list[AgentRunRecord] = Field(default_factory=list)
    agent_definitions: dict[str, AgentDefinition] = Field(default_factory=dict)
    run_id: str = Field(default="")
    workflow_id: str = Field(default="")
    workflow_timestamp: str = Field(default="")

    def record(
        self,
        agent_name: str,
        data: dict[str, Any],
        timestamp: str,
        hand_off: str,
        called_from: str,
    ) -> None:
        record = AgentRunRecord(
            id=agent_name + "_" + uuid.uuid4().hex[:4],
            agent=agent_name,
            data=data.copy(),
            timestamp=timestamp,
            hand_off=hand_off,
            called_from=called_from,
        )
        self.history.append(record)
        for key, value in data.items():
            self.set_variable(f"{agent_name}.{key}", value)
        self.set_variable(FLOCK_LAST_RESULT, data)
        self.set_variable(FLOCK_LAST_AGENT, agent_name)
        logger.info(
            f"Agent run recorded - run_id '{record.id}'",
            agent=agent_name,
            timestamp=timestamp,
            data=data,
        )
        current_span = trace.get_current_span()
        if current_span.get_span_context().is_valid:
            current_span.add_event(
                "record",
                attributes={"agent": agent_name, "timestamp": timestamp},
            )

    def get_variable(self, key: str, default: Any = None) -> Any:
        return self.state.get(key, default)

    def set_variable(self, key: str, value: Any) -> None:
        old_value = self.state.get(key)
        self.state[key] = value
        if old_value != value:
            escaped_value = str(value).replace("{", "{{").replace("}", "}}")

            logger.info(
                "Context variable updated - {} -> {}",
                key,
                escaped_value,  # Arguments in order
            )

            current_span = trace.get_current_span()
            if current_span.get_span_context().is_valid:
                current_span.add_event(
                    "set_variable",
                    attributes={
                        "key": key,
                        "old": str(old_value),
                        "new": str(value),
                    },
                )

    def deepcopy(self) -> "FlockContext":
        return FlockContext.from_dict(self.to_dict())

    def get_agent_history(self, agent_name: str) -> list[AgentRunRecord]:
        return [record for record in self.history if record.agent == agent_name]

    def next_input_for(self, agent) -> Any:
        try:
            if hasattr(agent, "input") and isinstance(agent.input, str):
                keys = [k.strip() for k in agent.input.split(",") if k.strip()]
                if len(keys) == 1:
                    return self.get_variable(keys[0])
                else:
                    return {key: self.get_variable(key) for key in keys}
            else:
                return self.get_variable("init_input")
        except Exception as e:
            logger.error(
                "Error getting next input for agent",
                agent=agent.name,
                error=str(e),
            )
            raise

    def get_most_recent_value(self, variable_name: str) -> Any:
        for history_record in reversed(self.history):
            if variable_name in history_record.data:
                return history_record.data[variable_name]

    def get_agent_definition(self, agent_name: str) -> AgentDefinition | None:
        return self.agent_definitions.get(agent_name)

    def add_agent_definition(
        self, agent_type: type, agent_name: str, agent_data: Any
    ) -> None:
        definition = AgentDefinition(
            agent_type=agent_type.__name__,
            agent_name=agent_name,
            agent_data=agent_data,
        )
        self.agent_definitions[agent_name] = definition

    # Use the reactive setter for dict-like access.
    def __getitem__(self, key: str) -> Any:
        return self.get_variable(key)

    def __setitem__(self, key: str, value: Any) -> None:
        self.set_variable(key, value)

    def to_dict(self) -> dict[str, Any]:
        def convert(obj):
            if isinstance(obj, datetime):
                return obj.isoformat()
            if hasattr(obj, "__dataclass_fields__"):
                return asdict(
                    obj, dict_factory=lambda x: {k: convert(v) for k, v in x}
                )
            return obj

        return convert(asdict(self))

    @classmethod
    def from_dict(cls, data: dict[str, Any]) -> "FlockContext":
        def convert(obj):
            if isinstance(obj, dict):
                if "timestamp" in obj:
                    return AgentRunRecord(
                        **{
                            **obj,
                            "timestamp": datetime.fromisoformat(
                                obj["timestamp"]
                            ),
                        }
                    )
                if "agent_type" in obj:
                    return AgentDefinition(**obj)
                return {k: convert(v) for k, v in obj.items()}
            if isinstance(obj, list):
                return [convert(v) for v in obj]
            return obj

        converted = convert(data)
        return cls(**converted)

```

---

### 62. src\flock\core\context\context_manager.py

- **File ID**: file_61
- **Type**: Code File
- **Line Count**: 37
- **Description**: Module for managing the FlockContext.
- **Dependencies**: None
- **Used By**: None

**Content**:
```
"""Module for managing the FlockContext."""

from flock.core.context.context import FlockContext
from flock.core.context.context_vars import (
    FLOCK_CURRENT_AGENT,
    FLOCK_INITIAL_INPUT,
    FLOCK_LOCAL_DEBUG,
    FLOCK_MODEL,
    FLOCK_RUN_ID,
)


def initialize_context(
    context: FlockContext,
    agent_name: str,
    input_data: dict,
    run_id: str,
    local_debug: bool,
    model: str,
) -> None:
    """Initialize the FlockContext with standard variables before running an agent.

    Args:
        context: The FlockContext instance.
        agent_name: The name of the current agent.
        input_data: A dictionary of inputs for the agent.
        run_id: A unique identifier for the run.
        local_debug: Flag indicating whether local debugging is enabled.
    """
    context.set_variable(FLOCK_CURRENT_AGENT, agent_name)
    for key, value in input_data.items():
        context.set_variable("flock." + key, value)
    context.set_variable(FLOCK_INITIAL_INPUT, input_data)
    context.set_variable(FLOCK_LOCAL_DEBUG, local_debug)
    context.run_id = run_id
    context.set_variable(FLOCK_RUN_ID, run_id)
    context.set_variable(FLOCK_MODEL, model)

```

---

### 63. src\flock\core\context\context_vars.py

- **File ID**: file_62
- **Type**: Code File
- **Line Count**: 10
- **Description**: Context variables for Flock.
- **Dependencies**: None
- **Used By**: None

**Content**:
```
"""Context variables for Flock."""

FLOCK_CURRENT_AGENT = "flock.current_agent"
FLOCK_INITIAL_INPUT = "flock.initial_input"
FLOCK_LOCAL_DEBUG = "flock.local_debug"
FLOCK_RUN_ID = "flock.run_id"
FLOCK_LAST_AGENT = "flock.last_agent"
FLOCK_LAST_RESULT = "flock.last_result"
FLOCK_MODEL = "flock.model"
FLOCK_BATCH_SILENT_MODE = "flock.batch_silent"

```

---

### 64. src\flock\core\evaluation\utils.py

- **File ID**: file_63
- **Type**: Code File
- **Line Count**: 312
- **Description**: File at src\flock\core\evaluation\utils.py
- **Dependencies**: None
- **Used By**: None

**Content**:
```
# src/flock/core/util/evaluation_helpers.py
import inspect
import sys
from collections.abc import Callable
from pathlib import Path
from typing import Any, Union

import pandas as pd
from box import Box
from datasets import get_dataset_config_names, load_dataset

from flock.core.flock_agent import FlockAgent
from flock.core.flock_evaluator import FlockEvaluator
from flock.core.logging.logging import get_logger

# Potentially import metrics libraries like rouge_score, nltk, sentence_transformers

logger_helpers = get_logger("util.evaluation")


def load_and_merge_all_configs(dataset_name: str) -> pd.DataFrame:
    all_configs = get_dataset_config_names(dataset_name)
    all_dfs = []

    for config in all_configs:
        dataset_dict = load_dataset(dataset_name, config)
        for split_name, split_dataset in dataset_dict.items():
            df = split_dataset.to_pandas()
            df["config"] = config
            df["split"] = split_name
            all_dfs.append(df)

    merged_df = pd.concat(all_dfs, ignore_index=True)
    return merged_df


def normalize_dataset(dataset: Any) -> pd.DataFrame:
    """Converts various dataset formats into a pandas DataFrame."""
    if isinstance(dataset, pd.DataFrame):
        return dataset.copy()
    elif isinstance(dataset, str | Path):
        path = Path(dataset)
        if not path.exists():
            try:
                return load_and_merge_all_configs(dataset)
            except Exception as e:
                raise FileNotFoundError(
                    f"Dataset file not found: {path}"
                ) from e
        if path.suffix.lower() == ".csv":
            return pd.read_csv(path)
        # Add support for json, jsonl etc. if needed
        else:
            raise ValueError(
                f"Unsupported file type for dataset: {path.suffix}"
            )
    elif isinstance(dataset, list):
        if not dataset or not isinstance(dataset[0], dict):
            raise ValueError("Dataset list must contain dictionaries.")
        return pd.DataFrame(dataset)
    elif "datasets" in sys.modules and isinstance(
        dataset, sys.modules["datasets"].Dataset
    ):
        # Requires 'datasets' library to be installed
        return dataset.to_pandas()
    else:
        raise TypeError(f"Unsupported dataset type: {type(dataset)}")


def extract_value_by_dot_notation(data: dict | Box, key: str) -> Any:
    """Retrieves a value from a nested dictionary or Box object using dot notation."""
    if not key:
        return None
    keys = key.split(".")
    value = data
    try:
        for k in keys:
            if isinstance(value, (dict, Box)):
                value = value.get(k)
            # Add list index handling if needed: e.g., 'results[0].field'
            # elif isinstance(value, list) and k.isdigit():
            #     value = value[int(k)]
            else:
                return None  # Cannot traverse further
            if value is None:
                return None  # Key not found at this level
        return value
    except (KeyError, IndexError, AttributeError):
        return None


def calculate_evaluation_metrics(
    metrics: list[Union[str, Callable, "FlockAgent", "FlockEvaluator"]],
    metric_configs: dict[str, dict[str, Any]],
    predicted_answers: dict[str, Any],
    expected_answers: dict[str, Any],
    agent_inputs: dict[str, Any],  # For context
    agent_output: Any,  # For context
) -> dict[str, Any]:
    """Calculates all specified metrics for a single evaluation item."""
    results = {}
    for metric in metrics:
        metric_name = ""
        metric_result = None
        try:
            if isinstance(metric, str):
                metric_name = metric
                # Find predicted/expected values relevant to this metric string
                # Simple case: metric name matches an answer_mapping key
                if (
                    metric_name in predicted_answers
                    and metric_name in expected_answers
                ):
                    predicted = predicted_answers[metric_name]
                    expected = expected_answers[metric_name]
                    metric_func = _get_metric_function(metric_name)
                    config = metric_configs.get(metric_name, {})
                    metric_result = metric_func(predicted, expected, **config)
                else:
                    logger_helpers.warning(
                        f"Could not find matching predicted/expected values for metric '{metric_name}' based on answer_mapping keys."
                    )
                    metric_result = None  # Or some error indicator

            elif isinstance(metric, Callable):
                metric_name = getattr(metric, "__name__", "custom_function")
                # Custom functions might need specific predicted/expected pairs, or all of them
                # Let's pass all for flexibility, user function needs to handle it
                config = metric_configs.get(metric_name, {})
                # Allow passing context if function signature supports it
                sig = inspect.signature(metric)
                call_kwargs = config.copy()
                if "agent_inputs" in sig.parameters:
                    call_kwargs["agent_inputs"] = agent_inputs
                if "agent_output" in sig.parameters:
                    call_kwargs["agent_output"] = agent_output

                metric_result = metric(
                    predicted_answers, expected_answers, **call_kwargs
                )

            # --- Placeholder for Agent/Evaluator based metrics ---
            elif "FlockAgent" in str(
                type(metric)
            ):  # Avoid hard import if possible
                metric_name = getattr(metric, "name", "judge_agent")
                config = metric_configs.get(metric_name, {})
                # Requires running the judge agent - needs async context
                # metric_result = asyncio.run(_run_judge_agent(metric, predicted_answers, expected_answers, config))
                logger_helpers.warning(
                    f"Agent-based metric '{metric_name}' execution not implemented in this sketch."
                )
                metric_result = "[Agent Judge Not Implemented]"

            elif "FlockEvaluator" in str(
                type(metric)
            ):  # Avoid hard import if possible
                metric_name = getattr(metric, "name", "judge_evaluator")
                config = metric_configs.get(metric_name, {})
                # Requires running the evaluator - needs async context
                # metric_result = asyncio.run(_run_judge_evaluator(metric, predicted_answers, expected_answers, config))
                logger_helpers.warning(
                    f"Evaluator-based metric '{metric_name}' execution not implemented in this sketch."
                )
                metric_result = "[Evaluator Judge Not Implemented]"
            # --- End Placeholder ---

            else:
                logger_helpers.warning(
                    f"Unsupported metric type: {type(metric)}"
                )
                continue

            # Store result - handle dict results from metrics
            if isinstance(metric_result, dict):
                for sub_key, sub_value in metric_result.items():
                    results[f"{metric_name}_{sub_key}"] = sub_value
            else:
                results[metric_name] = metric_result

        except Exception as e:
            logger_helpers.error(
                f"Error calculating metric '{metric_name}': {e}"
            )
            results[metric_name] = f"[Error: {e}]"

    return results


def _get_metric_function(metric_name: str) -> Callable:
    """Maps metric names to their implementation functions."""
    # Lazy load metric libraries
    if metric_name == "exact_match":
        return lambda pred, act, **kw: str(pred).strip() == str(act).strip()
    elif metric_name == "fuzzy_match":
        try:
            from thefuzz import fuzz

            return (
                lambda pred, act, threshold=85, **kw: fuzz.ratio(
                    str(pred), str(act)
                )
                >= threshold
            )
        except ImportError:
            logger_helpers.warning(
                "fuzzy_match requires 'thefuzz': pip install thefuzz[speedup]"
            )
            return lambda p, a, **kw: None
    elif metric_name.startswith("rouge"):  # rouge_1, rouge_2, rouge_l
        try:
            from rouge_score import rouge_scorer

            scorer = rouge_scorer.RougeScorer(
                [metric_name.replace("_", "")], use_stemmer=True
            )

            def calculate_rouge(pred, act, score_type="fmeasure", **kw):
                scores = scorer.score(str(act), str(pred))
                return (
                    scores[metric_name.replace("_", "")]
                    ._asdict()
                    .get(score_type, 0.0)
                )

            return calculate_rouge
        except ImportError:
            logger_helpers.warning(
                "rouge requires 'rouge-score': pip install rouge-score"
            )
            return lambda p, a, **kw: None
    elif metric_name == "semantic_similarity":
        try:
            from sentence_transformers import SentenceTransformer, util

            # Cache the model? Maybe pass it in via config?
            model = SentenceTransformer("all-MiniLM-L6-v2")

            def calculate_similarity(pred, act, **kw):
                emb1 = model.encode(str(pred), convert_to_tensor=True)
                emb2 = model.encode(str(act), convert_to_tensor=True)
                return util.pytorch_cos_sim(emb1, emb2).item()

            return calculate_similarity
        except ImportError:
            logger_helpers.warning(
                "semantic_similarity requires 'sentence-transformers': pip install sentence-transformers"
            )
            return lambda p, a, **kw: None
    # Add bleu, f1 etc.
    elif metric_name == "llm_judge":
        # This is handled by checking type in calculate_evaluation_metrics
        # but we need a placeholder callable here if we map by string first
        return lambda p, a, **kw: "[LLM Judge Not Implemented Directly]"
    else:
        raise ValueError(f"Unknown built-in metric: {metric_name}")


def aggregate_results(results_list: list[dict[str, Any]]) -> dict[str, Any]:
    """Aggregates evaluation results across all items."""
    summary = {"total_items": len(results_list), "errors": 0}
    metric_values: dict[str, list[float | bool]] = {}

    for item in results_list:
        if item.get("error"):
            summary["errors"] += 1
        metrics = item.get("metrics", {})
        for name, value in metrics.items():
            if isinstance(
                value, (float, int, bool)
            ):  # Only aggregate numerics/bools
                if name not in metric_values:
                    metric_values[name] = []
                metric_values[name].append(value)

    summary["metrics_summary"] = {}
    for name, values in metric_values.items():
        if not values:
            continue
        # Calculate different stats based on value type
        if all(isinstance(v, bool) for v in values):
            summary["metrics_summary"][name] = {
                "accuracy": sum(values) / len(values)
            }
        elif all(isinstance(v, (int, float)) for v in values):
            numeric_values = [v for v in values if isinstance(v, (int, float))]
            if numeric_values:
                summary["metrics_summary"][name] = {
                    "mean": sum(numeric_values) / len(numeric_values),
                    "count": len(numeric_values),
                    # Add min, max, stddev if needed
                }

    return summary


# --- Placeholder for async judge execution ---
# Need to run these within the main async context or manage loops carefully
async def _run_judge_agent(judge_agent, predicted, expected, config):
    # Prepare input for the judge agent based on its signature
    # E.g., judge_input = {"prediction": predicted_value, "reference": expected_value, "criteria": ...}
    # judge_result = await judge_agent.run_async(judge_input)
    # return judge_result # Or extract specific score/judgement
    return "[Agent Judge Not Implemented]"


async def _run_judge_evaluator(judge_evaluator, predicted, expected, config):
    # Prepare input for the judge evaluator based on its signature
    # judge_input = {"prediction": predicted_value, "reference": expected_value, **config}
    # judge_result = await judge_evaluator.evaluate(None, judge_input, []) # Agent might not be needed
    # return judge_result # Or extract specific score/judgement
    return "[Evaluator Judge Not Implemented]"

```

---

### 65. src\flock\core\execution\batch_executor.py

- **File ID**: file_64
- **Type**: Code File
- **Line Count**: 336
- **Description**: File at src\flock\core\execution\batch_executor.py
- **Dependencies**: None
- **Used By**: None

**Content**:
```
import asyncio
from pathlib import Path
from typing import TYPE_CHECKING, Any

from box import Box
from opentelemetry import trace
from pandas import DataFrame
from rich.progress import (  # Import Rich Progress
    BarColumn,
    Progress,
    SpinnerColumn,
    TextColumn,
    TimeElapsedColumn,
)

from flock.config import TELEMETRY
from flock.core.context.context import FlockContext
from flock.core.context.context_vars import FLOCK_BATCH_SILENT_MODE
from flock.core.flock_agent import FlockAgent
from flock.core.logging.logging import get_logger

try:
    import pandas as pd

    PANDAS_AVAILABLE = True
except ImportError:
    pd = None
    PANDAS_AVAILABLE = False

if TYPE_CHECKING:
    from flock.core.flock import Flock

logger = get_logger("flock")
TELEMETRY.setup_tracing()  # Setup OpenTelemetry
tracer = trace.get_tracer(__name__)


class BatchProcessor:
    def __init__(self, flock_instance: "Flock"):
        self.flock = flock_instance

    async def run_batch_async(
        self,
        start_agent: FlockAgent | str,
        batch_inputs: list[dict[str, Any]] | DataFrame | str,
        input_mapping: dict[str, str] | None = None,
        static_inputs: dict[str, Any] | None = None,
        parallel: bool = True,
        max_workers: int = 5,
        use_temporal: bool | None = None,
        box_results: bool = True,
        return_errors: bool = False,
        silent_mode: bool = False,
        write_to_csv: str | None = None,
        hide_columns: list[str] | None = None,
        delimiter: str = ",",
    ) -> list[Box | dict | None | Exception]:
        """Runs the specified agent/workflow for each item in a batch asynchronously.

        Args:
            start_agent: Agent instance or name to start each run.
            batch_inputs: Input data in one of these forms:
                - List of dictionaries, each representing inputs for one run
                - Pandas DataFrame where each row is inputs for one run
                - String path to a CSV file to load as DataFrame
            input_mapping: Maps DataFrame/CSV column names to agent input keys (required for DataFrame/CSV).
            static_inputs: Dictionary of inputs constant across all batch runs.
            parallel: Whether to run local jobs in parallel (ignored if use_temporal=True).
            max_workers: Max concurrent local workers (used if parallel=True and use_temporal=False).
            use_temporal: Override Flock's 'enable_temporal' setting for this batch.
            box_results: Wrap successful dictionary results in Box objects.
            return_errors: If True, return Exception objects for failed runs instead of raising.
            silent_mode: If True, suppress output and show progress bar instead.
            write_to_csv: Path to save results as CSV file.
            hide_columns: List of column names to hide from output.

        Returns:
            List containing results (Box/dict), None (if error and not return_errors),
            or Exception objects (if error and return_errors). Order matches input.

        Raises:
            ValueError: For invalid input combinations.
            ImportError: If DataFrame/CSV used without pandas.
            Exception: First exception from a run if return_errors is False.
        """
        effective_use_temporal = (
            use_temporal
            if use_temporal is not None
            else self.flock.enable_temporal
        )
        exec_mode = (
            "Temporal"
            if effective_use_temporal
            else ("Parallel Local" if parallel else "Sequential Local")
        )
        logger.info(
            f"Starting batch run for agent '{start_agent}'. Execution: {exec_mode}, Silent: {silent_mode}"
        )

        # --- Input Preparation ---
        prepared_batch_inputs: list[dict[str, Any]] = []

        if input_mapping == {}:
            input_mapping = None
        if static_inputs == {}:
            static_inputs = None

        if isinstance(batch_inputs, str):
            # Handle CSV file input
            try:
                df = pd.read_csv(batch_inputs)
                logger.debug(
                    f"Loaded CSV file with {len(df)} rows: {batch_inputs}"
                )
                batch_inputs = df  # Convert to DataFrame for unified handling
            except Exception as e:
                raise ValueError(
                    f"Failed to load CSV file '{batch_inputs}': {e}"
                )

        if isinstance(batch_inputs, DataFrame):
            # Handle DataFrame input
            logger.debug(
                f"Converting DataFrame ({len(batch_inputs)} rows) to batch inputs."
            )
            for _, row in batch_inputs.iterrows():
                if input_mapping:
                    item_input = {
                        agent_key: row[df_col]
                        for df_col, agent_key in input_mapping.items()
                        if df_col in row
                    }
                else:
                    item_input = row.to_dict()
                prepared_batch_inputs.append(item_input)
        else:
            # Handle list of dictionaries
            if not isinstance(batch_inputs, list):
                raise ValueError(
                    "batch_inputs must be a list of dictionaries, DataFrame, or CSV file path"
                )

            if input_mapping:
                # Apply mapping to dictionary inputs
                logger.debug("Applying input mapping to dictionary inputs")
                for item in batch_inputs:
                    mapped_input = {}
                    for df_col, agent_key in input_mapping.items():
                        if df_col in item:
                            mapped_input[agent_key] = item[df_col]
                        else:
                            logger.warning(
                                f"Input mapping key '{df_col}' not found in input dictionary"
                            )
                    prepared_batch_inputs.append(mapped_input)
            else:
                # Use dictionaries as-is if no mapping provided
                prepared_batch_inputs = batch_inputs

            logger.debug(
                f"Using provided list of {len(prepared_batch_inputs)} batch inputs."
            )

        if not prepared_batch_inputs:
            return []

        # --- Setup Progress Bar if Silent ---
        progress_context = None
        progress_task_id = None
        if silent_mode:
            progress = Progress(
                SpinnerColumn(),
                TextColumn("[progress.description]{task.description}"),
                BarColumn(),
                TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
                TextColumn("({task.completed}/{task.total})"),
                TimeElapsedColumn(),
                # transient=True # Optionally remove progress bar when done
            )
            progress_context = progress  # Use as context manager
            progress_task_id = progress.add_task(
                f"Processing Batch ({exec_mode})",
                total=len(prepared_batch_inputs),
            )
            progress.start()

        results = [None] * len(
            prepared_batch_inputs
        )  # Pre-allocate results list
        tasks = []
        semaphore = asyncio.Semaphore(
            max_workers if parallel and not effective_use_temporal else 1
        )  # Semaphore for parallel local

        async def worker(index, item_inputs):
            async with semaphore:
                full_input = {**(static_inputs or {}), **item_inputs}
                context = FlockContext()
                context.set_variable(FLOCK_BATCH_SILENT_MODE, silent_mode)

                run_desc = f"Batch item {index + 1}"
                logger.debug(f"{run_desc} started.")
                try:
                    result = await self.flock.run_async(
                        start_agent,
                        full_input,
                        box_result=box_results,
                        context=context,
                    )
                    results[index] = result
                    logger.debug(f"{run_desc} finished successfully.")
                except Exception as e:
                    logger.error(
                        f"{run_desc} failed: {e}", exc_info=not return_errors
                    )
                    if return_errors:
                        results[index] = e
                    else:
                        # If not returning errors, ensure the exception propagates
                        # to stop asyncio.gather if running in parallel.
                        if parallel and not effective_use_temporal:
                            raise  # Re-raise to stop gather
                        else:
                            # For sequential, we just store None or the exception if return_errors=True
                            # For Temporal, error handling happens within the workflow/activity usually
                            results[index] = e if return_errors else None
                finally:
                    if progress_context:
                        progress.update(
                            progress_task_id, advance=1
                        )  # Update progress

        try:
            if effective_use_temporal:
                # Temporal Batching (Simplified: sequential execution for this example)
                # A real implementation might use start_workflow or signals
                logger.info(
                    "Running batch using Temporal (executing sequentially for now)..."
                )
                for i, item_data in enumerate(prepared_batch_inputs):
                    await worker(i, item_data)  # Run sequentially for demo
                # TODO: Implement true parallel Temporal workflow execution if needed

            elif parallel:
                logger.info(
                    f"Running batch in parallel with max_workers={max_workers}..."
                )
                for i, item_data in enumerate(prepared_batch_inputs):
                    tasks.append(asyncio.create_task(worker(i, item_data)))
                await asyncio.gather(
                    *tasks
                )  # gather handles exceptions based on return_errors logic in worker

            else:  # Sequential Local
                logger.info("Running batch sequentially...")
                for i, item_data in enumerate(prepared_batch_inputs):
                    await worker(
                        i, item_data
                    )  # Already handles errors internally based on return_errors

            logger.info("Batch execution finished.")

        except Exception as batch_error:
            # This catch handles errors re-raised from workers when return_errors=False
            logger.error(f"Batch execution stopped due to error: {batch_error}")
            # No need to cancel tasks here as gather would have stopped
            if not return_errors:
                raise  # Re-raise the first error encountered if not returning errors
        finally:
            if progress_context:
                progress.stop()

        if write_to_csv:
            try:
                df = pd.DataFrame(results)
                if hide_columns:
                    df = df.drop(columns=hide_columns)
                # create write_to_csv directory if it doesn't exist
                Path(write_to_csv).parent.mkdir(parents=True, exist_ok=True)
                df.to_csv(write_to_csv, index=False, sep=delimiter)
                logger.info(f"Results written to CSV file: {write_to_csv}")
            except Exception as e:
                logger.error(f"Failed to write results to CSV: {e}")

        return results

    def run_batch(  # Synchronous wrapper
        self,
        start_agent: FlockAgent | str,
        batch_inputs: list[dict[str, Any]] | DataFrame | str,
        input_mapping: dict[str, str] | None = None,
        static_inputs: dict[str, Any] | None = None,
        parallel: bool = True,
        max_workers: int = 5,
        use_temporal: bool | None = None,
        box_results: bool = True,
        return_errors: bool = False,
        silent_mode: bool = False,
        write_to_csv: str | None = None,
        hide_columns: list[str] | None = None,
        delimiter: str = ",",
    ) -> list[Box | dict | None | Exception]:
        """Synchronous wrapper for run_batch_async."""
        # (Standard asyncio run wrapper - same as in previous suggestion)
        try:
            loop = asyncio.get_running_loop()
            if loop.is_closed():
                raise RuntimeError("Event loop is closed")
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)

        coro = self.run_batch_async(
            start_agent=start_agent,
            batch_inputs=batch_inputs,
            input_mapping=input_mapping,
            static_inputs=static_inputs,
            parallel=parallel,
            max_workers=max_workers,
            use_temporal=use_temporal,
            box_results=box_results,
            return_errors=return_errors,
            silent_mode=silent_mode,
            write_to_csv=write_to_csv,
            hide_columns=hide_columns,
            delimiter=delimiter,
        )

        if asyncio.get_event_loop() is loop and not loop.is_running():
            results = loop.run_until_complete(coro)
            # loop.close() # Avoid closing potentially shared loop
            return results
        else:
            # Run within an existing loop
            future = asyncio.ensure_future(coro)
            return loop.run_until_complete(future)

```

---

### 66. src\flock\core\execution\evaluation_executor.py

- **File ID**: file_65
- **Type**: Code File
- **Line Count**: 438
- **Description**: Contains the EvaluationProcessor class responsible for evaluating Flock agents
- **Dependencies**: None
- **Used By**: None

**Content**:
```
# src/flock/core/execution/evaluation_processor.py
"""Contains the EvaluationProcessor class responsible for evaluating Flock agents
against datasets using various metrics.
"""

import asyncio
import json
from collections.abc import Callable
from pathlib import Path
from typing import (
    TYPE_CHECKING,
    Any,
    Literal,
    Union,
)

from pandas import DataFrame

# Conditional pandas import
try:
    import pandas as pd

    PANDAS_AVAILABLE = True
except ImportError:
    pd = None  # type: ignore
    PANDAS_AVAILABLE = False

# Box for results
from box import Box
from datasets import Dataset as HFDataset

from flock.core.evaluation.utils import (
    aggregate_results,
    calculate_evaluation_metrics,
    extract_value_by_dot_notation,
    normalize_dataset,
    # Import metric calculation/aggregation helpers
)

# Flock core imports
from flock.core.logging.logging import get_logger

if TYPE_CHECKING:
    from flock.core.flock import Flock
    from flock.core.flock_agent import FlockAgent
    from flock.core.flock_evaluator import FlockEvaluator
    # Conditional types


logger = get_logger("execution.evaluation")


class EvaluationExecutor:
    """Handles the evaluation of Flock agents against datasets."""

    def __init__(self, flock_instance: "Flock"):
        """Initializes the EvaluationProcessor.

        Args:
            flock_instance: The Flock instance this processor will use.
        """
        self.flock = flock_instance

    async def evaluate_async(
        self,
        dataset: str | Path | list[dict[str, Any]] | DataFrame | HFDataset,
        start_agent: Union["FlockAgent", str],
        input_mapping: dict[str, str],
        answer_mapping: dict[str, str],
        metrics: list[
            Union[
                str,
                Callable[[Any, Any], bool | float | dict[str, Any]],
                "FlockAgent",
                "FlockEvaluator",
            ]
        ],
        metric_configs: dict[str, dict[str, Any]] | None = None,
        static_inputs: dict[str, Any] | None = None,
        parallel: bool = True,
        max_workers: int = 5,
        use_temporal: bool | None = None,
        error_handling: Literal["raise", "skip", "log"] = "log",
        output_file: str | Path | None = None,
        return_dataframe: bool = True,
        silent_mode: bool = False,
        metadata_columns: list[str] | None = None,  # Columns to pass through
        # dataset_split: Optional[str] = None # TODO: Add split support in normalize_dataset
    ) -> DataFrame | list[dict[str, Any]]:
        """Evaluates the Flock's performance against a dataset asynchronously."""
        effective_use_temporal = (
            use_temporal
            if use_temporal is not None
            else self.flock.enable_temporal
        )
        exec_mode = (
            "Temporal"
            if effective_use_temporal
            else ("Parallel Local" if parallel else "Sequential Local")
        )
        start_agent_name = (
            start_agent.name if hasattr(start_agent, "name") else start_agent
        )
        logger.info(
            f"Starting evaluation for agent '{start_agent_name}'. Execution: {exec_mode}, Silent: {silent_mode}"
        )

        # --- 1. Normalize Dataset ---
        try:
            df = normalize_dataset(dataset)  # Uses helper
            if df is None or df.empty:
                raise ValueError(
                    "Provided dataset is empty or could not be normalized."
                )
            logger.info(f"Normalized dataset with {len(df)} items.")
        except Exception as e:
            logger.error(
                f"Failed to load or normalize dataset: {e}", exc_info=True
            )
            raise ValueError(f"Dataset processing failed: {e}") from e

        # --- 2. Prepare Batch Items ---
        batch_items = []
        required_input_cols = list(input_mapping.keys())
        required_answer_cols = list(answer_mapping.values())
        required_metadata_cols = metadata_columns or []
        all_required_cols = set(
            required_input_cols + required_answer_cols + required_metadata_cols
        )

        missing_cols = all_required_cols - set(df.columns)
        if missing_cols:
            raise ValueError(
                f"Dataset missing required columns: {', '.join(missing_cols)}"
            )

        for index, row in df.iterrows():
            agent_input = {
                agent_key: row[df_col]
                for df_col, agent_key in input_mapping.items()
            }
            expected_answers = {
                agent_out_key: row[answer_col]
                for agent_out_key, answer_col in answer_mapping.items()
            }
            metadata = {col: row[col] for col in required_metadata_cols}
            batch_items.append(
                {
                    "_original_index": index,  # Store original DF index
                    "_agent_input": agent_input,
                    "_expected_answers": expected_answers,
                    "_metadata": metadata,
                }
            )

        if not batch_items:
            logger.warning("No items prepared for evaluation.")
            return pd.DataFrame() if return_dataframe else []

        # --- 3. Execute Workers ---
        results_dict = {}  # Store results keyed by original index
        tasks = []
        semaphore = asyncio.Semaphore(
            max_workers if parallel and not effective_use_temporal else 1
        )

        # --- Worker Function ---
        async def evaluate_worker(item_index: int, item_data: dict[str, Any]):
            nonlocal results_dict
            original_index = item_data["_original_index"]
            item_result_details = {
                "index": original_index,  # Use original index in result
                "inputs": item_data["_agent_input"],
                "expected_answers": item_data["_expected_answers"],
                "agent_output": None,
                "metrics": {},
                "error": None,
                **(item_data["_metadata"]),  # Include pass-through metadata
            }
            agent_inputs_with_static = {
                **(static_inputs or {}),
                **item_data["_agent_input"],
            }

            async with semaphore:  # Acquire semaphore
                run_desc = f"Evaluation item (original index: {original_index})"
                logger.debug(f"{run_desc} starting.")
                try:
                    # Run the agent/flock for this item
                    agent_output = await self.flock.run_async(
                        start_agent=start_agent,  # Name or instance
                        input=agent_inputs_with_static,
                        box_result=True,  # Use Box for easier access via dot notation
                        # context=... # Assuming isolated context for now
                    )
                    item_result_details["agent_output"] = (
                        agent_output  # Store Box or dict
                    )

                    # Extract predicted values based on answer_mapping
                    predicted_answers = {}
                    for agent_out_key in answer_mapping:
                        # Use helper to handle dot notation
                        predicted_answers[agent_out_key] = (
                            extract_value_by_dot_notation(
                                agent_output, agent_out_key
                            )
                        )

                    # Calculate metrics using helper
                    item_result_details["metrics"] = (
                        calculate_evaluation_metrics(
                            metrics=metrics,
                            metric_configs=metric_configs or {},
                            predicted_answers=predicted_answers,
                            expected_answers=item_data["_expected_answers"],
                            agent_inputs=agent_inputs_with_static,  # Pass context if needed
                            agent_output=agent_output,  # Pass context if needed
                        )
                    )
                    logger.debug(f"{run_desc} finished successfully.")

                except Exception as e:
                    logger.warning(
                        f"Error processing item {original_index}: {e}"
                    )
                    item_result_details["error"] = str(e)
                    if error_handling == "raise":
                        raise  # Re-raise to stop processing (if parallel, stops gather)
                    elif error_handling == "skip":
                        item_result_details["_skip"] = (
                            True  # Mark for filtering
                        )

                # Store result associated with original index
                results_dict[original_index] = item_result_details

                # Update progress bar if applicable (inside the worker is okay)
                if progress_context:
                    progress.update(progress_task_id, advance=1)

        # --- Setup Progress Bar if Silent ---
        progress_context = None
        progress_task_id = None
        if silent_mode:
            from rich.progress import (
                BarColumn,
                Progress,
                SpinnerColumn,
                TextColumn,
                TimeElapsedColumn,
            )

            progress = Progress(
                SpinnerColumn(),
                TextColumn("[progress.description]{task.description}"),
                BarColumn(),
                TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
                TextColumn("({task.completed}/{task.total})"),
                TimeElapsedColumn(),
            )
            progress_context = progress
            progress_task_id = progress.add_task(
                f"Evaluating {len(batch_items)} items...",
                total=len(batch_items),
            )
            progress.start()

        # --- Execute Tasks ---
        try:
            if effective_use_temporal:
                # TODO: Implement parallel Temporal evaluation
                logger.info(
                    "Running evaluation using Temporal (executing sequentially for now)..."
                )
                for i, item_data in enumerate(batch_items):
                    await evaluate_worker(i, item_data)  # Pass sequential index
            elif parallel:
                logger.info(
                    f"Running evaluation in parallel with max_workers={max_workers}..."
                )
                for i, item_data in enumerate(batch_items):
                    # Pass sequential index i, and the item_data which contains original_index
                    tasks.append(
                        asyncio.create_task(evaluate_worker(i, item_data))
                    )
                await asyncio.gather(*tasks)
            else:  # Sequential Local
                logger.info("Running evaluation sequentially...")
                for i, item_data in enumerate(batch_items):
                    await evaluate_worker(i, item_data)

            logger.info("Evaluation execution finished.")

        except Exception as batch_error:
            logger.error(
                f"Evaluation stopped due to an error in one of the items: {batch_error}"
            )
            if (
                not error_handling == "skip"
            ):  # If skipping, we continue; otherwise, re-raise if required
                if error_handling == "raise":
                    raise
        finally:
            if progress_context:
                progress.stop()

        # --- 4. Process Results ---
        # Reconstruct results list based on original order and filtering
        final_results_list = []
        for idx in df.index:  # Iterate through original DataFrame index
            res = results_dict.get(idx)
            if res:
                if error_handling == "skip" and res.get("_skip"):
                    continue  # Skip items marked for skipping
                # Remove internal skip flag if present
                res.pop("_skip", None)
                final_results_list.append(res)

        # Calculate aggregate summary using helper
        summary = aggregate_results(final_results_list)
        logger.info(
            "Evaluation Summary:", extra=summary
        )  # Log summary automatically

        # --- 5. Save and Return ---
        if output_file:
            output_path = Path(output_file)
            output_path.parent.mkdir(parents=True, exist_ok=True)
            try:
                results_df = pd.DataFrame(final_results_list)
                # Handle complex objects before saving
                if "agent_output" in results_df.columns:
                    results_df["agent_output"] = results_df[
                        "agent_output"
                    ].apply(lambda x: x.to_dict() if isinstance(x, Box) else x)
                if (
                    "expected_answers" in results_df.columns
                ):  # Flatten dicts for CSV
                    results_df = pd.concat(
                        [
                            results_df.drop(["expected_answers"], axis=1),
                            pd.json_normalize(
                                results_df["expected_answers"]
                            ).add_prefix("expected_"),
                        ],
                        axis=1,
                    )
                if "metrics" in results_df.columns:  # Flatten dicts for CSV
                    results_df = pd.concat(
                        [
                            results_df.drop(["metrics"], axis=1),
                            pd.json_normalize(results_df["metrics"]).add_prefix(
                                "metric_"
                            ),
                        ],
                        axis=1,
                    )
                if "inputs" in results_df.columns:  # Flatten dicts for CSV
                    results_df = pd.concat(
                        [
                            results_df.drop(["inputs"], axis=1),
                            pd.json_normalize(results_df["inputs"]).add_prefix(
                                "input_"
                            ),
                        ],
                        axis=1,
                    )

                # Convert lists/dicts in metadata columns for CSV saving
                for col in metadata_columns or []:
                    if col in results_df.columns:
                        # Check if column contains lists/dicts before converting
                        if (
                            results_df[col]
                            .apply(lambda x: isinstance(x, (list, dict)))
                            .any()
                        ):
                            results_df[col] = results_df[col].apply(json.dumps)

                if output_path.suffix.lower() == ".csv":
                    results_df.to_csv(output_path, index=False)
                elif output_path.suffix.lower() == ".json":
                    # Save list of dicts directly (before potential DataFrame manipulation)
                    # Need to handle non-serializable types like Box
                    serializable_results = []
                    for res_dict in final_results_list:
                        if "agent_output" in res_dict and isinstance(
                            res_dict["agent_output"], Box
                        ):
                            res_dict["agent_output"] = res_dict[
                                "agent_output"
                            ].to_dict()
                        serializable_results.append(res_dict)
                    with open(output_path, "w", encoding="utf-8") as f:
                        json.dump(
                            serializable_results, f, indent=2, default=str
                        )  # Use default=str for safety
                else:
                    logger.warning(
                        f"Unsupported output file format: {output_path.suffix}. Use .csv or .json."
                    )
                logger.info(
                    f"Detailed evaluation results saved to {output_path}"
                )
            except Exception as e:
                logger.error(
                    f"Failed to save evaluation results to {output_file}: {e}",
                    exc_info=True,
                )

        if return_dataframe:
            if not PANDAS_AVAILABLE:
                logger.error(
                    "Cannot return DataFrame: pandas library not installed."
                )
                return final_results_list  # Fallback to list
            # Ensure DataFrame is created if not done for saving
            if "results_df" not in locals():
                results_df = pd.DataFrame(final_results_list)
                # Convert Box if needed
                if "agent_output" in results_df.columns:
                    results_df["agent_output"] = results_df[
                        "agent_output"
                    ].apply(lambda x: x.to_dict() if isinstance(x, Box) else x)
            return results_df
        else:
            # Ensure Box objects are converted if returning list
            final_list = []
            for res_dict in final_results_list:
                if "agent_output" in res_dict and isinstance(
                    res_dict["agent_output"], Box
                ):
                    res_dict["agent_output"] = res_dict[
                        "agent_output"
                    ].to_dict()
                final_list.append(res_dict)
            return final_list

```

---

### 67. src\flock\core\execution\local_executor.py

- **File ID**: file_66
- **Type**: Code File
- **Line Count**: 31
- **Description**: File at src\flock\core\execution\local_executor.py
- **Dependencies**: None
- **Used By**: None

**Content**:
```
# src/your_package/core/execution/local_executor.py
from flock.core.context.context import FlockContext
from flock.core.logging.logging import get_logger
from flock.workflow.activities import (
    run_agent,  # This should be the local activity function
)

logger = get_logger("flock")


async def run_local_workflow(
    context: FlockContext, box_result: bool = True
) -> dict:
    """Execute the agent workflow locally (for debugging).

    Args:
        context: The FlockContext instance with state and history.
        output_formatter: Formatter options for displaying results.
        box_result: If True, wraps the result in a Box for nicer display.

    Returns:
        A dictionary containing the workflow result.
    """
    logger.info("Running local debug workflow")
    result = await run_agent(context)
    if box_result:
        from box import Box

        logger.debug("Boxing result")
        return Box(result)
    return result

```

---

### 68. src\flock\core\execution\temporal_executor.py

- **File ID**: file_67
- **Type**: Code File
- **Line Count**: 49
- **Description**: File at src\flock\core\execution\temporal_executor.py
- **Dependencies**: None
- **Used By**: None

**Content**:
```
# src/your_package/core/execution/temporal_executor.py

from flock.core.context.context import FlockContext
from flock.core.context.context_vars import FLOCK_RUN_ID
from flock.core.logging.logging import get_logger
from flock.workflow.activities import (
    run_agent,  # Activity function used in Temporal
)
from flock.workflow.temporal_setup import create_temporal_client, setup_worker
from flock.workflow.workflow import FlockWorkflow  # Your workflow class

logger = get_logger("flock")


async def run_temporal_workflow(
    context: FlockContext,
    box_result: bool = True,
) -> dict:
    """Execute the agent workflow via Temporal for robust, distributed processing.

    Args:
        context: The FlockContext instance with state and history.
        box_result: If True, wraps the result in a Box for nicer display.

    Returns:
        A dictionary containing the workflow result.
    """
    logger.info("Setting up Temporal workflow")
    await setup_worker(workflow=FlockWorkflow, activity=run_agent)
    logger.debug("Creating Temporal client")
    flock_client = await create_temporal_client()
    workflow_id = context.get_variable(FLOCK_RUN_ID)
    logger.info("Executing Temporal workflow", workflow_id=workflow_id)
    result = await flock_client.execute_workflow(
        FlockWorkflow.run,
        context.to_dict(),
        id=workflow_id,
        task_queue="flock-queue",
    )

    agent_name = context.get_variable("FLOCK_CURRENT_AGENT")
    logger.debug("Formatting Temporal result", agent=agent_name)

    if box_result:
        from box import Box

        logger.debug("Boxing Temporal result")
        return Box(result)
    return result

```

---

### 69. src\flock\core\flock.py

- **File ID**: file_68
- **Type**: Code File
- **Line Count**: 661
- **Description**: High-level orchestrator for managing and executing agents within the Flock framework.
- **Dependencies**: None
- **Used By**: None

**Content**:
```
# src/flock/core/flock.py
"""High-level orchestrator for managing and executing agents within the Flock framework."""

from __future__ import annotations  # Ensure forward references work

import asyncio
import os
import uuid
from collections.abc import Callable
from pathlib import Path
from typing import (
    TYPE_CHECKING,
    Any,
    Literal,
    TypeVar,
)

# Third-party imports
from box import Box
from datasets import Dataset
from opentelemetry import trace
from opentelemetry.baggage import get_baggage, set_baggage
from pandas import DataFrame
from pydantic import BaseModel, Field

# Flock core components & utilities
from flock.config import DEFAULT_MODEL, TELEMETRY
from flock.core.context.context import FlockContext
from flock.core.context.context_manager import initialize_context
from flock.core.execution.local_executor import run_local_workflow
from flock.core.execution.temporal_executor import run_temporal_workflow
from flock.core.flock_evaluator import FlockEvaluator
from flock.core.logging.logging import LOGGERS, get_logger, get_module_loggers
from flock.core.serialization.serializable import Serializable
from flock.core.util.cli_helper import init_console

# Import FlockAgent using TYPE_CHECKING to avoid circular import at runtime
if TYPE_CHECKING:
    # These imports are only for type hints
    from flock.core.flock_agent import FlockAgent


# Registry
from flock.core.flock_registry import get_registry

try:
    import pandas as pd

    PANDAS_AVAILABLE = True
except ImportError:
    pd = None
    PANDAS_AVAILABLE = False

logger = get_logger("flock")
TELEMETRY.setup_tracing()  # Setup OpenTelemetry
tracer = trace.get_tracer(__name__)
FlockRegistry = get_registry()  # Get the registry instance

# Define TypeVar for generic class methods like from_dict
T = TypeVar("T", bound="Flock")


class Flock(BaseModel, Serializable):
    """Orchestrator for managing and executing agent systems.

    Manages agent definitions, context, and execution flow (local or Temporal).
    Relies on FlockSerializer for serialization/deserialization logic.
    Inherits from Pydantic BaseModel and Serializable.
    """

    name: str | None = Field(
        default_factory=lambda: f"flock_{uuid.uuid4().hex[:8]}",
        description="A unique identifier for this Flock instance.",
    )
    model: str | None = Field(
        default=DEFAULT_MODEL,
        description="Default model identifier for agents if not specified otherwise.",
    )
    description: str | None = Field(
        default=None,
        description="A brief description of the purpose of this Flock configuration.",
    )
    enable_temporal: bool = Field(
        default=False,
        description="If True, execute workflows via Temporal; otherwise, run locally.",
    )
    enable_logging: bool = Field(
        default=False,
        description="If True, enable logging for the Flock instance.",
    )
    show_flock_banner: bool = Field(
        default=True,
        description="If True, show the Flock banner on console interactions.",
    )
    # Internal agent storage - not part of the Pydantic model for direct serialization
    _agents: dict[str, FlockAgent]
    _start_agent_name: str | None = None  # For potential pre-configuration
    _start_input: dict = {}  # For potential pre-configuration

    # Pydantic v2 model config
    model_config = {
        "arbitrary_types_allowed": True,
        "ignored_types": (type(FlockRegistry),),
    }

    def __init__(
        self,
        name: str | None = None,
        model: str | None = DEFAULT_MODEL,
        description: str | None = None,
        show_flock_banner: bool = True,
        enable_temporal: bool = False,
        enable_logging: bool | list[str] = False,
        agents: list[FlockAgent] | None = None,
        **kwargs,
    ):
        """Initialize the Flock orchestrator."""
        # Use provided name or generate default BEFORE super init if needed elsewhere
        effective_name = name or f"flock_{uuid.uuid4().hex[:8]}"

        # Initialize Pydantic fields
        super().__init__(
            name=effective_name,
            model=model,
            description=description,
            enable_temporal=enable_temporal,
            enable_logging=enable_logging,
            show_flock_banner=show_flock_banner,
            **kwargs,
        )

        # Initialize runtime attributes AFTER super().__init__()
        self._agents = {}
        self._start_agent_name = None
        self._start_input = {}

        # Set up logging based on the enable_logging flag
        self._configure_logging(enable_logging)  # Use instance attribute

        # Register passed agents
        if agents:
            from flock.core.flock_agent import FlockAgent as ConcreteFlockAgent

            for agent in agents:
                if isinstance(agent, ConcreteFlockAgent):
                    self.add_agent(agent)
                else:
                    logger.warning(
                        f"Item provided in 'agents' list is not a FlockAgent: {type(agent)}"
                    )

        # Initialize console if needed for banner
        init_console(clear_screen=True, show_banner=self.show_flock_banner)

        # Set Temporal debug environment variable
        self._set_temporal_debug_flag()

        # Ensure session ID exists in baggage
        self._ensure_session_id()

        logger.info(
            "Flock instance initialized",
            name=self.name,
            model=self.model,
            enable_temporal=self.enable_temporal,
        )

    def _configure_logging(self, enable_logging: bool | list[str]):
        """Configure logging levels based on the enable_logging flag."""
        is_enabled_globally = False
        enabled_loggers = []

        if isinstance(enable_logging, bool):
            is_enabled_globally = enable_logging
        elif isinstance(enable_logging, list):
            is_enabled_globally = bool(enable_logging)
            enabled_loggers = enable_logging

        # Configure core loggers
        for log_name in LOGGERS:
            log_instance = get_logger(log_name)
            if is_enabled_globally or log_name in enabled_loggers:
                log_instance.enable_logging = True
            else:
                log_instance.enable_logging = False

        # Configure module loggers (existing ones)
        module_loggers = get_module_loggers()
        for mod_log in module_loggers:
            if is_enabled_globally or mod_log.name in enabled_loggers:
                mod_log.enable_logging = True
            else:
                mod_log.enable_logging = False

    def _set_temporal_debug_flag(self):
        """Set or remove LOCAL_DEBUG env var based on enable_temporal."""
        if not self.enable_temporal:
            if "LOCAL_DEBUG" not in os.environ:
                os.environ["LOCAL_DEBUG"] = "1"
                logger.debug(
                    "Set LOCAL_DEBUG environment variable for local execution."
                )
        elif "LOCAL_DEBUG" in os.environ:
            del os.environ["LOCAL_DEBUG"]
            logger.debug(
                "Removed LOCAL_DEBUG environment variable for Temporal execution."
            )

    def _ensure_session_id(self):
        """Ensure a session_id exists in the OpenTelemetry baggage."""
        session_id = get_baggage("session_id")
        if not session_id:
            session_id = str(uuid.uuid4())
            set_baggage("session_id", session_id)
            logger.debug(f"Generated new session_id: {session_id}")

    def add_agent(self, agent: FlockAgent) -> FlockAgent:
        """Adds an agent instance to this Flock configuration and registry."""
        from flock.core.flock_agent import FlockAgent as ConcreteFlockAgent

        if not isinstance(agent, ConcreteFlockAgent):
            raise TypeError("Provided object is not a FlockAgent instance.")
        if not agent.name:
            raise ValueError("Agent must have a name.")

        if agent.name in self._agents:
            raise ValueError("Agent with this name already exists.")
        self._agents[agent.name] = agent
        FlockRegistry.register_agent(agent)  # Register globally

        # Set default model if agent doesn't have one
        if agent.model is None:
            if self.model:
                agent.set_model(self.model)
                logger.debug(
                    f"Agent '{agent.name}' using Flock default model: {self.model}"
                )
            else:
                logger.warning(
                    f"Agent '{agent.name}' has no model and Flock default model is not set."
                )

        logger.info(f"Agent '{agent.name}' added to Flock '{self.name}'.")
        return agent

    @property
    def agents(self) -> dict[str, FlockAgent]:
        """Returns the dictionary of agents managed by this Flock instance."""
        return self._agents

    def run(
        self,
        start_agent: FlockAgent | str | None = None,
        input: dict = {},
        context: FlockContext | None = None,
        run_id: str = "",
        box_result: bool = True,
        agents: list[FlockAgent] | None = None,
    ) -> Box | dict:
        """Entry point for running an agent system synchronously."""
        try:
            loop = asyncio.get_running_loop()
            # If loop exists, check if it's closed
            if loop.is_closed():
                raise RuntimeError("Event loop is closed")
        except RuntimeError:  # No running loop
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)

        # Ensure the loop runs the task and handles closure if we created it
        if asyncio.get_event_loop() is loop and not loop.is_running():
            result = loop.run_until_complete(
                self.run_async(
                    start_agent=start_agent,
                    input=input,
                    context=context,
                    run_id=run_id,
                    box_result=box_result,
                    agents=agents,
                )
            )
            return result
        else:
            future = asyncio.ensure_future(
                self.run_async(
                    start_agent=start_agent,
                    input=input,
                    context=context,
                    run_id=run_id,
                    box_result=box_result,
                    agents=agents,
                )
            )
            return loop.run_until_complete(future)

    async def run_async(
        self,
        start_agent: FlockAgent | str | None = None,
        input: dict | None = None,
        context: FlockContext | None = None,
        run_id: str = "",
        box_result: bool = True,
        agents: list[FlockAgent] | None = None,
    ) -> Box | dict:
        """Entry point for running an agent system asynchronously."""
        # Import here to allow forward reference resolution
        from flock.core.flock_agent import FlockAgent as ConcreteFlockAgent

        with tracer.start_as_current_span("flock.run_async") as span:
            # Add passed agents first
            if agents:
                for agent_obj in agents:
                    if isinstance(agent_obj, ConcreteFlockAgent):
                        self.add_agent(agent_obj)
                    else:
                        logger.warning(
                            f"Item in 'agents' list is not a FlockAgent: {type(agent_obj)}"
                        )

            # Determine starting agent name
            start_agent_name: str | None = None
            if isinstance(start_agent, ConcreteFlockAgent):
                start_agent_name = start_agent.name
                if start_agent_name not in self._agents:
                    self.add_agent(start_agent)
            elif isinstance(start_agent, str):
                start_agent_name = start_agent
            else:
                start_agent_name = self._start_agent_name

            # Default to first agent if only one exists and none specified
            if not start_agent_name and len(self._agents) == 1:
                start_agent_name = list(self._agents.keys())[0]
            elif not start_agent_name:
                raise ValueError(
                    "No start_agent specified and multiple/no agents exist."
                )

            run_input = input if input is not None else self._start_input
            effective_run_id = run_id or f"flockrun_{uuid.uuid4().hex[:8]}"

            span.set_attribute("start_agent", start_agent_name)
            span.set_attribute("input", str(run_input))
            span.set_attribute("run_id", effective_run_id)
            span.set_attribute("enable_temporal", self.enable_temporal)
            logger.info(
                f"Initiating Flock run '{self.name}'. Start Agent: '{start_agent_name}'. Temporal: {self.enable_temporal}."
            )

            try:
                resolved_start_agent = self._agents.get(start_agent_name)
                if not resolved_start_agent:
                    resolved_start_agent = FlockRegistry.get_agent(start_agent_name)
                    if not resolved_start_agent:
                        raise ValueError(f"Start agent '{start_agent_name}' not found.")
                    self.add_agent(resolved_start_agent)

                run_context = context if context else FlockContext()
                set_baggage("run_id", effective_run_id)

                initialize_context(
                    run_context,
                    start_agent_name,
                    run_input,
                    effective_run_id,
                    not self.enable_temporal,
                    self.model or resolved_start_agent.model or DEFAULT_MODEL,
                )
                # Add agent definitions to context for routing/serialization within workflow
                for agent_name, agent_instance in self.agents.items():
                    # Agents already handle their serialization
                    agent_dict_repr = agent_instance.to_dict()
                    run_context.add_agent_definition(
                        agent_type=type(agent_instance),
                        agent_name=agent_name,
                        agent_data=agent_dict_repr,  # Pass the serialized dict
                    )

                logger.info(
                    "Starting agent execution",
                    agent=start_agent_name,
                    enable_temporal=self.enable_temporal,
                )

                # Execute workflow
                if not self.enable_temporal:
                    result = await run_local_workflow(run_context, box_result=False)
                else:
                    result = await run_temporal_workflow(run_context, box_result=False)

                span.set_attribute("result.type", str(type(result)))
                result_str = str(result)
                span.set_attribute(
                    "result.preview",
                    result_str[:1000] + ("..." if len(result_str) > 1000 else ""),
                )

                if box_result:
                    try:
                        logger.debug("Boxing final result.")
                        return Box(result)
                    except ImportError:
                        logger.warning("Box library not installed, returning raw dict.")
                        return result
                else:
                    return result

            except Exception as e:
                logger.error(f"Flock run '{self.name}' failed: {e}", exc_info=True)
                span.record_exception(e)
                span.set_status(trace.Status(trace.StatusCode.ERROR, str(e)))
                return {
                    "error": str(e),
                    "details": f"Flock run '{self.name}' failed.",
                }

    # --- Batch Processing (Delegation) ---
    async def run_batch_async(
        self,
        start_agent: FlockAgent | str,
        batch_inputs: list[dict[str, Any]] | DataFrame | str,
        input_mapping: dict[str, str] | None = None,
        static_inputs: dict[str, Any] | None = None,
        parallel: bool = True,
        max_workers: int = 5,
        use_temporal: bool | None = None,
        box_results: bool = True,
        return_errors: bool = False,
        silent_mode: bool = False,
        write_to_csv: str | None = None,
        hide_columns: list[str] | None = None,
        delimiter: str = ",",
    ) -> list[Box | dict | None | Exception]:
        """Runs the specified agent/workflow for each item in a batch asynchronously (delegated)."""
        # Import processor locally
        from flock.core.execution.batch_executor import BatchProcessor

        processor = BatchProcessor(self)  # Pass self
        return await processor.run_batch_async(
            start_agent=start_agent,
            batch_inputs=batch_inputs,
            input_mapping=input_mapping,
            static_inputs=static_inputs,
            parallel=parallel,
            max_workers=max_workers,
            use_temporal=use_temporal,
            box_results=box_results,
            return_errors=return_errors,
            silent_mode=silent_mode,
            write_to_csv=write_to_csv,
            hide_columns=hide_columns,
            delimiter=delimiter,
        )

    def run_batch(
        self,
        start_agent: FlockAgent | str,
        batch_inputs: list[dict[str, Any]] | DataFrame | str,
        input_mapping: dict[str, str] | None = None,
        static_inputs: dict[str, Any] | None = None,
        parallel: bool = True,
        max_workers: int = 5,
        use_temporal: bool | None = None,
        box_results: bool = True,
        return_errors: bool = False,
        silent_mode: bool = False,
        write_to_csv: str | None = None,
        hide_columns: list[str] | None = None,
        delimiter: str = ",",
    ) -> list[Box | dict | None | Exception]:
        """Synchronous wrapper for run_batch_async."""
        # (Standard asyncio run wrapper logic)
        try:
            loop = asyncio.get_running_loop()
            if loop.is_closed():
                raise RuntimeError("Event loop is closed")
        except RuntimeError:  # No running loop
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)

        coro = self.run_batch_async(
            start_agent=start_agent,
            batch_inputs=batch_inputs,
            input_mapping=input_mapping,
            static_inputs=static_inputs,
            parallel=parallel,
            max_workers=max_workers,
            use_temporal=use_temporal,
            box_results=box_results,
            return_errors=return_errors,
            silent_mode=silent_mode,
            write_to_csv=write_to_csv,
            hide_columns=hide_columns,
            delimiter=delimiter,
        )

        if asyncio.get_event_loop() is loop and not loop.is_running():
            results = loop.run_until_complete(coro)
            return results
        else:
            future = asyncio.ensure_future(coro)
            return loop.run_until_complete(future)

    # --- Evaluation (Delegation) ---
    async def evaluate_async(
        self,
        dataset: str | Path | list[dict[str, Any]] | DataFrame | Dataset,
        start_agent: FlockAgent | str,
        input_mapping: dict[str, str],
        answer_mapping: dict[str, str],
        metrics: list[
            str
            | Callable[[Any, Any], bool | float | dict[str, Any]]
            | FlockAgent
            | FlockEvaluator
        ],
        metric_configs: dict[str, dict[str, Any]] | None = None,
        static_inputs: dict[str, Any] | None = None,
        parallel: bool = True,
        max_workers: int = 5,
        use_temporal: bool | None = None,
        error_handling: Literal["raise", "skip", "log"] = "log",
        output_file: str | Path | None = None,
        return_dataframe: bool = True,
        silent_mode: bool = False,
        metadata_columns: list[str] | None = None,
    ) -> DataFrame | list[dict[str, Any]]:
        """Evaluates the Flock's performance against a dataset (delegated)."""
        # Import processor locally
        from flock.core.execution.evaluation_executor import (
            EvaluationExecutor,
        )

        processor = EvaluationExecutor(self)  # Pass self
        return await processor.evaluate_async(
            dataset=dataset,
            start_agent=start_agent,
            input_mapping=input_mapping,
            answer_mapping=answer_mapping,
            metrics=metrics,
            metric_configs=metric_configs,
            static_inputs=static_inputs,
            parallel=parallel,
            max_workers=max_workers,
            use_temporal=use_temporal,
            error_handling=error_handling,
            output_file=output_file,
            return_dataframe=return_dataframe,
            silent_mode=silent_mode,
            metadata_columns=metadata_columns,
        )

    def evaluate(
        self,
        dataset: str | Path | list[dict[str, Any]] | DataFrame | Dataset,
        start_agent: FlockAgent | str,
        input_mapping: dict[str, str],
        answer_mapping: dict[str, str],
        metrics: list[
            str
            | Callable[[Any, Any], bool | float | dict[str, Any]]
            | FlockAgent
            | FlockEvaluator
        ],
        metric_configs: dict[str, dict[str, Any]] | None = None,
        static_inputs: dict[str, Any] | None = None,
        parallel: bool = True,
        max_workers: int = 5,
        use_temporal: bool | None = None,
        error_handling: Literal["raise", "skip", "log"] = "log",
        output_file: str | Path | None = None,
        return_dataframe: bool = True,
        silent_mode: bool = False,
        metadata_columns: list[str] | None = None,
    ) -> DataFrame | list[dict[str, Any]]:
        """Synchronous wrapper for evaluate_async."""
        # (Standard asyncio run wrapper logic)
        try:
            loop = asyncio.get_running_loop()
            if loop.is_closed():
                raise RuntimeError("Event loop is closed")
        except RuntimeError:  # No running loop
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)

        coro = self.evaluate_async(
            dataset=dataset,
            start_agent=start_agent,
            input_mapping=input_mapping,
            answer_mapping=answer_mapping,
            metrics=metrics,
            metric_configs=metric_configs,
            static_inputs=static_inputs,
            parallel=parallel,
            max_workers=max_workers,
            use_temporal=use_temporal,
            error_handling=error_handling,
            output_file=output_file,
            return_dataframe=return_dataframe,
            silent_mode=silent_mode,
            metadata_columns=metadata_columns,
        )

        if asyncio.get_event_loop() is loop and not loop.is_running():
            results = loop.run_until_complete(coro)
            return results
        else:
            future = asyncio.ensure_future(coro)
            return loop.run_until_complete(future)

    # --- API Server Starter ---
    def start_api(
        self,
        host: str = "127.0.0.1",
        port: int = 8344,
        server_name: str = "Flock API",
        create_ui: bool = False,
    ) -> None:
        """Starts a REST API server for this Flock instance."""
        # Import runner locally
        from flock.core.api.runner import start_flock_api

        start_flock_api(self, host, port, server_name, create_ui)

    # --- CLI Starter ---
    def start_cli(
        self,
        server_name: str = "Flock CLI",
        show_results: bool = False,
        edit_mode: bool = False,
    ) -> None:
        """Starts an interactive CLI for this Flock instance."""
        # Import runner locally
        from flock.cli.runner import start_flock_cli

        start_flock_cli(self, server_name, show_results, edit_mode)

    # --- Serialization Delegation Methods ---
    def to_dict(self, path_type: str = "relative") -> dict[str, Any]:
        """Serialize Flock instance to dictionary using FlockSerializer."""
        # Import locally to prevent circular imports at module level if structure is complex
        from flock.core.serialization.flock_serializer import FlockSerializer

        return FlockSerializer.serialize(self, path_type=path_type)

    @classmethod
    def from_dict(cls: type[T], data: dict[str, Any]) -> T:
        """Deserialize Flock instance from dictionary using FlockSerializer."""
        # Import locally
        from flock.core.serialization.flock_serializer import FlockSerializer

        return FlockSerializer.deserialize(cls, data)

    # --- Static Method Loader (Delegates to loader module) ---
    @staticmethod
    def load_from_file(file_path: str) -> Flock:
        """Load a Flock instance from various file formats (delegates to loader)."""
        # Import locally
        from flock.core.util.loader import load_flock_from_file

        return load_flock_from_file(file_path)

```

---

### 70. src\flock\core\flock_agent.py

- **File ID**: file_69
- **Type**: Code File
- **Line Count**: 784
- **Description**: FlockAgent is the core, declarative base class for all agents in the Flock framework.
- **Dependencies**: None
- **Used By**: None

**Content**:
```
# src/flock/core/flock_agent.py
"""FlockAgent is the core, declarative base class for all agents in the Flock framework."""

import asyncio
import json
import os
from abc import ABC
from collections.abc import Callable
from datetime import datetime
from typing import TYPE_CHECKING, Any, TypeVar

from flock.core.serialization.json_encoder import FlockJSONEncoder

if TYPE_CHECKING:
    from flock.core.context.context import FlockContext
    from flock.core.flock_evaluator import FlockEvaluator
    from flock.core.flock_module import FlockModule
    from flock.core.flock_router import FlockRouter

from opentelemetry import trace
from pydantic import BaseModel, Field
from rich.console import Console

# Core Flock components (ensure these are importable)
from flock.core.context.context import FlockContext
from flock.core.flock_evaluator import FlockEvaluator
from flock.core.flock_module import FlockModule
from flock.core.flock_router import FlockRouter
from flock.core.logging.logging import get_logger

# Mixins and Serialization components
from flock.core.mixin.dspy_integration import DSPyIntegrationMixin
from flock.core.serialization.serializable import (
    Serializable,  # Import Serializable base
)
from flock.core.serialization.serialization_utils import (
    deserialize_component,
    serialize_item,
)

console = Console()

logger = get_logger("agent")
tracer = trace.get_tracer(__name__)
T = TypeVar("T", bound="FlockAgent")


# Make FlockAgent inherit from Serializable
class FlockAgent(BaseModel, Serializable, DSPyIntegrationMixin, ABC):
    """Core, declarative base class for Flock agents, enabling serialization,
    modularity, and integration with evaluation and routing components.
    Inherits from Pydantic BaseModel, ABC, DSPyIntegrationMixin, and Serializable.
    """

    name: str = Field(..., description="Unique identifier for the agent.")
    model: str | None = Field(
        None,
        description="The model identifier to use (e.g., 'openai/gpt-4o'). If None, uses Flock's default.",
    )
    description: str | Callable[..., str] | None = Field(
        "",
        description="A human-readable description or a callable returning one.",
    )
    input: str | Callable[..., str] | None = Field(
        None,
        description=(
            "Signature for input keys. Supports type hints (:) and descriptions (|). "
            "E.g., 'query: str | Search query, context: dict | Conversation context'. Can be a callable."
        ),
    )
    output: str | Callable[..., str] | None = Field(
        None,
        description=(
            "Signature for output keys. Supports type hints (:) and descriptions (|). "
            "E.g., 'result: str | Generated result, summary: str | Brief summary'. Can be a callable."
        ),
    )
    tools: list[Callable[..., Any]] | None = (
        Field(  # Assume tools are always callable for serialization simplicity
            default=None,
            description="List of callable tools the agent can use. These must be registered.",
        )
    )
    write_to_file: bool = Field(
        default=False,
        description="Write the agent's output to a file.",
    )
    wait_for_input: bool = Field(
        default=False,
        description="Wait for user input after the agent's output is displayed.",
    )

    # --- Components ---
    evaluator: FlockEvaluator | None = Field(  # Make optional, allow None
        default=None,
        description="The evaluator instance defining the agent's core logic.",
    )
    handoff_router: FlockRouter | None = Field(  # Make optional, allow None
        default=None,
        description="Router determining the next agent in the workflow.",
    )
    modules: dict[str, FlockModule] = Field(  # Keep as dict
        default_factory=dict,
        description="Dictionary of FlockModules attached to this agent.",
    )

    # --- Runtime State (Excluded from Serialization) ---
    context: FlockContext | None = Field(
        default=None,
        exclude=True,  # Exclude context from model_dump and serialization
        description="Runtime context associated with the flock execution.",
    )

    # --- Existing Methods (add_module, remove_module, etc.) ---
    # (Keep these methods as they were, adding type hints where useful)
    def add_module(self, module: FlockModule) -> None:
        """Add a module to this agent."""
        if not module.name:
            logger.error("Module must have a name to be added.")
            return
        if module.name in self.modules:
            logger.warning(f"Overwriting existing module: {module.name}")
        self.modules[module.name] = module
        logger.debug(f"Added module '{module.name}' to agent '{self.name}'")

    def remove_module(self, module_name: str) -> None:
        """Remove a module from this agent."""
        if module_name in self.modules:
            del self.modules[module_name]
            logger.debug(
                f"Removed module '{module_name}' from agent '{self.name}'"
            )
        else:
            logger.warning(
                f"Module '{module_name}' not found on agent '{self.name}'."
            )

    def get_module(self, module_name: str) -> FlockModule | None:
        """Get a module by name."""
        return self.modules.get(module_name)

    def get_enabled_modules(self) -> list[FlockModule]:
        """Get a list of currently enabled modules attached to this agent."""
        return [m for m in self.modules.values() if m.config.enabled]

    # --- Lifecycle Hooks (Keep as they were) ---
    async def initialize(self, inputs: dict[str, Any]) -> None:
        """Initialize agent and run module initializers."""
        logger.debug(f"Initializing agent '{self.name}'")
        with tracer.start_as_current_span("agent.initialize") as span:
            span.set_attribute("agent.name", self.name)
            span.set_attribute("inputs", str(inputs))
            logger.info(
                f"agent.initialize",
                agent=self.name,
            )
            try:
                for module in self.get_enabled_modules():
                    await module.initialize(self, inputs, self.context)
            except Exception as module_error:
                logger.error(
                    "Error during initialize",
                    agent=self.name,
                    error=str(module_error),
                )
                span.record_exception(module_error)

    async def terminate(
        self, inputs: dict[str, Any], result: dict[str, Any]
    ) -> None:
        """Terminate agent and run module terminators."""
        logger.debug(f"Terminating agent '{self.name}'")
        with tracer.start_as_current_span("agent.terminate") as span:
            span.set_attribute("agent.name", self.name)
            span.set_attribute("inputs", str(inputs))
            span.set_attribute("result", str(result))
            logger.info(
                f"agent.terminate",
                agent=self.name,
            )
            try:
                for module in self.get_enabled_modules():
                    await module.terminate(self, inputs, result, self.context)

                if self.write_to_file:
                    self._save_output(self.name, result)
                if self.wait_for_input:
                    console.input(prompt="Press Enter to continue...")
            except Exception as module_error:
                logger.error(
                    "Error during terminate",
                    agent=self.name,
                    error=str(module_error),
                )
                span.record_exception(module_error)

    async def on_error(self, error: Exception, inputs: dict[str, Any]) -> None:
        """Handle errors and run module error handlers."""
        logger.error(f"Error occurred in agent '{self.name}': {error}")
        with tracer.start_as_current_span("agent.on_error") as span:
            span.set_attribute("agent.name", self.name)
            span.set_attribute("inputs", str(inputs))
            try:
                for module in self.get_enabled_modules():
                    await module.on_error(self, error, inputs, self.context)
            except Exception as module_error:
                logger.error(
                    "Error during on_error",
                    agent=self.name,
                    error=str(module_error),
                )
                span.record_exception(module_error)

    async def evaluate(self, inputs: dict[str, Any]) -> dict[str, Any]:
        """Core evaluation logic, calling the assigned evaluator and modules."""
        if not self.evaluator:
            raise RuntimeError(
                f"Agent '{self.name}' has no evaluator assigned."
            )
        with tracer.start_as_current_span("agent.evaluate") as span:
            span.set_attribute("agent.name", self.name)
            span.set_attribute("inputs", str(inputs))
            logger.info(
                f"agent.evaluate",
                agent=self.name,
            )

            logger.debug(f"Evaluating agent '{self.name}'")
            current_inputs = inputs

            # Pre-evaluate hooks
            for module in self.get_enabled_modules():
                current_inputs = await module.pre_evaluate(
                    self, current_inputs, self.context
                )

            # Actual evaluation
            try:
                # Pass registered tools if the evaluator needs them
                registered_tools = []
                if self.tools:
                    # Ensure tools are actually retrieved/validated if needed by evaluator type
                    # For now, assume evaluator handles tool resolution if necessary
                    registered_tools = self.tools

                result = await self.evaluator.evaluate(
                    self, current_inputs, registered_tools
                )
            except Exception as eval_error:
                logger.error(
                    "Error during evaluate",
                    agent=self.name,
                    error=str(eval_error),
                )
                span.record_exception(eval_error)
                await self.on_error(
                    eval_error, current_inputs
                )  # Call error hook
                raise  # Re-raise the exception

            # Post-evaluate hooks
            current_result = result
            for module in self.get_enabled_modules():
                current_result = await module.post_evaluate(
                    self, current_inputs, current_result, self.context
                )

            logger.debug(f"Evaluation completed for agent '{self.name}'")
            return current_result

    def run(self, inputs: dict[str, Any]) -> dict[str, Any]:
        """Synchronous wrapper for run_async."""
        try:
            loop = asyncio.get_running_loop()
        except (
            RuntimeError
        ):  # 'RuntimeError: There is no current event loop...'
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        return loop.run_until_complete(self.run_async(inputs))

    def set_model(self, model: str):
        """Set the model for the agent and its evaluator."""
        self.model = model
        if self.evaluator and hasattr(self.evaluator, "config"):
            self.evaluator.config.model = model
            logger.info(
                f"Set model to '{model}' for agent '{self.name}' and its evaluator."
            )
        elif self.evaluator:
            logger.warning(
                f"Evaluator for agent '{self.name}' does not have a standard config to set model."
            )
        else:
            logger.warning(
                f"Agent '{self.name}' has no evaluator to set model for."
            )

    async def run_async(self, inputs: dict[str, Any]) -> dict[str, Any]:
        """Asynchronous execution logic with lifecycle hooks."""
        with tracer.start_as_current_span("agent.run") as span:
            span.set_attribute("agent.name", self.name)
            span.set_attribute("inputs", str(inputs))
            try:
                await self.initialize(inputs)
                result = await self.evaluate(inputs)
                await self.terminate(inputs, result)
                span.set_attribute("result", str(result))
                logger.info("Agent run completed", agent=self.name)
                return result
            except Exception as run_error:
                logger.error(
                    "Error running agent", agent=self.name, error=str(run_error)
                )
                if "evaluate" not in str(
                    run_error
                ):  # Simple check, might need refinement
                    await self.on_error(run_error, inputs)
                logger.error(
                    f"Agent '{self.name}' run failed: {run_error}",
                    exc_info=True,
                )
                span.record_exception(run_error)
                raise  # Re-raise after handling

    async def run_temporal(self, inputs: dict[str, Any]) -> dict[str, Any]:
        with tracer.start_as_current_span("agent.run_temporal") as span:
            span.set_attribute("agent.name", self.name)
            span.set_attribute("inputs", str(inputs))
            try:
                from temporalio.client import Client

                from flock.workflow.agent_activities import (
                    run_flock_agent_activity,
                )
                from flock.workflow.temporal_setup import run_activity

                client = await Client.connect(
                    "localhost:7233", namespace="default"
                )
                agent_data = self.to_dict()
                inputs_data = inputs

                result = await run_activity(
                    client,
                    self.name,
                    run_flock_agent_activity,
                    {"agent_data": agent_data, "inputs": inputs_data},
                )
                span.set_attribute("result", str(result))
                logger.info("Temporal run successful", agent=self.name)
                return result
            except Exception as temporal_error:
                logger.error(
                    "Error in Temporal workflow",
                    agent=self.name,
                    error=str(temporal_error),
                )
                span.record_exception(temporal_error)
                raise

    # resolve_callables remains useful for dynamic definitions
    def resolve_callables(self, context: FlockContext | None = None) -> None:
        """Resolves callable fields (description, input, output) using context."""
        if callable(self.description):
            self.description = self.description(
                context
            )  # Pass context if needed by callable
        if callable(self.input):
            self.input = self.input(context)
        if callable(self.output):
            self.output = self.output(context)

    # --- Serialization Implementation ---

    def _save_output(self, agent_name: str, result: dict[str, Any]) -> None:
        """Save output to file if configured."""
        if not self.write_to_file:
            return

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{agent_name}_output_{timestamp}.json"
        filepath = os.path.join("output/", filename)
        os.makedirs("output/", exist_ok=True)

        output_data = {
            "agent": agent_name,
            "timestamp": timestamp,
            "output": result,
        }

        try:
            with open(filepath, "w") as f:
                json.dump(output_data, f, indent=2, cls=FlockJSONEncoder)
        except Exception as e:
            logger.warning(f"Failed to save output to file: {e}")

    def to_dict(self) -> dict[str, Any]:
        """Convert instance to dictionary representation suitable for serialization."""
        from flock.core.flock_registry import get_registry

        FlockRegistry = get_registry()

        exclude = ["context", "evaluator", "modules", "handoff_router", "tools"]

        is_descrition_callable = False
        is_input_callable = False
        is_output_callable = False

        # if self.description is a callable, exclude it
        if callable(self.description):
            is_descrition_callable = True
            exclude.append("description")
        # if self.input is a callable, exclude it
        if callable(self.input):
            is_input_callable = True
            exclude.append("input")
        # if self.output is a callable, exclude it
        if callable(self.output):
            is_output_callable = True
            exclude.append("output")

        logger.debug(f"Serializing agent '{self.name}' to dict.")
        # Use Pydantic's dump, exclude manually handled fields and runtime context
        data = self.model_dump(
            exclude=exclude,
            mode="json",  # Use json mode for better handling of standard types by Pydantic
            exclude_none=True,  # Exclude None values for cleaner output
        )
        logger.debug(f"Base agent data for '{self.name}': {list(data.keys())}")
        serialized_modules = {}

        def add_serialized_component(component: Any, field_name: str):
            if component:
                comp_type = type(component)
                type_name = FlockRegistry.get_component_type_name(
                    comp_type
                )  # Get registered name
                if type_name:
                    try:
                        serialized_component_data = serialize_item(component)

                        if not isinstance(serialized_component_data, dict):
                            logger.error(
                                f"Serialization of component {type_name} for field '{field_name}' did not result in a dictionary. Got: {type(serialized_component_data)}"
                            )
                            serialized_modules[field_name] = {
                                "type": type_name,
                                "name": getattr(component, "name", "unknown"),
                                "error": "serialization_failed_non_dict",
                            }
                        else:
                            serialized_component_data["type"] = type_name
                            serialized_modules[field_name] = (
                                serialized_component_data
                            )
                            logger.debug(
                                f"Successfully serialized component for field '{field_name}' (type: {type_name})"
                            )

                    except Exception as e:
                        logger.error(
                            f"Failed to serialize component {type_name} for field '{field_name}': {e}",
                            exc_info=True,
                        )
                        serialized_modules[field_name] = {
                            "type": type_name,
                            "name": getattr(component, "name", "unknown"),
                            "error": "serialization_failed",
                        }
                else:
                    logger.warning(
                        f"Cannot serialize unregistered component {comp_type.__name__} for field '{field_name}'"
                    )

        add_serialized_component(self.evaluator, "evaluator")
        if serialized_modules:
            data["evaluator"] = serialized_modules["evaluator"]
            logger.debug(f"Added evaluator to agent '{self.name}'")

        serialized_modules = {}
        add_serialized_component(self.handoff_router, "handoff_router")
        if serialized_modules:
            data["handoff_router"] = serialized_modules["handoff_router"]
            logger.debug(f"Added handoff_router to agent '{self.name}'")

        serialized_modules = {}
        for module in self.modules.values():
            add_serialized_component(module, module.name)

        if serialized_modules:
            data["modules"] = serialized_modules
            logger.debug(
                f"Added {len(serialized_modules)} modules to agent '{self.name}'"
            )

        # --- Serialize Tools (Callables) ---
        if self.tools:
            logger.debug(
                f"Serializing {len(self.tools)} tools for agent '{self.name}'"
            )
            serialized_tools = []
            for tool in self.tools:
                if callable(tool) and not isinstance(tool, type):
                    path_str = FlockRegistry.get_callable_path_string(tool)
                    if path_str:
                        # Get just the function name from the path string
                        # If it's a namespaced path like module.submodule.function_name
                        # Just use the function_name part
                        func_name = path_str.split(".")[-1]
                        serialized_tools.append(func_name)
                        logger.debug(
                            f"Added tool '{func_name}' (from path '{path_str}') to agent '{self.name}'"
                        )
                    else:
                        logger.warning(
                            f"Could not get path string for tool {tool} in agent '{self.name}'. Skipping."
                        )
                else:
                    logger.warning(
                        f"Non-callable item found in tools list for agent '{self.name}': {tool}. Skipping."
                    )
            if serialized_tools:
                data["tools"] = serialized_tools
                logger.debug(
                    f"Added {len(serialized_tools)} tools to agent '{self.name}'"
                )

        if is_descrition_callable:
            path_str = FlockRegistry.get_callable_path_string(self.description)
            if path_str:
                func_name = path_str.split(".")[-1]
                data["description_callable"] = func_name
                logger.debug(
                    f"Added description '{func_name}' (from path '{path_str}') to agent '{self.name}'"
                )
            else:
                logger.warning(
                    f"Could not get path string for description {self.description} in agent '{self.name}'. Skipping."
                )

        if is_input_callable:
            path_str = FlockRegistry.get_callable_path_string(self.input)
            if path_str:
                func_name = path_str.split(".")[-1]
                data["input_callable"] = func_name
                logger.debug(
                    f"Added input '{func_name}' (from path '{path_str}') to agent '{self.name}'"
                )
            else:
                logger.warning(
                    f"Could not get path string for input {self.input} in agent '{self.name}'. Skipping."
                )

        if is_output_callable:
            path_str = FlockRegistry.get_callable_path_string(self.output)
            if path_str:
                func_name = path_str.split(".")[-1]
                data["output_callable"] = func_name
                logger.debug(
                    f"Added output '{func_name}' (from path '{path_str}') to agent '{self.name}'"
                )
            else:
                logger.warning(
                    f"Could not get path string for output {self.output} in agent '{self.name}'. Skipping."
                )

        # No need to call _filter_none_values here as model_dump(exclude_none=True) handles it
        logger.info(
            f"Serialization of agent '{self.name}' complete with {len(data)} fields"
        )
        return data

    @classmethod
    def from_dict(cls: type[T], data: dict[str, Any]) -> T:
        """Create instance from dictionary representation."""
        from flock.core.flock_registry import get_registry

        logger.debug(
            f"Deserializing agent from dict. Provided keys: {list(data.keys())}"
        )
        if "name" not in data:
            raise ValueError("Agent data must include a 'name' field.")
        FlockRegistry = get_registry()
        agent_name = data["name"]  # For logging context
        logger.info(f"Deserializing agent '{agent_name}'")

        # Pop complex components to handle them after basic agent instantiation
        evaluator_data = data.pop("evaluator", None)
        router_data = data.pop("handoff_router", None)
        modules_data = data.pop("modules", {})
        tools_data = data.pop("tools", [])
        description_callable = data.pop("description_callable", None)
        input_callable = data.pop("input_callable", None)
        output_callable = data.pop("output_callable", None)

        logger.debug(
            f"Agent '{agent_name}' has {len(modules_data)} modules and {len(tools_data)} tools"
        )

        # Deserialize remaining data recursively (handles nested basic types/callables)
        # Note: Pydantic v2 handles most basic deserialization well if types match.
        # Explicit deserialize_item might be needed if complex non-pydantic structures exist.
        # For now, assume Pydantic handles basic fields based on type hints.
        deserialized_basic_data = data  # Assume Pydantic handles basic fields

        try:
            # Create the agent instance using Pydantic's constructor
            logger.debug(
                f"Creating agent instance with fields: {list(deserialized_basic_data.keys())}"
            )
            agent = cls(**deserialized_basic_data)
        except Exception as e:
            logger.error(
                f"Pydantic validation/init failed for agent '{agent_name}': {e}",
                exc_info=True,
            )
            raise ValueError(
                f"Failed to initialize agent '{agent_name}' from dict: {e}"
            ) from e

        # --- Deserialize and Attach Components ---
        # Evaluator
        if evaluator_data:
            try:
                logger.debug(
                    f"Deserializing evaluator for agent '{agent_name}'"
                )
                agent.evaluator = deserialize_component(
                    evaluator_data, FlockEvaluator
                )
                if agent.evaluator is None:
                    raise ValueError("deserialize_component returned None")
                logger.debug(
                    f"Deserialized evaluator '{agent.evaluator.name}' of type '{evaluator_data.get('type')}' for agent '{agent_name}'"
                )
            except Exception as e:
                logger.error(
                    f"Failed to deserialize evaluator for agent '{agent_name}': {e}",
                    exc_info=True,
                )
                # Decide: raise error or continue without evaluator?
                # raise ValueError(f"Failed to deserialize evaluator for agent '{agent_name}': {e}") from e

        # Router
        if router_data:
            try:
                logger.debug(f"Deserializing router for agent '{agent_name}'")
                agent.handoff_router = deserialize_component(
                    router_data, FlockRouter
                )
                if agent.handoff_router is None:
                    raise ValueError("deserialize_component returned None")
                logger.debug(
                    f"Deserialized router '{agent.handoff_router.name}' of type '{router_data.get('type')}' for agent '{agent_name}'"
                )
            except Exception as e:
                logger.error(
                    f"Failed to deserialize router for agent '{agent_name}': {e}",
                    exc_info=True,
                )
                # Decide: raise error or continue without router?

        # Modules
        if modules_data:
            agent.modules = {}  # Ensure it's initialized
            logger.debug(
                f"Deserializing {len(modules_data)} modules for agent '{agent_name}'"
            )
            for name, module_data in modules_data.items():
                try:
                    logger.debug(
                        f"Deserializing module '{name}' of type '{module_data.get('type')}' for agent '{agent_name}'"
                    )
                    module_instance = deserialize_component(
                        module_data, FlockModule
                    )
                    if module_instance:
                        # Ensure instance name matches key if possible
                        module_instance.name = module_data.get("name", name)
                        agent.add_module(
                            module_instance
                        )  # Use add_module for consistency
                        logger.debug(
                            f"Successfully added module '{name}' to agent '{agent_name}'"
                        )
                    else:
                        raise ValueError("deserialize_component returned None")
                except Exception as e:
                    logger.error(
                        f"Failed to deserialize module '{name}' for agent '{agent_name}': {e}",
                        exc_info=True,
                    )
                    # Decide: skip module or raise error?

        # --- Deserialize Tools ---
        agent.tools = []  # Initialize tools list
        if tools_data:
            # Get component registry to look up function imports
            registry = get_registry()
            components = getattr(registry, "_callables", {})
            logger.debug(
                f"Deserializing {len(tools_data)} tools for agent '{agent_name}'"
            )
            logger.debug(
                f"Available callables in registry: {list(components.keys())}"
            )

            for tool_name in tools_data:
                try:
                    logger.debug(f"Looking for tool '{tool_name}' in registry")
                    # First try to lookup by simple name in the registry's callables
                    found = False
                    for path_str, func in components.items():
                        if (
                            path_str.endswith("." + tool_name)
                            or path_str == tool_name
                        ):
                            agent.tools.append(func)
                            found = True
                            logger.info(
                                f"Found tool '{tool_name}' via path '{path_str}' for agent '{agent_name}'"
                            )
                            break

                    # If not found by simple name, try manual import
                    if not found:
                        logger.debug(
                            f"Attempting to import tool '{tool_name}' from modules"
                        )
                        # Check in relevant modules (could be customized based on project structure)
                        import __main__

                        if hasattr(__main__, tool_name):
                            agent.tools.append(getattr(__main__, tool_name))
                            found = True
                            logger.info(
                                f"Found tool '{tool_name}' in __main__ module for agent '{agent_name}'"
                            )

                    if not found:
                        logger.warning(
                            f"Could not find tool '{tool_name}' for agent '{agent_name}'"
                        )
                except Exception as e:
                    logger.error(
                        f"Error adding tool '{tool_name}' to agent '{agent_name}': {e}",
                        exc_info=True,
                    )

        if description_callable:
            logger.debug(
                f"Deserializing description callable '{description_callable}' for agent '{agent_name}'"
            )
            agent.description = components[description_callable]

        if input_callable:
            logger.debug(
                f"Deserializing input callable '{input_callable}' for agent '{agent_name}'"
            )
            agent.input = components[input_callable]

        if output_callable:
            logger.debug(
                f"Deserializing output callable '{output_callable}' for agent '{agent_name}'"
            )
            agent.output = components[output_callable]

        logger.info(
            f"Successfully deserialized agent '{agent_name}' with {len(agent.modules)} modules and {len(agent.tools)} tools"
        )
        return agent

    # --- Pydantic v2 Configuration ---
    class Config:
        arbitrary_types_allowed = (
            True  # Important for components like evaluator, router etc.
        )
        # Might need custom json_encoders if not using model_dump(mode='json') everywhere
        # json_encoders = {
        #      FlockEvaluator: lambda v: v.to_dict() if v else None,
        #      FlockRouter: lambda v: v.to_dict() if v else None,
        #      FlockModule: lambda v: v.to_dict() if v else None,
        # }

```

---

### 71. src\flock\core\flock_evaluator.py

- **File ID**: file_70
- **Type**: Code File
- **Line Count**: 53
- **Description**: Base classes and implementations for Flock evaluators.
- **Dependencies**: None
- **Used By**: None

**Content**:
```
"""Base classes and implementations for Flock evaluators."""

from abc import ABC, abstractmethod
from typing import Any, TypeVar

from pydantic import BaseModel, Field, create_model

T = TypeVar("T", bound="FlockEvaluatorConfig")


class FlockEvaluatorConfig(BaseModel):
    """Base configuration class for Flock modules.

    This class serves as the base for all module-specific configurations.
    Each module should define its own config class inheriting from this one.

    Example:
        class MemoryModuleConfig(FlockModuleConfig):
            file_path: str = Field(default="memory.json")
            save_after_update: bool = Field(default=True)
    """

    model: str = Field(
        default="", description="The model to use for evaluation"
    )

    @classmethod
    def with_fields(cls: type[T], **field_definitions) -> type[T]:
        """Create a new config class with additional fields."""
        return create_model(
            f"Dynamic{cls.__name__}", __base__=cls, **field_definitions
        )


class FlockEvaluator(ABC, BaseModel):
    """Base class for all evaluators in Flock.

    An evaluator is responsible for taking inputs and producing outputs using
    some evaluation strategy (e.g., DSPy, natural language, etc.).
    """

    name: str = Field(..., description="Unique identifier for this evaluator")
    config: FlockEvaluatorConfig = Field(
        default_factory=FlockEvaluatorConfig,
        description="Evaluator configuration",
    )

    @abstractmethod
    async def evaluate(
        self, agent: Any, inputs: dict[str, Any], tools: list[Any]
    ) -> dict[str, Any]:
        """Evaluate inputs to produce outputs."""
        pass

```

---

### 72. src\flock\core\flock_factory.py

- **File ID**: file_71
- **Type**: Code File
- **Line Count**: 86
- **Description**: Factory for creating pre-configured Flock agents.
- **Dependencies**: None
- **Used By**: None

**Content**:
```
"""Factory for creating pre-configured Flock agents."""

from collections.abc import Callable
from typing import Any

from flock.core.flock_agent import FlockAgent
from flock.core.logging.formatters.themes import OutputTheme
from flock.evaluators.declarative.declarative_evaluator import (
    DeclarativeEvaluator,
    DeclarativeEvaluatorConfig,
)
from flock.modules.output.output_module import OutputModule, OutputModuleConfig
from flock.modules.performance.metrics_module import (
    MetricsModule,
    MetricsModuleConfig,
)


class FlockFactory:
    """Factory for creating pre-configured Flock agents with common module setups."""

    @staticmethod
    def create_default_agent(
        name: str,
        description: str | Callable[..., str] | None = None,
        model: str | Callable[..., str] | None = None,
        input: str | Callable[..., str] | None = None,
        output: str | Callable[..., str] | None = None,
        tools: list[Callable[..., Any] | Any] | None = None,
        use_cache: bool = True,
        enable_rich_tables: bool = False,
        output_theme: OutputTheme = OutputTheme.abernathy,
        wait_for_input: bool = False,
        temperature: float = 0.0,
        max_tokens: int = 8192,
        alert_latency_threshold_ms: int = 30000,
        no_output: bool = False,
        print_context: bool = False,
        write_to_file: bool = False,
        stream: bool = False,
        include_thought_process: bool = False,
    ) -> FlockAgent:
        """Creates a default FlockAgent.

        The default agent includes a declarative evaluator with the following modules:
        - OutputModule

        It also includes often needed configurations like cache usage, rich tables, and output theme.
        """
        eval_config = DeclarativeEvaluatorConfig(
            model=model,
            use_cache=use_cache,
            max_tokens=max_tokens,
            temperature=temperature,
            stream=stream,
            include_thought_process=include_thought_process,
        )

        evaluator = DeclarativeEvaluator(name="default", config=eval_config)
        agent = FlockAgent(
            name=name,
            input=input,
            output=output,
            tools=tools,
            model=model,
            description=description,
            evaluator=evaluator,
            write_to_file=write_to_file,
            wait_for_input=wait_for_input,
        )
        output_config = OutputModuleConfig(
            render_table=enable_rich_tables,
            theme=output_theme,
            no_output=no_output,
            print_context=print_context,
        )
        output_module = OutputModule("output", config=output_config)

        metrics_config = MetricsModuleConfig(
            latency_threshold_ms=alert_latency_threshold_ms
        )
        metrics_module = MetricsModule("metrics", config=metrics_config)

        agent.add_module(output_module)
        agent.add_module(metrics_module)
        return agent

```

---

### 73. src\flock\core\flock_module.py

- **File ID**: file_72
- **Type**: Code File
- **Line Count**: 101
- **Description**: Base classes and implementations for the Flock module system.
- **Dependencies**: None
- **Used By**: None

**Content**:
```
"""Base classes and implementations for the Flock module system."""

from abc import ABC
from typing import Any, TypeVar

from pydantic import BaseModel, Field, create_model

from flock.core.context.context import FlockContext

T = TypeVar("T", bound="FlockModuleConfig")


class FlockModuleConfig(BaseModel):
    """Base configuration class for Flock modules.

    This class serves as the base for all module-specific configurations.
    Each module should define its own config class inheriting from this one.

    Example:
        class MemoryModuleConfig(FlockModuleConfig):
            file_path: str = Field(default="memory.json")
            save_after_update: bool = Field(default=True)
    """

    enabled: bool = Field(
        default=True, description="Whether the module is currently enabled"
    )

    @classmethod
    def with_fields(cls: type[T], **field_definitions) -> type[T]:
        """Create a new config class with additional fields."""
        return create_model(
            f"Dynamic{cls.__name__}", __base__=cls, **field_definitions
        )


class FlockModule(BaseModel, ABC):
    """Base class for all Flock modules.

    Modules can hook into agent lifecycle events and modify or enhance agent behavior.
    They are initialized when added to an agent and can maintain their own state.

    Each module should define its configuration requirements either by:
    1. Creating a subclass of FlockModuleConfig
    2. Using FlockModuleConfig.with_fields() to create a config class
    """

    name: str = Field(
        default="", description="Unique identifier for the module"
    )
    config: FlockModuleConfig = Field(
        default_factory=FlockModuleConfig, description="Module configuration"
    )

    async def initialize(
        self,
        agent: Any,
        inputs: dict[str, Any],
        context: FlockContext | None = None,
    ) -> None:
        """Called when the agent starts running."""
        pass

    async def pre_evaluate(
        self,
        agent: Any,
        inputs: dict[str, Any],
        context: FlockContext | None = None,
    ) -> dict[str, Any]:
        """Called before agent evaluation, can modify inputs."""
        return inputs

    async def post_evaluate(
        self,
        agent: Any,
        inputs: dict[str, Any],
        result: dict[str, Any],
        context: FlockContext | None = None,
    ) -> dict[str, Any]:
        """Called after agent evaluation, can modify results."""
        return result

    async def terminate(
        self,
        agent: Any,
        inputs: dict[str, Any],
        result: dict[str, Any],
        context: FlockContext | None = None,
    ) -> None:
        """Called when the agent finishes running."""
        pass

    async def on_error(
        self,
        agent: Any,
        error: Exception,
        inputs: dict[str, Any],
        context: FlockContext | None = None,
    ) -> None:
        """Called when an error occurs during agent execution."""
        pass

```

---

### 74. src\flock\core\flock_registry.py

- **File ID**: file_73
- **Type**: Code File
- **Line Count**: 596
- **Description**: Centralized registry for managing Agents, Callables, Types, and Component Classes...
- **Dependencies**: None
- **Used By**: None

**Content**:
```
# src/flock/core/flock_registry.py
"""Centralized registry for managing Agents, Callables, Types, and Component Classes
within the Flock framework to support dynamic lookup and serialization.
"""

from __future__ import annotations  # Add this at the very top

import builtins
import importlib
import inspect
import sys
from collections.abc import Callable, Mapping, Sequence
from dataclasses import is_dataclass
from typing import (  # Add TYPE_CHECKING
    TYPE_CHECKING,
    Any,
    Literal,
    Optional,
    TypeVar,
    Union,
    overload,
)

from pydantic import BaseModel

if TYPE_CHECKING:
    from flock.core.flock_agent import (
        FlockAgent,  # Import only for type checking
    )
    from flock.core.flock_evaluator import FlockEvaluator
    from flock.core.flock_module import FlockModule
    from flock.core.flock_router import FlockRouter

    COMPONENT_BASE_TYPES = (FlockModule, FlockEvaluator, FlockRouter)
    IS_COMPONENT_CHECK_ENABLED = True
else:
    # Define dummy types or skip check if not type checking
    FlockAgent = Any  # Or define a dummy class
    COMPONENT_BASE_TYPES = ()
    IS_COMPONENT_CHECK_ENABLED = False

# Fallback if core types aren't available during setup

from flock.core.logging.logging import get_logger

logger = get_logger("registry")
T = TypeVar("T")
ClassType = TypeVar("ClassType", bound=type)
FuncType = TypeVar("FuncType", bound=Callable)


class FlockRegistry:
    """Singleton registry for Agents, Callables (functions/methods).

    Types (Pydantic/Dataclasses used in signatures), and Component Classes
    (Modules, Evaluators, Routers).
    """

    _instance = None

    _agents: dict[str, FlockAgent]
    _callables: dict[str, Callable]
    _types: dict[str, type]
    _components: dict[str, type]  # For Module, Evaluator, Router classes

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._initialize()
            # logger.info("FlockRegistry instance created.")
        return cls._instance

    def _initialize(self):
        """Initialize the internal dictionaries."""
        self._agents = {}
        self._callables = {}
        self._types = {}
        self._components = {}
        # logger.debug("FlockRegistry initialized internal stores.")
        # Auto-register core Python types
        self._register_core_types()

    def _register_core_types(self):
        """Registers common built-in and typing types."""
        core_types = [
            str,
            int,
            float,
            bool,
            list,
            dict,
            tuple,
            set,
            Any,
            Mapping,
            Sequence,
            TypeVar,
            Literal,
            Optional,
            Union,  # Common typing generics
        ]
        for t in core_types:
            try:
                self.register_type(t)
            except Exception as e:
                logger.error(f"Failed to auto-register core type {t}: {e}")

    # --- Path String Generation ---
    @staticmethod
    def _get_path_string(obj: Callable | type) -> str | None:
        """Generates a unique path string 'module.ClassName' or 'module.function_name'."""
        try:
            module = obj.__module__
            name = obj.__name__
            if module == "builtins":
                return name
            # Check if it's nested (basic check, might not cover all edge cases)
            if "." in name and hasattr(sys.modules[module], name.split(".")[0]):
                # Likely a nested class/method - serialization might need custom handling or pickle
                logger.warning(
                    f"Object {name} appears nested in {module}. Path string might be ambiguous."
                )
            return f"{module}.{name}"
        except AttributeError:
            logger.warning(f"Could not determine module/name for object: {obj}")
            return None

    # --- Agent Registration ---
    def register_agent(self, agent: FlockAgent) -> None:
        """Registers a FlockAgent instance by its name."""
        if not hasattr(agent, "name") or not agent.name:
            logger.error(
                "Attempted to register an agent without a valid 'name' attribute."
            )
            return
        if agent.name in self._agents and self._agents[agent.name] != agent:
            logger.warning(
                f"Agent '{agent.name}' already registered. Overwriting."
            )
        self._agents[agent.name] = agent
        logger.debug(f"Registered agent: {agent.name}")

    def get_agent(self, name: str) -> FlockAgent | None:
        """Retrieves a registered FlockAgent instance by name."""
        agent = self._agents.get(name)
        if not agent:
            logger.warning(f"Agent '{name}' not found in registry.")
        return agent

    def get_all_agent_names(self) -> list[str]:
        """Returns a list of names of all registered agents."""
        return list(self._agents.keys())

    # --- Callable Registration ---
    def register_callable(
        self, func: Callable, name: str | None = None
    ) -> str | None:
        """Registers a callable (function/method). Returns its path string identifier."""
        path_str = name or self._get_path_string(func)
        if path_str:
            if (
                path_str in self._callables
                and self._callables[path_str] != func
            ):
                logger.warning(
                    f"Callable '{path_str}' already registered with a different function. Overwriting."
                )
            self._callables[path_str] = func
            logger.debug(f"Registered callable: '{path_str}' ({func.__name__})")
            return path_str
        logger.warning(
            f"Could not register callable {func.__name__}: Unable to determine path string"
        )
        return None

    def get_callable(self, name_or_path: str) -> Callable:
        """Retrieves a callable by its registered name or full path string.
        Attempts dynamic import if not found directly. Prioritizes exact match,
        then searches for matches ending with '.{name}'.
        """
        # 1. Try exact match first (covers full paths and simple names if registered that way)
        if name_or_path in self._callables:
            logger.debug(
                f"Found callable '{name_or_path}' directly in registry."
            )
            return self._callables[name_or_path]

        # 2. If not found, and it looks like a simple name, search registered paths
        if "." not in name_or_path:
            matches = []
            for path_str, func in self._callables.items():
                # Check if path ends with ".{simple_name}" or exactly matches simple_name
                if path_str == name_or_path or path_str.endswith(
                    f".{name_or_path}"
                ):
                    matches.append(func)

            if len(matches) == 1:
                logger.debug(
                    f"Found unique callable for simple name '{name_or_path}' via path '{self.get_callable_path_string(matches[0])}'."
                )
                return matches[0]
            elif len(matches) > 1:
                # Ambiguous simple name - require full path
                found_paths = [
                    self.get_callable_path_string(f) for f in matches
                ]
                logger.error(
                    f"Ambiguous callable name '{name_or_path}'. Found matches: {found_paths}. Use full path string for lookup."
                )
                raise KeyError(
                    f"Ambiguous callable name '{name_or_path}'. Use full path string."
                )
            # else: Not found by simple name search in registry, proceed to dynamic import

        # 3. Attempt dynamic import if it looks like a full path
        if "." in name_or_path:
            logger.debug(
                f"Callable '{name_or_path}' not in registry cache, attempting dynamic import."
            )
            try:
                module_name, func_name = name_or_path.rsplit(".", 1)
                module = importlib.import_module(module_name)
                func = getattr(module, func_name)
                if callable(func):
                    self.register_callable(
                        func, name_or_path
                    )  # Cache dynamically imported
                    logger.info(
                        f"Successfully imported and registered module callable '{name_or_path}'"
                    )
                    return func
                else:
                    raise TypeError(
                        f"Dynamically imported object '{name_or_path}' is not callable."
                    )
            except (ImportError, AttributeError, TypeError) as e:
                logger.error(
                    f"Failed to dynamically load/find callable '{name_or_path}': {e}",
                    exc_info=False,
                )
                # Fall through to raise KeyError
        # 4. Handle built-ins if not found yet (might be redundant if simple name check worked)
        elif name_or_path in builtins.__dict__:
            func = builtins.__dict__[name_or_path]
            if callable(func):
                self.register_callable(func, name_or_path)  # Cache it
                logger.info(
                    f"Found and registered built-in callable '{name_or_path}'"
                )
                return func

        # 5. Final failure
        logger.error(
            f"Callable '{name_or_path}' not found in registry or via import."
        )
        raise KeyError(f"Callable '{name_or_path}' not found.")

    def get_callable_path_string(self, func: Callable) -> str | None:
        """Gets the path string for a callable, registering it if necessary."""
        # First try to find by direct identity
        for path_str, registered_func in self._callables.items():
            if func == registered_func:
                logger.debug(
                    f"Found existing path string for callable: '{path_str}'"
                )
                return path_str

        # If not found by identity, generate path, register, and return
        path_str = self.register_callable(func)
        if path_str:
            logger.debug(
                f"Generated and registered new path string for callable: '{path_str}'"
            )
        else:
            logger.warning(
                f"Failed to generate path string for callable {func.__name__}"
            )

        return path_str

    # --- Type Registration ---
    def register_type(
        self, type_obj: type, name: str | None = None
    ) -> str | None:
        """Registers a class/type (Pydantic, Dataclass, etc.) used in signatures."""
        type_name = name or type_obj.__name__
        if type_name:
            if type_name in self._types and self._types[type_name] != type_obj:
                logger.warning(
                    f"Type '{type_name}' already registered. Overwriting."
                )
            self._types[type_name] = type_obj
            logger.debug(f"Registered type: {type_name}")
            return type_name
        return None

    def get_type(self, type_name: str) -> type:
        """Retrieves a registered type by its name."""
        if type_name in self._types:
            return self._types[type_name]
        else:
            # Consider adding dynamic import attempts for types if needed,
            # but explicit registration is generally safer for types.
            logger.error(f"Type '{type_name}' not found in registry.")
            raise KeyError(
                f"Type '{type_name}' not found. Ensure it is registered."
            )

    # --- Component Class Registration ---
    def register_component(
        self, component_class: type, name: str | None = None
    ) -> str | None:
        """Registers a component class (Module, Evaluator, Router)."""
        type_name = name or component_class.__name__
        if type_name:
            # Optional: Add check if it's a subclass of expected bases
            # if COMPONENT_BASE_TYPES and not issubclass(component_class, COMPONENT_BASE_TYPES):
            #     logger.warning(f"Registering class '{type_name}' which is not a standard Flock component type.")
            if (
                type_name in self._components
                and self._components[type_name] != component_class
            ):
                logger.warning(
                    f"Component class '{type_name}' already registered. Overwriting."
                )
            self._components[type_name] = component_class
            logger.debug(f"Registered component class: {type_name}")
            return type_name
        return None

    def get_component(self, type_name: str) -> type:
        """Retrieves a component class by its type name."""
        if type_name in self._components:
            return self._components[type_name]
        else:
            # Dynamic import attempts similar to get_callable could be added here if desired,
            # targeting likely module locations based on type_name conventions.
            logger.error(
                f"Component class '{type_name}' not found in registry."
            )
            raise KeyError(
                f"Component class '{type_name}' not found. Ensure it is registered."
            )

    def get_component_type_name(self, component_class: type) -> str | None:
        """Gets the type name for a component class, registering it if necessary."""
        for type_name, registered_class in self._components.items():
            if component_class == registered_class:
                return type_name
        # If not found, register using class name and return
        return self.register_component(component_class)

    # --- Auto-Registration ---
    def register_module_components(self, module_or_path: Any) -> None:
        """Scans a module (object or path string) and automatically registers.

        - Functions as callables.
        - Pydantic Models and Dataclasses as types.
        - Subclasses of FlockModule, FlockEvaluator, FlockRouter as components.
        """
        try:
            if isinstance(module_or_path, str):
                module = importlib.import_module(module_or_path)
            elif inspect.ismodule(module_or_path):
                module = module_or_path
            else:
                logger.error(
                    f"Invalid input for auto-registration: {module_or_path}. Must be module object or path string."
                )
                return

            logger.info(
                f"Auto-registering components from module: {module.__name__}"
            )
            registered_count = {"callable": 0, "type": 0, "component": 0}

            for name, obj in inspect.getmembers(module):
                if name.startswith("_"):
                    continue  # Skip private/internal

                # Register Functions as Callables
                if (
                    inspect.isfunction(obj)
                    and obj.__module__ == module.__name__
                ):
                    if self.register_callable(obj):
                        registered_count["callable"] += 1

                # Register Classes (Types and Components)
                elif inspect.isclass(obj) and obj.__module__ == module.__name__:
                    is_component = False
                    # Register as Component if subclass of base types
                    if (
                        COMPONENT_BASE_TYPES
                        and issubclass(obj, COMPONENT_BASE_TYPES)
                        and self.register_component(obj)
                    ):
                        registered_count["component"] += 1
                        is_component = True  # Mark as component

                    # Register as Type if Pydantic Model or Dataclass
                    # A component can also be a type used in signatures
                    base_model_or_dataclass = isinstance(obj, type) and (
                        issubclass(obj, BaseModel) or is_dataclass(obj)
                    )
                    if (
                        base_model_or_dataclass
                        and self.register_type(obj)
                        and not is_component
                    ):
                        # Only increment type count if it wasn't already counted as component
                        registered_count["type"] += 1

            logger.info(
                f"Auto-registration summary for {module.__name__}: "
                f"{registered_count['callable']} callables, "
                f"{registered_count['type']} types, "
                f"{registered_count['component']} components."
            )

        except Exception as e:
            logger.error(
                f"Error during auto-registration for {module_or_path}: {e}",
                exc_info=True,
            )


# --- Initialize Singleton ---
_registry_instance = FlockRegistry()


# --- Convenience Access ---
# Provide a function to easily get the singleton instance
def get_registry() -> FlockRegistry:
    """Returns the singleton FlockRegistry instance."""
    return _registry_instance


# Type hinting for decorators to preserve signature
@overload
def flock_component(cls: ClassType) -> ClassType: ...
@overload
def flock_component(
    *, name: str | None = None
) -> Callable[[ClassType], ClassType]: ...


def flock_component(
    cls: ClassType | None = None, *, name: str | None = None
) -> Any:
    """Decorator to register a Flock Component class (Module, Evaluator, Router).

    Usage:
        @flock_component
        class MyModule(FlockModule): ...

        @flock_component(name="CustomRouterAlias")
        class MyRouter(FlockRouter): ...
    """
    registry = get_registry()

    def decorator(inner_cls: ClassType) -> ClassType:
        if not inspect.isclass(inner_cls):
            raise TypeError("@flock_component can only decorate classes.")
        component_name = name or inner_cls.__name__
        registry.register_component(inner_cls, name=component_name)
        return inner_cls

    if cls is None:
        # Called as @flock_component(name="...")
        return decorator
    else:
        # Called as @flock_component
        return decorator(cls)


# Type hinting for decorators
@overload
def flock_tool(func: FuncType) -> FuncType: ...
@overload
def flock_tool(
    *, name: str | None = None
) -> Callable[[FuncType], FuncType]: ...


def flock_tool(func: FuncType | None = None, *, name: str | None = None) -> Any:
    """Decorator to register a callable function/method as a Tool (or general callable).

    Usage:
        @flock_tool
        def my_web_search(query: str): ...

        @flock_tool(name="utils.calculate_pi")
        def compute_pi(): ...
    """
    registry = get_registry()

    def decorator(inner_func: FuncType) -> FuncType:
        if not callable(inner_func):
            raise TypeError("@flock_tool can only decorate callables.")
        # Let registry handle default name generation if None
        registry.register_callable(inner_func, name=name)
        return inner_func

    if func is None:
        # Called as @flock_tool(name="...")
        return decorator
    else:
        # Called as @flock_tool
        return decorator(func)


# Alias for clarity if desired
flock_callable = flock_tool


@overload
def flock_type(cls: ClassType) -> ClassType: ...
@overload
def flock_type(
    *, name: str | None = None
) -> Callable[[ClassType], ClassType]: ...


def flock_type(cls: ClassType | None = None, *, name: str | None = None) -> Any:
    """Decorator to register a Type (Pydantic Model, Dataclass) used in signatures.

    Usage:
        @flock_type
        class MyDataModel(BaseModel): ...

        @flock_type(name="UserInput")
        @dataclass
        class UserQuery: ...
    """
    registry = get_registry()

    def decorator(inner_cls: ClassType) -> ClassType:
        if not inspect.isclass(inner_cls):
            raise TypeError("@flock_type can only decorate classes.")
        type_name = name or inner_cls.__name__
        registry.register_type(inner_cls, name=type_name)
        return inner_cls

    if cls is None:
        # Called as @flock_type(name="...")
        return decorator
    else:
        # Called as @flock_type
        return decorator(cls)


# --- Auto-register known core components and tools ---
def _auto_register_by_path():
    components_to_register = [
        (
            "flock.evaluators.declarative.declarative_evaluator",
            "DeclarativeEvaluator",
        ),
        ("flock.evaluators.memory.memory_evaluator", "MemoryEvaluator"),
        ("flock.modules.output.output_module", "OutputModule"),
        ("flock.modules.performance.metrics_module", "MetricsModule"),
        ("flock.modules.memory.memory_module", "MemoryModule"),
        # ("flock.modules.hierarchical.module", "HierarchicalMemoryModule"), # Uncomment if exists
        ("flock.routers.default.default_router", "DefaultRouter"),
        ("flock.routers.llm.llm_router", "LLMRouter"),
        ("flock.routers.agent.agent_router", "AgentRouter"),
    ]
    for module_path, class_name in components_to_register:
        try:
            module = importlib.import_module(module_path)
            component_class = getattr(module, class_name)
            _registry_instance.register_component(component_class)
        except (ImportError, AttributeError) as e:
            logger.warning(f"{class_name} not found for auto-registration: {e}")

    # Auto-register standard tools by scanning modules
    tool_modules = [
        "flock.core.tools.basic_tools",
        "flock.core.tools.azure_tools",
        "flock.core.tools.dev_tools.github",
        "flock.core.tools.llm_tools",
        "flock.core.tools.markdown_tools",
    ]
    for module_path in tool_modules:
        try:
            _registry_instance.register_module_components(module_path)
        except ImportError as e:
            logger.warning(
                f"Could not auto-register tools from {module_path}: {e}"
            )


# Bootstrapping the registry
# _auto_register_by_path()

```

---

### 75. src\flock\core\flock_router.py

- **File ID**: file_74
- **Type**: Code File
- **Line Count**: 83
- **Description**: Base router class for the Flock framework.
- **Dependencies**: None
- **Used By**: None

**Content**:
```
"""Base router class for the Flock framework."""

from abc import ABC, abstractmethod
from typing import Any, Literal

from pydantic import BaseModel, Field

from flock.core.context.context import FlockContext


class HandOffRequest(BaseModel):
    """Base class for handoff returns."""

    next_agent: str = Field(default="", description="Next agent to invoke")
    # match = use the output fields of the current agent that also exists as input field of the next agent
    # add = add the output of the current agent to the input of the next agent
    output_to_input_merge_strategy: Literal["match", "add"] = Field(
        default="match"
    )
    add_input_fields: list[str] | None = Field(
        default=None,
        description="List of input fields to add to the next agent",
    )
    add_output_fields: list[str] | None = Field(
        default=None,
        description="List of output fields to add to the next agent",
    )
    add_description: str | None = Field(
        default=None, description="Add this description to the next agent"
    )
    override_next_agent: Any | None = Field(
        default=None,
        description="Override the next agent to hand off to",
    )
    override_context: FlockContext | None = Field(
        default=None, description="Override context parameters"
    )


class FlockRouterConfig(BaseModel):
    """Configuration for a router.

    This class defines the configuration parameters for a router.
    Subclasses can extend this to add additional parameters.
    """

    enabled: bool = Field(
        default=True, description="Whether the router is enabled"
    )
    # agents: list[str] | None = Field(
    #     default=None,
    #     description="List of agents to choose from",
    # )


class FlockRouter(BaseModel, ABC):
    """Base class for all routers.

    A router is responsible for determining the next agent in a workflow
    based on the current agent's output.
    """

    name: str = Field(..., description="Name of the router")
    config: FlockRouterConfig = Field(default_factory=FlockRouterConfig)

    @abstractmethod
    async def route(
        self,
        current_agent: Any,
        result: dict[str, Any],
        context: FlockContext,
    ) -> HandOffRequest:
        """Determine the next agent to hand off to based on the current agent's output.

        Args:
            current_agent: The agent that just completed execution
            result: The output from the current agent
            context: The global execution context

        Returns:
            A HandOff object containing the next agent and input data
        """
        pass

```

---

### 76. src\flock\core\interpreter\python_interpreter.py

- **File ID**: file_75
- **Type**: Code File
- **Line Count**: 683
- **Description**: File at src\flock\core\interpreter\python_interpreter.py
- **Dependencies**: None
- **Used By**: None

**Content**:
```
# =========== Copyright 2023 @ CAMEL-AI.org. All Rights Reserved. ===========
# Licensed under the Apache License, Version 2.0 (the “License”);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an “AS IS” BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# =========== Copyright 2023 @ CAMEL-AI.org. All Rights Reserved. ===========
import ast
import builtins
import difflib
import importlib
import re
import typing
from collections.abc import Mapping
from typing import (
    Any,
)

from opentelemetry import trace

from flock.core.logging.logging import get_logger

tracer = trace.get_tracer(__name__)
logger = get_logger("interpreter")


class InterpreterError(ValueError):
    r"""An error raised when the interpreter cannot evaluate a Python
    expression, due to syntax error or unsupported operations.
    """

    pass


class PythonInterpreter:
    r"""A customized python interpreter to control the execution of
    LLM-generated codes. The interpreter makes sure the code can only execute
    functions given in action space and import white list. It also supports
    fuzzy variable matching to receive uncertain input variable name.

    [Documentation omitted for brevity]

    Args:
        action_space (Dict[str, Any]): A dictionary mapping action names to
            their corresponding functions or objects.
        import_white_list (Optional[List[str]], optional): A list of allowed modules.
        verbose (bool, optional): If True, the interpreter prints log messages
            as it executes the code. (default: False)
    """

    def __init__(
        self,
        action_space: dict[str, Any],
        import_white_list: list[str] | None = None,
        verbose: bool = False,
    ) -> None:
        self.action_space = action_space
        self.state = self.action_space.copy()
        self.fuzz_state: dict[str, Any] = {}
        self.import_white_list = import_white_list or [
            "math",
            "random",
            "datetime",
            "time",
            "string",
            "collections",
            "itertools",
            "functools",
            "typing",
            "enum",
            "json",
            "ast",
        ]  # default imports
        self.verbose = verbose

    def log(self, message: str) -> None:
        """Print a log message immediately."""
        # print(message, flush=True)
        logger.info(message, flush=True)

    def execute(
        self,
        code: str,
        state: dict[str, Any] | None = None,
        fuzz_state: dict[str, Any] | None = None,
        keep_state: bool = True,
    ) -> Any:
        r"""Execute the input python codes in a secure environment.

        [Documentation omitted for brevity]
        """
        if state is not None:
            self.state.update(state)
        if fuzz_state is not None:
            self.fuzz_state.update(fuzz_state)

        try:
            expression = ast.parse(code)
        except SyntaxError as e:
            error_line = code.splitlines()[e.lineno - 1]
            raise InterpreterError(
                f"Syntax error in code at line {e.lineno}: {error_line}\nError: {e}"
            )

        result = None
        if self.verbose:
            self.log("[Interpreter] Starting code execution...")

        for idx, node in enumerate(expression.body):
            # Log the AST node being executed (using unparse if available)
            if self.verbose:
                try:
                    node_repr = ast.unparse(node)
                except Exception:
                    node_repr = ast.dump(node)
                self.log(f"[Interpreter] Executing node {idx}: {node_repr}")

            try:
                line_result = self._execute_ast(node)
            except InterpreterError as e:
                if not keep_state:
                    self.clear_state()
                msg = f"Evaluation of the code stopped at node {idx}. See:\n{e}"
                raise InterpreterError(msg)
            if line_result is not None:
                result = line_result
                if self.verbose:
                    self.log(f"[Interpreter] Node {idx} result: {result}")

        if self.verbose:
            self.log("[Interpreter] Finished code execution.")
        if not keep_state:
            self.clear_state()

        return result

    def clear_state(self) -> None:
        r"""Initialize :obj:`state` and :obj:`fuzz_state`"""
        self.state = self.action_space.copy()
        self.fuzz_state = {}

    # ast.Index is deprecated after python 3.9, which cannot pass type check,
    # but is still necessary for older versions.
    @typing.no_type_check
    def _execute_ast(self, expression: ast.AST) -> Any:
        if isinstance(expression, ast.Assign):
            return self._execute_assign(expression)
        elif isinstance(expression, ast.Attribute):
            value = self._execute_ast(expression.value)
            return getattr(value, expression.attr)
        elif isinstance(expression, ast.AugAssign):
            return self._execute_augassign(expression)
        elif isinstance(expression, ast.BinOp):
            return self._execute_binop(expression)
        elif isinstance(expression, ast.BoolOp):
            return self._execute_condition(expression)
        elif isinstance(expression, ast.Call):
            return self._execute_call(expression)
        elif isinstance(expression, ast.Compare):
            return self._execute_condition(expression)
        elif isinstance(expression, ast.Constant):
            return expression.value
        elif isinstance(expression, ast.Dict):
            result: dict = {}
            for k, v in zip(expression.keys, expression.values):
                if k is not None:
                    result[self._execute_ast(k)] = self._execute_ast(v)
                else:
                    result.update(self._execute_ast(v))
            return result
        elif isinstance(expression, ast.Expr):
            return self._execute_ast(expression.value)
        elif isinstance(expression, ast.For):
            return self._execute_for(expression)
        elif isinstance(expression, ast.FormattedValue):
            return self._execute_ast(expression.value)
        elif isinstance(expression, ast.FunctionDef):
            self.state[expression.name] = expression
            return None
        elif isinstance(expression, ast.GeneratorExp):
            return self._execute_generatorexp(expression)
        elif isinstance(expression, ast.If):
            return self._execute_if(expression)
        elif isinstance(expression, ast.IfExp):
            return self._execute_ifexp(expression)
        elif isinstance(expression, ast.Import):
            self._execute_import(expression)
            return None
        elif isinstance(expression, ast.ImportFrom):
            self._execute_import_from(expression)
            return None
        elif hasattr(ast, "Index") and isinstance(expression, ast.Index):
            return self._execute_ast(expression.value)
        elif isinstance(expression, ast.JoinedStr):
            return "".join(
                [str(self._execute_ast(v)) for v in expression.values]
            )
        elif isinstance(expression, ast.Lambda):
            return self._execute_lambda(expression)
        elif isinstance(expression, ast.List):
            return [self._execute_ast(elt) for elt in expression.elts]
        elif isinstance(expression, ast.Name):
            return self._execute_name(expression)
        elif isinstance(expression, ast.Return):
            return self._execute_ast(expression.value)
        elif isinstance(expression, ast.Subscript):
            return self._execute_subscript(expression)
        elif isinstance(expression, ast.Tuple):
            return tuple([self._execute_ast(elt) for elt in expression.elts])
        elif isinstance(expression, ast.UnaryOp):
            return self._execute_unaryop(expression)
        elif isinstance(expression, ast.While):
            return self._execute_while(expression)
        elif isinstance(expression, ast.ListComp):
            return self._execute_listcomp(expression)
        elif isinstance(expression, ast.DictComp):
            return self._execute_dictcomp(expression)
        elif isinstance(expression, ast.SetComp):
            return self._execute_setcomp(expression)
        elif isinstance(expression, ast.Break):
            raise BreakException()
        elif isinstance(expression, ast.Continue):
            raise ContinueException()
        elif isinstance(expression, ast.Try):
            return self._execute_try(expression)
        elif isinstance(expression, ast.Raise):
            return self._execute_raise(expression)
        elif isinstance(expression, ast.Pass):
            return None
        elif isinstance(expression, ast.Assert):
            return self._execute_assert(expression)
        else:
            raise InterpreterError(
                f"{expression.__class__.__name__} is not supported."
            )

    def _execute_assign(self, assign: ast.Assign) -> Any:
        targets = assign.targets
        result = self._execute_ast(assign.value)

        for target in targets:
            self._assign(target, result)
        return result

    def _assign(self, target: ast.expr, value: Any):
        if isinstance(target, ast.Name):
            self.state[target.id] = value
        elif isinstance(target, ast.Tuple):
            if not isinstance(value, tuple):
                raise InterpreterError(
                    f"Expected type tuple, but got {value.__class__.__name__} instead."
                )
            if len(target.elts) != len(value):
                raise InterpreterError(
                    f"Expected {len(target.elts)} values but got {len(value)}."
                )
            for t, v in zip(target.elts, value):
                self.state[self._execute_ast(t)] = v
        else:
            raise InterpreterError(
                f"Unsupported variable type. Expected ast.Name or ast.Tuple, got {target.__class__.__name__} instead."
            )

    def _execute_call(self, call: ast.Call) -> Any:
        callable_func = self._execute_ast(call.func)

        args = [self._execute_ast(arg) for arg in call.args]
        kwargs = {
            keyword.arg: self._execute_ast(keyword.value)
            for keyword in call.keywords
        }
        if isinstance(callable_func, ast.FunctionDef):
            old_state = self.state.copy()
            for param_name, arg_value in zip(
                [param.arg for param in callable_func.args.args], args
            ):
                self.state[param_name] = arg_value
            result = None
            for stmt in callable_func.body:
                result = self._execute_ast(stmt)
                if isinstance(stmt, ast.Return):
                    break
            self.state = old_state
            return result
        return callable_func(*args, **kwargs)

    def _execute_augassign(self, augassign: ast.AugAssign):
        current_value = self.state[augassign.target.id]
        increment_value = self._execute_ast(augassign.value)
        if not (
            isinstance(current_value, (int, float))
            and isinstance(increment_value, (int, float))
        ):
            raise InterpreterError(
                f"Invalid types for augmented assignment: {type(current_value)}, {type(increment_value)}"
            )
        if isinstance(augassign.op, ast.Add):
            new_value = current_value + increment_value
        elif isinstance(augassign.op, ast.Sub):
            new_value = current_value - increment_value
        elif isinstance(augassign.op, ast.Mult):
            new_value = current_value * increment_value
        elif isinstance(augassign.op, ast.Div):
            new_value = current_value / increment_value
        else:
            raise InterpreterError(
                f"Augmented assignment operator {augassign.op} is not supported"
            )
        self._assign(augassign.target, new_value)
        return new_value

    def _execute_subscript(self, subscript: ast.Subscript):
        index = self._execute_ast(subscript.slice)
        value = self._execute_ast(subscript.value)
        if not isinstance(subscript.ctx, ast.Load):
            raise InterpreterError(
                f"{subscript.ctx.__class__.__name__} is not supported for subscript."
            )
        if isinstance(value, (list, tuple)):
            return value[int(index)]
        if index in value:
            return value[index]
        if isinstance(index, str) and isinstance(value, Mapping):
            close_matches = difflib.get_close_matches(index, list(value.keys()))
            if len(close_matches) > 0:
                return value[close_matches[0]]
        raise InterpreterError(f"Could not index {value} with '{index}'.")

    def _execute_name(self, name: ast.Name):
        if name.id in dir(builtins):
            return getattr(builtins, name.id)
        if isinstance(name.ctx, ast.Store):
            return name.id
        elif isinstance(name.ctx, ast.Load):
            return self._get_value_from_state(name.id)
        else:
            raise InterpreterError(f"{name.ctx} is not supported.")

    def _execute_condition(self, condition):
        if isinstance(condition, ast.BoolOp):
            if isinstance(condition.op, ast.And):
                results = [
                    self._execute_ast(value) for value in condition.values
                ]
                return all(results)
            elif isinstance(condition.op, ast.Or):
                results = [
                    self._execute_ast(value) for value in condition.values
                ]
                return any(results)
            else:
                raise InterpreterError(
                    f"Boolean operator {condition.op} is not supported"
                )
        elif isinstance(condition, ast.Compare):
            if len(condition.ops) > 1:
                raise InterpreterError(
                    "Cannot evaluate conditions with multiple operators"
                )
            left = self._execute_ast(condition.left)
            comparator = condition.ops[0]
            right = self._execute_ast(condition.comparators[0])
            if isinstance(comparator, ast.Eq):
                return left == right
            elif isinstance(comparator, ast.NotEq):
                return left != right
            elif isinstance(comparator, ast.Lt):
                return left < right
            elif isinstance(comparator, ast.LtE):
                return left <= right
            elif isinstance(comparator, ast.Gt):
                return left > right
            elif isinstance(comparator, ast.GtE):
                return left >= right
            elif isinstance(comparator, ast.Is):
                return left is right
            elif isinstance(comparator, ast.IsNot):
                return left is not right
            elif isinstance(comparator, ast.In):
                return left in right
            elif isinstance(comparator, ast.NotIn):
                return left not in right
            else:
                raise InterpreterError("Unsupported comparison operator")
        elif isinstance(condition, ast.UnaryOp):
            return self._execute_unaryop(condition)
        elif isinstance(condition, ast.Name) or isinstance(condition, ast.Call):
            return bool(self._execute_ast(condition))
        elif isinstance(condition, ast.Constant):
            return bool(condition.value)
        else:
            raise InterpreterError(
                f"Unsupported condition type: {type(condition).__name__}"
            )

    def _execute_if(self, if_statement: ast.If):
        result = None
        if self._execute_condition(if_statement.test):
            for line in if_statement.body:
                line_result = self._execute_ast(line)
                if line_result is not None:
                    result = line_result
        else:
            for line in if_statement.orelse:
                line_result = self._execute_ast(line)
                if line_result is not None:
                    result = line_result
        return result

    def _execute_ifexp(self, ifexp: ast.IfExp) -> Any:
        test_result = self._execute_condition(ifexp.test)
        if test_result:
            return self._execute_ast(ifexp.body)
        else:
            return self._execute_ast(ifexp.orelse)

    def _execute_import(self, import_module: ast.Import) -> None:
        for module in import_module.names:
            self._validate_import(module.name)
            alias = module.asname or module.name
            self.state[alias] = importlib.import_module(module.name)

    def _execute_import_from(self, import_from: ast.ImportFrom):
        if import_from.module is None:
            raise InterpreterError('"from . import" is not supported.')
        for import_name in import_from.names:
            full_name = import_from.module + f".{import_name.name}"
            self._validate_import(full_name)
            imported_module = importlib.import_module(import_from.module)
            alias = import_name.asname or import_name.name
            self.state[alias] = getattr(imported_module, import_name.name)

    # Note: Two versions of _execute_for and _execute_while appear in this file.
    # We keep both as provided, but you may wish to consolidate these in your code.

    def _execute_for(self, for_statement: ast.For):
        class BreakException(Exception):
            pass

        class ContinueException(Exception):
            pass

        result = None
        try:
            for value in self._execute_ast(for_statement.iter):
                self._assign(for_statement.target, value)
                try:
                    for line in for_statement.body:
                        line_result = self._execute_ast(line)
                        if line_result is not None:
                            result = line_result
                except ContinueException:
                    continue
        except BreakException:
            pass
        return result

    def _execute_while(self, while_statement: ast.While):
        class BreakException(Exception):
            pass

        class ContinueException(Exception):
            pass

        result = None
        try:
            while self._execute_condition(while_statement.test):
                try:
                    for line in while_statement.body:
                        line_result = self._execute_ast(line)
                        if line_result is not None:
                            result = line_result
                except ContinueException:
                    continue
        except BreakException:
            pass
        return result

    def _execute_try(self, try_statement: ast.Try):
        try:
            for line in try_statement.body:
                self._execute_ast(line)
        except Exception as e:
            handled = False
            for handler in try_statement.handlers:
                if handler.type is None or isinstance(
                    e, self._execute_ast(handler.type)
                ):
                    if handler.name:
                        self.state[handler.name.id] = e
                    for line in handler.body:
                        self._execute_ast(line)
                    handled = True
                    break
            if not handled:
                raise
        finally:
            for line in try_statement.finalbody:
                self._execute_ast(line)

    def _execute_raise(self, raise_statement: ast.Raise):
        if raise_statement.exc:
            exception = self._execute_ast(raise_statement.exc)
            raise exception
        else:
            raise

    def _execute_assert(self, assert_statement: ast.Assert):
        test_result = self._execute_condition(assert_statement.test)
        if not test_result:
            if assert_statement.msg:
                msg = self._execute_ast(assert_statement.msg)
                raise AssertionError(msg)
            else:
                raise AssertionError

    def _execute_lambda(self, lambda_node: ast.Lambda) -> Any:
        def lambda_function(*args):
            old_state = self.state.copy()
            for param, arg in zip(lambda_node.args.args, args):
                self.state[param.arg] = arg
            result = self._execute_ast(lambda_node.body)
            self.state = old_state  # Restore the state
            return result

        return lambda_function

    def _validate_import(self, full_name: str):
        tmp_name = ""
        found_name = False
        for name in full_name.split("."):
            tmp_name += name if tmp_name == "" else f".{name}"
            if tmp_name in self.import_white_list:
                found_name = True
                return
        if not found_name:
            raise InterpreterError(
                f"It is not permitted to import modules "
                f"than module white list (try to import {full_name})."
            )

    def _execute_binop(self, binop: ast.BinOp):
        left = self._execute_ast(binop.left)
        operator = binop.op
        right = self._execute_ast(binop.right)

        if isinstance(operator, ast.Add):
            return left + right
        elif isinstance(operator, ast.Sub):
            return left - right
        elif isinstance(operator, ast.Mult):
            return left * right
        elif isinstance(operator, ast.Div):
            return left / right
        elif isinstance(operator, ast.FloorDiv):
            return left // right
        elif isinstance(operator, ast.Mod):
            return left % right
        elif isinstance(operator, ast.Pow):
            return left**right
        elif isinstance(operator, ast.LShift):
            return left << right
        elif isinstance(operator, ast.RShift):
            return left >> right
        elif isinstance(operator, ast.BitAnd):
            return left & right
        elif isinstance(operator, ast.BitOr):
            return left | right
        elif isinstance(operator, ast.BitXor):
            return left ^ right
        elif isinstance(operator, ast.MatMult):
            return left @ right
        else:
            raise InterpreterError(f"Operator not supported: {operator}")

    def _execute_unaryop(self, unaryop: ast.UnaryOp):
        operand = self._execute_ast(unaryop.operand)
        operator = unaryop.op

        if isinstance(operator, ast.UAdd):
            return +operand
        elif isinstance(operator, ast.USub):
            return -operand
        elif isinstance(operator, ast.Not):
            return not operand
        elif isinstance(operator, ast.Invert):
            return ~operand
        else:
            raise InterpreterError(f"Operator not supported: {operator}")

    def _execute_listcomp(self, comp: ast.ListComp):
        return [self._execute_comp(comp.elt, comp.generators)]

    def _execute_dictcomp(self, comp: ast.DictComp):
        return {
            self._execute_comp(comp.key, comp.generators): self._execute_comp(
                comp.value, comp.generators
            )
        }

    def _execute_setcomp(self, comp: ast.SetComp):
        return {self._execute_comp(comp.elt, comp.generators)}

    def _execute_comp(self, elt, generators):
        if not generators:
            return self._execute_ast(elt)
        gen = generators[0]
        result = []
        for value in self._execute_ast(gen.iter):
            self._assign(gen.target, value)
            if all(self._execute_condition(if_cond) for if_cond in gen.ifs):
                result.extend(self._execute_comp(elt, generators[1:]))
        return result

    def _execute_generatorexp(self, genexp: ast.GeneratorExp):
        def generator():
            for value in self._execute_comp(genexp.elt, genexp.generators):
                yield value

        return generator()

    def _get_value_from_state(self, key: str) -> Any:
        if key in self.state:
            return self.state[key]
        elif key in self.fuzz_state:
            return self.fuzz_state[key]
        else:
            raise InterpreterError(f"The variable `{key}` is not defined.")


class TextPrompt(str):
    r"""A class that represents a text prompt. The :obj:`TextPrompt` class
    extends the built-in :obj:`str` class to provide a property for retrieving
    the set of keywords in the prompt.
    """

    @property
    def key_words(self) -> set[str]:
        pattern = re.compile(r"\{([^{}]+)\}")
        found = pattern.findall(self)
        return set(found)

    def format(self, *args: Any, **kwargs: Any) -> "TextPrompt":
        default_kwargs = {key: "{" + f"{key}" + "}" for key in self.key_words}
        default_kwargs.update(kwargs)
        return TextPrompt(super().format(*args, **default_kwargs))


class CodePrompt(TextPrompt):
    r"""A class that represents a code prompt. It extends the :obj:`TextPrompt`
    class with a :obj:`code_type` property.
    """

    def __new__(cls, *args: Any, **kwargs: Any) -> "CodePrompt":
        code_type = kwargs.pop("code_type", None)
        instance = super().__new__(cls, *args, **kwargs)
        instance._code_type = code_type
        return instance

    @property
    def code_type(self) -> str | None:
        return self._code_type

    def set_code_type(self, code_type: str) -> None:
        self._code_type = code_type

    def execute(
        self,
        interpreter: PythonInterpreter | None = None,
        user_variable: dict[str, Any] | None = None,
    ) -> tuple[Any, PythonInterpreter]:
        if not interpreter:
            interpreter = PythonInterpreter(action_space=globals())
        execution_res = interpreter.execute(
            self, fuzz_state=user_variable, keep_state=True
        )
        return execution_res, interpreter

```

---

### 77. src\flock\core\logging\__init__.py

- **File ID**: file_76
- **Type**: Code File
- **Line Count**: 2
- **Description**: Flock logging system with Rich integration and structured logging support.
- **Dependencies**: None
- **Used By**: None

**Content**:
```
"""Flock logging system with Rich integration and structured logging support."""


```

---

### 78. src\flock\core\logging\formatters\enum_builder.py

- **File ID**: file_77
- **Type**: Code File
- **Line Count**: 38
- **Description**: Enum Builder.
- **Dependencies**: None
- **Used By**: None

**Content**:
```
"""Enum Builder."""

import os
import pathlib
import re

theme_folder = pathlib.Path(__file__).parent.parent.parent.parent / "themes"

if not theme_folder.exists():
    raise FileNotFoundError(f"Theme folder not found: {theme_folder}")

theme_files = [
    pathlib.Path(f.path).stem for f in os.scandir(theme_folder) if f.is_file()
]

theme_enum_entries = {}
for theme in theme_files:
    safe_name = (
        theme.replace("-", "_")
        .replace(" ", "_")
        .replace("(", "_")
        .replace(")", "_")
        .replace("+", "_")
        .replace(".", "_")
    )

    if re.match(r"^\d", safe_name):
        safe_name = f"_{safe_name}"

    theme_enum_entries[safe_name] = theme

with open("theme_enum.py", "w") as f:
    f.write("from enum import Enum\n\n")
    f.write("class OutputOptionsTheme(Enum):\n")
    for safe_name, original_name in theme_enum_entries.items():
        f.write(f'    {safe_name} = "{original_name}"\n')

print("Generated theme_enum.py ✅")

```

---

### 79. src\flock\core\logging\formatters\theme_builder.py

- **File ID**: file_78
- **Type**: Code File
- **Line Count**: 476
- **Description**: A simple interactive theme builder....
- **Dependencies**: None
- **Used By**: None

**Content**:
```
#!/usr/bin/env python
"""A simple interactive theme builder.

Steps:
1. Load theme files from a folder (or pick N random ones).
2. Display each theme’s color palette (colors only).
3. Let the user choose a palette.
4. Generate a number of sample tables using that palette (with randomized non-color settings).
5. Let the user select one sample table and save its configuration to a TOML file.
"""

import pathlib
import random
import re
from typing import Any

import toml
from rich import box
from rich.console import Console, Group
from rich.panel import Panel
from rich.table import Table
from rich.text import Text


def resolve_style_string(style_str: str, theme: dict) -> str:
    """Replace tokens of the form "color.<section>.<key>" in style_str with
    the value from theme["colors"][<section>][<key>].
    """
    pattern = r"color\.(\w+)\.(\w+)"

    def repl(match):
        section = match.group(1)
        key = match.group(2)
        try:
            return theme["colors"][section][key]
        except KeyError:
            return match.group(0)

    return re.sub(pattern, repl, style_str)


def generate_default_rich_block(theme: dict | None = None) -> dict[str, Any]:
    """Generate a default [rich] block that includes:
    - Color properties computed from the theme's [colors] blocks.
    - Extra color tokens (so tokens like "color.bright.green" can be used).
    - Non-color table layout properties, randomly chosen.
    """

    def random_background():
        return random.choice(
            [
                f"{normal_black}",
                f"{normal_blue}",
                f"{primary_background}",
                f"{selection_background}",
                f"{cursor_cursor}",
            ]
        )

    if theme is not None:
        bright = theme["colors"].get("bright", {})
        normal = theme["colors"].get("normal", {})
        cursor = theme["colors"].get("cursor", {})
        primary = theme["colors"].get("primary", {})
        selection = theme["colors"].get("selection", {})

        bright_black = bright.get("black", "#000000")
        bright_blue = bright.get("blue", "#96cbfe")
        bright_cyan = bright.get("cyan", "#85befd")
        bright_green = bright.get("green", "#94fa36")
        bright_magenta = bright.get("magenta", "#b9b6fc")
        bright_red = bright.get("red", "#fd5ff1")
        bright_white = bright.get("white", "#e0e0e0")
        bright_yellow = bright.get("yellow", "#f5ffa8")

        normal_black = normal.get("black", "#000000")
        normal_blue = normal.get("blue", "#85befd")
        normal_cyan = normal.get("cyan", "#85befd")
        normal_green = normal.get("green", "#87c38a")
        normal_magenta = normal.get("magenta", "#b9b6fc")
        normal_red = normal.get("red", "#fd5ff1")
        normal_white = normal.get("white", "#e0e0e0")
        normal_yellow = normal.get("yellow", "#ffd7b1")

        cursor_cursor = cursor.get("cursor", "#d0d0d0")
        cursor_text = cursor.get("text", "#151515")

        primary_background = primary.get("background", "#161719")
        primary_foreground = primary.get("foreground", "#c5c8c6")
        selection_background = selection.get("background", "#444444")
        selection_text = selection.get("text", primary_foreground)
    else:
        # Fallback default values.
        bright_black = "black"
        bright_blue = "blue"
        bright_cyan = "cyan"
        bright_green = "green"
        bright_magenta = "magenta"
        bright_red = "red"
        bright_white = "white"
        bright_yellow = "yellow"
        normal_black = "black"
        normal_blue = "blue"
        normal_cyan = "cyan"
        normal_green = "green"
        normal_magenta = "magenta"
        normal_red = "red"
        normal_white = "white"
        normal_yellow = "yellow"
        cursor_cursor = "gray"
        cursor_text = "white"
        primary_background = "black"
        primary_foreground = "white"
        selection_background = "gray"
        selection_text = "white"

    # Color properties.
    default_color_props = {
        "panel_style": f"on {random_background()}",
        "table_header_style": f"bold {selection_text} on {selection_background}",
        "table_title_style": f"bold {primary_foreground}",
        "table_border_style": bright_blue,
        "panel_border_style": bright_blue,
        "column_output": f"bold {primary_foreground}",
        "column_value": primary_foreground,
    }
    # Extra color tokens.
    extra_color_props = {
        "bright_black": bright_black,
        "bright_blue": bright_blue,
        "bright_cyan": bright_cyan,
        "bright_green": bright_green,
        "bright_magenta": bright_magenta,
        "bright_red": bright_red,
        "bright_white": bright_white,
        "bright_yellow": bright_yellow,
        "normal_black": normal_black,
        "normal_blue": normal_blue,
        "normal_cyan": normal_cyan,
        "normal_green": normal_green,
        "normal_magenta": normal_magenta,
        "normal_red": normal_red,
        "normal_white": normal_white,
        "normal_yellow": normal_yellow,
        "cursor_cursor": cursor_cursor,
        "cursor_text": cursor_text,
    }
    # Non-color layout properties, randomly chosen.
    default_non_color_props = {
        "table_show_lines": random.choice([True, False]),
        "table_box": random.choice(
            ["ROUNDED", "SIMPLE", "SQUARE", "MINIMAL", "HEAVY", "DOUBLE_EDGE"]
        ),
        "panel_padding": random.choice([[1, 2], [1, 1], [2, 2], [0, 2]]),
        "panel_title_align": random.choice(["left", "center", "right"]),
        "table_row_styles": random.choice(
            [["", "dim"], ["", "italic"], ["", "underline"]]
        ),
    }
    # Extra table layout properties (non-content).
    default_extra_table_props = {
        "table_safe_box": True,
        "table_padding": [0, 1],
        "table_collapse_padding": False,
        "table_pad_edge": True,
        "table_expand": False,
        "table_show_footer": False,
        "table_show_edge": True,
        "table_leading": 0,
        "table_style": "none",
        "table_footer_style": "none",
        "table_caption": "",
        "table_caption_style": "none",
        "table_title_justify": "center",
        "table_caption_justify": "center",
        "table_highlight": False,
    }
    defaults = {
        **default_color_props,
        **extra_color_props,
        **default_non_color_props,
        **default_extra_table_props,
    }
    return defaults


def load_theme_from_file(filepath: str) -> dict:
    """Load a theme from a TOML file.

    If the file does not contain a [rich] block, one is generated and saved.
    """
    with open(filepath) as f:
        theme = toml.load(f)
    if "rich" not in theme:
        theme["rich"] = generate_default_rich_block(theme)
        with open(filepath, "w") as f:
            toml.dump(theme, f)
    return theme


def get_default_styles(theme: dict | None) -> dict[str, Any]:
    """Build a style mapping from the theme by merging defaults with any overrides
    in the [rich] block. Also resolves any color tokens.
    """
    if theme is None:
        final_styles = generate_default_rich_block(None)
    else:
        defaults = generate_default_rich_block(theme)
        rich_props = theme.get("rich", {})
        final_styles = {
            key: rich_props.get(key, defaults[key]) for key in defaults
        }
    # Ensure tuple for padding properties.
    final_styles["panel_padding"] = tuple(final_styles["panel_padding"])
    if "table_padding" in final_styles:
        final_styles["table_padding"] = tuple(final_styles["table_padding"])
    # Resolve tokens.
    if theme is not None:
        for key, value in final_styles.items():
            if isinstance(value, str):
                final_styles[key] = resolve_style_string(value, theme)
    return final_styles


def create_rich_renderable(
    value: Any,
    level: int = 0,
    theme: dict | None = None,
    styles: dict[str, Any] | None = None,
) -> Any:
    """Recursively creates a Rich renderable.

    - If value is a dict, renders it as a Table.
    - If a list/tuple, renders each item.
    - Otherwise, returns the string representation.
    """
    if styles is None:
        styles = get_default_styles(theme)

    if isinstance(value, dict):
        box_style = (
            getattr(box, styles["table_box"])
            if isinstance(styles["table_box"], str)
            else styles["table_box"]
        )
        table_kwargs = {
            "show_header": True,
            "header_style": styles["table_header_style"],
            "title": f"Subtable (Level {level})" if level > 0 else None,
            "title_style": styles["table_title_style"],
            "border_style": styles["table_border_style"],
            "show_lines": styles["table_show_lines"],
            "box": box_style,
            "row_styles": styles["table_row_styles"],
            "safe_box": styles.get("table_safe_box"),
            "padding": styles.get("table_padding"),
            "collapse_padding": styles.get("table_collapse_padding"),
            "pad_edge": styles.get("table_pad_edge"),
            "expand": styles.get("table_expand"),
            "show_footer": styles.get("table_show_footer"),
            "show_edge": styles.get("table_show_edge"),
            "leading": styles.get("table_leading"),
            "style": styles.get("table_style"),
            "footer_style": styles.get("table_footer_style"),
            "caption": styles.get("table_caption"),
            "caption_style": styles.get("table_caption_style"),
            "title_justify": styles.get("table_title_justify"),
            "caption_justify": styles.get("table_caption_justify"),
            "highlight": styles.get("table_highlight"),
        }
        table = Table(**table_kwargs)
        table.add_column("Key", style=styles["column_output"])
        table.add_column("Value", style=styles["column_value"])
        for k, v in value.items():
            table.add_row(
                str(k), create_rich_renderable(v, level + 1, theme, styles)
            )
        return table

    elif isinstance(value, (list, tuple)):
        if all(isinstance(item, dict) for item in value):
            sub_tables = []
            for i, item in enumerate(value):
                sub_tables.append(f"[bold]Item {i + 1}[/bold]")
                sub_tables.append(
                    create_rich_renderable(item, level + 1, theme, styles)
                )
            return Group(*sub_tables)
        else:
            rendered_items = [
                create_rich_renderable(item, level + 1, theme, styles)
                for item in value
            ]
            if all(isinstance(item, str) for item in rendered_items):
                return "\n".join(rendered_items)
            else:
                return Group(*rendered_items)
    else:
        if isinstance(value, str) and "\n" in value:
            return f"\n{value}\n"
        return str(value)


# --- Theme Builder Functions --- #


def load_theme_files(theme_dir: pathlib.Path) -> list[pathlib.Path]:
    """Return a list of .toml theme files in the given directory."""
    return list(theme_dir.glob("*.toml"))


def display_color_palette(theme: dict) -> None:
    """Display the color palette from a theme's [colors] sections with a color preview."""
    console = Console()
    palette_table = Table(
        title="Color Palette", show_header=True, header_style="bold"
    )
    palette_table.add_column("Section", style="bold")
    palette_table.add_column("Key", style="italic")
    palette_table.add_column("Value", style="bold")
    palette_table.add_column("Preview", justify="center")

    # Iterate over the colors in each section.
    for section, colors in theme.get("colors", {}).items():
        for key, value in colors.items():
            # Create a Text object with a fixed-width string (here, six spaces)
            # styled with a background color of the actual color value.
            preview = Text("      ", style=f"on {value}")
            palette_table.add_row(section, key, value, preview)

    console.print(palette_table)


def generate_sample_rich_blocks(
    chosen_theme: dict, count: int
) -> list[dict[str, Any]]:
    """Generate a list of sample rich blocks (randomized layout) using the chosen theme's colors."""
    samples = []
    for _ in range(count):
        samples.append(generate_default_rich_block(chosen_theme))
    return samples


def generate_sample_table(sample_theme: dict, dummy_data: dict) -> Panel:
    """Generate a sample table using the given theme dictionary (which includes a [rich] block)
    and some dummy data.
    """
    # Here we use our create_rich_renderable to build a table for dummy_data.
    # For simplicity, we create our own panel.
    styles = get_default_styles(sample_theme)
    # Build a basic table (using our earlier functions)
    table = create_rich_renderable(
        dummy_data, theme=sample_theme, styles=styles
    )
    return Panel(
        table,
        title="Sample Table",
        title_align=styles["panel_title_align"],
        border_style=styles["panel_border_style"],
        padding=styles["panel_padding"],
        style=styles["panel_style"],
    )


def save_theme(theme: dict, filename: pathlib.Path) -> None:
    """Save the given theme dictionary to the specified TOML file."""
    with open(filename, "w") as f:
        toml.dump(theme, f)


# --- Main Interactive Loop --- #


def theme_builder():
    console = Console(force_terminal=True, color_system="truecolor")
    themes_dir = pathlib.Path(__file__).parent.parent.parent.parent / "themes"
    theme_files = load_theme_files(themes_dir)

    if not theme_files:
        console.print("[red]No theme files found in the themes folder.[/red]")
        return

    # Ask the user: load all themes or N random themes?
    console.print("[bold]Theme Builder[/bold]")
    choice = console.input(
        "Load [bold](a)ll[/bold] themes or [bold](n)[/bold] random ones? (a/n): "
    )
    if choice.lower() == "n":
        n = console.input("How many random themes? ")
        try:
            n = int(n)
        except ValueError:
            n = len(theme_files)
        theme_files = random.sample(theme_files, min(n, len(theme_files)))

    # Display palettes for each theme file.
    console.print("\n[underline]Available Color Palettes:[/underline]")
    palettes = []
    for idx, tf in enumerate(theme_files):
        theme_dict = load_theme_from_file(str(tf))
        palettes.append((tf, theme_dict))
        console.print(f"\n[bold]Theme #{idx} - {tf.name}[/bold]")
        display_color_palette(theme_dict)

    # Let the user choose a palette by index.
    sel = console.input("\nEnter the number of the palette to use: ")
    try:
        sel = int(sel)
        chosen_theme = palettes[sel][1]
    except (ValueError, IndexError):
        console.print("[red]Invalid selection. Exiting.[/red]")
        return

    console.print("\n[underline]Selected Palette:[/underline]")
    display_color_palette(chosen_theme)

    # Ask the user how many sample tables to generate.
    count = console.input("\nHow many sample tables to generate? (default 3): ")
    try:
        count = int(count)
    except ValueError:
        count = 3

    # Generate sample rich blocks from the chosen theme.
    sample_rich_blocks = generate_sample_rich_blocks(chosen_theme, count)

    # For each sample, create a new theme dict that uses the chosen palette and the sample rich block.
    dummy_data = {
        "Agent": "Test Agent",
        "Status": "Running",
        "Metrics": {
            "CPU": "20%",
            "Memory": "512MB",
            "Nested": {"value1": 1, "value2": 2},
        },
        "Logs": [
            "Initialization complete",
            "Running process...",
            {"Step": "Completed", "Time": "2025-02-07T12:00:00Z"},
        ],
    }

    samples = []
    for i, rich_block in enumerate(sample_rich_blocks):
        # Build a sample theme: copy the chosen theme and override its [rich] block.
        sample_theme = dict(
            chosen_theme
        )  # shallow copy (good enough if colors remain unchanged)
        sample_theme["rich"] = rich_block
        sample_table = generate_sample_table(sample_theme, dummy_data)
        samples.append((sample_theme, sample_table))
        console.print(f"\n[bold]Sample Table #{i}[/bold]")
        console.print(sample_table)

    # Let the user choose one sample or regenerate.
    sel2 = console.input(
        "\nEnter the number of the sample table you like, or type [bold]r[/bold] to regenerate: "
    )
    if sel2.lower() == "r":
        console.print("Regenerating samples...")
        theme_builder()  # restart the builder
        return
    try:
        sel2 = int(sel2)
        chosen_sample_theme = samples[sel2][0]
    except (ValueError, IndexError):
        console.print("[red]Invalid selection. Exiting.[/red]")
        return

    # Ask for file name to save the chosen theme.
    filename = console.input(
        "\nEnter a filename to save the chosen theme (e.g. mytheme.toml): "
    )
    save_path = themes_dir / filename
    save_theme(chosen_sample_theme, save_path)
    console.print(f"\n[green]Theme saved as {save_path}.[/green]")

```

---

### 80. src\flock\core\logging\formatters\themed_formatter.py

- **File ID**: file_79
- **Type**: Code File
- **Line Count**: 546
- **Description**: A Rich-based formatter for agent results with theme support.
- **Dependencies**: None
- **Used By**: None

**Content**:
```
"""A Rich-based formatter for agent results with theme support."""

import pathlib
import random
import re
from typing import Any

from temporalio import workflow

from flock.core.logging.formatters.themes import OutputTheme

with workflow.unsafe.imports_passed_through():
    from pygments.style import Style
    from pygments.token import Token
    from rich import box
    from rich.console import Console, Group
    from rich.panel import Panel
    from rich.syntax import PygmentsSyntaxTheme, Syntax
    from rich.table import Table
    from rich.theme import Theme

import toml  # install with: pip install toml


def resolve_style_string(style_str: str, theme: dict) -> str:
    """Replace tokens in a style string of the form.

        color.<section>.<key>

    with the corresponding value from theme["colors"][<section>][<key>].
    If the token cannot be resolved, it is left unchanged.
    """
    pattern = r"color\.(\w+)\.(\w+)"

    def repl(match):
        section = match.group(1)
        key = match.group(2)
        try:
            return theme["colors"][section][key]
        except KeyError:
            return match.group(0)  # leave token unchanged if not found

    return re.sub(pattern, repl, style_str)


def generate_default_rich_block(theme: dict | None = None) -> dict[str, Any]:
    """Generate a default [rich] block with *all* styling properties.

    For the color mapping properties the defaults are computed from the
    theme's [colors] blocks (if available). This includes colors from the
    "bright", "normal", and "cursor" sections.

    Non color properties (layout and table specific properties) are randomly
    chosen from a set of sensible alternatives.
    """
    if theme is not None:
        # Retrieve colors from the theme.
        bright_black = theme["colors"]["bright"].get("black", "#000000")
        bright_blue = theme["colors"]["bright"].get("blue", "#96cbfe")
        bright_cyan = theme["colors"]["bright"].get("cyan", "#85befd")
        bright_green = theme["colors"]["bright"].get("green", "#94fa36")
        bright_magenta = theme["colors"]["bright"].get("magenta", "#b9b6fc")
        bright_red = theme["colors"]["bright"].get("red", "#fd5ff1")
        bright_white = theme["colors"]["bright"].get("white", "#e0e0e0")
        bright_yellow = theme["colors"]["bright"].get("yellow", "#f5ffa8")

        normal_black = theme["colors"]["normal"].get("black", "#000000")
        normal_blue = theme["colors"]["normal"].get("blue", "#85befd")
        normal_cyan = theme["colors"]["normal"].get("cyan", "#85befd")
        normal_green = theme["colors"]["normal"].get("green", "#87c38a")
        normal_magenta = theme["colors"]["normal"].get("magenta", "#b9b6fc")
        normal_red = theme["colors"]["normal"].get("red", "#fd5ff1")
        normal_white = theme["colors"]["normal"].get("white", "#e0e0e0")
        normal_yellow = theme["colors"]["normal"].get("yellow", "#ffd7b1")

        cursor_cursor = theme["colors"]["cursor"].get("cursor", "#d0d0d0")
        cursor_text = theme["colors"]["cursor"].get("text", "#151515")

        primary_background = theme["colors"]["primary"].get(
            "background", "#161719"
        )
        primary_foreground = theme["colors"]["primary"].get(
            "foreground", "#c5c8c6"
        )
        selection_background = theme["colors"]["selection"].get(
            "background", "#444444"
        )
        selection_text = theme["colors"]["selection"].get(
            "text", primary_foreground
        )
    else:
        bright_black = "black"
        bright_blue = "blue"
        bright_cyan = "cyan"
        bright_green = "green"
        bright_magenta = "magenta"
        bright_red = "red"
        bright_white = "white"
        bright_yellow = "yellow"

        normal_black = "black"
        normal_blue = "blue"
        normal_cyan = "cyan"
        normal_green = "green"
        normal_magenta = "magenta"
        normal_red = "red"
        normal_white = "white"
        normal_yellow = "yellow"

        cursor_cursor = "gray"
        cursor_text = "white"

        primary_background = "black"
        primary_foreground = "white"
        selection_background = "gray"
        selection_text = "white"

    # Color properties computed from the theme.
    default_color_props = {
        "panel_style": f"on {primary_background}",
        "table_header_style": f"bold {selection_text} on {selection_background}",
        "table_title_style": f"bold {primary_foreground}",
        "table_border_style": bright_blue,
        "panel_border_style": bright_blue,
        "column_output": f"bold {primary_foreground}",
        "column_value": primary_foreground,
    }
    # Extra color tokens so they can be used via tokens like color.bright.black, etc.
    extra_color_props = {
        "bright_black": bright_black,
        "bright_blue": bright_blue,
        "bright_cyan": bright_cyan,
        "bright_green": bright_green,
        "bright_magenta": bright_magenta,
        "bright_red": bright_red,
        "bright_white": bright_white,
        "bright_yellow": bright_yellow,
        "normal_black": normal_black,
        "normal_blue": normal_blue,
        "normal_cyan": normal_cyan,
        "normal_green": normal_green,
        "normal_magenta": normal_magenta,
        "normal_red": normal_red,
        "normal_white": normal_white,
        "normal_yellow": normal_yellow,
        "cursor_cursor": cursor_cursor,
        "cursor_text": cursor_text,
    }
    # Randomly choose non color properties.
    default_non_color_props = {
        "table_show_lines": random.choice([True, False]),
        "table_box": random.choice(
            ["ROUNDED", "SIMPLE", "SQUARE", "MINIMAL", "HEAVY", "DOUBLE_EDGE"]
        ),
        "panel_padding": random.choice([[1, 2], [1, 1], [2, 2], [0, 2]]),
        "panel_title_align": random.choice(["left", "center", "right"]),
        # Add table_row_styles property.
        "table_row_styles": random.choice(
            [["", "dim"], ["", "italic"], ["", "underline"]]
        ),
    }
    # Extra table layout properties (non content properties).
    default_extra_table_props = {
        "table_safe_box": True,
        "table_padding": [0, 1],
        "table_collapse_padding": False,
        "table_pad_edge": True,
        "table_expand": False,
        "table_show_footer": False,
        "table_show_edge": True,
        "table_leading": 0,
        "table_style": "none",
        "table_footer_style": "none",
        "table_caption": None,
        "table_caption_style": "none",
        "table_title_justify": "center",
        "table_caption_justify": "center",
        "table_highlight": False,
    }
    # Combine all defaults.
    defaults = {
        **default_color_props,
        **extra_color_props,
        **default_non_color_props,
        **default_extra_table_props,
    }
    return defaults


def load_theme_from_file(filepath: str) -> dict:
    """Load a theme from a TOML file.

    The theme is expected to contain color blocks like [colors.primary],
    [colors.selection], [colors.normal], [colors.cursor], etc.
    If the file does not contain a [rich] block for styling properties,
    one is generated (with all properties including color mappings) and
    written back into the file.
    """
    with open(filepath) as f:
        theme = toml.load(f)

    if "rich" not in theme:
        theme["rich"] = generate_default_rich_block(theme)
        # Write the updated theme back into the file.
        with open(filepath, "w") as f:
            toml.dump(theme, f)

    return theme


def get_default_styles(theme: dict | None) -> dict[str, Any]:
    """Build a style mapping from the theme.

    It first computes defaults from the [colors] block (via generate_default_rich_block)
    and then overrides any property found in the [rich] block.
    Finally, for every property that is a string, tokens of the form
    "color.<section>.<key>" are resolved.
    """
    if theme is None:
        final_styles = generate_default_rich_block(None)
    else:
        defaults = generate_default_rich_block(theme)
        rich_props = theme.get("rich", {})
        final_styles = {
            key: rich_props.get(key, defaults[key]) for key in defaults
        }

    # Ensure that panel_padding and table_padding are tuples.
    final_styles["panel_padding"] = tuple(final_styles["panel_padding"])
    if "table_padding" in final_styles:
        final_styles["table_padding"] = tuple(final_styles["table_padding"])

    # Resolve tokens in every string value.
    if theme is not None:
        for key, value in final_styles.items():
            if isinstance(value, str):
                final_styles[key] = resolve_style_string(value, theme)

    return final_styles


def create_rich_renderable(
    value: Any,
    level: int = 0,
    theme: dict | None = None,
    styles: dict[str, Any] | None = None,
    max_length: int = -1,
) -> Any:
    """Recursively creates a Rich renderable for a given value.

    - For dicts: creates a Table with headers styled via the computed properties.
    - For lists/tuples: if every item is a dict, returns a Group of subtables;
      otherwise, renders each item recursively.
    - Other types: returns a string (adding extra newlines for multi-line strings).
    """
    if styles is None:
        styles = get_default_styles(theme)

    # If the value is a dictionary, render it as a table.
    if isinstance(value, dict):
        # Convert table_box string into an actual box style.
        box_style = (
            getattr(box, styles["table_box"])
            if isinstance(styles["table_box"], str)
            else styles["table_box"]
        )
        # Gather all table-related keyword arguments.
        table_kwargs = {
            "show_header": True,
            "header_style": styles["table_header_style"],
            "title": f"Subtable (Level {level})" if level > 0 else None,
            "title_style": styles["table_title_style"],
            "border_style": styles["table_border_style"],
            "show_lines": styles["table_show_lines"],
            "box": box_style,
            "row_styles": styles["table_row_styles"],
            "safe_box": styles.get("table_safe_box"),
            "padding": styles.get("table_padding"),
            "collapse_padding": styles.get("table_collapse_padding"),
            "pad_edge": styles.get("table_pad_edge"),
            "expand": styles.get("table_expand"),
            "show_footer": styles.get("table_show_footer"),
            "show_edge": styles.get("table_show_edge"),
            "leading": styles.get("table_leading"),
            "style": styles.get("table_style"),
            "footer_style": styles.get("table_footer_style"),
            "caption": styles.get("table_caption"),
            "caption_style": styles.get("table_caption_style"),
            "title_justify": styles.get("table_title_justify"),
            "caption_justify": styles.get("table_caption_justify"),
            "highlight": styles.get("table_highlight"),
        }
        table = Table(**table_kwargs)
        table.add_column("Key", style=styles["column_output"])
        table.add_column("Value", style=styles["column_value"])
        for k, v in value.items():
            table.add_row(
                str(k),
                create_rich_renderable(v, level + 1, theme, styles, max_length),
            )
        return table

    # If the value is a list or tuple, render each item.
    elif isinstance(value, list | tuple):
        if all(isinstance(item, dict) for item in value):
            sub_tables = []
            for i, item in enumerate(value):
                sub_tables.append(f"[bold]Item {i + 1}[/bold]")
                sub_tables.append(
                    create_rich_renderable(
                        item, level + 1, theme, styles, max_length=max_length
                    )
                )
            return Group(*sub_tables)
        else:
            rendered_items = [
                create_rich_renderable(
                    item, level + 1, theme, styles, max_length=max_length
                )
                for item in value
            ]
            if all(isinstance(item, str) for item in rendered_items):
                return "\n".join(rendered_items)
            else:
                return Group(*rendered_items)

    # Otherwise, return a string representation.
    else:
        s = str(value).strip()
        if max_length > 0 and len(s) > max_length:
            omitted = len(s) - max_length
            s = (
                s[:max_length]
                + f"[bold bright_yellow]...(+{omitted}chars)[/bold bright_yellow]"
            )
        if isinstance(value, str) and "\n" in value:
            return f"\n{s}\n"
        return s


def load_syntax_theme_from_file(filepath: str) -> dict:
    """Load a syntax highlighting theme from a TOML file and map it to Rich styles."""
    with open(filepath) as f:
        theme = toml.load(f)

    if "colors" not in theme:
        raise ValueError(
            f"Theme file {filepath} does not contain a 'colors' section."
        )

    # Map theme colors to syntax categories
    syntax_theme = {
        "background": theme["colors"]["primary"].get("background", "#161719"),
        "text": theme["colors"]["primary"].get("foreground", "#c5c8c6"),
        "comment": theme["colors"]["normal"].get("black", "#666666"),
        "keyword": theme["colors"]["bright"].get("magenta", "#ff79c6"),
        "builtin": theme["colors"]["bright"].get("cyan", "#8be9fd"),
        "string": theme["colors"]["bright"].get("green", "#50fa7b"),
        "name": theme["colors"]["bright"].get("blue", "#6272a4"),
        "number": theme["colors"]["bright"].get("yellow", "#f1fa8c"),
        "operator": theme["colors"]["bright"].get("red", "#ff5555"),
        "punctuation": theme["colors"]["normal"].get("white", "#bbbbbb"),
        "error": theme["colors"]["bright"].get("red", "#ff5555"),
    }

    return syntax_theme


def create_rich_syntax_theme(syntax_theme: dict) -> Theme:
    """Convert a syntax theme dict to a Rich-compatible Theme."""
    return Theme(
        {
            "background": f"on {syntax_theme['background']}",
            "text": syntax_theme["text"],
            "keyword": f"bold {syntax_theme['keyword']}",
            "builtin": f"bold {syntax_theme['builtin']}",
            "string": syntax_theme["string"],
            "name": syntax_theme["name"],
            "number": syntax_theme["number"],
            "operator": syntax_theme["operator"],
            "punctuation": syntax_theme["punctuation"],
            "error": f"bold {syntax_theme['error']}",
        }
    )


def create_pygments_syntax_theme(syntax_theme: dict) -> PygmentsSyntaxTheme:
    """Convert a syntax theme dict to a Pygments-compatible Rich syntax theme."""

    class CustomSyntaxStyle(Style):
        """Dynamically generated Pygments style based on the loaded theme."""

        background_color = syntax_theme["background"]
        styles = {
            Token.Text: syntax_theme["text"],
            Token.Comment: f"italic {syntax_theme['comment']}",
            Token.Keyword: f"bold {syntax_theme['keyword']}",
            Token.Name.Builtin: f"bold {syntax_theme['builtin']}",
            Token.String: syntax_theme["string"],
            Token.Name: syntax_theme["name"],
            Token.Number: syntax_theme["number"],
            Token.Operator: syntax_theme["operator"],
            Token.Punctuation: syntax_theme["punctuation"],
            Token.Error: f"bold {syntax_theme['error']}",
        }

    return PygmentsSyntaxTheme(CustomSyntaxStyle)


class ThemedAgentResultFormatter:
    """Formats agent results in a Rich table with nested subtables and theme support."""

    def __init__(
        self,
        theme: OutputTheme = OutputTheme.afterglow,
        max_length: int = -1,
        render_table: bool = True,
    ):
        """Initialize the formatter with a theme and optional max length."""
        self.theme = theme
        self.styles = None
        self.max_length = max_length
        self.render_table = render_table

    def format_result(
        self,
        result: dict[str, Any],
        agent_name: str,
        theme,
        styles,
    ) -> Panel:
        from devtools import pformat

        """Format an agent's result as a Rich Panel containing a table."""
        box_style = (
            getattr(box, styles["table_box"])
            if isinstance(styles["table_box"], str)
            else styles["table_box"]
        )

        # Gather table properties for the main table.
        table_kwargs = {
            "show_header": True,
            "header_style": styles["table_header_style"],
            "title": f"Agent Results: {agent_name}",
            "title_style": styles["table_title_style"],
            "border_style": styles["table_border_style"],
            "show_lines": styles["table_show_lines"],
            "box": box_style,
            "row_styles": styles["table_row_styles"],
            "safe_box": styles.get("table_safe_box"),
            "padding": styles.get("table_padding"),
            "collapse_padding": styles.get("table_collapse_padding"),
            "pad_edge": styles.get("table_pad_edge"),
            "expand": styles.get("table_expand"),
            "show_footer": styles.get("table_show_footer"),
            "show_edge": styles.get("table_show_edge"),
            "leading": styles.get("table_leading"),
            "style": styles.get("table_style"),
            "footer_style": styles.get("table_footer_style"),
            "caption": styles.get("table_caption"),
            "caption_style": styles.get("table_caption_style"),
            "title_justify": styles.get("table_title_justify"),
            "caption_justify": styles.get("table_caption_justify"),
            "highlight": styles.get("table_highlight"),
        }

        table = Table(**table_kwargs)
        table.add_column("Output", style=styles["column_output"])
        table.add_column("Value", style=styles["column_value"])
        for key, value in result.items():
            rich_renderable = create_rich_renderable(
                value,
                level=0,
                theme=theme,
                styles=styles,
                max_length=self.max_length,
            )
            table.add_row(key, rich_renderable)

        s = pformat(result, highlight=False)

        if self.render_table:
            return Panel(
                table,
                title="🐤🐧🐓🦆",
                title_align=styles["panel_title_align"],
                border_style=styles["panel_border_style"],
                padding=styles["panel_padding"],
                style=styles["panel_style"],
            )
        else:
            syntax = Syntax(
                s,  # The formatted string
                "python",  # Highlight as Python (change this for other formats)
                theme=self.syntax_style,  # Choose a Rich theme (matches your color setup)
                line_numbers=False,
            )
            return Panel(
                syntax,
                title=agent_name,
                title_align=styles["panel_title_align"],
                border_style=styles["panel_border_style"],
                padding=styles["panel_padding"],
                style=styles["panel_style"],
            )

    def display_result(self, result: dict[str, Any], agent_name: str) -> None:
        """Print an agent's result using Rich formatting."""
        theme = self.theme
        themes_dir = (
            pathlib.Path(__file__).parent.parent.parent.parent / "themes"
        )
        all_themes = list(themes_dir.glob("*.toml"))
        theme = (
            theme.value + ".toml"
            if not theme.value.endswith(".toml")
            else theme.value
        )
        theme = (
            pathlib.Path(__file__).parent.parent.parent.parent
            / "themes"
            / theme
        )

        if pathlib.Path(theme) not in all_themes:
            raise ValueError(
                f"Invalid theme: {theme}\nAvailable themes: {all_themes}"
            )

        theme_dict = load_theme_from_file(theme)

        styles = get_default_styles(theme_dict)
        self.styles = styles
        self.syntax_style = create_pygments_syntax_theme(
            load_syntax_theme_from_file(theme)
        )

        console = Console()
        panel = self.format_result(
            result=result,
            agent_name=agent_name,
            theme=theme_dict,
            styles=styles,
        )
        console.print(panel)

```

---

### 81. src\flock\core\logging\formatters\themes.py

- **File ID**: file_80
- **Type**: Code File
- **Line Count**: 340
- **Description**: File at src\flock\core\logging\formatters\themes.py
- **Dependencies**: None
- **Used By**: None

**Content**:
```
from enum import Enum


class OutputTheme(str, Enum):
    tomorrow_night_eighties = "tomorrow-night-eighties"
    builtin_light = "builtin-light"
    iterm2_dark_background = "iterm2-dark-background"
    zenbones = "zenbones"
    iterm2_tango_dark = "iterm2-tango-dark"
    gruber_darker = "gruber-darker"
    scarlet_protocol = "scarlet-protocol"
    purplepeter = "purplepeter"
    seashells = "seashells"
    monokai_soda = "monokai-soda"
    wildcherry = "wildcherry"
    builtin_solarized_light = "builtin-solarized-light"
    firewatch = "firewatch"
    builtin_tango_dark = "builtin-tango-dark"
    spacedust = "spacedust"
    paraiso_dark = "paraiso-dark"
    nightlion_v2 = "nightlion-v2"
    misterioso = "misterioso"
    shades_of_purple = "shades-of-purple"
    red_planet = "red-planet"
    flat = "flat"
    terafox = "terafox"
    crayonponyfish = "crayonponyfish"
    elementary = "elementary"
    blulocolight = "blulocolight"
    blazer = "blazer"
    purple_rain = "purple-rain"
    aurora = "aurora"
    neutron = "neutron"
    alienblood = "alienblood"
    symfonic = "symfonic"
    pro = "pro"
    highway = "highway"
    grape = "grape"
    hax0r_blue = "hax0r-blue"
    zenwritten_light = "zenwritten-light"
    spacegray = "spacegray"
    everblush = "everblush"
    popping_and_locking = "popping-and-locking"
    zenburn = "zenburn"
    monalisa = "monalisa"
    deep = "deep"
    ir_black = "ir-black"
    wombat = "wombat"
    zenbones_light = "zenbones-light"
    darkermatrix = "darkermatrix"
    wez = "wez"
    matrix = "matrix"
    farmhouse_light = "farmhouse-light"
    sublette = "sublette"
    nocturnal_winter = "nocturnal-winter"
    ryuuko = "ryuuko"
    jackie_brown = "jackie-brown"
    framer = "framer"
    _3024_day = "3024-day"
    lovelace = "lovelace"
    teerb = "teerb"
    fairyfloss = "fairyfloss"
    tokyonight = "tokyonight"
    xcodelighthc = "xcodelighthc"
    iceberg_light = "iceberg-light"
    gruvboxlight = "gruvboxlight"
    tomorrow = "tomorrow"
    sleepyhollow = "sleepyhollow"
    monokai_vivid = "monokai-vivid"
    synthwave_everything = "synthwave-everything"
    tomorrow_night_burns = "tomorrow-night-burns"
    hurtado = "hurtado"
    dotgov = "dotgov"
    adventure = "adventure"
    tomorrow_night = "tomorrow-night"
    arthur = "arthur"
    fahrenheit = "fahrenheit"
    oxocarbon = "oxocarbon"
    violet_dark = "violet-dark"
    adventuretime = "adventuretime"
    vesper = "vesper"
    overnight_slumber = "overnight-slumber"
    japanesque = "japanesque"
    encom = "encom"
    brogrammer = "brogrammer"
    _3024_night = "3024-night"
    hivacruz = "hivacruz"
    darkmatrix = "darkmatrix"
    synthwavealpha = "synthwavealpha"
    aardvark_blue = "aardvark-blue"
    xcodewwdc = "xcodewwdc"
    chester = "chester"
    flatland = "flatland"
    n0tch2k = "n0tch2k"
    molokai = "molokai"
    violet_light = "violet-light"
    solarized_darcula = "solarized-darcula"
    espresso = "espresso"
    darkside = "darkside"
    flexoki_light = "flexoki-light"
    bright_lights = "bright-lights"
    clrs = "clrs"
    firefly_traditional = "firefly-traditional"
    forestblue = "forestblue"
    batman = "batman"
    snazzy = "snazzy"
    wryan = "wryan"
    kurokula = "kurokula"
    iterm2_pastel_dark_background = "iterm2-pastel-dark-background"
    afterglow = "afterglow"
    seoulbones_light = "seoulbones-light"
    ollie = "ollie"
    shaman = "shaman"
    liquidcarbontransparent = "liquidcarbontransparent"
    ayu_mirage = "ayu-mirage"
    kolorit = "kolorit"
    red_sands = "red-sands"
    funforrest = "funforrest"
    unikitty = "unikitty"
    espresso_libre = "espresso-libre"
    ultraviolent = "ultraviolent"
    ayu_light = "ayu-light"
    terminal_basic = "terminal-basic"
    paulmillr = "paulmillr"
    github = "github"
    hacktober = "hacktober"
    ayu_copy = "ayu copy"
    material = "material"
    vimbones = "vimbones"
    arcoiris = "arcoiris"
    wilmersdorf = "wilmersdorf"
    desert = "desert"
    rouge_2 = "rouge-2"
    doom_peacock = "doom-peacock"
    smyck = "smyck"
    cutiepro = "cutiepro"
    nvimlight = "nvimlight"
    hipster_green = "hipster-green"
    spiderman = "spiderman"
    nvimdark = "nvimdark"
    sugarplum = "sugarplum"
    catppuccin_latte = "catppuccin-latte"
    dayfox = "dayfox"
    seafoam_pastel = "seafoam-pastel"
    peppermint = "peppermint"
    tokyonight_storm = "tokyonight-storm"
    mariana = "mariana"
    novel = "novel"
    argonaut_copy = "argonaut copy"
    twilight = "twilight"
    xcodelight = "xcodelight"
    homebrew = "homebrew"
    ateliersulphurpool = "ateliersulphurpool"
    thayer_bright = "thayer-bright"
    konsolas = "konsolas"
    iterm2_solarized_light = "iterm2-solarized-light"
    midnight_in_mojave = "midnight-in-mojave"
    materialdarker = "materialdarker"
    royal = "royal"
    builtin_tango_light = "builtin-tango-light"
    idletoes = "idletoes"
    operator_mono_dark = "operator-mono-dark"
    cyberdyne = "cyberdyne"
    atom = "atom"
    hybrid = "hybrid"
    slate = "slate"
    duckbones = "duckbones"
    tinacious_design__dark_ = "tinacious-design-(dark)"
    kibble = "kibble"
    sakura = "sakura"
    lab_fox = "lab-fox"
    blue_matrix = "blue-matrix"
    materialdesigncolors = "materialdesigncolors"
    seoulbones_dark = "seoulbones-dark"
    seti = "seti"
    solarized_dark_higher_contrast = "solarized-dark-higher-contrast"
    chalkboard = "chalkboard"
    mathias = "mathias"
    neobones_dark = "neobones-dark"
    alabaster = "alabaster"
    djangorebornagain = "djangorebornagain"
    ayu = "ayu"
    iterm2_default = "iterm2-default"
    mirage = "mirage"
    firefoxdev = "firefoxdev"
    nightfox = "nightfox"
    grey_green = "grey-green"
    broadcast = "broadcast"
    solarized_dark___patched = "solarized-dark---patched"
    flexoki_dark = "flexoki-dark"
    challengerdeep = "challengerdeep"
    onehalflight = "onehalflight"
    earthsong = "earthsong"
    kanagawabones = "kanagawabones"
    gruvboxdarkhard = "gruvboxdarkhard"
    abernathy = "abernathy"
    oceanicmaterial = "oceanicmaterial"
    medallion = "medallion"
    pnevma = "pnevma"
    birdsofparadise = "birdsofparadise"
    toychest = "toychest"
    dimidium = "dimidium"
    cyberpunk = "cyberpunk"
    duotone_dark = "duotone-dark"
    whimsy = "whimsy"
    nord_light = "nord-light"
    belafonte_day = "belafonte-day"
    square = "square"
    retro = "retro"
    pandora = "pandora"
    galaxy = "galaxy"
    the_hulk = "the-hulk"
    rose_pine_moon = "rose-pine-moon"
    coffee_theme = "coffee-theme"
    tomorrow_night_bright = "tomorrow-night-bright"
    blulocodark = "blulocodark"
    sundried = "sundried"
    rippedcasts = "rippedcasts"
    glacier = "glacier"
    zenwritten_dark = "zenwritten-dark"
    xcodedarkhc = "xcodedarkhc"
    iterm2_solarized_dark = "iterm2-solarized-dark"
    softserver = "softserver"
    jubi = "jubi"
    fishtank = "fishtank"
    spacegray_eighties_dull = "spacegray-eighties-dull"
    raycast_light = "raycast-light"
    tinacious_design__light_ = "tinacious-design-(light)"
    gruvboxdark = "gruvboxdark"
    piatto_light = "piatto-light"
    grass = "grass"
    catppuccin_mocha = "catppuccin-mocha"
    hardcore = "hardcore"
    tokyonight_day = "tokyonight-day"
    underthesea = "underthesea"
    guezwhoz = "guezwhoz"
    borland = "borland"
    argonaut = "argonaut"
    farmhouse_dark = "farmhouse-dark"
    rapture = "rapture"
    zenbones_dark = "zenbones-dark"
    iceberg_dark = "iceberg-dark"
    pro_light = "pro-light"
    jellybeans = "jellybeans"
    later_this_evening = "later-this-evening"
    blueberrypie = "blueberrypie"
    vibrantink = "vibrantink"
    dimmedmonokai = "dimmedmonokai"
    catppuccin_macchiato = "catppuccin-macchiato"
    ocean = "ocean"
    banana_blueberry = "banana-blueberry"
    dark_ = "dark+"
    neopolitan = "neopolitan"
    relaxed = "relaxed"
    galizur = "galizur"
    liquidcarbon = "liquidcarbon"
    hax0r_gr33n = "hax0r-gr33n"
    ic_orange_ppl = "ic-orange-ppl"
    niji = "niji"
    liquidcarbontransparentinverse = "liquidcarbontransparentinverse"
    github_dark = "github-dark"
    zenburned = "zenburned"
    django = "django"
    rose_pine_dawn = "rose-pine-dawn"
    builtin_dark = "builtin-dark"
    iterm2_smoooooth = "iterm2-smoooooth"
    neon = "neon"
    raycast_dark = "raycast-dark"
    palenighthc = "palenighthc"
    laser = "laser"
    builtin_solarized_dark = "builtin-solarized-dark"
    cobalt2 = "cobalt2"
    breeze = "breeze"
    apple_classic = "apple-classic"
    c64 = "c64"
    calamity = "calamity"
    onehalfdark = "onehalfdark"
    neobones_light = "neobones-light"
    dracula = "dracula"
    spring = "spring"
    monokai_remastered = "monokai-remastered"
    lavandula = "lavandula"
    night_owlish_light = "night-owlish-light"
    builtin_pastel_dark = "builtin-pastel-dark"
    frontenddelight = "frontenddelight"
    tango_adapted = "tango-adapted"
    ubuntu = "ubuntu"
    oceanic_next = "oceanic-next"
    primary = "primary"
    materialdark = "materialdark"
    doomone = "doomone"
    rose_pine = "rose-pine"
    chalk = "chalk"
    andromeda = "andromeda"
    djangosmooth = "djangosmooth"
    red_alert = "red-alert"
    warmneon = "warmneon"
    man_page = "man-page"
    hopscotch = "hopscotch"
    urple = "urple"
    tomorrow_night_blue = "tomorrow-night-blue"
    atomonelight = "atomonelight"
    pencillight = "pencillight"
    ciapre = "ciapre"
    dracula_ = "dracula+"
    hopscotch_256 = "hopscotch.256"
    fideloper = "fideloper"
    treehouse = "treehouse"
    ic_green_ppl = "ic-green-ppl"
    tango_half_adapted = "tango-half-adapted"
    belafonte_night = "belafonte-night"
    iterm2_light_background = "iterm2-light-background"
    harper = "harper"
    mellifluous = "mellifluous"
    rebecca = "rebecca"
    cga = "cga"
    cobalt_neon = "cobalt-neon"
    synthwave = "synthwave"
    pencildark = "pencildark"
    cyberpunkscarletprotocol = "cyberpunkscarletprotocol"
    iterm2_tango_light = "iterm2-tango-light"
    subliminal = "subliminal"
    idea = "idea"
    xcodedark = "xcodedark"
    apple_system_colors = "apple-system-colors"
    hax0r_r3d = "hax0r-r3d"
    atom_test = "atom_test"
    floraverse = "floraverse"
    materialocean = "materialocean"
    nord = "nord"
    vaughn = "vaughn"
    obsidian = "obsidian"
    jetbrains_darcula = "jetbrains-darcula"
    elemental = "elemental"
    spacegray_eighties = "spacegray-eighties"
    nightlion_v1 = "nightlion-v1"
    bluedolphin = "bluedolphin"
    catppuccin_frappe = "catppuccin-frappe"
    dark_pastel = "dark-pastel"
    ultradark = "ultradark"

```

---

### 82. src\flock\core\logging\logging.py

- **File ID**: file_81
- **Type**: Code File
- **Line Count**: 464
- **Description**: A unified logging module for Flock that works both in local/worker contexts and inside Temporal workflows....
- **Dependencies**: None
- **Used By**: None

**Content**:
```
# File: src/flock/core/logging.py
"""A unified logging module for Flock that works both in local/worker contexts and inside Temporal workflows.

Key points:
  - We always have Temporal imported, so we cannot decide based on import.
  - Instead, we dynamically check if we're in a workflow context by trying
    to call `workflow.info()`.
  - In a workflow, we use Temporal's built-in logger and skip debug/info/warning
    logs during replay.
  - Outside workflows, we use Loguru with rich formatting.
"""

import sys

from opentelemetry import trace

# Always import Temporal workflow (since it's part of the project)
from temporalio import workflow

with workflow.unsafe.imports_passed_through():
    from loguru import logger as loguru_logger


def in_workflow_context() -> bool:
    """Returns True if this code is running inside a Temporal workflow context.

    It does this by attempting to call workflow.info() and returning True
    if successful. Otherwise, it returns False.
    """
    try:
        workflow.logger.debug("Checking if in workflow context...")
        # loguru_logger.debug("Checking if in workflow context...")
        # This call will succeed only if we're in a workflow context.
        return bool(hasattr(workflow.info(), "is_replaying"))
    except Exception:
        return False


def get_current_trace_id() -> str:
    """Fetch the current trace ID from OpenTelemetry, if available."""
    current_span = trace.get_current_span()
    span_context = current_span.get_span_context()
    # Format the trace_id as hex (if valid)
    if span_context.is_valid:
        return format(span_context.trace_id, "032x")
    return "no-trace"


COLOR_MAP = {
    # Core & Orchestration
    "flock": "magenta",  # Color only
    "agent": "blue",  # Color only
    "workflow": "cyan",  # Color only
    "activities": "cyan",
    "context": "green",
    # Components & Mechanisms
    "registry": "yellow",  # Color only
    "serialization": "yellow",
    "serialization.utils": "light-yellow",
    "evaluator": "light-blue",
    "module": "light-green",
    "router": "light-magenta",
    "mixin.dspy": "yellow",
    # Specific Modules (Examples)
    "memory": "yellow",
    "module.output": "green",
    "module.metrics": "blue",
    "module.zep": "red",
    "module.hierarchical": "light-green",
    # Tools & Execution
    "tools": "light-black",
    "interpreter": "light-yellow",
    # API Components
    "api": "white",  # Color only
    "api.main": "white",
    "api.endpoints": "light-black",
    "api.run_store": "light-black",
    "api.ui": "light-blue",  # Color only
    "api.ui.routes": "light-blue",
    "api.ui.utils": "cyan",
    # Default/Unknown
    "evaluators.declarative": "light-green",
    "unknown": "light-black",
}

LOGGERS = [
    "flock",  # Core Flock orchestration
    "agent",  # General agent operations
    "context",  # Context management
    "registry",  # Unified registry operations (new)
    "serialization",  # General serialization (new - can be base for others)
    "serialization.utils",  # Serialization helpers (new, more specific)
    "evaluator",  # Base evaluator category (new/optional)
    "evaluators.declarative",  # Declarative evaluator specifics
    "module",  # Base module category (new/optional)
    "router",  # Base router category (new/optional)
    "mixin.dspy",  # DSPy integration specifics (new)
    "memory",  # Memory module specifics
    "module.output",  # Output module specifics (example specific module)
    "module.metrics",  # Metrics module specifics (example specific module)
    "module.zep",  # Zep module specifics (example specific module)
    "module.hierarchical",  # Hierarchical memory specifics (example specific module)
    "interpreter",  # Code interpreter (if still used)
    "activities",  # Temporal activities
    "workflow",  # Temporal workflow logic
    "tools",  # Tool execution/registration
    "api",  # General API server (new)
    "api.main",  # API main setup (new)
    "api.endpoints",  # API endpoints (new)
    "api.run_store",  # API run state management (new)
    "api.ui",  # UI general (new)
    "api.ui.routes",  # UI routes (new)
    "api.ui.utils",  # UI utils (new)
]

BOLD_CATEGORIES = [
    "flock",
    "agent",
    "workflow",
    "registry",
    "api",
    "api.ui",
]


def color_for_category(category: str) -> str:
    """Return the Rich markup color code name for the given category."""
    # Handle potentially nested names like 'serialization.utils'
    # Try exact match first, then go up the hierarchy
    if category in COLOR_MAP:
        return COLOR_MAP[category]
    parts = category.split(".")
    for i in range(len(parts) - 1, 0, -1):
        parent_category = ".".join(parts[:i])
        if parent_category in COLOR_MAP:
            return COLOR_MAP[parent_category]
    # Fallback to default 'unknown' color
    return COLOR_MAP.get("unknown", "light-black")  # Final fallback


def custom_format(record):
    """A formatter that applies truncation and sequential styling tags."""
    t = record["time"].strftime("%Y-%m-%d %H:%M:%S")
    level_name = record["level"].name
    category = record["extra"].get("category", "unknown")
    trace_id = record["extra"].get("trace_id", "no-trace")
    color_tag = color_for_category(category)  # Get the color tag name (e.g., "yellow")

    message = record["message"]
    message = message.replace("{", "{{").replace("}", "}}")

    # MAX_LENGTH = 500 # Example value
    if len(message) > MAX_LENGTH:
        truncated_chars = len(message) - MAX_LENGTH
        message = (
            message[:MAX_LENGTH] + f"<yellow>...+({truncated_chars} chars)</yellow>"
        )

    # Determine if category needs bolding (can refine this logic)
    needs_bold = category in BOLD_CATEGORIES

    # Apply tags sequentially
    category_styled = f"[{category}]"  # Start with the plain category name
    category_styled = f"<{color_tag}>{category_styled}</{color_tag}>"  # Wrap with color
    if needs_bold:
        category_styled = f"<bold>{category_styled}</bold>"  # Wrap with bold if needed

    # Final format string using sequential tags for category
    return (
        f"<green>{t}</green> | <level>{level_name: <8}</level> | "
        f"<cyan>[trace_id: {trace_id}]</cyan> | "
        f"{category_styled} | {message}\n"  # Apply the sequentially styled category
    )


class ImmediateFlushSink:
    """A custom Loguru sink that writes to a stream and flushes immediately after each message.

    This ensures that logs appear in real time.
    """

    def __init__(self, stream=None):
        """Initialize the ImmediateFlushSink.

        Args:
            stream (Stream, optional): The stream to write to. Defaults to sys.stderr.
        """
        self._stream = stream if stream else sys.stderr

    def write(self, message):
        """Write a message to the stream and flush immediately.

        Args:
            message (str): The message to write.
        """
        self._stream.write(message)
        self._stream.flush()

    def flush(self):
        """Flush the stream."""
        self._stream.flush()


class PrintAndFlushSink:
    """A Loguru sink.

    forcibly prints each log record and flushes immediately,
    mimicking print(..., flush=True).
    """

    def write(self, message: str):
        """Write a message to the stream and flush immediately.

        Args:
            message (str): The message to write.
        """
        # message already ends with a newline
        print(message, end="", flush=True)

    def flush(self):
        """Flush the stream.

        Already flushed on every write call.
        """
        pass


# Configure Loguru for non-workflow (local/worker) contexts.
# Note that in workflow code, we will use Temporal's workflow.logger instead.
loguru_logger.remove()
loguru_logger.add(
    PrintAndFlushSink(),
    level="DEBUG",
    colorize=True,
    format=custom_format,
)
# Optionally add a file handler, e.g.:
# loguru_logger.add("logs/flock.log", rotation="100 MB", retention="30 days", level="DEBUG")


# Define a dummy logger that does nothing
class DummyLogger:
    """A dummy logger that does nothing when called."""

    def debug(self, *args, **kwargs):  # noqa: D102
        pass

    def info(self, *args, **kwargs):  # noqa: D102
        pass

    def warning(self, *args, **kwargs):  # noqa: D102
        pass

    def error(self, *args, **kwargs):  # noqa: D102
        pass

    def exception(self, *args, **kwargs):  # noqa: D102
        pass

    def success(self, *args, **kwargs):  # noqa: D102
        pass


dummy_logger = DummyLogger()


# Maximum length for log messages before truncation
MAX_LENGTH = 500


class FlockLogger:
    """A unified logger that selects the appropriate logging mechanism based on context.

    - If running in a workflow context, it uses Temporal's built-in logger.
      Additionally, if workflow.info().is_replaying is True, it suppresses debug/info/warning logs.
    - Otherwise, it uses Loguru.
    """

    def __init__(self, name: str, enable_logging: bool = False):
        """Initialize the FlockLogger.

        Args:
            name (str): The name of the logger.
            enable_logging (bool, optional): Whether to enable logging. Defaults to False.
        """
        self.name = name
        self.enable_logging = enable_logging

    def _get_logger(self):
        if not self.enable_logging:
            return dummy_logger
        if in_workflow_context():
            # Use Temporal's workflow.logger inside a workflow context.
            return workflow.logger
        # Bind our logger with category and trace_id
        return loguru_logger.bind(
            name=self.name,
            category=self.name,  # Customize this per module (e.g., "flock", "agent", "context")
            trace_id=get_current_trace_id(),
        )

    def _truncate_message(self, message: str, max_length: int) -> str:
        """Truncate a message if it exceeds max_length and add truncation indicator."""
        if len(message) > max_length:
            truncated_chars = len(message) - max_length
            return (
                message[:max_length] + f"...<yellow>+({truncated_chars} chars)</yellow>"
            )
        return message

    def debug(
        self,
        message: str,
        *args,
        flush: bool = False,
        max_length: int = MAX_LENGTH,
        **kwargs,
    ) -> None:
        """Debug a message.

        Args:
            message (str): The message to debug.
            flush (bool, optional): Whether to flush the message. Defaults to False.
            max_length (int, optional): The maximum length of the message. Defaults to MAX_LENGTH.
        """
        message = self._truncate_message(message, max_length)
        self._get_logger().debug(message, *args, **kwargs)

    def info(
        self,
        message: str,
        *args,
        flush: bool = False,
        max_length: int = MAX_LENGTH,
        **kwargs,
    ) -> None:
        """Info a message.

        Args:
            message (str): The message to info.
            flush (bool, optional): Whether to flush the message. Defaults to False.
            max_length (int, optional): The maximum length of the message. Defaults to MAX_LENGTH.
        """
        message = self._truncate_message(message, max_length)
        self._get_logger().info(message, *args, **kwargs)

    def warning(
        self,
        message: str,
        *args,
        flush: bool = False,
        max_length: int = MAX_LENGTH,
        **kwargs,
    ) -> None:
        """Warning a message.

        Args:
            message (str): The message to warning.
            flush (bool, optional): Whether to flush the message. Defaults to False.
            max_length (int, optional): The maximum length of the message. Defaults to MAX_LENGTH.
        """
        message = self._truncate_message(message, max_length)
        self._get_logger().warning(message, *args, **kwargs)

    def error(
        self,
        message: str,
        *args,
        flush: bool = False,
        max_length: int = MAX_LENGTH,
        **kwargs,
    ) -> None:
        """Error a message.

        Args:
            message (str): The message to error.
            flush (bool, optional): Whether to flush the message. Defaults to False.
            max_length (int, optional): The maximum length of the message. Defaults to MAX_LENGTH.
        """
        message = self._truncate_message(message, max_length)
        self._get_logger().error(message, *args, **kwargs)

    def exception(
        self,
        message: str,
        *args,
        flush: bool = False,
        max_length: int = MAX_LENGTH,
        **kwargs,
    ) -> None:
        """Exception a message.

        Args:
            message (str): The message to exception.
            flush (bool, optional): Whether to flush the message. Defaults to False.
            max_length (int, optional): The maximum length of the message. Defaults to MAX_LENGTH.
        """
        message = self._truncate_message(message, max_length)
        self._get_logger().exception(message, *args, **kwargs)

    def success(
        self,
        message: str,
        *args,
        flush: bool = False,
        max_length: int = MAX_LENGTH,
        **kwargs,
    ) -> None:
        """Success a message.

        Args:
            message (str): The message to success.
            flush (bool, optional): Whether to flush the message. Defaults to False.
            max_length (int, optional): The maximum length of the message. Defaults to MAX_LENGTH.
        """
        message = self._truncate_message(message, max_length)
        self._get_logger().success(message, *args, **kwargs)


_LOGGER_CACHE: dict[str, FlockLogger] = {}


def get_logger(name: str = "flock", enable_logging: bool = True) -> FlockLogger:
    """Return a cached FlockLogger instance for the given name.

    If the logger doesn't exist, create it.
    If it does exist, update 'enable_logging' if a new value is passed.
    """
    if name not in _LOGGER_CACHE:
        _LOGGER_CACHE[name] = FlockLogger(name, enable_logging)
    else:
        _LOGGER_CACHE[name].enable_logging = enable_logging
    return _LOGGER_CACHE[name]


def get_module_loggers() -> list[FlockLogger]:
    """Return a cached FlockLogger instance for the given module name."""
    result = []
    for kvp in _LOGGER_CACHE:
        if kvp.startswith("module."):
            result.append(_LOGGER_CACHE[kvp])

    return result


def truncate_for_logging(obj, max_item_length=100, max_items=10):
    """Truncate large data structures for logging purposes."""
    if isinstance(obj, str) and len(obj) > max_item_length:
        return obj[:max_item_length] + f"... ({len(obj) - max_item_length} more chars)"
    elif isinstance(obj, dict):
        if len(obj) > max_items:
            return {
                k: truncate_for_logging(v)
                for i, (k, v) in enumerate(obj.items())
                if i < max_items
            }
        return {k: truncate_for_logging(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        if len(obj) > max_items:
            return [truncate_for_logging(item) for item in obj[:max_items]] + [
                f"... ({len(obj) - max_items} more items)"
            ]
        return [truncate_for_logging(item) for item in obj]
    return obj

```

---

### 83. src\flock\core\logging\span_middleware\baggage_span_processor.py

- **File ID**: file_82
- **Type**: Code File
- **Line Count**: 31
- **Description**: File at src\flock\core\logging\span_middleware\baggage_span_processor.py
- **Dependencies**: None
- **Used By**: None

**Content**:
```
from opentelemetry.baggage import get_baggage
from opentelemetry.sdk.trace import SpanProcessor


class BaggageAttributeSpanProcessor(SpanProcessor):
    """A custom span processor that, on span start, inspects the baggage items from the parent context
    and attaches specified baggage keys as attributes on the span.
    """

    def __init__(self, baggage_keys=None):
        # baggage_keys: list of baggage keys to attach to spans (e.g. ["session_id", "run_id"])
        if baggage_keys is None:
            baggage_keys = []
        self.baggage_keys = baggage_keys

    def on_start(self, span, parent_context):
        # For each desired key, look up its value in the parent context baggage and set it as an attribute.
        for key in self.baggage_keys:
            value = get_baggage(key, context=parent_context)
            if value is not None:
                span.set_attribute(key, value)

    def on_end(self, span):
        # No action required on span end for this processor.
        pass

    def shutdown(self):
        pass

    def force_flush(self, timeout_millis: int = 30000):
        pass

```

---

### 84. src\flock\core\logging\telemetry.py

- **File ID**: file_83
- **Type**: Code File
- **Line Count**: 138
- **Description**: This module sets up OpenTelemetry tracing for a service.
- **Dependencies**: None
- **Used By**: None

**Content**:
```
"""This module sets up OpenTelemetry tracing for a service."""

import sys

from opentelemetry import trace
from opentelemetry.sdk.resources import Resource
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import SimpleSpanProcessor
from temporalio import workflow

from flock.core.logging.span_middleware.baggage_span_processor import (
    BaggageAttributeSpanProcessor,
)

with workflow.unsafe.imports_passed_through():
    from flock.core.logging.telemetry_exporter.file_exporter import (
        FileSpanExporter,
    )
    from flock.core.logging.telemetry_exporter.sqlite_exporter import (
        SqliteTelemetryExporter,
    )


class TelemetryConfig:
    """This configuration class sets up OpenTelemetry tracing.

      - Export spans to a Jaeger collector using gRPC.
      - Write spans to a file.
      - Save spans in a SQLite database.

    Only exporters with a non-None configuration will be activated.
    """

    def __init__(
        self,
        service_name: str,
        jaeger_endpoint: str | None = None,
        jaeger_transport: str = "grpc",
        local_logging_dir: str | None = None,
        file_export_name: str | None = None,
        sqlite_db_name: str | None = None,
        enable_jaeger: bool = True,
        enable_file: bool = True,
        enable_sql: bool = True,
        batch_processor_options: dict | None = None,
    ):
        """:param service_name: Name of your service.

        :param jaeger_endpoint: The Jaeger collector gRPC endpoint (e.g., "localhost:14250").
        :param file_export_path: If provided, spans will be written to this file.
        :param sqlite_db_path: If provided, spans will be stored in this SQLite DB.
        :param batch_processor_options: Dict of options for BatchSpanProcessor (e.g., {"max_export_batch_size": 10}).
        """
        self.service_name = service_name
        self.jaeger_endpoint = jaeger_endpoint
        self.jaeger_transport = jaeger_transport
        self.file_export_name = file_export_name
        self.sqlite_db_name = sqlite_db_name
        self.local_logging_dir = local_logging_dir
        self.batch_processor_options = batch_processor_options or {}
        self.enable_jaeger = enable_jaeger
        self.enable_file = enable_file
        self.enable_sql = enable_sql
        self.global_tracer = None

    def setup_tracing(self):
        """Set up OpenTelemetry tracing with the specified exporters."""
        # Create a Resource with the service name.
        resource = Resource(attributes={"service.name": self.service_name})
        provider = TracerProvider(resource=resource)
        trace.set_tracer_provider(provider)

        # List to collect our span processors.
        span_processors = []

        # If a Jaeger endpoint is specified, add the Jaeger exporter.
        if self.jaeger_endpoint and self.enable_jaeger:
            if self.jaeger_transport == "grpc":
                from opentelemetry.exporter.jaeger.proto.grpc import (
                    JaegerExporter,
                )

                jaeger_exporter = JaegerExporter(
                    endpoint=self.jaeger_endpoint,
                    insecure=True,
                )
            elif self.jaeger_transport == "http":
                from opentelemetry.exporter.jaeger.thrift import JaegerExporter

                jaeger_exporter = JaegerExporter(
                    collector_endpoint=self.jaeger_endpoint,
                )
            else:
                raise ValueError(
                    "Invalid JAEGER_TRANSPORT specified. Use 'grpc' or 'http'."
                )

            span_processors.append(SimpleSpanProcessor(jaeger_exporter))

        # If a file path is provided, add the custom file exporter.
        if self.file_export_name and self.enable_file:
            file_exporter = FileSpanExporter(
                self.local_logging_dir, self.file_export_name
            )
            span_processors.append(SimpleSpanProcessor(file_exporter))

        # If a SQLite database path is provided, ensure the DB exists and add the SQLite exporter.
        if self.sqlite_db_name and self.enable_sql:
            sqlite_exporter = SqliteTelemetryExporter(
                self.local_logging_dir, self.sqlite_db_name
            )
            span_processors.append(SimpleSpanProcessor(sqlite_exporter))

        # Register all span processors with the provider.
        for processor in span_processors:
            provider.add_span_processor(processor)

        provider.add_span_processor(
            BaggageAttributeSpanProcessor(baggage_keys=["session_id", "run_id"])
        )
        # self.global_tracer = trace.get_tracer("flock")
        sys.excepthook = self.log_exception_to_otel

    def log_exception_to_otel(self, exc_type, exc_value, exc_traceback):
        """Log unhandled exceptions to OpenTelemetry."""
        if issubclass(exc_type, KeyboardInterrupt):
            # Allow normal handling of KeyboardInterrupt
            sys.__excepthook__(exc_type, exc_value, exc_traceback)
            return

        # Use OpenTelemetry to record the exception
        with self.global_tracer.start_as_current_span(
            "UnhandledException"
        ) as span:
            span.record_exception(exc_value)
            span.set_status(
                trace.Status(trace.StatusCode.ERROR, str(exc_value))
            )

```

---

### 85. src\flock\core\logging\telemetry_exporter\base_exporter.py

- **File ID**: file_84
- **Type**: Code File
- **Line Count**: 38
- **Description**: Base class for custom OpenTelemetry exporters.
- **Dependencies**: None
- **Used By**: None

**Content**:
```
"""Base class for custom OpenTelemetry exporters."""

from abc import ABC, abstractmethod

from opentelemetry.sdk.trace.export import SpanExporter, SpanExportResult


class TelemetryExporter(SpanExporter, ABC):
    """Base class for custom OpenTelemetry exporters."""

    def __init__(self):
        """Base class for custom OpenTelemetry exporters."""
        super().__init__()

    def _export(self, spans):
        """Forward spans to the Jaeger exporter."""
        try:
            result = self.export(spans)
            if result is None:
                return SpanExportResult.SUCCESS
            return result
        except Exception:
            return SpanExportResult.FAILURE
        finally:
            self.shutdown()

    @abstractmethod
    def export(self, spans) -> SpanExportResult | None:
        """Export spans to the configured backend.

        To be implemented by subclasses.
        """
        raise NotImplementedError("Subclasses must implement the export method")

    @abstractmethod
    def shutdown(self):
        """Cleanup resources, if any. Optional for subclasses."""
        pass

```

---

### 86. src\flock\core\logging\telemetry_exporter\file_exporter.py

- **File ID**: file_85
- **Type**: Code File
- **Line Count**: 85
- **Description**: A simple exporter that writes span data as JSON lines into a file.
- **Dependencies**: None
- **Used By**: None

**Content**:
```
"""A simple exporter that writes span data as JSON lines into a file."""

import json

from opentelemetry.sdk.trace.export import SpanExportResult
from opentelemetry.trace import Status, StatusCode
from temporalio import workflow

from flock.core.logging.telemetry_exporter.base_exporter import (
    TelemetryExporter,
)

with workflow.unsafe.imports_passed_through():
    from pathlib import Path


class FileSpanExporter(TelemetryExporter):
    """A simple exporter that writes span data as JSON lines into a file."""

    def __init__(self, dir: str, file_path: str = "flock_events.jsonl"):
        """Initialize the exporter with a file path."""
        super().__init__()
        self.telemetry_path = Path(dir)
        self.telemetry_path.mkdir(parents=True, exist_ok=True)
        self.file_path = self.telemetry_path.joinpath(file_path).__str__()

    def _span_to_json(self, span):
        """Convert a ReadableSpan to a JSON-serializable dict."""
        context = span.get_span_context()
        status = span.status or Status(StatusCode.UNSET)

        return {
            "name": span.name,
            "context": {
                "trace_id": format(context.trace_id, "032x"),
                "span_id": format(context.span_id, "016x"),
                "trace_flags": context.trace_flags,
                "trace_state": str(context.trace_state),
            },
            "kind": span.kind.name if span.kind else None,
            "start_time": span.start_time,
            "end_time": span.end_time,
            "status": {
                "status_code": status.status_code.name,
                "description": status.description,
            },
            "attributes": dict(span.attributes or {}),
            "events": [
                {
                    "name": event.name,
                    "timestamp": event.timestamp,
                    "attributes": dict(event.attributes or {}),
                }
                for event in span.events
            ],
            "links": [
                {
                    "context": {
                        "trace_id": format(link.context.trace_id, "032x"),
                        "span_id": format(link.context.span_id, "016x"),
                    },
                    "attributes": dict(link.attributes or {}),
                }
                for link in span.links
            ],
            "resource": {
                attr_key: attr_value
                for attr_key, attr_value in span.resource.attributes.items()
            },
        }

    def export(self, spans):
        """Write spans to a log file."""
        try:
            with open(self.file_path, "a") as f:
                for span in spans:
                    json_span = self._span_to_json(span)
                    f.write(f"{json.dumps(json_span)}\n")
            return SpanExportResult.SUCCESS
        except Exception:
            return SpanExportResult.FAILURE

    def shutdown(self) -> None:
        # Nothing special needed on shutdown.
        pass

```

---

### 87. src\flock\core\logging\telemetry_exporter\sqlite_exporter.py

- **File ID**: file_86
- **Type**: Code File
- **Line Count**: 103
- **Description**: Exporter for storing OpenTelemetry spans in SQLite.
- **Dependencies**: None
- **Used By**: None

**Content**:
```
"""Exporter for storing OpenTelemetry spans in SQLite."""

import json
import sqlite3
from pathlib import Path
from typing import Any

from opentelemetry.sdk.trace.export import SpanExportResult

from flock.core.logging.telemetry_exporter.base_exporter import (
    TelemetryExporter,
)


class SqliteTelemetryExporter(TelemetryExporter):
    """Exporter for storing OpenTelemetry spans in SQLite."""

    def __init__(self, dir: str, db_path: str = "flock_events.db"):
        """Initialize the SQLite exporter.

        Args:
            db_path: Path to the SQLite database file
        """
        super().__init__()
        self.telemetry_path = Path(dir)
        self.telemetry_path.mkdir(parents=True, exist_ok=True)
        # Create an absolute path to the database file:
        self.db_path = self.telemetry_path.joinpath(db_path).resolve().__str__()
        # Use the absolute path when connecting:
        self.conn = sqlite3.connect(self.db_path, check_same_thread=False)
        self._initialize_database()

    def _initialize_database(self):
        """Set up the SQLite database schema."""
        cursor = self.conn.cursor()
        cursor.execute(
            """
            CREATE TABLE IF NOT EXISTS spans (
                id TEXT PRIMARY KEY,
                name TEXT,
                trace_id TEXT,
                span_id TEXT,
                start_time INTEGER,
                end_time INTEGER,
                attributes TEXT,
                status TEXT
            )
            """
        )
        self.conn.commit()

    def _convert_attributes(self, attributes: dict[str, Any]) -> str:
        """Convert span attributes to a JSON string.

        Args:
            attributes: Dictionary of span attributes

        Returns:
            JSON string representation of attributes
        """
        # Convert attributes to a serializable format
        serializable_attrs = {}
        for key, value in attributes.items():
            # Convert complex types to strings if needed
            if isinstance(value, dict | list | tuple):
                serializable_attrs[key] = json.dumps(value)
            else:
                serializable_attrs[key] = str(value)
        return json.dumps(serializable_attrs)

    def export(self, spans) -> SpanExportResult:
        """Export spans to SQLite."""
        try:
            cursor = self.conn.cursor()
            for span in spans:
                span_id = format(span.context.span_id, "016x")
                trace_id = format(span.context.trace_id, "032x")
                cursor.execute(
                    """
                    INSERT OR REPLACE INTO spans 
                    (id, name, trace_id, span_id, start_time, end_time, attributes, status)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                    """,
                    (
                        span_id,
                        span.name,
                        trace_id,
                        span_id,
                        span.start_time,
                        span.end_time,
                        self._convert_attributes(span.attributes),
                        str(span.status),
                    ),
                )
            self.conn.commit()
            return SpanExportResult.SUCCESS
        except Exception as e:
            print("Error exporting spans to SQLite:", e)
            return SpanExportResult.FAILURE

    def shutdown(self) -> None:
        """Cleanup resources."""
        pass

```

---

### 88. src\flock\core\logging\trace_and_logged.py

- **File ID**: file_87
- **Type**: Code File
- **Line Count**: 59
- **Description**: A decorator that wraps a function in an OpenTelemetry span and logs its inputs, outputs, and exceptions.
- **Dependencies**: None
- **Used By**: None

**Content**:
```
"""A decorator that wraps a function in an OpenTelemetry span and logs its inputs, outputs, and exceptions."""

import functools
import inspect

from opentelemetry import trace

from flock.core.logging.logging import get_logger

logger = get_logger("tools")
tracer = trace.get_tracer(__name__)


def traced_and_logged(func):
    """A decorator that wraps a function in an OpenTelemetry span.

    and logs its inputs,
    outputs, and exceptions. Supports both synchronous and asynchronous functions.
    """
    if inspect.iscoroutinefunction(func):

        @functools.wraps(func)
        async def async_wrapper(*args, **kwargs):
            with tracer.start_as_current_span(func.__name__) as span:
                span.set_attribute("args", str(args))
                span.set_attribute("kwargs", str(kwargs))
                try:
                    result = await func(*args, **kwargs)
                    span.set_attribute("result", str(result))
                    logger.debug(
                        f"{func.__name__} executed successfully", result=result
                    )
                    return result
                except Exception as e:
                    logger.error(f"Error in {func.__name__}", error=str(e))
                    span.record_exception(e)
                    raise

        return async_wrapper
    else:

        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            with tracer.start_as_current_span(func.__name__) as span:
                span.set_attribute("args", str(args))
                span.set_attribute("kwargs", str(kwargs))
                try:
                    result = func(*args, **kwargs)
                    span.set_attribute("result", str(result))
                    logger.debug(
                        f"{func.__name__} executed successfully", result=result
                    )
                    return result
                except Exception as e:
                    logger.error(f"Error in {func.__name__}", error=str(e))
                    span.record_exception(e)
                    raise

        return wrapper

```

---

### 89. src\flock\core\mixin\dspy_integration.py

- **File ID**: file_88
- **Type**: Code File
- **Line Count**: 423
- **Description**: Mixin class for integrating with the dspy library.
- **Dependencies**: None
- **Used By**: None

**Content**:
```
# src/flock/core/mixin/dspy_integration.py
"""Mixin class for integrating with the dspy library."""

import re  # Import re for parsing
import typing
from typing import Any, Literal

from flock.core.logging.logging import get_logger

# Import split_top_level (assuming it's moved or copied appropriately)
# Option 1: If moved to a shared util
# from flock.core.util.parsing_utils import split_top_level
# Option 2: If kept within this file (as in previous example)
# Define split_top_level here or ensure it's imported

logger = get_logger("mixin.dspy")

# Type definition for agent type override
AgentType = Literal["ReAct", "Completion", "ChainOfThought"] | None


# Helper function needed by _resolve_type_string (copied from input_resolver.py/previous response)
def split_top_level(s: str) -> list[str]:
    """Split a string on commas that are not enclosed within brackets, parentheses, or quotes."""
    parts = []
    current = []
    level = 0
    in_quote = False
    quote_char = ""
    i = 0
    while i < len(s):
        char = s[i]
        # Handle escapes within quotes
        if in_quote and char == "\\" and i + 1 < len(s):
            current.append(char)
            current.append(s[i + 1])
            i += 1  # Skip next char
        elif in_quote:
            current.append(char)
            if char == quote_char:
                in_quote = False
        elif char in ('"', "'"):
            in_quote = True
            quote_char = char
            current.append(char)
        elif char in "([{":
            level += 1
            current.append(char)
        elif char in ")]}":
            level -= 1
            current.append(char)
        elif char == "," and level == 0:
            parts.append("".join(current).strip())
            current = []
        else:
            current.append(char)
        i += 1
    if current:
        parts.append("".join(current).strip())
    # Filter out empty strings that might result from trailing commas etc.
    return [part for part in parts if part]


# Helper function to resolve type strings (can be static or module-level)
def _resolve_type_string(type_str: str) -> type:
    """Resolves a type string into a Python type object.
    Handles built-ins, registered types, and common typing generics like
    List, Dict, Optional, Union, Literal.
    """
    # Import registry here to avoid circular imports
    from flock.core.flock_registry import get_registry

    FlockRegistry = get_registry()

    type_str = type_str.strip()
    logger.debug(f"Attempting to resolve type string: '{type_str}'")

    # 1. Check built-ins and registered types directly
    try:
        # This covers str, int, bool, Any, and types registered by name
        resolved_type = FlockRegistry.get_type(type_str)
        logger.debug(f"Resolved '{type_str}' via registry to: {resolved_type}")
        return resolved_type
    except KeyError:
        logger.debug(
            f"'{type_str}' not found directly in registry, attempting generic parsing."
        )
        pass  # Not found, continue parsing generics

    # 2. Handle typing generics (List, Dict, Optional, Union, Literal)
    # Use regex to match pattern like Generic[InnerType1, InnerType2, ...]
    generic_match = re.fullmatch(r"(\w+)\s*\[(.*)\]", type_str)
    if generic_match:
        base_name = generic_match.group(1).strip()
        args_str = generic_match.group(2).strip()
        logger.debug(f"Detected generic pattern: Base='{base_name}', Args='{args_str}'")

        try:
            # Get the base generic type (e.g., list, dict, Optional) from registry/builtins
            BaseType = FlockRegistry.get_type(
                base_name
            )  # Expects List, Dict etc. to be registered
            logger.debug(f"Resolved base generic type '{base_name}' to: {BaseType}")

            # Special handling for Literal
            if BaseType is typing.Literal:
                # Split literal values, remove quotes, strip whitespace
                literal_args_raw = split_top_level(args_str)
                literal_args = tuple(s.strip().strip("'\"") for s in literal_args_raw)
                logger.debug(
                    f"Parsing Literal arguments: {literal_args_raw} -> {literal_args}"
                )
                resolved_type = typing.Literal[literal_args]  # type: ignore
                logger.debug(f"Constructed Literal type: {resolved_type}")
                return resolved_type

            # Recursively resolve arguments for other generics
            logger.debug(f"Splitting generic arguments: '{args_str}'")
            arg_strs = split_top_level(args_str)
            logger.debug(f"Split arguments: {arg_strs}")
            if not arg_strs:
                raise ValueError("Generic type has no arguments.")

            resolved_arg_types = tuple(_resolve_type_string(arg) for arg in arg_strs)
            logger.debug(f"Resolved generic arguments: {resolved_arg_types}")

            # Construct the generic type hint
            if BaseType is typing.Optional:
                if len(resolved_arg_types) != 1:
                    raise ValueError("Optional requires exactly one argument.")
                resolved_type = typing.Union[resolved_arg_types[0], type(None)]  # type: ignore
                logger.debug(f"Constructed Optional type as Union: {resolved_type}")
                return resolved_type
            elif BaseType is typing.Union:
                if not resolved_arg_types:
                    raise ValueError("Union requires at least one argument.")
                resolved_type = typing.Union[resolved_arg_types]  # type: ignore
                logger.debug(f"Constructed Union type: {resolved_type}")
                return resolved_type
            elif hasattr(
                BaseType, "__getitem__"
            ):  # Check if subscriptable (like list, dict, List, Dict)
                resolved_type = BaseType[resolved_arg_types]  # type: ignore
                logger.debug(f"Constructed subscripted generic type: {resolved_type}")
                return resolved_type
            else:
                # Base type found but cannot be subscripted
                logger.warning(
                    f"Base type '{base_name}' found but is not a standard subscriptable generic. Returning base type."
                )
                return BaseType

        except (KeyError, ValueError, IndexError, TypeError) as e:
            logger.warning(
                f"Failed to parse generic type '{type_str}': {e}. Falling back."
            )
            # Fall through to raise KeyError below if base type itself wasn't found or parsing failed

    # 3. If not resolved by now, raise error
    logger.error(f"Type string '{type_str}' could not be resolved.")
    raise KeyError(f"Type '{type_str}' could not be resolved.")


class DSPyIntegrationMixin:
    """Mixin class for integrating with the dspy library."""

    def create_dspy_signature_class(
        self, agent_name, description_spec, fields_spec
    ) -> Any:
        """Creates a dynamic DSPy Signature class from string specifications,
        resolving types using the FlockRegistry.
        """
        try:
            import dspy
        except ImportError:
            logger.error(
                "DSPy library is not installed. Cannot create DSPy signature. "
                "Install with: pip install dspy-ai"
            )
            raise ImportError("DSPy is required for this functionality.")

        base_class = dspy.Signature
        class_dict = {"__doc__": description_spec, "__annotations__": {}}

        if "->" in fields_spec:
            inputs_spec, outputs_spec = fields_spec.split("->", 1)
        else:
            inputs_spec, outputs_spec = (
                fields_spec,
                "",
            )  # Assume only inputs if no '->'

        def parse_field(field_str):
            """Parses 'name: type_str | description' using _resolve_type_string."""
            field_str = field_str.strip()
            if not field_str:
                return None

            parts = field_str.split("|", 1)
            main_part = parts[0].strip()
            desc = parts[1].strip() if len(parts) > 1 else None

            if ":" in main_part:
                name, type_str = [s.strip() for s in main_part.split(":", 1)]
            else:
                name = main_part
                type_str = "str"  # Default type

            try:
                field_type = _resolve_type_string(type_str)
            except Exception as e:  # Catch resolution errors
                logger.error(
                    f"Failed to resolve type '{type_str}' for field '{name}': {e}. Defaulting to str."
                )
                field_type = str

            return name, field_type, desc

        def process_fields(fields_string, field_kind):
            """Process fields and add to class_dict."""
            if not fields_string or not fields_string.strip():
                return

            for field in split_top_level(fields_string):
                if field.strip():
                    parsed = parse_field(field)
                    if not parsed:
                        continue
                    name, field_type, desc = parsed
                    class_dict["__annotations__"][name] = (
                        field_type  # Use resolved type
                    )

                    FieldClass = (
                        dspy.InputField if field_kind == "input" else dspy.OutputField
                    )
                    # DSPy Fields use 'desc' for description
                    class_dict[name] = (
                        FieldClass(desc=desc) if desc is not None else FieldClass()
                    )

        try:
            process_fields(inputs_spec, "input")
            process_fields(outputs_spec, "output")
        except Exception as e:
            logger.error(
                f"Error processing fields for DSPy signature '{agent_name}': {e}",
                exc_info=True,
            )
            raise ValueError(f"Could not process fields for signature: {e}") from e

        # Create and return the dynamic class
        try:
            DynamicSignature = type("dspy_" + agent_name, (base_class,), class_dict)
            logger.info(
                f"Successfully created DSPy Signature: {DynamicSignature.__name__} "
                f"with fields: {DynamicSignature.__annotations__}"
            )
            return DynamicSignature
        except Exception as e:
            logger.error(
                f"Failed to create dynamic type 'dspy_{agent_name}': {e}",
                exc_info=True,
            )
            raise TypeError(f"Could not create DSPy signature type: {e}") from e

    def _configure_language_model(
        self,
        model: str | None,
        use_cache: bool,
        temperature: float,
        max_tokens: int,
    ) -> None:
        """Initialize and configure the language model using dspy."""
        if model is None:
            logger.warning(
                "No model specified for DSPy configuration. Using DSPy default."
            )
            # Rely on DSPy's global default or raise error if none configured
            # import dspy
            # if dspy.settings.lm is None:
            #      raise ValueError("No model specified for agent and no global DSPy LM configured.")
            return

        try:
            import dspy
        except ImportError:
            logger.error(
                "DSPy library is not installed. Cannot configure language model."
            )
            return  # Or raise

        try:
            # Ensure 'cache' parameter is handled correctly (might not exist on dspy.LM directly)
            # DSPy handles caching globally or via specific optimizers typically.
            # We'll configure the LM without explicit cache control here.
            lm_instance = dspy.LM(
                model=model,
                temperature=temperature,
                max_tokens=max_tokens,
                cache=use_cache,
                # Add other relevant parameters if needed, e.g., API keys via dspy.settings
            )
            dspy.settings.configure(lm=lm_instance)
            logger.info(
                f"DSPy LM configured with model: {model}, temp: {temperature}, max_tokens: {max_tokens}"
            )
            # Note: DSPy caching is usually configured globally, e.g., dspy.settings.configure(cache=...)
            # or handled by optimizers. Setting `cache=use_cache` on dspy.LM might not be standard.
        except Exception as e:
            logger.error(
                f"Failed to configure DSPy language model '{model}': {e}",
                exc_info=True,
            )

    def _select_task(
        self,
        signature: Any,
        override_evaluator_type: AgentType,
        tools: list[Any] | None = None,
        kwargs: dict[str, Any] = {},
    ) -> Any:
        """Select and instantiate the appropriate DSPy Program/Module."""
        try:
            import dspy
        except ImportError:
            logger.error("DSPy library is not installed. Cannot select DSPy task.")
            raise ImportError("DSPy is required for this functionality.")

        processed_tools = []
        if tools:
            for tool in tools:
                if callable(tool):  # Basic check
                    processed_tools.append(tool)
                # Could add more sophisticated tool wrapping/validation here if needed
                else:
                    logger.warning(
                        f"Item '{tool}' in tools list is not callable, skipping."
                    )

        dspy_program = None
        selected_type = override_evaluator_type

        # Determine type if not overridden
        if not selected_type:
            selected_type = "ReAct" if processed_tools else "Predict"  # Default logic

        logger.debug(
            f"Selecting DSPy program type: {selected_type} (Tools provided: {bool(processed_tools)})"
        )

        try:
            if selected_type == "ChainOfThought":
                dspy_program = dspy.ChainOfThought(signature, **kwargs)
            elif selected_type == "ReAct":
                if not kwargs:
                    kwargs = {"max_iters": 10}
                dspy_program = dspy.ReAct(
                    signature, tools=processed_tools or [], **kwargs
                )
            elif selected_type == "Predict":  # Default or explicitly Completion
                dspy_program = dspy.Predict(signature)
            else:  # Fallback or handle unknown type
                logger.warning(
                    f"Unknown or unsupported agent_type_override '{selected_type}'. Defaulting to dspy.Predict."
                )
                dspy_program = dspy.Predict(signature)

            logger.info(f"Instantiated DSPy program: {type(dspy_program).__name__}")
            return dspy_program
        except Exception as e:
            logger.error(
                f"Failed to instantiate DSPy program of type '{selected_type}': {e}",
                exc_info=True,
            )
            raise RuntimeError(f"Could not create DSPy program: {e}") from e

    def _process_result(
        self, result: Any, inputs: dict[str, Any]
    ) -> tuple[dict[str, Any], float, list]:
        """Convert the DSPy result object to a dictionary."""
        import dspy

        if result is None:
            logger.warning("DSPy program returned None result.")
            return {}
        try:
            # DSPy Prediction objects often behave like dicts or have .keys() / items()
            if hasattr(result, "items") and callable(result.items):
                output_dict = dict(result.items())
            elif hasattr(result, "__dict__"):  # Fallback for other object types
                output_dict = {
                    k: v for k, v in result.__dict__.items() if not k.startswith("_")
                }
            else:
                # If it's already a dict (less common for DSPy results directly)
                if isinstance(result, dict):
                    output_dict = result
                else:  # Final fallback
                    logger.warning(
                        f"Could not reliably convert DSPy result of type {type(result)} to dict. Returning as is."
                    )
                    output_dict = {"raw_result": result}

            logger.debug(f"Processed DSPy result to dict: {output_dict}")
            # Optionally merge inputs back if desired (can make result dict large)
            final_result = {**inputs, **output_dict}

            lm = dspy.settings.get("lm")
            cost = sum([x["cost"] for x in lm.history if x["cost"] is not None])
            lm_history = lm.history

            return final_result, cost, lm_history

        except Exception as conv_error:
            logger.error(
                f"Failed to process DSPy result into dictionary: {conv_error}",
                exc_info=True,
            )
            return {
                "error": "Failed to process result",
                "raw_result": str(result),
            }

```

---

### 90. src\flock\core\mixin\prompt_parser.py

- **File ID**: file_89
- **Type**: Code File
- **Line Count**: 125
- **Description**: A mixin class for parsing agent prompts and building clean signatures for DSPy.
- **Dependencies**: None
- **Used By**: None

**Content**:
```
"""A mixin class for parsing agent prompts and building clean signatures for DSPy."""

# DEPRECATED! This mixin is no longer used in the current version of Flock. It was used to parse agent prompts and build clean signatures for DSPy.
# TODO: DELETE THIS FILE!

from flock.core.util.input_resolver import split_top_level


class PromptParserMixin:
    """A mixin class for parsing agent prompts and building clean signatures for DSPy."""

    def _parse_key_descriptions(self, keys_str: str) -> list[tuple[str, str]]:
        """Parse a comma-separated string into a list of (key, description) tuples.

        This function processes a configuration string that defines one or more keys, where each key may
        include a type hint and an optional human-readable description. The expected format for each key is:

            key: type_hint | description

        If the pipe symbol ("|") is absent, the description is set to an empty string.

        The splitting is performed using split_top_level() so that commas inside type hints are preserved.

        For example, given:
            "query: str | The search query, context: dict | The full conversation context"
        it returns:
            [("query", "The search query"), ("context", "The full conversation context")]

        Args:
            keys_str (str): A comma-separated string of key definitions.

        Returns:
            List[Tuple[str, str]]: A list of (key, description) tuples.
        """
        key_descs = []
        for part in split_top_level(keys_str):
            if not part:
                continue
            if "|" in part:
                key_type_part, desc = part.split("|", 1)
                desc = desc.strip()
            else:
                key_type_part = part
                desc = ""
            key = key_type_part.split(":", 1)[0].strip()
            key_descs.append((key, desc))
        return key_descs

    def _build_clean_signature(self, keys_str: str) -> str:
        """Build a clean signature string from the configuration string by removing the description parts.

        Given a string like:
            "query: str | The search query, context: dict | The full conversation context"
        this method returns:
            "query: str, context: dict"

        This function uses split_top_level() to avoid splitting on commas that are inside type hints.

        Args:
            keys_str (str): The configuration string containing keys, type hints, and optional descriptions.

        Returns:
            str: A clean signature string with only keys and type hints.
        """
        parts = []
        for part in split_top_level(keys_str):
            if not part:
                continue
            if "|" in part:
                clean_part = part.split("|", 1)[0].strip()
            else:
                clean_part = part.strip()
            parts.append(clean_part)
        return ", ".join(parts)

    def _build_descriptions(self) -> tuple[dict[str, str], dict[str, str]]:
        """Build dictionaries of input and output descriptions from the agent's configuration.

        Returns:
            A tuple containing:
            - input_desc: A dictionary mapping each input key (without type hints) to its description.
            - output_desc: A dictionary mapping each output key (without type hints) to its description.
        """
        input_desc: dict[str, str] = {}
        if self.input:
            for key, desc in self._parse_key_descriptions(self.input):
                input_desc[key] = desc

        output_desc: dict[str, str] = {}
        if self.output:
            for key, desc in self._parse_key_descriptions(self.output):
                output_desc[key] = desc

        return input_desc, output_desc

    def _build_prompt(
        self, input_desc: dict[str, str], output_desc: dict[str, str]
    ) -> str:
        """Build a clean signature prompt from the agent's configuration.

        This method uses the original input and output strings (removing the description parts)
        to create a signature string that is passed to DSPy. For example, if:
        - self.input is "query: str | The search query, context: dict | The full conversation context"
        - self.output is "result: str | The result"
        then the prompt will be:
        "query: str, context: dict -> result: str"

        **Note:** The descriptive metadata is preserved in the dictionaries obtained from _build_descriptions,
        which are passed separately to DSPy.

        Args:
            input_desc: Dictionary of input key descriptions (for metadata only).
            output_desc: Dictionary of output key descriptions (for metadata only).

        Returns:
            A clean signature string for DSPy.
        """
        clean_input = (
            self._build_clean_signature(self.input) if self.input else ""
        )
        clean_output = (
            self._build_clean_signature(self.output) if self.output else ""
        )
        # Combine the clean input and output signatures using "->"
        return f"{clean_input} -> {clean_output}"

```

---

### 91. src\flock\core\serialization\__init__.py

- **File ID**: file_90
- **Type**: Code File
- **Line Count**: 13
- **Description**: Serialization utilities for Flock objects.
- **Dependencies**: None
- **Used By**: None

**Content**:
```
"""Serialization utilities for Flock objects."""

from flock.core.serialization.callable_registry import CallableRegistry
from flock.core.serialization.json_encoder import FlockJSONEncoder
from flock.core.serialization.secure_serializer import SecureSerializer
from flock.core.serialization.serializable import Serializable

__all__ = [
    "CallableRegistry",
    "FlockJSONEncoder",
    "SecureSerializer",
    "Serializable",
]

```

---

### 92. src\flock\core\serialization\callable_registry.py

- **File ID**: file_91
- **Type**: Code File
- **Line Count**: 52
- **Description**: Registry system for callable objects to support serialization.
- **Dependencies**: None
- **Used By**: None

**Content**:
```
"""Registry system for callable objects to support serialization."""

from collections.abc import Callable


class CallableRegistry:
    """Registry for callable objects.

    This class serves as a central registry for callable objects (functions, methods)
    that can be referenced by name in serialized formats.

    This is a placeholder implementation that will be fully implemented in task US007-T004.
    """

    _registry: dict[str, Callable] = {}

    @classmethod
    def register(cls, name: str, callable_obj: Callable) -> None:
        """Register a callable object with the given name.

        Args:
            name: Unique name for the callable
            callable_obj: Function or method to register
        """
        cls._registry[name] = callable_obj

    @classmethod
    def get(cls, name: str) -> Callable:
        """Get a callable object by name.

        Args:
            name: Name of the callable to retrieve

        Returns:
            The registered callable

        Raises:
            KeyError: If no callable with the given name is registered
        """
        return cls._registry[name]

    @classmethod
    def contains(cls, name: str) -> bool:
        """Check if a callable with the given name is registered.

        Args:
            name: Name to check

        Returns:
            True if registered, False otherwise
        """
        return name in cls._registry

```

---

### 93. src\flock\core\serialization\flock_serializer.py

- **File ID**: file_92
- **Type**: Code File
- **Line Count**: 777
- **Description**: Handles serialization and deserialization logic for Flock instances.
- **Dependencies**: None
- **Used By**: None

**Content**:
```
# src/flock/core/serialization/flock_serializer.py
"""Handles serialization and deserialization logic for Flock instances."""

import builtins
import importlib
import importlib.util
import inspect
import os
import re
import sys
from dataclasses import is_dataclass
from typing import TYPE_CHECKING, Any, Literal

from pydantic import BaseModel, create_model

# Need registry access
from flock.core.flock_registry import get_registry
from flock.core.logging.logging import get_logger
from flock.core.serialization.serialization_utils import (
    extract_pydantic_models_from_type_string,  # Assuming this handles basic serialization needs
)

if TYPE_CHECKING:
    from flock.core.flock import Flock


logger = get_logger("serialization.flock")
FlockRegistry = get_registry()


class FlockSerializer:
    """Provides static methods for serializing and deserializing Flock instances."""

    @staticmethod
    def serialize(
        flock_instance: "Flock",
        path_type: Literal["absolute", "relative"] = "relative",
    ) -> dict[str, Any]:
        """Convert Flock instance to dictionary representation.

        Args:
            flock_instance: The Flock instance to serialize.
            path_type: How file paths should be formatted ('absolute' or 'relative').
        """
        logger.debug(
            f"Serializing Flock instance '{flock_instance.name}' to dict."
        )
        # Use Pydantic's dump for base fields defined in Flock's model
        data = flock_instance.model_dump(mode="json", exclude_none=True)
        logger.info(
            f"Serializing Flock '{flock_instance.name}' with {len(flock_instance._agents)} agents"
        )

        data["agents"] = {}
        custom_types = {}
        components = {}

        for name, agent_instance in flock_instance._agents.items():
            try:
                logger.debug(f"Serializing agent '{name}'")
                # Agents handle their own serialization via their to_dict
                agent_data = (
                    agent_instance.to_dict()
                )  # This now uses the agent's refined to_dict
                data["agents"][name] = agent_data

                # --- Extract Types from Agent Signatures ---
                input_types = []
                if agent_instance.input:
                    input_types = FlockSerializer._extract_types_from_signature(
                        agent_instance.input
                    )
                    if input_types:
                        logger.debug(
                            f"Found input types in agent '{name}': {input_types}"
                        )

                output_types = []
                if agent_instance.output:
                    output_types = (
                        FlockSerializer._extract_types_from_signature(
                            agent_instance.output
                        )
                    )
                    if output_types:
                        logger.debug(
                            f"Found output types in agent '{name}': {output_types}"
                        )

                all_types = set(input_types + output_types)
                if all_types:
                    custom_types.update(
                        FlockSerializer._get_type_definitions(list(all_types))
                    )

                # --- Extract Component Information ---
                # Evaluator
                if (
                    "evaluator" in agent_data
                    and agent_data["evaluator"]
                    and "type" in agent_data["evaluator"]
                ):
                    component_type = agent_data["evaluator"]["type"]
                    if component_type not in components:
                        logger.debug(
                            f"Adding evaluator component '{component_type}' from agent '{name}'"
                        )
                        components[component_type] = (
                            FlockSerializer._get_component_definition(
                                component_type, path_type
                            )
                        )

                # Modules
                if "modules" in agent_data:
                    for module_name, module_data in agent_data[
                        "modules"
                    ].items():
                        if module_data and "type" in module_data:
                            component_type = module_data["type"]
                            if component_type not in components:
                                logger.debug(
                                    f"Adding module component '{component_type}' from module '{module_name}' in agent '{name}'"
                                )
                                components[component_type] = (
                                    FlockSerializer._get_component_definition(
                                        component_type, path_type
                                    )
                                )

                # Router
                if (
                    "handoff_router" in agent_data
                    and agent_data["handoff_router"]
                    and "type" in agent_data["handoff_router"]
                ):
                    component_type = agent_data["handoff_router"]["type"]
                    if component_type not in components:
                        logger.debug(
                            f"Adding router component '{component_type}' from agent '{name}'"
                        )
                        components[component_type] = (
                            FlockSerializer._get_component_definition(
                                component_type, path_type
                            )
                        )

                # Description (Callables)
                if agent_data.get("description_callable"):
                    logger.debug(
                        f"Adding description callable '{agent_data['description_callable']}' from agent '{name}'"
                    )
                    description_callable_name = agent_data[
                        "description_callable"
                    ]
                    description_callable = agent_instance.description
                    path_str = FlockRegistry.get_callable_path_string(
                        description_callable
                    )
                    if path_str:
                        logger.debug(
                            f"Adding description callable '{description_callable_name}' (from path '{path_str}') to components"
                        )
                        components[description_callable_name] = (
                            FlockSerializer._get_callable_definition(
                                path_str, description_callable_name, path_type
                            )
                        )

                if agent_data.get("input_callable"):
                    logger.debug(
                        f"Adding input callable '{agent_data['input_callable']}' from agent '{name}'"
                    )
                    input_callable_name = agent_data["input_callable"]
                    input_callable = agent_instance.input
                    path_str = FlockRegistry.get_callable_path_string(
                        input_callable
                    )
                    if path_str:
                        logger.debug(
                            f"Adding input callable '{input_callable_name}' (from path '{path_str}') to components"
                        )
                        components[input_callable_name] = (
                            FlockSerializer._get_callable_definition(
                                path_str, input_callable_name, path_type
                            )
                        )

                if agent_data.get("output_callable"):
                    logger.debug(
                        f"Adding output callable '{agent_data['output_callable']}' from agent '{name}'"
                    )
                    output_callable_name = agent_data["output_callable"]
                    output_callable = agent_instance.output
                    path_str = FlockRegistry.get_callable_path_string(
                        output_callable
                    )
                    if path_str:
                        logger.debug(
                            f"Adding output callable '{output_callable_name}' (from path '{path_str}') to components"
                        )
                        components[output_callable_name] = (
                            FlockSerializer._get_callable_definition(
                                path_str, output_callable_name, path_type
                            )
                        )

                # Tools (Callables)
                if agent_data.get("tools"):
                    logger.debug(
                        f"Extracting tool information from agent '{name}': {agent_data['tools']}"
                    )
                    tool_objs = (
                        agent_instance.tools if agent_instance.tools else []
                    )
                    for i, tool_name in enumerate(agent_data["tools"]):
                        if tool_name not in components and i < len(tool_objs):
                            tool = tool_objs[i]
                            if callable(tool) and not isinstance(tool, type):
                                path_str = (
                                    FlockRegistry.get_callable_path_string(tool)
                                )
                                if path_str:
                                    logger.debug(
                                        f"Adding tool '{tool_name}' (from path '{path_str}') to components"
                                    )
                                    components[tool_name] = (
                                        FlockSerializer._get_callable_definition(
                                            path_str, tool_name, path_type
                                        )
                                    )

            except Exception as e:
                logger.error(
                    f"Failed to serialize agent '{name}' within Flock: {e}",
                    exc_info=True,
                )

        if custom_types:
            logger.info(f"Adding {len(custom_types)} custom type definitions")
            data["types"] = custom_types
        if components:
            logger.info(
                f"Adding {len(components)} component/callable definitions"
            )
            data["components"] = components

        data["dependencies"] = FlockSerializer._get_dependencies()
        data["metadata"] = {
            "path_type": path_type,
            "flock_version": "0.4.0",
        }  # Example version

        logger.debug(f"Flock '{flock_instance.name}' serialization complete.")
        return data

    @staticmethod
    def deserialize(cls: type["Flock"], data: dict[str, Any]) -> "Flock":
        """Create Flock instance from dictionary representation."""
        # Import concrete types needed for instantiation
        from flock.core.flock import Flock  # Import the actual class
        from flock.core.flock_agent import FlockAgent as ConcreteFlockAgent

        logger.debug(
            f"Deserializing Flock from dict. Provided keys: {list(data.keys())}"
        )

        metadata = data.pop("metadata", {})
        path_type = metadata.get(
            "path_type", "relative"
        )  # Default to relative for loading flexibility
        logger.debug(
            f"Using path_type '{path_type}' from metadata for component loading"
        )

        if "types" in data:
            logger.info(f"Processing {len(data['types'])} type definitions")
            FlockSerializer._register_type_definitions(data.pop("types"))

        if "components" in data:
            logger.info(
                f"Processing {len(data['components'])} component/callable definitions"
            )
            FlockSerializer._register_component_definitions(
                data.pop("components"), path_type
            )

        if "dependencies" in data:
            logger.debug(f"Checking {len(data['dependencies'])} dependencies")
            FlockSerializer._check_dependencies(data.pop("dependencies"))

        agents_data = data.pop("agents", {})
        logger.info(f"Found {len(agents_data)} agents to deserialize")

        try:
            # Pass only fields defined in Flock's Pydantic model to constructor
            init_data = {
                k: v for k, v in data.items() if k in Flock.model_fields
            }
            logger.debug(
                f"Creating Flock instance with fields: {list(init_data.keys())}"
            )
            flock_instance = cls(**init_data)  # Use cls which is Flock
        except Exception as e:
            logger.error(
                f"Pydantic validation/init failed for Flock: {e}", exc_info=True
            )
            raise ValueError(
                f"Failed to initialize Flock from dict: {e}"
            ) from e

        # Deserialize and add agents AFTER Flock instance exists
        for name, agent_data in agents_data.items():
            try:
                logger.debug(f"Deserializing agent '{name}'")
                agent_data.setdefault("name", name)
                agent_instance = ConcreteFlockAgent.from_dict(agent_data)
                flock_instance.add_agent(agent_instance)
                logger.debug(f"Successfully added agent '{name}' to Flock")
            except Exception as e:
                logger.error(
                    f"Failed to deserialize/add agent '{name}': {e}",
                    exc_info=True,
                )

        logger.info(
            f"Successfully deserialized Flock '{flock_instance.name}' with {len(flock_instance._agents)} agents"
        )
        return flock_instance

    # --- Helper methods moved from Flock ---
    # (Keep all the _extract..., _get..., _register..., _create... methods here)
    # Ensure they use FlockSerializer._... or are standalone functions called directly.
    # Make static if they don't need instance state (which they shouldn't here).

    @staticmethod
    def _extract_types_from_signature(signature: str) -> list[str]:
        """Extract type names from an input/output signature string."""
        if not signature:
            return []
        from flock.core.util.input_resolver import (
            split_top_level,  # Import locally if needed
        )

        type_names = set()
        try:
            parts = split_top_level(signature)
            for part in parts:
                if ":" in part:
                    type_str = part.split(":", 1)[1].split("|", 1)[0].strip()
                    # Use the more robust extractor
                    models = extract_pydantic_models_from_type_string(type_str)
                    for model in models:
                        type_names.add(model.__name__)
        except Exception as e:
            logger.warning(
                f"Could not fully parse types from signature '{signature}': {e}"
            )
        return list(type_names)

    @staticmethod
    def _get_type_definitions(type_names: list[str]) -> dict[str, Any]:
        """Get definitions for the specified custom types from the registry."""
        type_definitions = {}
        for type_name in type_names:
            try:
                type_obj = FlockRegistry.get_type(
                    type_name
                )  # Throws KeyError if not found
                type_def = FlockSerializer._extract_type_definition(
                    type_name, type_obj
                )
                if type_def:
                    type_definitions[type_name] = type_def
            except KeyError:
                logger.warning(
                    f"Type '{type_name}' requested but not found in registry."
                )
            except Exception as e:
                logger.warning(
                    f"Could not extract definition for type {type_name}: {e}"
                )
        return type_definitions

    @staticmethod
    def _extract_type_definition(
        type_name: str, type_obj: type
    ) -> dict[str, Any] | None:
        """Extract a definition for a custom type (Pydantic or Dataclass)."""
        # Definition includes module path and schema/fields
        module_path = getattr(type_obj, "__module__", "unknown")
        type_def = {"module_path": module_path}
        try:
            if issubclass(type_obj, BaseModel):
                type_def["type"] = "pydantic.BaseModel"
                schema = type_obj.model_json_schema()
                if "title" in schema and schema["title"] == type_name:
                    del schema["title"]
                type_def["schema"] = schema
                return type_def
            elif is_dataclass(type_obj):
                type_def["type"] = "dataclass"
                fields = {}
                for field_name, field in getattr(
                    type_obj, "__dataclass_fields__", {}
                ).items():
                    # Attempt to get a string representation of the type
                    try:
                        type_repr = str(field.type)
                    except Exception:
                        type_repr = "unknown"
                    fields[field_name] = {
                        "type": type_repr,
                        "default": str(field.default)
                        if field.default is not inspect.Parameter.empty
                        else None,
                    }
                type_def["fields"] = fields
                return type_def
            else:
                logger.debug(
                    f"Type '{type_name}' is not Pydantic or Dataclass, skipping detailed definition."
                )
                return (
                    None  # Don't include non-data types in the 'types' section
                )
        except Exception as e:
            logger.warning(f"Error extracting definition for {type_name}: {e}")
            return None

    @staticmethod
    def _get_component_definition(
        component_type_name: str, path_type: Literal["absolute", "relative"]
    ) -> dict[str, Any]:
        """Get definition for a component type from the registry."""
        component_def = {
            "type": "flock_component",
            "module_path": "unknown",
            "file_path": None,
        }
        try:
            component_class = FlockRegistry.get_component(
                component_type_name
            )  # Raises KeyError if not found
            component_def["module_path"] = getattr(
                component_class, "__module__", "unknown"
            )
            component_def["description"] = (
                inspect.getdoc(component_class)
                or f"{component_type_name} component"
            )

            # Get file path
            try:
                file_path_abs = inspect.getfile(component_class)
                component_def["file_path"] = (
                    os.path.relpath(file_path_abs)
                    if path_type == "relative"
                    else file_path_abs
                )
            except (TypeError, ValueError) as e:
                logger.debug(
                    f"Could not determine file path for component {component_type_name}: {e}"
                )

        except KeyError:
            logger.warning(
                f"Component class '{component_type_name}' not found in registry."
            )
            component_def["description"] = (
                f"{component_type_name} component (class not found in registry)"
            )
        except Exception as e:
            logger.warning(
                f"Could not extract full definition for component {component_type_name}: {e}"
            )
        return component_def

    @staticmethod
    def _get_callable_definition(
        callable_path: str,
        func_name: str,
        path_type: Literal["absolute", "relative"],
    ) -> dict[str, Any]:
        """Get definition for a callable using its registry path."""
        callable_def = {
            "type": "flock_callable",
            "module_path": "unknown",
            "file_path": None,
        }
        try:
            func = FlockRegistry.get_callable(
                callable_path
            )  # Raises KeyError if not found
            callable_def["module_path"] = getattr(func, "__module__", "unknown")
            callable_def["description"] = (
                inspect.getdoc(func) or f"Callable function {func_name}"
            )
            # Get file path
            try:
                file_path_abs = inspect.getfile(func)
                callable_def["file_path"] = (
                    os.path.relpath(file_path_abs)
                    if path_type == "relative"
                    else file_path_abs
                )
            except (TypeError, ValueError) as e:
                logger.debug(
                    f"Could not determine file path for callable {callable_path}: {e}"
                )

        except KeyError:
            logger.warning(
                f"Callable '{callable_path}' (for tool '{func_name}') not found in registry."
            )
            callable_def["description"] = (
                f"Callable {func_name} (function not found in registry)"
            )
        except Exception as e:
            logger.warning(
                f"Could not extract full definition for callable {callable_path}: {e}"
            )
        return callable_def

    @staticmethod
    def _get_dependencies() -> list[str]:
        """Get list of core dependencies required by Flock."""
        # Basic static list for now
        return [
            "pydantic>=2.0.0",
            "flock-core>=0.4.0",
        ]  # Update version as needed

    @staticmethod
    def _register_type_definitions(type_defs: dict[str, Any]) -> None:
        """Register type definitions from serialized data."""
        # (Logic remains largely the same as original, ensure it uses FlockRegistry)
        for type_name, type_def in type_defs.items():
            logger.debug(f"Registering type definition for: {type_name}")
            # Prioritize direct import
            module_path = type_def.get("module_path")
            registered = False
            if module_path and module_path != "unknown":
                try:
                    module = importlib.import_module(module_path)
                    if hasattr(module, type_name):
                        type_obj = getattr(module, type_name)
                        FlockRegistry.register_type(type_obj, type_name)
                        logger.info(
                            f"Registered type '{type_name}' from module '{module_path}'"
                        )
                        registered = True
                except ImportError:
                    logger.debug(
                        f"Could not import module {module_path} for type {type_name}"
                    )
                except Exception as e:
                    logger.warning(
                        f"Error registering type {type_name} from module: {e}"
                    )

            if registered:
                continue

            # Attempt dynamic creation if direct import failed or wasn't possible
            type_kind = type_def.get("type")
            if type_kind == "pydantic.BaseModel" and "schema" in type_def:
                FlockSerializer._create_pydantic_model(type_name, type_def)
            elif type_kind == "dataclass" and "fields" in type_def:
                FlockSerializer._create_dataclass(type_name, type_def)
            else:
                logger.warning(
                    f"Cannot dynamically register type '{type_name}' with kind '{type_kind}'"
                )

    @staticmethod
    def _create_pydantic_model(
        type_name: str, type_def: dict[str, Any]
    ) -> None:
        """Dynamically create and register a Pydantic model from schema."""
        # (Logic remains the same, ensure it uses FlockRegistry.register_type)
        schema = type_def.get("schema", {})
        try:
            fields = {}
            properties = schema.get("properties", {})
            required = schema.get("required", [])
            for field_name, field_schema in properties.items():
                field_type = FlockSerializer._get_type_from_schema(field_schema)
                default = ... if field_name in required else None
                fields[field_name] = (field_type, default)

            DynamicModel = create_model(type_name, **fields)
            FlockRegistry.register_type(DynamicModel, type_name)
            logger.info(
                f"Dynamically created and registered Pydantic model: {type_name}"
            )
        except Exception as e:
            logger.error(f"Failed to create Pydantic model {type_name}: {e}")

    @staticmethod
    def _get_type_from_schema(field_schema: dict[str, Any]) -> Any:
        """Convert JSON schema type to Python type."""
        # (Logic remains the same)
        schema_type = field_schema.get("type")
        type_mapping = {
            "string": str,
            "integer": int,
            "number": float,
            "boolean": bool,
            "array": list,
            "object": dict,
        }
        if schema_type in type_mapping:
            return type_mapping[schema_type]
        if "enum" in field_schema:
            from typing import Literal

            return Literal[tuple(field_schema["enum"])]  # type: ignore
        return Any

    @staticmethod
    def _create_dataclass(type_name: str, type_def: dict[str, Any]) -> None:
        """Dynamically create and register a dataclass."""
        # (Logic remains the same, ensure it uses FlockRegistry.register_type)
        from dataclasses import make_dataclass

        fields_def = type_def.get("fields", {})
        try:
            fields = []
            for field_name, field_props in fields_def.items():
                # Safely evaluate type string - requires care!
                field_type_str = field_props.get("type", "str")
                try:
                    field_type = eval(
                        field_type_str,
                        {"__builtins__": builtins.__dict__},
                        {"List": list, "Dict": dict},
                    )  # Allow basic types
                except Exception:
                    field_type = Any
                fields.append((field_name, field_type))

            DynamicDataclass = make_dataclass(type_name, fields)
            FlockRegistry.register_type(DynamicDataclass, type_name)
            logger.info(
                f"Dynamically created and registered dataclass: {type_name}"
            )
        except Exception as e:
            logger.error(f"Failed to create dataclass {type_name}: {e}")

    @staticmethod
    def _register_component_definitions(
        component_defs: dict[str, Any],
        path_type: Literal["absolute", "relative"],
    ) -> None:
        """Register component/callable definitions from serialized data."""
        # (Logic remains the same, ensure it uses FlockRegistry.register_component/register_callable)
        # Key change: Ensure file_path is handled correctly based on path_type from metadata
        for name, comp_def in component_defs.items():
            logger.debug(
                f"Registering component/callable definition for: {name}"
            )
            kind = comp_def.get("type")
            module_path = comp_def.get("module_path")
            file_path = comp_def.get("file_path")
            registered = False

            # Resolve file path if relative
            if (
                path_type == "relative"
                and file_path
                and not os.path.isabs(file_path)
            ):
                abs_file_path = os.path.abspath(file_path)
                logger.debug(
                    f"Resolved relative path '{file_path}' to absolute '{abs_file_path}'"
                )
                file_path = abs_file_path  # Use absolute path for loading

            # 1. Try importing from module_path
            if module_path and module_path != "unknown":
                try:
                    module = importlib.import_module(module_path)
                    if hasattr(module, name):
                        obj = getattr(module, name)
                        if kind == "flock_callable" and callable(obj):
                            FlockRegistry.register_callable(
                                obj, name
                            )  # Register by simple name
                            # Also register by full path if possible
                            full_path = f"{module_path}.{name}"
                            if full_path != name:
                                FlockRegistry.register_callable(obj, full_path)
                            logger.info(
                                f"Registered callable '{name}' from module '{module_path}'"
                            )
                            registered = True
                        elif kind == "flock_component" and isinstance(
                            obj, type
                        ):
                            FlockRegistry.register_component(obj, name)
                            logger.info(
                                f"Registered component '{name}' from module '{module_path}'"
                            )
                            registered = True
                except (ImportError, AttributeError):
                    logger.debug(
                        f"Could not import '{name}' from module '{module_path}', trying file path."
                    )
                except Exception as e:
                    logger.warning(
                        f"Error registering '{name}' from module '{module_path}': {e}"
                    )

            if registered:
                continue

            # 2. Try importing from file_path if module import failed or wasn't possible
            if file_path and os.path.exists(file_path):
                logger.debug(
                    f"Attempting to load '{name}' from file: {file_path}"
                )
                try:
                    mod_name = f"flock_dynamic_{name}"  # Unique module name
                    spec = importlib.util.spec_from_file_location(
                        mod_name, file_path
                    )
                    if spec and spec.loader:
                        module = importlib.util.module_from_spec(spec)
                        sys.modules[spec.name] = (
                            module  # Important for pickle/cloudpickle
                        )
                        spec.loader.exec_module(module)
                        if hasattr(module, name):
                            obj = getattr(module, name)
                            if kind == "flock_callable" and callable(obj):
                                FlockRegistry.register_callable(obj, name)
                                logger.info(
                                    f"Registered callable '{name}' from file '{file_path}'"
                                )
                            elif kind == "flock_component" and isinstance(
                                obj, type
                            ):
                                FlockRegistry.register_component(obj, name)
                                logger.info(
                                    f"Registered component '{name}' from file '{file_path}'"
                                )
                        else:
                            logger.warning(
                                f"'{name}' not found in loaded file '{file_path}'"
                            )
                    else:
                        logger.warning(
                            f"Could not create import spec for file '{file_path}'"
                        )
                except Exception as e:
                    logger.error(
                        f"Error loading '{name}' from file '{file_path}': {e}",
                        exc_info=True,
                    )
            elif not registered:
                logger.warning(
                    f"Could not register '{name}'. No valid module or file path found."
                )

    @staticmethod
    def _check_dependencies(dependencies: list[str]) -> None:
        """Check if required dependencies are available (basic check)."""
        # (Logic remains the same)
        for dep in dependencies:
            match = re.match(r"([^>=<]+)", dep)
            if match:
                pkg_name = match.group(1).replace("-", "_")
                try:
                    importlib.import_module(pkg_name)
                except ImportError:
                    logger.warning(f"Dependency '{dep}' might be missing.")

```

---

### 94. src\flock\core\serialization\json_encoder.py

- **File ID**: file_93
- **Type**: Code File
- **Line Count**: 41
- **Description**: JSON encoder utilities for Flock objects.
- **Dependencies**: None
- **Used By**:
  - file_95

**Content**:
```
"""JSON encoder utilities for Flock objects."""

import json
from datetime import datetime
from typing import Any


class FlockJSONEncoder(json.JSONEncoder):
    """Custom JSON encoder for handling Pydantic models and other non-serializable objects."""

    def default(self, obj: Any) -> Any:
        from pydantic import BaseModel

        # Handle Pydantic models
        if isinstance(obj, BaseModel):
            return obj.model_dump()

        # Handle datetime objects
        if isinstance(obj, datetime):
            return obj.isoformat()

        # Handle sets, convert to list
        if isinstance(obj, set):
            return list(obj)

        # Handle objects with a to_dict method
        if hasattr(obj, "to_dict") and callable(getattr(obj, "to_dict")):
            return obj.to_dict()

        # Handle objects with a __dict__ attribute
        if hasattr(obj, "__dict__"):
            return {
                k: v for k, v in obj.__dict__.items() if not k.startswith("_")
            }

        # Let the parent class handle it or raise TypeError
        try:
            return super().default(obj)
        except TypeError:
            # If all else fails, convert to string
            return str(obj)

```

---

### 95. src\flock\core\serialization\secure_serializer.py

- **File ID**: file_94
- **Type**: Code File
- **Line Count**: 175
- **Description**: File at src\flock\core\serialization\secure_serializer.py
- **Dependencies**: None
- **Used By**: None

**Content**:
```
import cloudpickle


class SecureSerializer:
    """Security-focused serialization system with capability controls for Flock objects."""

    # Define capability levels for different modules
    MODULE_CAPABILITIES = {
        # Core Python - unrestricted
        "builtins": "unrestricted",
        "datetime": "unrestricted",
        "re": "unrestricted",
        "math": "unrestricted",
        "json": "unrestricted",
        # Framework modules - unrestricted
        "flock": "unrestricted",
        # System modules - restricted but allowed
        "os": "restricted",
        "io": "restricted",
        "sys": "restricted",
        "subprocess": "high_risk",
        # Network modules - high risk
        "socket": "high_risk",
        "requests": "high_risk",
    }

    # Functions that should never be serialized
    BLOCKED_FUNCTIONS = {
        "os.system",
        "os.popen",
        "os.spawn",
        "os.exec",
        "subprocess.call",
        "subprocess.run",
        "subprocess.Popen",
        "eval",
        "exec",
        "__import__",
    }

    @staticmethod
    def _get_module_capability(module_name):
        """Get the capability level for a module."""
        for prefix, level in SecureSerializer.MODULE_CAPABILITIES.items():
            if module_name == prefix or module_name.startswith(f"{prefix}."):
                return level
        return "unknown"  # Default to unknown for unlisted modules

    @staticmethod
    def _is_safe_callable(obj):
        """Check if a callable is safe to serialize."""
        if not callable(obj) or isinstance(obj, type):
            return True, "Not a callable function"

        module = obj.__module__
        func_name = (
            f"{module}.{obj.__name__}"
            if hasattr(obj, "__name__")
            else "unknown"
        )

        # Check against blocked functions
        if func_name in SecureSerializer.BLOCKED_FUNCTIONS:
            return False, f"Function {func_name} is explicitly blocked"

        # Check module capability level
        capability = SecureSerializer._get_module_capability(module)
        if capability == "unknown":
            return False, f"Module {module} has unknown security capability"

        return True, capability

    @staticmethod
    def serialize(obj, allow_restricted=True, allow_high_risk=False):
        """Serialize an object with capability checks."""
        if callable(obj) and not isinstance(obj, type):
            is_safe, capability = SecureSerializer._is_safe_callable(obj)

            if not is_safe:
                raise ValueError(
                    f"Cannot serialize unsafe callable: {capability}"
                )

            if capability == "high_risk" and not allow_high_risk:
                raise ValueError(
                    f"High risk callable {obj.__module__}.{obj.__name__} requires explicit permission"
                )

            if capability == "restricted" and not allow_restricted:
                raise ValueError(
                    f"Restricted callable {obj.__module__}.{obj.__name__} requires explicit permission"
                )

            # Store metadata about the callable for verification during deserialization
            metadata = {
                "module": obj.__module__,
                "name": getattr(obj, "__name__", "unknown"),
                "capability": capability,
            }

            return {
                "__serialized_callable__": True,
                "data": cloudpickle.dumps(obj).hex(),
                "metadata": metadata,
            }

        if isinstance(obj, list):
            return [
                SecureSerializer.serialize(
                    item, allow_restricted, allow_high_risk
                )
                for item in obj
            ]

        if isinstance(obj, dict):
            return {
                k: SecureSerializer.serialize(
                    v, allow_restricted, allow_high_risk
                )
                for k, v in obj.items()
            }

        return obj

    @staticmethod
    def deserialize(obj, allow_restricted=True, allow_high_risk=False):
        """Deserialize an object with capability enforcement."""
        if isinstance(obj, dict) and obj.get("__serialized_callable__") is True:
            # Validate the capability level during deserialization
            metadata = obj.get("metadata", {})
            capability = metadata.get("capability", "unknown")

            if capability == "high_risk" and not allow_high_risk:
                raise ValueError(
                    f"Cannot deserialize high risk callable {metadata.get('module')}.{metadata.get('name')}"
                )

            if capability == "restricted" and not allow_restricted:
                raise ValueError(
                    f"Cannot deserialize restricted callable {metadata.get('module')}.{metadata.get('name')}"
                )

            try:
                callable_obj = cloudpickle.loads(bytes.fromhex(obj["data"]))

                # Additional verification that the deserialized object matches its metadata
                if callable_obj.__module__ != metadata.get("module") or (
                    hasattr(callable_obj, "__name__")
                    and callable_obj.__name__ != metadata.get("name")
                ):
                    raise ValueError(
                        "Callable metadata mismatch - possible tampering detected"
                    )

                return callable_obj
            except Exception as e:
                raise ValueError(f"Failed to deserialize callable: {e!s}")

        if isinstance(obj, list):
            return [
                SecureSerializer.deserialize(
                    item, allow_restricted, allow_high_risk
                )
                for item in obj
            ]

        if isinstance(obj, dict) and "__serialized_callable__" not in obj:
            return {
                k: SecureSerializer.deserialize(
                    v, allow_restricted, allow_high_risk
                )
                for k, v in obj.items()
            }

        return obj

```

---

### 96. src\flock\core\serialization\serializable.py

- **File ID**: file_95
- **Type**: Code File
- **Line Count**: 342
- **Description**: File at src\flock\core\serialization\serializable.py
- **Dependencies**:
  - file_93
- **Used By**: None

**Content**:
```
# src/flock/core/serialization/serializable.py
import json
from abc import ABC, abstractmethod
from pathlib import Path
from typing import Any, Literal, TypeVar

# Use yaml if available, otherwise skip yaml methods
try:
    import yaml

    YAML_AVAILABLE = True
except ImportError:
    YAML_AVAILABLE = False

# Use msgpack if available
try:
    import msgpack

    MSGPACK_AVAILABLE = True
except ImportError:
    MSGPACK_AVAILABLE = False

# Use cloudpickle
try:
    import cloudpickle

    PICKLE_AVAILABLE = True
except ImportError:
    PICKLE_AVAILABLE = False


T = TypeVar("T", bound="Serializable")


class Serializable(ABC):
    """Base class for all serializable objects in the system.

    Provides methods for serializing/deserializing objects to various formats.
    Subclasses MUST implement to_dict and from_dict.
    """

    @abstractmethod
    def to_dict(self) -> dict[str, Any]:
        """Convert instance to a dictionary representation suitable for serialization.
        This method should handle converting nested Serializable objects and callables.
        """
        pass

    @classmethod
    @abstractmethod
    def from_dict(cls: type[T], data: dict[str, Any]) -> T:
        """Create instance from a dictionary representation.
        This method should handle reconstructing nested Serializable objects and callables.
        """
        pass

    # --- JSON Methods ---
    def to_json(self, indent: int | None = 2) -> str:
        """Serialize to JSON string."""
        # Import encoder locally to avoid making it a hard dependency if JSON isn't used
        from .json_encoder import FlockJSONEncoder

        try:
            # Note: to_dict should ideally prepare the structure fully.
            # FlockJSONEncoder is a fallback for types missed by to_dict.
            return json.dumps(
                self.to_dict(), cls=FlockJSONEncoder, indent=indent
            )
        except Exception as e:
            raise RuntimeError(
                f"Failed to serialize {self.__class__.__name__} to JSON: {e}"
            ) from e

    @classmethod
    def from_json(cls: type[T], json_str: str) -> T:
        """Create instance from JSON string."""
        try:
            data = json.loads(json_str)
            return cls.from_dict(data)
        except json.JSONDecodeError as e:
            raise ValueError(f"Invalid JSON string: {e}") from e
        except Exception as e:
            raise RuntimeError(
                f"Failed to deserialize {cls.__name__} from JSON: {e}"
            ) from e

    # --- YAML Methods ---
    def to_yaml(
        self,
        path_type: Literal["absolute", "relative"] = "relative",
        sort_keys=False,
        default_flow_style=False,
    ) -> str:
        """Serialize to YAML string.

        Args:
            path_type: How file paths should be formatted ('absolute' or 'relative')
            sort_keys: Whether to sort dictionary keys
            default_flow_style: YAML flow style setting
        """
        if not YAML_AVAILABLE:
            raise NotImplementedError(
                "YAML support requires PyYAML: pip install pyyaml"
            )
        try:
            # If to_dict supports path_type, pass it; otherwise use standard to_dict
            if "path_type" in self.to_dict.__code__.co_varnames:
                dict_data = self.to_dict(path_type=path_type)
            else:
                dict_data = self.to_dict()

            return yaml.dump(
                dict_data,
                sort_keys=sort_keys,
                default_flow_style=default_flow_style,
                allow_unicode=True,
            )
        except Exception as e:
            raise RuntimeError(
                f"Failed to serialize {self.__class__.__name__} to YAML: {e}"
            ) from e

    @classmethod
    def from_yaml(cls: type[T], yaml_str: str) -> T:
        """Create instance from YAML string."""
        if not YAML_AVAILABLE:
            raise NotImplementedError(
                "YAML support requires PyYAML: pip install pyyaml"
            )
        try:
            data = yaml.safe_load(yaml_str)
            if not isinstance(data, dict):
                raise TypeError(
                    f"YAML did not yield a dictionary for {cls.__name__}"
                )
            return cls.from_dict(data)
        except yaml.YAMLError as e:
            raise ValueError(f"Invalid YAML string: {e}") from e
        except Exception as e:
            raise RuntimeError(
                f"Failed to deserialize {cls.__name__} from YAML: {e}"
            ) from e

    def to_yaml_file(
        self,
        path: Path | str,
        path_type: Literal["absolute", "relative"] = "relative",
        **yaml_dump_kwargs,
    ) -> None:
        """Serialize to YAML file.

        Args:
            path: File path to write to
            path_type: How file paths should be formatted ('absolute' or 'relative')
            **yaml_dump_kwargs: Additional arguments to pass to yaml.dump
        """
        if not YAML_AVAILABLE:
            raise NotImplementedError(
                "YAML support requires PyYAML: pip install pyyaml"
            )
        path = Path(path)
        try:
            path.parent.mkdir(parents=True, exist_ok=True)
            yaml_str = self.to_yaml(path_type=path_type, **yaml_dump_kwargs)
            path.write_text(yaml_str, encoding="utf-8")
        except Exception as e:
            raise RuntimeError(
                f"Failed to write {self.__class__.__name__} to YAML file {path}: {e}"
            ) from e

    @classmethod
    def from_yaml_file(cls: type[T], path: Path | str) -> T:
        """Create instance from YAML file."""
        if not YAML_AVAILABLE:
            raise NotImplementedError(
                "YAML support requires PyYAML: pip install pyyaml"
            )
        path = Path(path)
        try:
            yaml_str = path.read_text(encoding="utf-8")
            return cls.from_yaml(yaml_str)
        except FileNotFoundError:
            raise
        except Exception as e:
            raise RuntimeError(
                f"Failed to read {cls.__name__} from YAML file {path}: {e}"
            ) from e

    # --- MsgPack Methods ---
    def to_msgpack(self) -> bytes:
        """Serialize to msgpack bytes."""
        if not MSGPACK_AVAILABLE:
            raise NotImplementedError(
                "MsgPack support requires msgpack: pip install msgpack"
            )
        try:
            # Use default hook for complex types if needed, or rely on to_dict
            return msgpack.packb(self.to_dict(), use_bin_type=True)
        except Exception as e:
            raise RuntimeError(
                f"Failed to serialize {self.__class__.__name__} to MsgPack: {e}"
            ) from e

    @classmethod
    def from_msgpack(cls: type[T], msgpack_bytes: bytes) -> T:
        """Create instance from msgpack bytes."""
        if not MSGPACK_AVAILABLE:
            raise NotImplementedError(
                "MsgPack support requires msgpack: pip install msgpack"
            )
        try:
            # Use object_hook if custom deserialization is needed beyond from_dict
            data = msgpack.unpackb(msgpack_bytes, raw=False)
            if not isinstance(data, dict):
                raise TypeError(
                    f"MsgPack did not yield a dictionary for {cls.__name__}"
                )
            return cls.from_dict(data)
        except Exception as e:
            raise RuntimeError(
                f"Failed to deserialize {cls.__name__} from MsgPack: {e}"
            ) from e

    def to_msgpack_file(self, path: Path | str) -> None:
        """Serialize to msgpack file."""
        if not MSGPACK_AVAILABLE:
            raise NotImplementedError(
                "MsgPack support requires msgpack: pip install msgpack"
            )
        path = Path(path)
        try:
            path.parent.mkdir(parents=True, exist_ok=True)
            msgpack_bytes = self.to_msgpack()
            path.write_bytes(msgpack_bytes)
        except Exception as e:
            raise RuntimeError(
                f"Failed to write {self.__class__.__name__} to MsgPack file {path}: {e}"
            ) from e

    @classmethod
    def from_msgpack_file(cls: type[T], path: Path | str) -> T:
        """Create instance from msgpack file."""
        if not MSGPACK_AVAILABLE:
            raise NotImplementedError(
                "MsgPack support requires msgpack: pip install msgpack"
            )
        path = Path(path)
        try:
            msgpack_bytes = path.read_bytes()
            return cls.from_msgpack(msgpack_bytes)
        except FileNotFoundError:
            raise
        except Exception as e:
            raise RuntimeError(
                f"Failed to read {cls.__name__} from MsgPack file {path}: {e}"
            ) from e

    # --- Pickle Methods (Use with caution due to security risks) ---
    def to_pickle(self) -> bytes:
        """Serialize to pickle bytes using cloudpickle."""
        if not PICKLE_AVAILABLE:
            raise NotImplementedError(
                "Pickle support requires cloudpickle: pip install cloudpickle"
            )
        try:
            return cloudpickle.dumps(self)
        except Exception as e:
            raise RuntimeError(
                f"Failed to serialize {self.__class__.__name__} to Pickle: {e}"
            ) from e

    @classmethod
    def from_pickle(cls: type[T], pickle_bytes: bytes) -> T:
        """Create instance from pickle bytes using cloudpickle."""
        if not PICKLE_AVAILABLE:
            raise NotImplementedError(
                "Pickle support requires cloudpickle: pip install cloudpickle"
            )
        try:
            instance = cloudpickle.loads(pickle_bytes)
            if not isinstance(instance, cls):
                raise TypeError(
                    f"Deserialized object is not of type {cls.__name__}"
                )
            return instance
        except Exception as e:
            raise RuntimeError(
                f"Failed to deserialize {cls.__name__} from Pickle: {e}"
            ) from e

    def to_pickle_file(self, path: Path | str) -> None:
        """Serialize to pickle file using cloudpickle."""
        if not PICKLE_AVAILABLE:
            raise NotImplementedError(
                "Pickle support requires cloudpickle: pip install cloudpickle"
            )
        path = Path(path)
        try:
            path.parent.mkdir(parents=True, exist_ok=True)
            pickle_bytes = self.to_pickle()
            path.write_bytes(pickle_bytes)
        except Exception as e:
            raise RuntimeError(
                f"Failed to write {self.__class__.__name__} to Pickle file {path}: {e}"
            ) from e

    @classmethod
    def from_pickle_file(cls: type[T], path: Path | str) -> T:
        """Create instance from pickle file using cloudpickle."""
        if not PICKLE_AVAILABLE:
            raise NotImplementedError(
                "Pickle support requires cloudpickle: pip install cloudpickle"
            )
        path = Path(path)
        try:
            pickle_bytes = path.read_bytes()
            return cls.from_pickle(pickle_bytes)
        except FileNotFoundError:
            raise
        except Exception as e:
            raise RuntimeError(
                f"Failed to read {cls.__name__} from Pickle file {path}: {e}"
            ) from e

    # _filter_none_values remains unchanged
    @staticmethod
    def _filter_none_values(data: Any) -> Any:
        """Filter out None values from dictionaries and lists recursively."""
        if isinstance(data, dict):
            return {
                k: Serializable._filter_none_values(v)
                for k, v in data.items()
                if v is not None
            }
        elif isinstance(data, list):
            # Filter None from list items AND recursively filter within items
            return [
                Serializable._filter_none_values(item)
                for item in data
                if item is not None
            ]
        return data

```

---

### 97. src\flock\core\serialization\serialization_utils.py

- **File ID**: file_96
- **Type**: Code File
- **Line Count**: 409
- **Description**: Utilities for recursive serialization/deserialization with callable handling.
- **Dependencies**: None
- **Used By**: None

**Content**:
```
# src/flock/core/serialization/serialization_utils.py
"""Utilities for recursive serialization/deserialization with callable handling."""

import ast
import builtins
import importlib
import sys
import types
import typing
from collections.abc import Mapping, Sequence
from enum import Enum
from typing import (
    TYPE_CHECKING,
    Any,
    Literal,
    Union,
    get_args,
    get_origin,
)

from pydantic import BaseModel

# Use TYPE_CHECKING to avoid circular imports
if TYPE_CHECKING:
    pass

from flock.core.flock_registry import get_registry
from flock.core.logging.logging import get_logger

logger = get_logger("serialization.utils")

# Remove this line to avoid circular import at module level
# FlockRegistry = get_registry()  # Get singleton instance

# --- Serialization Helper ---

# src/flock/util/hydrator.py (or import from serialization_utils)


def _format_type_to_string(type_hint: type) -> str:
    """Converts a Python type object back into its string representation."""
    # This needs to handle various typing scenarios (List, Dict, Union, Optional, Literal, custom types)
    origin = typing.get_origin(type_hint)
    args = typing.get_args(type_hint)

    # Handle common cases first
    if origin is list or origin is list:
        if args:
            return f"list[{_format_type_to_string(args[0])}]"
        return "list[Any]"  # Or just "list"
    elif origin is dict or origin is dict:
        if args and len(args) == 2:
            return f"dict[{_format_type_to_string(args[0])}, {_format_type_to_string(args[1])}]"
        return "dict[Any, Any]"  # Or just "dict"
    elif origin is Union or origin is types.UnionType:
        # Handle Optional[T] as Union[T, NoneType]
        if len(args) == 2 and type(None) in args:
            inner_type = next(t for t in args if t is not type(None))
            return _format_type_to_string(inner_type)
        # return f"Optional[{_format_type_to_string(inner_type)}]"
        return (
            f"Union[{', '.join(_format_type_to_string(arg) for arg in args)}]"
        )
    elif origin is Literal:
        formatted_args = []
        for arg in args:
            if isinstance(arg, str):
                formatted_args.append(f"'{arg}'")
            else:
                formatted_args.append(str(arg))
        return f"Literal[{', '.join(formatted_args)}]"
    elif hasattr(
        type_hint, "__forward_arg__"
    ):  # Handle ForwardRefs if necessary
        return type_hint.__forward_arg__
    elif hasattr(type_hint, "__name__"):
        # Handle custom types registered in registry (get preferred name)
        registry = get_registry()
        for (
            name,
            reg_type,
        ) in registry._types.items():  # Access internal for lookup
            if reg_type == type_hint:
                return name  # Return registered name
        return type_hint.__name__  # Fallback to class name if not registered
    else:
        # Fallback for complex types or types not handled above
        type_repr = str(type_hint).replace("typing.", "")  # Basic cleanup
        type_repr = str(type_hint).replace("| None", "")
        type_repr = type_repr.strip()
        logger.debug(
            f"Using fallback string representation for type: {type_repr}"
        )
        return type_repr


def extract_identifiers_from_type_str(type_str: str) -> set[str]:
    """Extract all identifiers from a type annotation string using the AST."""
    tree = ast.parse(type_str, mode="eval")
    identifiers = set()

    class IdentifierVisitor(ast.NodeVisitor):
        def visit_Name(self, node):
            identifiers.add(node.id)

        def visit_Attribute(self, node):
            # Optionally support dotted names like mymodule.MyModel
            full_name = []
            while isinstance(node, ast.Attribute):
                full_name.append(node.attr)
                node = node.value
            if isinstance(node, ast.Name):
                full_name.append(node.id)
                identifiers.add(".".join(reversed(full_name)))

    IdentifierVisitor().visit(tree)
    return identifiers


def resolve_name(name: str):
    """Resolve a name to a Python object from loaded modules."""
    # Try dotted names first
    parts = name.split(".")
    obj = None

    if len(parts) == 1:
        # Search globals and builtins
        if parts[0] in globals():
            return globals()[parts[0]]
        if parts[0] in builtins.__dict__:
            return builtins.__dict__[parts[0]]
    else:
        try:
            obj = sys.modules[parts[0]]
            for part in parts[1:]:
                obj = getattr(obj, part)
            return obj
        except Exception:
            return None

    # Try all loaded modules' symbols
    for module in list(sys.modules.values()):
        if module is None or not hasattr(module, "__dict__"):
            continue
        if parts[0] in module.__dict__:
            return module.__dict__[parts[0]]

    return None


def extract_pydantic_models_from_type_string(
    type_str: str,
) -> list[type[BaseModel]]:
    identifiers = extract_identifiers_from_type_str(type_str)
    models = []
    for name in identifiers:
        resolved = resolve_name(name)
        if (
            isinstance(resolved, type)
            and issubclass(resolved, BaseModel)
            and resolved is not BaseModel
        ):
            models.append(resolved)
    return models


def collect_pydantic_models(
    type_hint, seen: set[type[BaseModel]] | None = None
) -> set[type[BaseModel]]:
    if seen is None:
        seen = set()

    origin = get_origin(type_hint)
    args = get_args(type_hint)

    # Direct BaseModel
    if isinstance(type_hint, type) and issubclass(type_hint, BaseModel):
        seen.add(type_hint)
        return seen

    # For Unions, Lists, Dicts, Tuples, etc.
    if origin is not None:
        for arg in args:
            collect_pydantic_models(arg, seen)

    return seen


def serialize_item(item: Any) -> Any:
    """Recursively prepares an item for serialization (e.g., to dict for YAML/JSON).
    Converts known callables to their path strings using FlockRegistry.
    Converts Pydantic models using model_dump.
    """
    # Import the registry lazily when needed
    from flock.core.flock_registry import get_registry

    FlockRegistry = get_registry()

    if callable(item) and not isinstance(item, type):
        path_str = FlockRegistry.get_callable_path_string(
            item
        )  # Use registry helper
        if path_str:
            # Store the simple name (last part of the path) for cleaner YAML/JSON
            simple_name = path_str.split(".")[-1]
            logger.debug(
                f"Serializing callable '{getattr(item, '__name__', 'unknown')}' as reference: '{simple_name}' (from path '{path_str}')"
            )
            return {
                "__callable_ref__": simple_name
            }  # Use simple name convention
        else:
            # Handle unregistered callables (e.g., lambdas defined inline)
            # Option 1: Raise error (stricter)
            # raise ValueError(f"Cannot serialize unregistered callable: {getattr(item, '__name__', item)}")
            # Option 2: Store string representation with warning (more lenient)
            logger.warning(
                f"Cannot serialize unregistered callable {getattr(item, '__name__', item)}, storing as string."
            )
            return str(item)
    elif isinstance(item, BaseModel):
        logger.debug(
            f"Serializing Pydantic model instance: {item.__class__.__name__}"
        )
        serialized_dict = {}
        # Iterate through defined fields in the model
        fields_to_iterate = {}
        if hasattr(item, "model_fields"):  # Pydantic v2
            fields_to_iterate = item.model_fields

        for field_name in fields_to_iterate:
            # Get the value *from the instance*
            try:
                value = getattr(item, field_name)
                if value is not None:  # Exclude None values
                    # Recursively serialize the field's value
                    serialized_dict[field_name] = serialize_item(value)
            except AttributeError:
                # Should not happen if iterating model_fields/__fields__ but handle defensively
                logger.warning(
                    f"Attribute '{field_name}' not found on instance of {item.__class__.__name__} during serialization."
                )
        return serialized_dict
    elif isinstance(item, Mapping):
        return {key: serialize_item(value) for key, value in item.items()}
    elif isinstance(item, Sequence) and not isinstance(item, str):
        return [serialize_item(sub_item) for sub_item in item]
    elif isinstance(
        item, type
    ):  # Handle type objects themselves (e.g. if stored directly)
        type_name = FlockRegistry.get_component_type_name(
            item
        )  # Check components first
        if type_name:
            return {"__component_ref__": type_name}
        type_name = FlockRegistry._get_path_string(
            item
        )  # Check regular types/classes by path
        if type_name:
            return {"__type_ref__": type_name}
        logger.warning(
            f"Could not serialize type object {item}, storing as string."
        )
        return str(item)
    elif isinstance(item, Enum):
        return item.value
    else:
        # Return basic types as is
        return item


# --- Deserialization Helper ---


def deserialize_item(item: Any) -> Any:
    """Recursively processes a deserialized item (e.g., from YAML/JSON dict).
    Converts reference dicts back to actual callables or types using FlockRegistry.
    Handles nested lists and dicts.
    """
    # Import the registry lazily when needed
    from flock.core.flock_registry import get_registry

    FlockRegistry = get_registry()

    if isinstance(item, Mapping):
        if "__callable_ref__" in item and len(item) == 1:
            ref_name = item["__callable_ref__"]
            try:
                # The registry's get_callable needs to handle lookup by simple name OR full path
                # Or we assume get_callable handles finding the right function from the simple name
                resolved_callable = FlockRegistry.get_callable(ref_name)
                logger.debug(
                    f"Deserialized callable reference '{ref_name}' to {resolved_callable}"
                )
                return resolved_callable
            except KeyError:
                logger.error(
                    f"Callable reference '{ref_name}' not found during deserialization."
                )
                return None  # Or raise?
            except Exception as e:
                logger.error(
                    f"Error resolving callable reference '{ref_name}': {e}",
                    exc_info=True,
                )
                return None
        elif "__component_ref__" in item and len(item) == 1:
            type_name = item["__component_ref__"]
            try:
                return FlockRegistry.get_component(type_name)
            except KeyError:
                logger.error(
                    f"Component reference '{type_name}' not found during deserialization."
                )
                return None
        elif "__type_ref__" in item and len(item) == 1:
            type_name = item["__type_ref__"]
            try:
                # For general types, use get_type or fallback to dynamic import like get_callable
                # Using get_type for now, assuming it needs registration
                return FlockRegistry.get_type(type_name)
            except KeyError:
                # Attempt dynamic import as fallback if get_type fails (similar to get_callable)
                try:
                    if "." not in type_name:  # Builtins?
                        mod = importlib.import_module("builtins")
                    else:
                        module_name, class_name = type_name.rsplit(".", 1)
                        mod = importlib.import_module(module_name)
                    type_obj = getattr(mod, class_name)
                    if isinstance(type_obj, type):
                        FlockRegistry.register_type(
                            type_obj, type_name
                        )  # Cache it
                        return type_obj
                    else:
                        raise TypeError()
                except Exception:
                    logger.error(
                        f"Type reference '{type_name}' not found in registry or via dynamic import."
                    )
                    return None

        else:
            # Recursively deserialize dictionary values
            return {key: deserialize_item(value) for key, value in item.items()}
    elif isinstance(item, Sequence) and not isinstance(item, str):
        return [deserialize_item(sub_item) for sub_item in item]
    else:
        # Return basic types as is
        return item


# --- Component Deserialization Helper ---
def deserialize_component(
    data: dict | None, expected_base_type: type
) -> Any | None:
    """Deserializes a component (Module, Evaluator, Router) from its dict representation.
    Uses the 'type' field to find the correct class via FlockRegistry.
    """
    # Import the registry and COMPONENT_BASE_TYPES lazily when needed
    from flock.core.flock_registry import COMPONENT_BASE_TYPES, get_registry

    FlockRegistry = get_registry()

    if data is None:
        return None
    if not isinstance(data, dict):
        logger.error(
            f"Expected dict for component deserialization, got {type(data)}"
        )
        return None

    type_name = data.get(
        "type"
    )  # Assuming 'type' key holds the class name string
    if not type_name:
        logger.error(f"Component data missing 'type' field: {data}")
        return None

    try:
        ComponentClass = FlockRegistry.get_component(type_name)  # Use registry
        # Optional: Keep the base type check
        if COMPONENT_BASE_TYPES and not issubclass(
            ComponentClass, expected_base_type
        ):
            raise TypeError(
                f"Deserialized class {type_name} is not a subclass of {expected_base_type.__name__}"
            )

        # Recursively deserialize the data *before* passing to Pydantic constructor
        # This handles nested callables/types within the component's config/data
        deserialized_data_for_init = {}
        for k, v in data.items():
            # Don't pass the 'type' field itself to the constructor if it matches class name
            if k == "type" and v == ComponentClass.__name__:
                continue
            deserialized_data_for_init[k] = deserialize_item(v)

        # Use Pydantic constructor directly. Assumes keys match field names.
        # from_dict could be added to components for more complex logic if needed.
        return ComponentClass(**deserialized_data_for_init)

    except (KeyError, TypeError, Exception) as e:
        logger.error(
            f"Failed to deserialize component of type '{type_name}': {e}",
            exc_info=True,
        )
        return None

```

---

### 98. src\flock\core\tools\azure_tools.py

- **File ID**: file_97
- **Type**: Code File
- **Line Count**: 497
- **Description**: File at src\flock\core\tools\azure_tools.py
- **Dependencies**: None
- **Used By**: None

**Content**:
```
import os
from typing import Any

from azure.core.credentials import AzureKeyCredential
from azure.search.documents import SearchClient
from azure.search.documents.indexes import SearchIndexClient
from azure.search.documents.indexes.models import (
    ExhaustiveKnnAlgorithmConfiguration,
    HnswAlgorithmConfiguration,
    SearchableField,
    SearchField,
    SearchFieldDataType,
    SearchIndex,
    SimpleField,
    VectorSearch,
    VectorSearchProfile,
)
from azure.search.documents.models import VectorizedQuery

from flock.core.logging.trace_and_logged import traced_and_logged


def _get_default_endpoint() -> str:
    """Get the default Azure Search endpoint from environment variables."""
    endpoint = os.environ.get("AZURE_SEARCH_ENDPOINT")
    if not endpoint:
        raise ValueError(
            "AZURE_SEARCH_ENDPOINT environment variable is not set"
        )
    return endpoint


def _get_default_api_key() -> str:
    """Get the default Azure Search API key from environment variables."""
    api_key = os.environ.get("AZURE_SEARCH_API_KEY")
    if not api_key:
        raise ValueError("AZURE_SEARCH_API_KEY environment variable is not set")
    return api_key


def _get_default_index_name() -> str:
    """Get the default Azure Search index name from environment variables."""
    index_name = os.environ.get("AZURE_SEARCH_INDEX_NAME")
    if not index_name:
        raise ValueError(
            "AZURE_SEARCH_INDEX_NAME environment variable is not set"
        )
    return index_name


@traced_and_logged
def azure_search_initialize_clients(
    endpoint: str | None = None,
    api_key: str | None = None,
    index_name: str | None = None,
) -> dict[str, Any]:
    """Initialize Azure AI Search clients.

    Args:
        endpoint: The Azure AI Search service endpoint URL (defaults to AZURE_SEARCH_ENDPOINT env var)
        api_key: The Azure AI Search API key (defaults to AZURE_SEARCH_API_KEY env var)
        index_name: Optional index name for SearchClient initialization (defaults to AZURE_SEARCH_INDEX_NAME env var if not None)

    Returns:
        Dictionary containing the initialized clients
    """
    # Use environment variables as defaults if not provided
    endpoint = endpoint or _get_default_endpoint()
    api_key = api_key or _get_default_api_key()

    credential = AzureKeyCredential(api_key)

    # Create the search index client
    search_index_client = SearchIndexClient(
        endpoint=endpoint, credential=credential
    )

    # Create clients dictionary
    clients = {
        "index_client": search_index_client,
    }

    # Add search client if index_name was provided or available in env
    if index_name is None and os.environ.get("AZURE_SEARCH_INDEX_NAME"):
        index_name = _get_default_index_name()

    if index_name:
        search_client = SearchClient(
            endpoint=endpoint, index_name=index_name, credential=credential
        )
        clients["search_client"] = search_client

    return clients


@traced_and_logged
def azure_search_create_index(
    index_name: str | None = None,
    fields: list[SearchField] = None,
    vector_search: VectorSearch | None = None,
    endpoint: str | None = None,
    api_key: str | None = None,
) -> dict[str, Any]:
    """Create a new search index in Azure AI Search.

    Args:
        index_name: Name of the search index to create (defaults to AZURE_SEARCH_INDEX_NAME env var)
        fields: List of field definitions for the index
        vector_search: Optional vector search configuration
        endpoint: The Azure AI Search service endpoint URL (defaults to AZURE_SEARCH_ENDPOINT env var)
        api_key: The Azure AI Search API key (defaults to AZURE_SEARCH_API_KEY env var)

    Returns:
        Dictionary containing information about the created index
    """
    # Use environment variables as defaults if not provided
    endpoint = endpoint or _get_default_endpoint()
    api_key = api_key or _get_default_api_key()
    index_name = index_name or _get_default_index_name()

    if fields is None:
        raise ValueError("Fields must be provided for index creation")

    clients = azure_search_initialize_clients(endpoint, api_key)
    index_client = clients["index_client"]

    # Create the index
    index = SearchIndex(
        name=index_name, fields=fields, vector_search=vector_search
    )

    result = index_client.create_or_update_index(index)

    return {
        "index_name": result.name,
        "fields": [field.name for field in result.fields],
        "created": True,
    }


@traced_and_logged
def azure_search_upload_documents(
    documents: list[dict[str, Any]],
    index_name: str | None = None,
    endpoint: str | None = None,
    api_key: str | None = None,
) -> dict[str, Any]:
    """Upload documents to an Azure AI Search index.

    Args:
        documents: List of documents to upload (as dictionaries)
        index_name: Name of the search index (defaults to AZURE_SEARCH_INDEX_NAME env var)
        endpoint: The Azure AI Search service endpoint URL (defaults to AZURE_SEARCH_ENDPOINT env var)
        api_key: The Azure AI Search API key (defaults to AZURE_SEARCH_API_KEY env var)

    Returns:
        Dictionary containing the upload results
    """
    # Use environment variables as defaults if not provided
    endpoint = endpoint or _get_default_endpoint()
    api_key = api_key or _get_default_api_key()
    index_name = index_name or _get_default_index_name()

    clients = azure_search_initialize_clients(endpoint, api_key, index_name)
    search_client = clients["search_client"]

    result = search_client.upload_documents(documents=documents)

    # Process results
    succeeded = sum(1 for r in result if r.succeeded)

    return {
        "succeeded": succeeded,
        "failed": len(result) - succeeded,
        "total": len(result),
    }


@traced_and_logged
def azure_search_query(
    search_text: str | None = None,
    filter: str | None = None,
    select: list[str] | None = None,
    top: int | None = 50,
    vector: list[float] | None = None,
    vector_field: str | None = None,
    vector_k: int | None = 10,
    index_name: str | None = None,
    endpoint: str | None = None,
    api_key: str | None = None,
) -> list[dict[str, Any]]:
    """Search documents in an Azure AI Search index.

    Args:
        search_text: Optional text to search for (keyword search)
        filter: Optional OData filter expression
        select: Optional list of fields to return
        top: Maximum number of results to return
        vector: Optional vector for vector search
        vector_field: Name of the field containing vectors for vector search
        vector_k: Number of nearest neighbors to retrieve in vector search
        index_name: Name of the search index (defaults to AZURE_SEARCH_INDEX_NAME env var)
        endpoint: The Azure AI Search service endpoint URL (defaults to AZURE_SEARCH_ENDPOINT env var)
        api_key: The Azure AI Search API key (defaults to AZURE_SEARCH_API_KEY env var)

    Returns:
        List of search results as dictionaries
    """
    # Use environment variables as defaults if not provided
    endpoint = endpoint or _get_default_endpoint()
    api_key = api_key or _get_default_api_key()
    index_name = index_name or _get_default_index_name()

    clients = azure_search_initialize_clients(endpoint, api_key, index_name)
    search_client = clients["search_client"]

    # Set up vector query if vector is provided
    vectorized_query = None
    if vector and vector_field:
        vectorized_query = VectorizedQuery(
            vector=vector, k=vector_k, fields=[vector_field]
        )

    # Execute the search
    results = search_client.search(
        search_text=search_text,
        filter=filter,
        select=select,
        top=top,
        vector_queries=[vectorized_query] if vectorized_query else None,
    )

    # Convert results to list of dictionaries
    # filter out the text_vector field
    result_list = [{**dict(result), "text_vector": ""} for result in results]

    return result_list


@traced_and_logged
def azure_search_get_document(
    key: str,
    select: list[str] | None = None,
    index_name: str | None = None,
    endpoint: str | None = None,
    api_key: str | None = None,
) -> dict[str, Any]:
    """Retrieve a specific document from an Azure AI Search index by key.

    Args:
        key: The unique key of the document to retrieve
        select: Optional list of fields to return
        index_name: Name of the search index (defaults to AZURE_SEARCH_INDEX_NAME env var)
        endpoint: The Azure AI Search service endpoint URL (defaults to AZURE_SEARCH_ENDPOINT env var)
        api_key: The Azure AI Search API key (defaults to AZURE_SEARCH_API_KEY env var)

    Returns:
        The retrieved document as a dictionary
    """
    # Use environment variables as defaults if not provided
    endpoint = endpoint or _get_default_endpoint()
    api_key = api_key or _get_default_api_key()
    index_name = index_name or _get_default_index_name()

    clients = azure_search_initialize_clients(endpoint, api_key, index_name)
    search_client = clients["search_client"]

    result = search_client.get_document(key=key, selected_fields=select)

    return dict(result)


@traced_and_logged
def azure_search_delete_documents(
    keys: list[str],
    key_field_name: str = "id",
    index_name: str | None = None,
    endpoint: str | None = None,
    api_key: str | None = None,
) -> dict[str, Any]:
    """Delete documents from an Azure AI Search index.

    Args:
        keys: List of document keys to delete
        key_field_name: Name of the key field (defaults to "id")
        index_name: Name of the search index (defaults to AZURE_SEARCH_INDEX_NAME env var)
        endpoint: The Azure AI Search service endpoint URL (defaults to AZURE_SEARCH_ENDPOINT env var)
        api_key: The Azure AI Search API key (defaults to AZURE_SEARCH_API_KEY env var)

    Returns:
        Dictionary containing the deletion results
    """
    # Use environment variables as defaults if not provided
    endpoint = endpoint or _get_default_endpoint()
    api_key = api_key or _get_default_api_key()
    index_name = index_name or _get_default_index_name()

    clients = azure_search_initialize_clients(endpoint, api_key, index_name)
    search_client = clients["search_client"]

    # Format documents for deletion (only need the key field)
    documents_to_delete = [{key_field_name: key} for key in keys]

    result = search_client.delete_documents(documents=documents_to_delete)

    # Process results
    succeeded = sum(1 for r in result if r.succeeded)

    return {
        "succeeded": succeeded,
        "failed": len(result) - succeeded,
        "total": len(result),
    }


@traced_and_logged
def azure_search_list_indexes(
    endpoint: str | None = None, api_key: str | None = None
) -> list[dict[str, Any]]:
    """List all indexes in the Azure AI Search service.

    Args:
        endpoint: The Azure AI Search service endpoint URL (defaults to AZURE_SEARCH_ENDPOINT env var)
        api_key: The Azure AI Search API key (defaults to AZURE_SEARCH_API_KEY env var)

    Returns:
        List of indexes as dictionaries
    """
    # Use environment variables as defaults if not provided
    endpoint = endpoint or _get_default_endpoint()
    api_key = api_key or _get_default_api_key()

    clients = azure_search_initialize_clients(endpoint, api_key)
    index_client = clients["index_client"]

    result = index_client.list_indexes()

    # Convert index objects to dictionaries with basic information
    indexes = [
        {
            "name": index.name,
            "fields": [field.name for field in index.fields],
            "field_count": len(index.fields),
        }
        for index in result
    ]

    return indexes


@traced_and_logged
def azure_search_get_index_statistics(
    index_name: str | None = None,
    endpoint: str | None = None,
    api_key: str | None = None,
) -> dict[str, Any]:
    """Get statistics for a specific Azure AI Search index.

    Args:
        index_name: Name of the search index (defaults to AZURE_SEARCH_INDEX_NAME env var)
        endpoint: The Azure AI Search service endpoint URL (defaults to AZURE_SEARCH_ENDPOINT env var)
        api_key: The Azure AI Search API key (defaults to AZURE_SEARCH_API_KEY env var)

    Returns:
        Dictionary containing index statistics
    """
    # Use environment variables as defaults if not provided
    endpoint = endpoint or _get_default_endpoint()
    api_key = api_key or _get_default_api_key()
    index_name = index_name or _get_default_index_name()

    clients = azure_search_initialize_clients(endpoint, api_key, index_name)
    search_client = clients["search_client"]

    stats = search_client.get_document_count()

    return {"document_count": stats}


@traced_and_logged
def azure_search_create_vector_index(
    fields: list[dict[str, Any]],
    vector_dimensions: int,
    index_name: str | None = None,
    algorithm_kind: str = "hnsw",
    endpoint: str | None = None,
    api_key: str | None = None,
) -> dict[str, Any]:
    """Create a vector search index in Azure AI Search.

    Args:
        fields: List of field configurations (dicts with name, type, etc.)
        vector_dimensions: Dimensions of the vector field
        index_name: Name of the search index (defaults to AZURE_SEARCH_INDEX_NAME env var)
        algorithm_kind: Vector search algorithm ("hnsw" or "exhaustive")
        endpoint: The Azure AI Search service endpoint URL (defaults to AZURE_SEARCH_ENDPOINT env var)
        api_key: The Azure AI Search API key (defaults to AZURE_SEARCH_API_KEY env var)

    Returns:
        Dictionary with index creation result
    """
    # Use environment variables as defaults if not provided
    endpoint = endpoint or _get_default_endpoint()
    api_key = api_key or _get_default_api_key()
    index_name = index_name or _get_default_index_name()

    clients = azure_search_initialize_clients(endpoint, api_key)
    index_client = clients["index_client"]

    # Convert field configurations to SearchField objects
    index_fields = []
    vector_fields = []

    for field_config in fields:
        field_name = field_config["name"]
        field_type = field_config["type"]
        field_searchable = field_config.get("searchable", False)
        field_filterable = field_config.get("filterable", False)
        field_sortable = field_config.get("sortable", False)
        field_key = field_config.get("key", False)
        field_vector = field_config.get("vector", False)

        if field_searchable and field_type == "string":
            field = SearchableField(
                name=field_name,
                type=SearchFieldDataType.String,
                key=field_key,
                filterable=field_filterable,
                sortable=field_sortable,
            )
        else:
            data_type = None
            if field_type == "string":
                data_type = SearchFieldDataType.String
            elif field_type == "int":
                data_type = SearchFieldDataType.Int32
            elif field_type == "double":
                data_type = SearchFieldDataType.Double
            elif field_type == "boolean":
                data_type = SearchFieldDataType.Boolean
            elif field_type == "collection":
                data_type = SearchFieldDataType.Collection(
                    SearchFieldDataType.String
                )

            field = SimpleField(
                name=field_name,
                type=data_type,
                key=field_key,
                filterable=field_filterable,
                sortable=field_sortable,
            )

        index_fields.append(field)

        if field_vector:
            vector_fields.append(field_name)

    # Set up vector search configuration
    algorithm_config = None
    if algorithm_kind.lower() == "hnsw":
        algorithm_config = HnswAlgorithmConfiguration(
            name="hnsw-config",
            parameters={"m": 4, "efConstruction": 400, "efSearch": 500},
        )
    else:
        algorithm_config = ExhaustiveKnnAlgorithmConfiguration(
            name="exhaustive-config"
        )

    # Create vector search configuration
    vector_search = VectorSearch(
        algorithms=[algorithm_config],
        profiles=[
            VectorSearchProfile(
                name="vector-profile",
                algorithm_configuration_name=algorithm_config.name,
            )
        ],
    )

    # Create the search index
    index = SearchIndex(
        name=index_name, fields=index_fields, vector_search=vector_search
    )

    try:
        result = index_client.create_or_update_index(index)
        return {
            "index_name": result.name,
            "vector_fields": vector_fields,
            "vector_dimensions": vector_dimensions,
            "algorithm": algorithm_kind,
            "created": True,
        }
    except Exception as e:
        return {"error": str(e), "created": False}

```

---

### 99. src\flock\core\tools\basic_tools.py

- **File ID**: file_98
- **Type**: Code File
- **Line Count**: 317
- **Description**: This module contains basic agentic tools for performing various tasks.
- **Dependencies**: None
- **Used By**: None

**Content**:
```
"""This module contains basic agentic tools for performing various tasks."""

import importlib
import json
import os
import re
from typing import Any, Literal

from flock.core.interpreter.python_interpreter import PythonInterpreter
from flock.core.logging.trace_and_logged import traced_and_logged


@traced_and_logged
def web_search_tavily(query: str):
    if importlib.util.find_spec("tavily") is not None:
        from tavily import TavilyClient

        client = TavilyClient(api_key=os.getenv("TAVILY_API_KEY"))
        try:
            response = client.search(query, include_answer=True)  # type: ignore
            return response
        except Exception:
            raise
    else:
        raise ImportError(
            "Optional tool dependencies not installed. Install with 'pip install flock-core[tools]'."
        )


@traced_and_logged
def web_search_duckduckgo(
    keywords: str, search_type: Literal["news", "web"] = "web"
):
    try:
        from duckduckgo_search import DDGS

        if search_type == "news":
            response = DDGS().news(keywords)
        else:
            response = DDGS().text(keywords)

        return response
    except Exception:
        raise


@traced_and_logged
def web_search_bing(keywords: str):
    try:
        import httpx

        subscription_key = os.environ["BING_SEARCH_V7_SUBSCRIPTION_KEY"]
        endpoint = "https://api.bing.microsoft.com/v7.0/search"

        # Query term(s) to search for.
        query = keywords

        # Construct a request
        mkt = "en-US"
        params = {"q": query, "mkt": mkt}
        headers = {"Ocp-Apim-Subscription-Key": subscription_key}

        response = httpx.get(endpoint, headers=headers, params=params)
        response.raise_for_status()
        search_results = response.json()
        return search_results["webPages"]
    except Exception:
        raise


def extract_links_from_markdown(markdown: str, url: str) -> list:
    # Regular expression to find all markdown links
    link_pattern = re.compile(r"\[([^\]]+)\]\(([^)]+)\)")
    links = link_pattern.findall(markdown)
    return [url + link[1] for link in links]


@traced_and_logged
def get_web_content_as_markdown(url: str) -> str:
    if (
        importlib.util.find_spec("httpx") is not None
        and importlib.util.find_spec("markdownify") is not None
    ):
        import httpx
        from markdownify import markdownify as md

        try:
            response = httpx.get(url)
            response.raise_for_status()
            markdown = md(response.text)
            return markdown
        except Exception:
            raise
    else:
        raise ImportError(
            "Optional tool dependencies not installed. Install with 'pip install flock-core[tools]'."
        )


@traced_and_logged
def get_anything_as_markdown(url_or_file_path: str):
    if importlib.util.find_spec("docling") is not None:
        from docling.document_converter import DocumentConverter

        try:
            converter = DocumentConverter()
            result = converter.convert(url_or_file_path)
            markdown = result.document.export_to_markdown()
            return markdown
        except Exception:
            raise
    else:
        raise ImportError(
            "Optional tool dependencies not installed. Install with 'pip install flock-core[all-tools]'."
        )


@traced_and_logged
def evaluate_math(expression: str) -> float:
    try:
        result = PythonInterpreter(
            {},
            [
                "os",
                "math",
                "random",
                "datetime",
                "time",
                "string",
                "collections",
                "itertools",
                "functools",
                "typing",
                "enum",
                "json",
                "ast",
            ],
            verbose=True,
        ).execute(expression)
        return result
    except Exception:
        raise


@traced_and_logged
def code_eval(python_code: str) -> str:
    try:
        result = PythonInterpreter(
            {},
            [
                "os",
                "math",
                "random",
                "datetime",
                "time",
                "string",
                "collections",
                "itertools",
                "functools",
                "typing",
                "enum",
                "json",
                "ast",
            ],
            verbose=True,
        ).execute(python_code)
        return result
    except Exception:
        raise


@traced_and_logged
def get_current_time() -> str:
    import datetime

    time = datetime.datetime.now().isoformat()
    return time


@traced_and_logged
def count_words(text: str) -> int:
    count = len(text.split())
    return count


@traced_and_logged
def extract_urls(text: str) -> list[str]:
    import re

    url_pattern = r"https?://(?:[-\w.]|(?:%[\da-fA-F]{2}))+"
    urls = re.findall(url_pattern, text)
    return urls


@traced_and_logged
def extract_numbers(text: str) -> list[float]:
    import re

    numbers = [float(x) for x in re.findall(r"-?\d*\.?\d+", text)]
    return numbers


@traced_and_logged
def json_parse_safe(text: str) -> dict:
    try:
        result = json.loads(text)
        return result
    except Exception:
        return {}


@traced_and_logged
def save_to_file(content: str, filename: str):
    try:
        with open(filename, "w") as f:
            f.write(content)
    except Exception:
        raise


@traced_and_logged
def read_from_file(filename: str) -> str:
    with open(filename, encoding="utf-8") as file:
        return file.read()


@traced_and_logged
def json_search(
    json_file_path: str, search_query: str, case_sensitive: bool = False
) -> list:
    """Search a JSON file for objects containing the specified search query.

    Args:
        json_file_path (str): Path to the JSON file to search
        search_query (str): Text to search for within the JSON objects
        case_sensitive (bool, optional): Whether to perform a case-sensitive search. Defaults to False.

    Returns:
        list: List of JSON objects (as dicts) that contain the search query

    Example:
        >>> matching_tickets = json_search("tickets.json", "error 404")
        >>> print(
        ...     f"Found {len(matching_tickets)} tickets mentioning '404 error'"
        ... )
    """
    try:
        # Read the JSON file
        file_content = read_from_file(json_file_path)

        # Parse the JSON content
        json_data = json_parse_safe(file_content)

        # Convert search query to lowercase if case-insensitive search
        if not case_sensitive:
            search_query = search_query.lower()

        results = []

        # Determine if the JSON root is an object or array
        if isinstance(json_data, dict):
            # Handle case where root is a dictionary object
            for key, value in json_data.items():
                if isinstance(value, list):
                    # If this key contains a list of objects, search within them
                    matching_items = _search_in_list(
                        value, search_query, case_sensitive
                    )
                    results.extend(matching_items)
                elif _contains_text(value, search_query, case_sensitive):
                    # The entire object matches
                    results.append(json_data)
                    break
        elif isinstance(json_data, list):
            # Handle case where root is an array
            matching_items = _search_in_list(
                json_data, search_query, case_sensitive
            )
            results.extend(matching_items)

        return results

    except Exception as e:
        return [{"error": f"Error searching JSON file: {e!s}"}]


def _search_in_list(
    items: list, search_query: str, case_sensitive: bool
) -> list:
    """Helper function to search for text in a list of items."""
    matching_items = []
    for item in items:
        if _contains_text(item, search_query, case_sensitive):
            matching_items.append(item)
    return matching_items


def _contains_text(obj: Any, search_query: str, case_sensitive: bool) -> bool:
    """Recursively check if an object contains the search query in any of its string values."""
    if isinstance(obj, str):
        # For string values, check if they contain the search query
        if case_sensitive:
            return search_query in obj
        else:
            return search_query in obj.lower()
    elif isinstance(obj, dict):
        # For dictionaries, check each value
        for value in obj.values():
            if _contains_text(value, search_query, case_sensitive):
                return True
    elif isinstance(obj, list):
        # For lists, check each item
        for item in obj:
            if _contains_text(item, search_query, case_sensitive):
                return True
    # For other types (numbers, booleans, None), return False
    return False

```

---

### 100. src\flock\core\tools\dev_tools\github.py

- **File ID**: file_99
- **Type**: Code File
- **Line Count**: 157
- **Description**: This module provides tools for interacting with GitHub repositories.
- **Dependencies**: None
- **Used By**: None

**Content**:
```
"""This module provides tools for interacting with GitHub repositories."""

import base64
import os

import httpx

from flock.core.logging.trace_and_logged import traced_and_logged


@traced_and_logged
def create_user_stories_as_github_issue(title: str, body: str) -> str:
    github_pat = os.getenv("GITHUB_PAT")
    github_repo = os.getenv("GITHUB_REPO")

    url = f"https://api.github.com/repos/{github_repo}/issues"
    headers = {
        "Authorization": f"Bearer {github_pat}",
        "Accept": "application/vnd.github+json",
    }
    issue_title = title
    issue_body = body

    payload = {"title": issue_title, "body": issue_body}
    response = httpx.post(url, json=payload, headers=headers)

    if response.status_code == 201:
        return "Issue created successfully."
    else:
        return "Failed to create issue. Please try again later."


@traced_and_logged
def upload_readme(content: str):
    GITHUB_USERNAME = os.getenv("GITHUB_USERNAME")
    REPO_NAME = os.getenv("GITHUB_REPO")
    GITHUB_TOKEN = os.getenv("GITHUB_PAT")

    if not GITHUB_USERNAME or not REPO_NAME or not GITHUB_TOKEN:
        raise ValueError(
            "Missing environment variables: GITHUB_USERNAME, GITHUB_REPO, or GITHUB_PAT"
        )

    GITHUB_API_URL = f"https://api.github.com/repos/{GITHUB_USERNAME}/{REPO_NAME}/contents/README.md"

    encoded_content = base64.b64encode(content.encode()).decode()

    with httpx.Client() as client:
        response = client.get(
            GITHUB_API_URL,
            headers={
                "Authorization": f"Bearer {GITHUB_TOKEN}",
                "Accept": "application/vnd.github.v3+json",
            },
        )

        data = response.json()
        sha = data.get("sha", None)

        payload = {
            "message": "Updating README.md",
            "content": encoded_content,
            "branch": "main",
        }

        if sha:
            payload["sha"] = sha

        response = client.put(
            GITHUB_API_URL,
            json=payload,
            headers={
                "Authorization": f"Bearer {GITHUB_TOKEN}",
                "Accept": "application/vnd.github.v3+json",
            },
        )

        if response.status_code in [200, 201]:
            print("README.md successfully uploaded/updated!")
        else:
            print("Failed to upload README.md:", response.json())


@traced_and_logged
def create_files(file_paths) -> str:
    """Create multiple files in a GitHub repository with a predefined content.

    This function iterates over a list of file paths (relative to the repository root) and creates
    each file in the specified GitHub repository with the content "#created by flock". For each file,
    it checks whether the file already exists; if it does, that file is skipped. The function
    uses the following environment variables for authentication and repository information:

      - GITHUB_USERNAME: Your GitHub username.
      - GITHUB_REPO: The name of the repository.
      - GITHUB_PAT: Your GitHub Personal Access Token for authentication.

    Parameters:
        file_paths (list of str): A list of file paths (relative to the repository root) to be created.

    Returns:
        str: A message indicating whether the files were created successfully or if there was a failure.
    """
    try:
        GITHUB_USERNAME = os.getenv("GITHUB_USERNAME")
        REPO_NAME = os.getenv("GITHUB_REPO")
        GITHUB_TOKEN = os.getenv("GITHUB_PAT")

        if not GITHUB_USERNAME or not REPO_NAME or not GITHUB_TOKEN:
            raise ValueError(
                "Missing environment variables: GITHUB_USERNAME, GITHUB_REPO, or GITHUB_PAT"
            )

        encoded_content = base64.b64encode(b"#created by flock").decode()

        with httpx.Client() as client:
            for file_path in file_paths:
                GITHUB_API_URL = f"https://api.github.com/repos/{GITHUB_USERNAME}/{REPO_NAME}/contents/{file_path}"

                response = client.get(
                    GITHUB_API_URL,
                    headers={
                        "Authorization": f"token {GITHUB_TOKEN}",
                        "Accept": "application/vnd.github.v3+json",
                    },
                )

                data = response.json()
                sha = data.get("sha", None)

                payload = {
                    "message": f"Creating {file_path}",
                    "content": encoded_content,
                    "branch": "main",
                }

                if sha:
                    print(f"Skipping {file_path}, file already exists.")
                    continue

                response = client.put(
                    GITHUB_API_URL,
                    json=payload,
                    headers={
                        "Authorization": f"token {GITHUB_TOKEN}",
                        "Accept": "application/vnd.github.v3+json",
                    },
                )

                if response.status_code in [200, 201]:
                    print(f"{file_path} successfully created!")
                else:
                    print(f"Failed to create {file_path}:", response.json())

        return "Files created successfully."

    except Exception:
        return "Failed to create file. Please try again later."

```

---

### 101. src\flock\core\tools\llm_tools.py

- **File ID**: file_100
- **Type**: Code File
- **Line Count**: 788
- **Description**: File at src\flock\core\tools\llm_tools.py
- **Dependencies**: None
- **Used By**: None

**Content**:
```
import hashlib
import json
import re
from collections.abc import Callable
from typing import Any

import nltk

from flock.core.logging.trace_and_logged import traced_and_logged

# Ensure NLTK data is downloaded
try:
    nltk.data.find("tokenizers/punkt")
except LookupError:
    nltk.download("punkt")

try:
    nltk.data.find("corpora/stopwords")
except LookupError:
    nltk.download("stopwords")


@traced_and_logged
def split_by_sentences(text: str) -> list[str]:
    return nltk.sent_tokenize(text)


@traced_and_logged
def split_by_characters(
    text: str, chunk_size: int = 4000, overlap: int = 200
) -> list[str]:
    if chunk_size <= 0:
        raise ValueError("chunk_size must be positive")

    if overlap >= chunk_size:
        raise ValueError("overlap must be smaller than chunk_size")

    if not text:
        return []

    chunks = []
    start = 0
    text_length = len(text)

    while start < text_length:
        end = min(start + chunk_size, text_length)

        # If we're not at the end and the next character isn't a space, try to find a suitable break point
        if end < text_length and text[end] not in [
            " ",
            "\n",
            ".",
            ",",
            "!",
            "?",
            ";",
            ":",
            "-",
        ]:
            # Look for the last occurrence of a good break character
            break_chars = [" ", "\n", ".", ",", "!", "?", ";", ":", "-"]
            for i in range(end, max(start, end - 100), -1):
                if text[i] in break_chars:
                    end = i + 1  # Include the break character
                    break

        chunks.append(text[start:end])
        start = end - overlap if end < text_length else text_length

    return chunks


@traced_and_logged
def split_by_tokens(
    text: str,
    tokenizer: Callable[[str], list[str]],
    max_tokens: int = 1024,
    overlap_tokens: int = 100,
) -> list[str]:
    tokens = tokenizer(text)
    chunks = []

    i = 0
    while i < len(tokens):
        chunk = tokens[i : i + max_tokens]
        chunks.append("".join(chunk))
        i += max_tokens - overlap_tokens

    return chunks


@traced_and_logged
def split_by_separator(text: str, separator: str = "\n\n") -> list[str]:
    if not text:
        return []

    chunks = text.split(separator)
    return [chunk for chunk in chunks if chunk.strip()]


@traced_and_logged
def recursive_text_splitter(
    text: str,
    chunk_size: int = 4000,
    separators: list[str] = ["\n\n", "\n", ". ", ", ", " ", ""],
    keep_separator: bool = True,
) -> list[str]:
    if not text:
        return []

    if len(text) <= chunk_size:
        return [text]

    if not separators:
        return [
            text[:chunk_size],
            *recursive_text_splitter(text[chunk_size:], chunk_size, separators),
        ]

    separator = separators[0]
    new_separators = separators[1:]

    if separator == "":
        # If we're at the character level, just split by characters
        return split_by_characters(text, chunk_size=chunk_size, overlap=0)

    splits = text.split(separator)
    separator_len = len(separator) if keep_separator else 0

    # Add separator back to the chunks if needed
    if keep_separator and separator:
        splits = [f"{split}{separator}" for split in splits[:-1]] + [splits[-1]]

    # Process each split
    result = []
    current_chunk = []
    current_length = 0

    for split in splits:
        split_len = len(split)

        if split_len > chunk_size:
            # If current split is too large, handle current chunk and recursively split this large piece
            if current_chunk:
                result.append("".join(current_chunk))
                current_chunk = []
                current_length = 0

            # Recursively split this large piece
            smaller_chunks = recursive_text_splitter(
                split, chunk_size, new_separators, keep_separator
            )
            result.extend(smaller_chunks)
        elif current_length + split_len <= chunk_size:
            # If we can fit this split in the current chunk, add it
            current_chunk.append(split)
            current_length += split_len
        else:
            # If we can't fit this split, complete the current chunk and start a new one
            result.append("".join(current_chunk))
            current_chunk = [split]
            current_length = split_len

    # Don't forget the last chunk
    if current_chunk:
        result.append("".join(current_chunk))

    return result


@traced_and_logged
def chunk_text_for_embedding(
    text: str, file_name: str, chunk_size: int = 1000, overlap: int = 100
) -> list[dict[str, Any]]:
    chunks = split_by_characters(text, chunk_size=chunk_size, overlap=overlap)

    # Create metadata for each chunk
    result = []
    for i, chunk in enumerate(chunks):
        result.append(
            {
                "chunk_id": file_name + "_" + str(i),
                "text": chunk,
                "file": file_name,
                "total_chunks": len(chunks),
            }
        )

    return result


@traced_and_logged
def split_code_by_functions(code: str) -> list[dict[str, Any]]:
    if not code:
        return []

    # Basic pattern for Python functions
    function_pattern = re.compile(
        r"(^|\n)def\s+([a-zA-Z_][a-zA-Z0-9_]*)\s*\((.*?)\)(?:\s*->.*?)?:"
    )
    matches = list(function_pattern.finditer(code))

    if not matches:
        return [{"name": "Main", "content": code, "type": "code"}]

    functions = []

    # Process each function
    for i in range(len(matches)):
        current_match = matches[i]
        function_name = current_match.group(2)

        # Determine function content
        if i < len(matches) - 1:
            next_function_start = matches[i + 1].start()
            content = code[current_match.start() : next_function_start]
        else:
            content = code[current_match.start() :]

        functions.append(
            {
                "name": function_name,
                "content": content.strip(),
                "type": "function",
            }
        )

    # Check if there's content before the first function
    if matches[0].start() > 0:
        preamble = code[: matches[0].start()].strip()
        if preamble:
            functions.insert(
                0,
                {"name": "Imports/Setup", "content": preamble, "type": "code"},
            )

    return functions


@traced_and_logged
def count_tokens(text: str, model: str = "gpt-3.5-turbo") -> int:
    """Count tokens using tiktoken."""
    if not text:
        return 0

    try:
        import tiktoken

        # Map model names to encoding types
        if model.startswith(("gpt-4", "gpt-3.5")):
            encoding_name = "cl100k_base"  # For newer OpenAI models
        elif model.startswith("text-davinci"):
            encoding_name = "p50k_base"  # For older OpenAI models
        elif "llama" in model.lower() or "mistral" in model.lower():
            encoding_name = (
                "cl100k_base"  # Best approximation for LLaMA/Mistral
            )
        else:
            # Default to cl100k_base as fallback
            encoding_name = "cl100k_base"

        # Try to get the specific encoder for the model if available
        try:
            encoding = tiktoken.encoding_for_model(model)
        except KeyError:
            # Fall back to the encoding name
            encoding = tiktoken.get_encoding(encoding_name)

        # Count tokens
        token_integers = encoding.encode(text)
        return len(token_integers)

    except ImportError:
        # Fallback to character-based estimation if tiktoken is not installed
        return count_tokens_estimate(text, model)


@traced_and_logged
def count_tokens_estimate(text: str, model: str = "gpt-3.5-turbo") -> int:
    """Estimate token count for different models."""
    if not text:
        return 0

    # Rough token estimations for different models
    if model.startswith(("gpt-3", "gpt-4")):
        # OpenAI models: ~4 chars per token
        return len(text) // 4 + 1
    elif model.startswith("claude"):
        # Anthropic models: ~3.5 chars per token
        return len(text) // 3.5 + 1
    elif "llama" in model.lower():
        # LLaMA-based models: ~3.7 chars per token
        return len(text) // 3.7 + 1
    else:
        # Default estimation
        return len(text) // 4 + 1


@traced_and_logged
def truncate_to_token_limit(
    text: str, max_tokens: int = 4000, model: str = "gpt-3.5-turbo"
) -> str:
    if not text:
        return ""

    # Try to use tiktoken for accurate truncation
    try:
        import tiktoken

        # Get appropriate encoding
        try:
            encoding = tiktoken.encoding_for_model(model)
        except KeyError:
            # Fall back to cl100k_base (used by most newer models)
            encoding = tiktoken.get_encoding("cl100k_base")

        # Encode the text to tokens
        tokens = encoding.encode(text)

        # If we're already under the limit, return the original text
        if len(tokens) <= max_tokens:
            return text

        # Truncate tokens and decode back to text
        truncated_tokens = tokens[:max_tokens]
        return encoding.decode(truncated_tokens)

    except ImportError:
        # Fallback to the character-based method if tiktoken is not available
        estimated_tokens = count_tokens_estimate(text, model)

        if estimated_tokens <= max_tokens:
            return text

        # Calculate approximate character limit
        char_per_token = 4  # Default for most models
        if model.startswith("claude"):
            char_per_token = 3.5
        elif "llama" in model.lower():
            char_per_token = 3.7

        char_limit = int(max_tokens * char_per_token)

        # Try to find a good breaking point
        if char_limit < len(text):
            # Look for sentence or paragraph break near the limit
            for i in range(char_limit - 1, max(0, char_limit - 100), -1):
                if i < len(text) and text[i] in [".", "!", "?", "\n"]:
                    return text[: i + 1]

        # Fallback to hard truncation
        return text[:char_limit]


@traced_and_logged
def extract_keywords(text: str, top_n: int = 10) -> list[str]:
    if not text:
        return []

    # Get stopwords
    try:
        from nltk.corpus import stopwords

        stop_words = set(stopwords.words("english"))
    except:
        # Fallback basic stopwords if NLTK data isn't available
        stop_words = {
            "i",
            "me",
            "my",
            "myself",
            "we",
            "our",
            "ours",
            "ourselves",
            "you",
            "you're",
            "you've",
            "you'll",
            "you'd",
            "your",
            "yours",
            "yourself",
            "yourselves",
            "he",
            "him",
            "his",
            "himself",
            "she",
            "she's",
            "her",
            "hers",
            "herself",
            "it",
            "it's",
            "its",
            "itself",
            "they",
            "them",
            "their",
            "theirs",
            "themselves",
            "what",
            "which",
            "who",
            "whom",
            "this",
            "that",
            "that'll",
            "these",
            "those",
            "am",
            "is",
            "are",
            "was",
            "were",
            "be",
            "been",
            "being",
            "have",
            "has",
            "had",
            "having",
            "do",
            "does",
            "did",
            "doing",
            "a",
            "an",
            "the",
            "and",
            "but",
            "if",
            "or",
            "because",
            "as",
            "until",
            "while",
            "of",
            "at",
            "by",
            "for",
            "with",
            "about",
            "against",
            "between",
            "into",
            "through",
            "during",
            "before",
            "after",
            "above",
            "below",
            "to",
            "from",
            "up",
            "down",
            "in",
            "out",
            "on",
            "off",
            "over",
            "under",
            "again",
            "further",
            "then",
            "once",
        }

    # Tokenize and remove punctuation
    words = re.findall(r"\b[a-zA-Z]{3,}\b", text.lower())

    # Remove stopwords
    words = [word for word in words if word not in stop_words]

    # Count word frequencies
    word_freq = {}
    for word in words:
        if word in word_freq:
            word_freq[word] += 1
        else:
            word_freq[word] = 1

    # Sort by frequency
    sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)

    # Return top N keywords
    return [word for word, freq in sorted_words[:top_n]]


@traced_and_logged
def clean_text(
    text: str,
    remove_urls: bool = True,
    remove_html: bool = True,
    normalize_whitespace: bool = True,
) -> str:
    if not text:
        return ""

    result = text

    # Remove URLs
    if remove_urls:
        result = re.sub(r"https?://\S+|www\.\S+", "", result)

    # Remove HTML tags
    if remove_html:
        result = re.sub(r"<.*?>", "", result)

    # Normalize whitespace
    if normalize_whitespace:
        # Replace multiple spaces, tabs, newlines with a single space
        result = re.sub(r"\s+", " ", result)
        result = result.strip()

    return result


@traced_and_logged
def format_chat_history(
    messages: list[dict[str, str]],
    format_type: str = "text",
    system_prefix: str = "System: ",
    user_prefix: str = "User: ",
    assistant_prefix: str = "Assistant: ",
) -> str:
    if not messages:
        return ""

    result = []

    if format_type == "text":
        for msg in messages:
            role = msg.get("role", "").lower()
            content = msg.get("content", "")

            if role == "system":
                result.append(f"{system_prefix}{content}")
            elif role == "user":
                result.append(f"{user_prefix}{content}")
            elif role == "assistant":
                result.append(f"{assistant_prefix}{content}")
            else:
                result.append(f"{role.capitalize()}: {content}")

        return "\n\n".join(result)

    elif format_type == "markdown":
        for msg in messages:
            role = msg.get("role", "").lower()
            content = msg.get("content", "")

            if role == "system":
                result.append(f"**{system_prefix.strip()}** {content}")
            elif role == "user":
                result.append(f"**{user_prefix.strip()}** {content}")
            elif role == "assistant":
                result.append(f"**{assistant_prefix.strip()}** {content}")
            else:
                result.append(f"**{role.capitalize()}:** {content}")

        return "\n\n".join(result)

    else:
        raise ValueError(f"Unsupported format type: {format_type}")


@traced_and_logged
def extract_json_from_text(text: str) -> dict[str, Any] | None:
    if not text:
        return None

    # Find JSON-like patterns between curly braces
    json_pattern = re.compile(r"({[\s\S]*?})")
    json_matches = json_pattern.findall(text)

    # Try to parse each match
    for json_str in json_matches:
        try:
            return json.loads(json_str)
        except json.JSONDecodeError:
            continue

    # Try to find JSON with markdown code blocks
    code_block_pattern = re.compile(r"```(?:json)?\s*([\s\S]*?)\s*```")
    code_blocks = code_block_pattern.findall(text)

    for block in code_blocks:
        # Clean up any trailing ``` that might have been captured
        block = block.replace("```", "")
        try:
            return json.loads(block)
        except json.JSONDecodeError:
            continue

    # No valid JSON found
    return None


@traced_and_logged
def calculate_text_hash(text: str, algorithm: str = "sha256") -> str:
    if not text:
        return ""

    if algorithm == "md5":
        return hashlib.md5(text.encode()).hexdigest()
    elif algorithm == "sha1":
        return hashlib.sha1(text.encode()).hexdigest()
    elif algorithm == "sha256":
        return hashlib.sha256(text.encode()).hexdigest()
    else:
        raise ValueError(f"Unsupported hash algorithm: {algorithm}")


@traced_and_logged
def format_table_from_dicts(data: list[dict[str, Any]]) -> str:
    if not data:
        return ""

    # Extract all possible keys
    keys = set()
    for item in data:
        keys.update(item.keys())

    # Convert to list and sort for consistent output
    keys = sorted(list(keys))

    # Calculate column widths
    widths = {key: len(key) for key in keys}
    for item in data:
        for key in keys:
            if key in item:
                value_str = str(item[key])
                widths[key] = max(widths[key], len(value_str))

    # Create header
    header = " | ".join(f"{key:{widths[key]}}" for key in keys)
    separator = "-+-".join("-" * widths[key] for key in keys)

    # Create rows
    rows = []
    for item in data:
        row = " | ".join(f"{item.get(key, '')!s:{widths[key]}}" for key in keys)
        rows.append(row)

    # Combine everything
    return f"{header}\n{separator}\n" + "\n".join(rows)


@traced_and_logged
def detect_language(text: str) -> str:
    """Simple language detection"""
    if not text or len(text.strip()) < 10:
        return "unknown"

    try:
        # Try to use langdetect if available
        from langdetect import detect

        return detect(text)
    except ImportError:
        # Fallback to simple detection based on character frequency
        # This is very simplistic and only works for a few common languages
        text = text.lower()

        # Count character frequencies that may indicate certain languages
        special_chars = {
            "á": 0,
            "é": 0,
            "í": 0,
            "ó": 0,
            "ú": 0,
            "ü": 0,
            "ñ": 0,  # Spanish
            "ä": 0,
            "ö": 0,
            "ß": 0,  # German
            "ç": 0,
            "à": 0,
            "è": 0,
            "ù": 0,  # French
            "å": 0,
            "ø": 0,  # Nordic
            "й": 0,
            "ы": 0,
            "ъ": 0,
            "э": 0,  # Russian/Cyrillic
            "的": 0,
            "是": 0,
            "在": 0,  # Chinese
            "の": 0,
            "は": 0,
            "で": 0,  # Japanese
            "한": 0,
            "국": 0,
            "어": 0,  # Korean
        }

        for char in text:
            if char in special_chars:
                special_chars[char] += 1

        # Detect based on character frequencies
        spanish = sum(
            special_chars[c] for c in ["á", "é", "í", "ó", "ú", "ü", "ñ"]
        )
        german = sum(special_chars[c] for c in ["ä", "ö", "ß"])
        french = sum(special_chars[c] for c in ["ç", "à", "è", "ù"])
        nordic = sum(special_chars[c] for c in ["å", "ø"])
        russian = sum(special_chars[c] for c in ["й", "ы", "ъ", "э"])
        chinese = sum(special_chars[c] for c in ["的", "是", "在"])
        japanese = sum(special_chars[c] for c in ["の", "は", "で"])
        korean = sum(special_chars[c] for c in ["한", "국", "어"])

        scores = {
            "es": spanish,
            "de": german,
            "fr": french,
            "no": nordic,
            "ru": russian,
            "zh": chinese,
            "ja": japanese,
            "ko": korean,
        }

        # If we have a clear signal from special characters
        max_score = max(scores.values())
        if max_score > 0:
            return max(scores, key=scores.get)

        # Otherwise assume English (very simplistic)
        return "en"


@traced_and_logged
def tiktoken_split(
    text: str,
    model: str = "gpt-3.5-turbo",
    chunk_size: int = 1000,
    overlap: int = 50,
) -> list[str]:
    """Split text based on tiktoken tokens with proper overlap handling."""
    if not text:
        return []

    try:
        import tiktoken

        try:
            encoding = tiktoken.encoding_for_model(model)
        except KeyError:
            encoding = tiktoken.get_encoding("cl100k_base")

        # Encode the text to tokens
        tokens = encoding.encode(text)
        total_tokens = len(tokens)

        # Check if we need to split at all
        if total_tokens <= chunk_size:
            return [text]

        # Create chunks with overlap
        chunks = []
        start_idx = 0

        while start_idx < total_tokens:
            # Define the end of this chunk
            end_idx = min(start_idx + chunk_size, total_tokens)

            # Decode this chunk of tokens back to text
            chunk_tokens = tokens[start_idx:end_idx]
            chunk_text = encoding.decode(chunk_tokens)
            chunks.append(chunk_text)

            # Move to the next chunk, accounting for overlap
            start_idx += chunk_size - overlap

            # Avoid tiny final chunks
            if start_idx < total_tokens and start_idx + overlap >= total_tokens:
                break

        return chunks
    except ImportError:
        # Fallback to character-based chunking if tiktoken is not available
        return split_by_characters(
            text, chunk_size=chunk_size * 4, overlap=overlap * 4
        )

```

---

### 102. src\flock\core\tools\markdown_tools.py

- **File ID**: file_101
- **Type**: Code File
- **Line Count**: 195
- **Description**: File at src\flock\core\tools\markdown_tools.py
- **Dependencies**: None
- **Used By**: None

**Content**:
```
import re
from typing import Any

from flock.core.logging.trace_and_logged import traced_and_logged


@traced_and_logged
def split_markdown_by_headers(
    markdown_text: str, min_header_level: int = 1, max_header_level: int = 2
) -> list[dict[str, Any]]:
    if not markdown_text:
        return []

    # Pattern to match headers from level min_header_level to max_header_level
    header_pattern = re.compile(
        f"^({'#' * min_header_level}){{'1,{max_header_level - min_header_level + 1}'}}\\s+(.+)$",
        re.MULTILINE,
    )

    # Find all headers
    headers = list(header_pattern.finditer(markdown_text))

    if not headers:
        return [{"title": "Text", "content": markdown_text, "level": 0}]

    chunks = []

    # Process each section
    for i in range(len(headers)):
        current_header = headers[i]
        header_text = current_header.group(2).strip()
        header_level = len(current_header.group(1))

        # Determine section content
        if i < len(headers) - 1:
            next_header_start = headers[i + 1].start()
            content = markdown_text[current_header.end() : next_header_start]
        else:
            content = markdown_text[current_header.end() :]

        chunks.append(
            {
                "title": header_text,
                "content": content.strip(),
                "level": header_level,
            }
        )

    # Check if there's content before the first header
    if headers[0].start() > 0:
        preamble = markdown_text[: headers[0].start()].strip()
        if preamble:
            chunks.insert(
                0, {"title": "Preamble", "content": preamble, "level": 0}
            )

    return chunks


@traced_and_logged
def extract_code_blocks(
    markdown_text: str, language: str = None
) -> list[dict[str, str]]:
    if not markdown_text:
        return []

    # Pattern to match markdown code blocks
    if language:
        # Match only code blocks with the specified language
        pattern = rf"```{language}\s*([\s\S]*?)\s*```"
    else:
        # Match all code blocks, capturing the language specifier if present
        pattern = r"```(\w*)\s*([\s\S]*?)\s*```"

    blocks = []

    if language:
        # If language is specified, we only capture the code content
        matches = re.finditer(pattern, markdown_text)
        for match in matches:
            blocks.append(
                {"language": language, "code": match.group(1).strip()}
            )
    else:
        # If no language is specified, we capture both language and code content
        matches = re.finditer(pattern, markdown_text)
        for match in matches:
            lang = match.group(1).strip() if match.group(1) else "text"
            blocks.append({"language": lang, "code": match.group(2).strip()})

    return blocks


@traced_and_logged
def extract_links(markdown_text: str) -> list[dict[str, str]]:
    if not markdown_text:
        return []

    # Pattern to match markdown links [text](url)
    link_pattern = re.compile(r"\[([^\]]+)\]\(([^)]+)\)")
    matches = link_pattern.findall(markdown_text)

    return [{"text": text, "url": url} for text, url in matches]


@traced_and_logged
def extract_tables(markdown_text: str) -> list[dict[str, Any]]:
    if not markdown_text:
        return []

    # Split the text by lines
    lines = markdown_text.split("\n")

    tables = []
    current_table = None
    header_row = None

    for line in lines:
        line = line.strip()

        # Table rows are indicated by starting with |
        if line.startswith("|") and line.endswith("|"):
            if current_table is None:
                current_table = []
                # This is the header row
                header_row = [
                    cell.strip() for cell in line.strip("|").split("|")
                ]
            elif "|--" in line or "|:-" in line:
                # This is the separator row, ignore it
                pass
            else:
                # This is a data row
                row_data = [cell.strip() for cell in line.strip("|").split("|")]

                # Create a dictionary mapping headers to values
                row_dict = {}
                for i, header in enumerate(header_row):
                    if i < len(row_data):
                        row_dict[header] = row_data[i]
                    else:
                        row_dict[header] = ""

                current_table.append(row_dict)
        else:
            # End of table
            if current_table is not None:
                tables.append({"headers": header_row, "rows": current_table})
                current_table = None
                header_row = None

    # Don't forget to add the last table if we're at the end of the document
    if current_table is not None:
        tables.append({"headers": header_row, "rows": current_table})

    return tables


@traced_and_logged
def markdown_to_plain_text(markdown_text: str) -> str:
    if not markdown_text:
        return ""

    # Replace headers
    text = re.sub(r"^#{1,6}\s+(.+)$", r"\1", markdown_text, flags=re.MULTILINE)

    # Replace bold and italic
    text = re.sub(r"\*\*(.*?)\*\*", r"\1", text)
    text = re.sub(r"__(.*?)__", r"\1", text)
    text = re.sub(r"\*(.*?)\*", r"\1", text)
    text = re.sub(r"_(.*?)_", r"\1", text)

    # Replace links
    text = re.sub(r"\[(.*?)\]\((.*?)\)", r"\1 (\2)", text)

    # Replace code blocks
    text = re.sub(r"```(?:\w+)?\s*([\s\S]*?)\s*```", r"\1", text)
    text = re.sub(r"`([^`]*?)`", r"\1", text)

    # Replace bullet points
    text = re.sub(r"^[\*\-\+]\s+(.+)$", r"• \1", text, flags=re.MULTILINE)

    # Replace numbered lists (keeping the numbers)
    text = re.sub(r"^\d+\.\s+(.+)$", r"\1", text, flags=re.MULTILINE)

    # Replace blockquotes
    text = re.sub(r"^>\s+(.+)$", r"\1", text, flags=re.MULTILINE)

    # Remove HTML tags
    text = re.sub(r"<.*?>", "", text)

    # Normalize whitespace
    text = re.sub(r"\n{3,}", "\n\n", text)

    return text.strip()

```

---

### 103. src\flock\core\tools\zendesk_tools.py

- **File ID**: file_102
- **Type**: Code File
- **Line Count**: 99
- **Description**: Tools for interacting with Zendesk.
- **Dependencies**: None
- **Used By**: None

**Content**:
```
"""Tools for interacting with Zendesk."""

import os

import httpx

ZENDESK_BEARER_TOKEN = os.getenv("ZENDESK_BEARER_TOKEN")

HEADERS = {
    "Authorization": f"Bearer {ZENDESK_BEARER_TOKEN}",
    "Accept": "application/json",
}


def get_tickets(number_of_tickets: int = 10) -> list[dict]:
    """Get all tickets."""
    ZENDESK_SUBDOMAIN = os.getenv("ZENDESK_SUBDOMAIN_TICKET")
    BASE_URL = f"https://{ZENDESK_SUBDOMAIN}.zendesk.com"
    url = f"{BASE_URL}/api/v2/tickets.json"
    all_tickets = []
    with httpx.Client(headers=HEADERS, timeout=30.0) as client:
        while url and len(all_tickets) < number_of_tickets:
            response = client.get(url)
            response.raise_for_status()

            data = response.json()
            tickets = data.get("tickets", [])
            all_tickets.extend(tickets)

            url = data.get("next_page")
    return all_tickets


def get_ticket_by_id(ticket_id: str) -> dict:
    """Get a ticket by ID."""
    ZENDESK_SUBDOMAIN = os.getenv("ZENDESK_SUBDOMAIN_TICKET")
    BASE_URL = f"https://{ZENDESK_SUBDOMAIN}.zendesk.com"
    url = f"{BASE_URL}/api/v2/tickets/{ticket_id}"
    with httpx.Client(headers=HEADERS, timeout=30.0) as client:
        response = client.get(url)
        response.raise_for_status()
        return response.json()["ticket"]


def get_comments_by_ticket_id(ticket_id: str) -> list[dict]:
    """Get all comments for a ticket."""
    ZENDESK_SUBDOMAIN = os.getenv("ZENDESK_SUBDOMAIN_TICKET")
    BASE_URL = f"https://{ZENDESK_SUBDOMAIN}.zendesk.com"
    url = f"{BASE_URL}/api/v2/tickets/{ticket_id}/comments"
    with httpx.Client(headers=HEADERS, timeout=30.0) as client:
        response = client.get(url)
        response.raise_for_status()
        return response.json()["comments"]


def get_article_by_id(article_id: str) -> dict:
    """Get an article by ID."""
    ZENDESK_LOCALE = os.getenv("ZENDESK_ARTICLE_LOCALE")
    ZENDESK_SUBDOMAIN = os.getenv("ZENDESK_SUBDOMAIN_ARTICLE")
    BASE_URL = f"https://{ZENDESK_SUBDOMAIN}.zendesk.com"
    url = (
        f"{BASE_URL}/api/v2/help_center/{ZENDESK_LOCALE}/articles/{article_id}"
    )
    with httpx.Client(headers=HEADERS, timeout=30.0) as client:
        response = client.get(url)
        response.raise_for_status()
        return response.json()["article"]


def get_articles() -> list[dict]:
    """Get all articles."""
    ZENDESK_LOCALE = os.getenv("ZENDESK_ARTICLE_LOCALE")
    ZENDESK_SUBDOMAIN = os.getenv("ZENDESK_SUBDOMAIN_ARTICLE")
    BASE_URL = f"https://{ZENDESK_SUBDOMAIN}.zendesk.com"
    url = f"{BASE_URL}/api/v2/help_center/{ZENDESK_LOCALE}/articles.json"
    with httpx.Client(headers=HEADERS, timeout=30.0) as client:
        response = client.get(url)
        response.raise_for_status()
        return response.json()["articles"]


def search_articles(query: str) -> list[dict]:
    """Search Zendesk Help Center articles using a query string."""
    ZENDESK_LOCALE = os.getenv("ZENDESK_ARTICLE_LOCALE")  # e.g., "en-us"
    ZENDESK_SUBDOMAIN = os.getenv("ZENDESK_SUBDOMAIN_ARTICLE")
    BASE_URL = f"https://{ZENDESK_SUBDOMAIN}.zendesk.com"
    url = f"{BASE_URL}/api/v2/help_center/articles/search.json"

    params = {
        "query": query,
        "locale": ZENDESK_LOCALE,
        "sort_by": "updated_at",
        "sort_order": "desc",
    }

    with httpx.Client(headers=HEADERS, timeout=30.0) as client:
        response = client.get(url, params=params)
        response.raise_for_status()
        return response.json().get("results", [])

```

---

### 104. src\flock\core\util\cli_helper.py

- **File ID**: file_103
- **Type**: Code File
- **Line Count**: 87
- **Description**: File at src\flock\core\util\cli_helper.py
- **Dependencies**: None
- **Used By**: None

**Content**:
```
from importlib.metadata import PackageNotFoundError, version

from rich.console import Console
from rich.syntax import Text

try:
    __version__ = version("flock-core")
except PackageNotFoundError:
    __version__ = "0.2.0"

console = Console()


def display_hummingbird():
    """Display the hummingbird."""
    print("""
\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m
\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m
\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m
\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m
\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m
\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;24;23;22m▀\033[0m\033[38;2;0;0;0;48;2;47;44;40m▀\033[0m\033[38;2;0;0;0;48;2;30;28;27m▀\033[0m\033[38;2;0;0;0;48;2;1;1;1m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m
\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;14;14;13m▀\033[0m\033[38;2;2;2;2;48;2;173;161;143m▀\033[0m\033[38;2;97;92;83;48;2;243;226;198m▀\033[0m\033[38;2;204;190;168;48;2;245;226;197m▀\033[0m\033[38;2;243;225;197;48;2;245;225;195m▀\033[0m\033[38;2;243;226;198;48;2;181;168;147m▀\033[0m\033[38;2;243;226;199;48;2;193;179;158m▀\033[0m\033[38;2;213;198;176;48;2;245;226;198m▀\033[0m\033[38;2;110;102;89;48;2;245;226;197m▀\033[0m\033[38;2;7;7;6;48;2;217;202;178m▀\033[0m\033[38;2;0;0;0;48;2;119;111;99m▀\033[0m\033[38;2;0;0;0;48;2;56;54;50m▀\033[0m\033[38;2;0;0;0;48;2;22;22;20m▀\033[0m\033[38;2;0;0;0;48;2;6;6;6m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m
\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;11;11;10;48;2;141;131;119m▀\033[0m\033[38;2;192;179;159;48;2;243;225;197m▀\033[0m\033[38;2;243;225;197;48;2;245;226;197m▀\033[0m\033[38;2;245;226;197;48;2;245;226;195m▀\033[0m\033[38;2;246;226;197;48;2;245;226;197m▀\033[0m\033[38;2;222;205;179;48;2;239;220;193m▀\033[0m\033[38;2;2;2;2;48;2;64;59;52m▀\033[0m\033[38;2;17;17;15;48;2;91;85;77m▀\033[0m\033[38;2;243;226;199;48;2;243;226;198m▀\033[0m\033[38;2;245;226;198;48;2;238;221;193m▀\033[0m\033[38;2;245;226;197;48;2;170;158;139m▀\033[0m\033[38;2;243;225;197;48;2;99;92;81m▀\033[0m\033[38;2;243;224;197;48;2;50;47;43m▀\033[0m\033[38;2;242;224;198;48;2;14;13;12m▀\033[0m\033[38;2;227;213;191;48;2;0;0;0m▀\033[0m\033[38;2;204;192;172;48;2;0;0;0m▀\033[0m\033[38;2;187;175;158;48;2;0;0;0m▀\033[0m\033[38;2;168;159;143;48;2;0;0;0m▀\033[0m\033[38;2;147;140;128;48;2;0;0;0m▀\033[0m\033[38;2;128;120;111;48;2;0;0;0m▀\033[0m\033[38;2;99;93;85;48;2;1;1;1m▀\033[0m\033[38;2;58;55;51;48;2;9;9;8m▀\033[0m\033[38;2;6;6;6;48;2;21;21;21m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m
\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;32;31;29;48;2;121;113;102m▀\033[0m\033[38;2;240;223;195;48;2;243;226;195m▀\033[0m\033[38;2;245;226;197;48;2;245;226;195m▀\033[0m\033[38;2;245;226;197;48;2;245;226;197m▀\033[0m\033[38;2;246;226;197;48;2;246;226;197m▀\033[0m\033[38;2;245;225;195;48;2;245;226;197m▀\033[0m\033[38;2;245;226;195;48;2;245;226;195m▀\033[0m\033[38;2;243;225;198;48;2;239;221;195m▀\033[0m\033[38;2;240;223;199;48;2;71;65;60m▀\033[0m\033[38;2;136;127;111;48;2;0;0;0m▀\033[0m\033[38;2;21;20;19;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m
\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;186;173;153;48;2;220;204;179m▀\033[0m\033[38;2;243;225;197;48;2;245;226;198m▀\033[0m\033[38;2;245;226;197;48;2;245;225;197m▀\033[0m\033[38;2;245;226;197;48;2;245;225;195m▀\033[0m\033[38;2;245;226;197;48;2;245;225;195m▀\033[0m\033[38;2;245;226;197;48;2;245;225;197m▀\033[0m\033[38;2;245;225;195;48;2;227;211;187m▀\033[0m\033[38;2;112;104;93;48;2;6;6;5m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m
\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;219;204;179;48;2;188;175;156m▀\033[0m\033[38;2;245;226;197;48;2;245;226;198m▀\033[0m\033[38;2;245;226;195;48;2;245;226;197m▀\033[0m\033[38;2;245;226;195;48;2;245;225;195m▀\033[0m\033[38;2;245;225;195;48;2;245;225;195m▀\033[0m\033[38;2;245;225;195;48;2;245;225;195m▀\033[0m\033[38;2;238;220;192;48;2;245;226;195m▀\033[0m\033[38;2;149;137;121;48;2;243;225;197m▀\033[0m\033[38;2;16;14;13;48;2;197;183;161m▀\033[0m\033[38;2;0;0;0;48;2;15;14;13m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m
\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;20;19;17m▀\033[0m\033[38;2;0;0;0;48;2;146;136;121m▀\033[0m\033[38;2;16;15;14;48;2;235;219;194m▀\033[0m\033[38;2;161;149;134;48;2;243;226;198m▀\033[0m\033[38;2;245;226;198;48;2;243;226;197m▀\033[0m\033[38;2;245;226;195;48;2;245;226;197m▀\033[0m\033[38;2;245;226;195;48;2;245;225;195m▀\033[0m\033[38;2;245;226;195;48;2;245;225;197m▀\033[0m\033[38;2;245;225;195;48;2;243;225;197m▀\033[0m\033[38;2;243;225;198;48;2;192;178;158m▀\033[0m\033[38;2;235;218;190;48;2;152;141;125m▀\033[0m\033[38;2;245;226;197;48;2;245;226;197m▀\033[0m\033[38;2;163;151;134;48;2;243;225;198m▀\033[0m\033[38;2;0;0;0;48;2;46;43;39m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m
\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;40;38;35m▀\033[0m\033[38;2;56;52;48;48;2;232;216;193m▀\033[0m\033[38;2;220;204;182;48;2;207;193;170m▀\033[0m\033[38;2;243;227;199;48;2;72;67;60m▀\033[0m\033[38;2;211;195;175;48;2;1;1;1m▀\033[0m\033[38;2;151;140;125;48;2;0;0;0m▀\033[0m\033[38;2;130;120;108;48;2;0;0;0m▀\033[0m\033[38;2;138;128;113;48;2;0;0;0m▀\033[0m\033[38;2;163;151;133;48;2;0;0;0m▀\033[0m\033[38;2;170;158;140;48;2;0;0;0m▀\033[0m\033[38;2;121;113;101;48;2;0;0;0m▀\033[0m\033[38;2;16;15;14;48;2;39;36;32m▀\033[0m\033[38;2;155;145;129;48;2;231;215;191m▀\033[0m\033[38;2;245;226;197;48;2;245;226;198m▀\033[0m\033[38;2;245;226;197;48;2;246;226;198m▀\033[0m\033[38;2;118;109;97;48;2;151;140;124m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m
\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;1;1;1;48;2;75;71;65m▀\033[0m\033[38;2;183;170;153;48;2;211;197;177m▀\033[0m\033[38;2;182;170;151;48;2;18;16;15m▀\033[0m\033[38;2;12;12;11;48;2;112;104;95m▀\033[0m\033[38;2;12;11;10;48;2;227;211;187m▀\033[0m\033[38;2;86;81;73;48;2;234;217;193m▀\033[0m\033[38;2;127;119;107;48;2;192;179;160m▀\033[0m\033[38;2;138;129;116;48;2;174;162;144m▀\033[0m\033[38;2;126;118;105;48;2;172;160;142m▀\033[0m\033[38;2;105;97;88;48;2;170;158;140m▀\033[0m\033[38;2;97;92;83;48;2;159;147;131m▀\033[0m\033[38;2;141;131;119;48;2;121;113;101m▀\033[0m\033[38;2;227;211;191;48;2;47;44;40m▀\033[0m\033[38;2;194;181;161;48;2;139;130;116m▀\033[0m\033[38;2;245;226;198;48;2;245;226;198m▀\033[0m\033[38;2;245;226;197;48;2;245;226;197m▀\033[0m\033[38;2;149;138;123;48;2;113;105;94m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m
\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;163;154;139;48;2;219;205;182m▀\033[0m\033[38;2;100;94;85;48;2;102;96;87m▀\033[0m\033[38;2;165;156;141;48;2;210;195;175m▀\033[0m\033[38;2;223;207;184;48;2;29;28;24m▀\033[0m\033[38;2;97;92;81;48;2;52;48;44m▀\033[0m\033[38;2;9;9;8;48;2;110;103;93m▀\033[0m\033[38;2;0;0;0;48;2;140;131;118m▀\033[0m\033[38;2;0;0;0;48;2;142;131;119m▀\033[0m\033[38;2;0;0;0;48;2;121;113;102m▀\033[0m\033[38;2;0;0;0;48;2;109;102;93m▀\033[0m\033[38;2;0;0;0;48;2;128;120;107m▀\033[0m\033[38;2;1;1;0;48;2;195;181;162m▀\033[0m\033[38;2;91;86;79;48;2;243;225;198m▀\033[0m\033[38;2;241;222;195;48;2;245;225;197m▀\033[0m\033[38;2;245;226;197;48;2;245;226;198m▀\033[0m\033[38;2;243;225;197;48;2;190;176;157m▀\033[0m\033[38;2;47;44;40;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m
\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;9;9;8;48;2;21;20;19m▀\033[0m\033[38;2;197;184;163;48;2;192;179;162m▀\033[0m\033[38;2;190;177;159;48;2;236;220;195m▀\033[0m\033[38;2;101;95;84;48;2;235;217;193m▀\033[0m\033[38;2;197;184;163;48;2;135;125;111m▀\033[0m\033[38;2;243;226;199;48;2;40;39;36m▀\033[0m\033[38;2;231;215;190;48;2;2;1;1m▀\033[0m\033[38;2;194;179;160;48;2;0;0;0m▀\033[0m\033[38;2;179;166;147;48;2;10;9;9m▀\033[0m\033[38;2;179;166;147;48;2;64;60;55m▀\033[0m\033[38;2;182;170;152;48;2;169;158;143m▀\033[0m\033[38;2;240;224;195;48;2;245;226;199m▀\033[0m\033[38;2;245;226;197;48;2;245;226;198m▀\033[0m\033[38;2;245;225;197;48;2;245;226;197m▀\033[0m\033[38;2;245;225;197;48;2;243;225;199m▀\033[0m\033[38;2;243;224;195;48;2;117;110;97m▀\033[0m\033[38;2;56;52;48;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m
\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;15;14;13;48;2;6;6;5m▀\033[0m\033[38;2;235;220;195;48;2;242;225;201m▀\033[0m\033[38;2;243;225;199;48;2;243;225;199m▀\033[0m\033[38;2;80;75;67;48;2;226;210;187m▀\033[0m\033[38;2;65;60;56;48;2;243;226;199m▀\033[0m\033[38;2;126;118;105;48;2;195;182;162m▀\033[0m\033[38;2;173;163;146;48;2;136;127;113m▀\033[0m\033[38;2;210;195;177;48;2;89;84;77m▀\033[0m\033[38;2;238;222;198;48;2;52;50;47m▀\033[0m\033[38;2;234;219;195;48;2;92;87;80m▀\033[0m\033[38;2;186;174;157;48;2;209;195;172m▀\033[0m\033[38;2;236;218;192;48;2;243;226;198m▀\033[0m\033[38;2;245;226;198;48;2;221;205;181m▀\033[0m\033[38;2;243;225;198;48;2;81;76;68m▀\033[0m\033[38;2;131;124;111;48;2;0;0;0m▀\033[0m\033[38;2;1;1;1;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m
\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;2;2;2;48;2;0;0;0m▀\033[0m\033[38;2;238;222;197;48;2;233;218;194m▀\033[0m\033[38;2;243;225;198;48;2;169;157;141m▀\033[0m\033[38;2;177;163;147;48;2;13;12;11m▀\033[0m\033[38;2;42;39;36;48;2;116;108;99m▀\033[0m\033[38;2;4;4;4;48;2;219;204;182m▀\033[0m\033[38;2;68;64;58;48;2;243;226;199m▀\033[0m\033[38;2;153;142;128;48;2;225;210;186m▀\033[0m\033[38;2;224;209;187;48;2;147;137;121m▀\033[0m\033[38;2;243;225;199;48;2;69;65;59m▀\033[0m\033[38;2;222;206;182;48;2;6;6;5m▀\033[0m\033[38;2;118;110;99;48;2;0;0;0m▀\033[0m\033[38;2;14;13;12;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m
\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;230;216;193;48;2;192;181;163m▀\033[0m\033[38;2;59;55;52;48;2;189;176;157m▀\033[0m\033[38;2;202;188;168;48;2;242;224;198m▀\033[0m\033[38;2;245;227;200;48;2;133;123;110m▀\033[0m\033[38;2;216;199;176;48;2;10;9;8m▀\033[0m\033[38;2;95;89;79;48;2;0;0;0m▀\033[0m\033[38;2;6;5;5;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m
\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;222;207;186;48;2;225;210;189m▀\033[0m\033[38;2;240;222;195;48;2;105;100;89m▀\033[0m\033[38;2;81;75;67;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m
\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;161;149;136;48;2;40;36;36m▀\033[0m\033[38;2;1;1;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m
\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m
\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m
\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m
\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m

""")


def init_console(clear_screen: bool = True, show_banner: bool = True):
    """Display the Flock banner."""
    banner_text = Text(
        f"""
🦆    🐓     🐤     🐧
╭━━━━━━━━━━━━━━━━━━━━━━━━╮
│ ▒█▀▀▀ █░░ █▀▀█ █▀▀ █░█ │
│ ▒█▀▀▀ █░░ █░░█ █░░ █▀▄ │
│ ▒█░░░ ▀▀▀ ▀▀▀▀ ▀▀▀ ▀░▀ │
╰━━━━━━━━v{__version__}━━━━━━━╯
🦆     🐤    🐧     🐓
""",
        justify="center",
        style="bold orange3",
    )
    if clear_screen:
        console.clear()

    if show_banner:
        console.print(banner_text)
    console.print(
        "[italic]'Magpie'[/] milestone - [bold]white duck GmbH[/] - [cyan]https://whiteduck.de[/]\n"
    )


def display_banner_no_version():
    """Display the Flock banner."""
    banner_text = Text(
        """
🦆    🐓     🐤     🐧
╭━━━━━━━━━━━━━━━━━━━━━━━━╮
│ ▒█▀▀▀ █░░ █▀▀█ █▀▀ █░█ │
│ ▒█▀▀▀ █░░ █░░█ █░░ █▀▄ │
│ ▒█░░░ ▀▀▀ ▀▀▀▀ ▀▀▀ ▀░▀ │
╰━━━━━━━━━━━━━━━━━━━━━━━━╯
🦆     🐤    🐧     🐓
""",
        justify="center",
        style="bold orange3",
    )
    console.print(banner_text)
    console.print("[bold]white duck GmbH[/] - [cyan]https://whiteduck.de[/]\n")

```

---

### 105. src\flock\core\util\file_path_utils.py

- **File ID**: file_104
- **Type**: Code File
- **Line Count**: 223
- **Description**: Utility functions for handling file paths in Flock....
- **Dependencies**: None
- **Used By**: None

**Content**:
```
"""Utility functions for handling file paths in Flock.

This module provides utilities for working with file paths,
especially for components that may be loaded from file system paths
rather than module imports.
"""

import importlib.util
import inspect
import os
import sys
from pathlib import Path
from typing import Any


def get_file_path(obj: Any) -> str | None:
    """Get the file path for a Python object.

    Args:
        obj: The object to get the file path for

    Returns:
        The file path if it can be determined, None otherwise
    """
    try:
        if inspect.ismodule(obj):
            return obj.__file__
        elif inspect.isclass(obj) or inspect.isfunction(obj):
            return inspect.getfile(obj)
        return None
    except (TypeError, ValueError):
        return None


def normalize_path(path: str) -> str:
    """Normalize a path for consistent representation.

    Args:
        path: The path to normalize

    Returns:
        The normalized path
    """
    return os.path.normpath(path)


def is_same_path(path1: str, path2: str) -> bool:
    """Check if two paths point to the same file.

    Args:
        path1: The first path
        path2: The second path

    Returns:
        True if the paths point to the same file, False otherwise
    """
    return os.path.normpath(os.path.abspath(path1)) == os.path.normpath(
        os.path.abspath(path2)
    )


def get_relative_path(path: str, base_path: str | None = None) -> str:
    """Get a path relative to a base path.

    Args:
        path: The path to make relative
        base_path: The base path (defaults to current working directory)

    Returns:
        The relative path
    """
    if base_path is None:
        base_path = os.getcwd()

    return os.path.relpath(path, base_path)


def load_class_from_file(file_path: str, class_name: str) -> type | None:
    """Load a class from a file.

    Args:
        file_path: The path to the file
        class_name: The name of the class to load

    Returns:
        The loaded class, or None if it could not be loaded
    """
    try:
        # Generate a unique module name to avoid conflicts
        module_name = f"flock_dynamic_import_{hash(file_path)}"

        # Create a spec for the module
        spec = importlib.util.spec_from_file_location(module_name, file_path)
        if not spec or not spec.loader:
            return None

        # Create and load the module
        module = importlib.util.module_from_spec(spec)
        sys.modules[module_name] = module
        spec.loader.exec_module(module)

        # Get the class from the module
        if not hasattr(module, class_name):
            return None

        return getattr(module, class_name)
    except Exception:
        return None


def get_project_root() -> Path:
    """Get the project root directory.

    Returns:
        The project root path
    """
    # Try to find the directory containing pyproject.toml or setup.py
    current_dir = Path(os.getcwd())

    # Walk up the directory tree looking for project markers
    for path in [current_dir, *current_dir.parents]:
        if (path / "pyproject.toml").exists() or (path / "setup.py").exists():
            return path

    # Default to current directory if no project markers found
    return current_dir


def component_path_to_file_path(component_path: str) -> str | None:
    """Convert a component path (module.ClassName) to a file path.

    Args:
        component_path: The component path in the form module.ClassName

    Returns:
        The file path if it can be determined, None otherwise
    """
    try:
        # Split into module path and class name
        if "." not in component_path:
            return None

        module_path, class_name = component_path.rsplit(".", 1)

        # Import the module
        module = importlib.import_module(module_path)

        # Get the file path
        if hasattr(module, "__file__"):
            return module.__file__

        return None
    except (ImportError, AttributeError):
        return None


def file_path_to_component_path(file_path: str, class_name: str) -> str | None:
    """Convert a file path and class name to a component path (module.ClassName).

    This is approximate and may not work in all cases, especially for non-standard
    module structures.

    Args:
        file_path: The file path to the module
        class_name: The name of the class

    Returns:
        The component path if it can be determined, None otherwise
    """
    try:
        # Convert the file path to an absolute path
        abs_path = os.path.abspath(file_path)

        # Get the project root
        root = get_project_root()

        # Get the relative path from the project root
        rel_path = os.path.relpath(abs_path, root)

        # Convert to a module path
        module_path = os.path.splitext(rel_path)[0].replace(os.sep, ".")

        # Remove 'src.' prefix if present (common in Python projects)
        if module_path.startswith("src."):
            module_path = module_path[4:]

        # Combine with the class name
        return f"{module_path}.{class_name}"
    except Exception:
        return None


def register_file_paths_in_registry(
    component_paths: dict[str, str], registry: Any | None = None
) -> bool:
    """Register file paths in the registry.

    Args:
        component_paths: Dictionary mapping component paths to file paths
        registry: The registry to register in (defaults to the global registry)

    Returns:
        True if all paths were registered, False otherwise
    """
    try:
        # Get the global registry if none provided
        if registry is None:
            from flock.core.flock_registry import get_registry

            registry = get_registry()

        # Initialize component_file_paths if needed
        if not hasattr(registry, "_component_file_paths"):
            registry._component_file_paths = {}

        # Register each path
        for component_name, file_path in component_paths.items():
            if component_name in registry._components:
                registry._component_file_paths[component_name] = file_path

        return True
    except Exception:
        return False

```

---

### 106. src\flock\core\util\hydrator.py

- **File ID**: file_105
- **Type**: Code File
- **Line Count**: 310
- **Description**: File at src\flock\core\util\hydrator.py
- **Dependencies**: None
- **Used By**: None

**Content**:
```
# src/flock/core/util/hydrator.py (Revised - Simpler)

import asyncio
import json
from typing import (
    Any,
    TypeVar,
    get_type_hints,
)

from pydantic import BaseModel

# Import necessary Flock components
from flock.core import Flock, FlockFactory
from flock.core.logging.logging import get_logger

# Import helper to format type hints back to strings
from flock.core.serialization.serialization_utils import _format_type_to_string

logger = get_logger("hydrator")
T = TypeVar("T", bound=BaseModel)


def flockclass(model: str = "openai/gpt-4o", agent_description: str | None = None):
    """Decorator to add a .hydrate() method to a Pydantic class.
    Leverages a dynamic Flock agent to fill missing (None) fields.

    Args:
        model: The default LLM model identifier to use for hydration.
        agent_description: An optional description for the dynamically created agent.
    """

    def decorator(cls: type[T]) -> type[T]:
        if not issubclass(cls, BaseModel):
            raise TypeError(
                "@flockclass can only decorate Pydantic BaseModel subclasses."
            )

        # Store metadata on the class
        setattr(cls, "__flock_model__", model)
        setattr(cls, "__flock_agent_description__", agent_description)

        # --- Attach the async hydrate method directly ---
        async def hydrate_async(self) -> T:
            """Hydrates the object by filling None fields using a dynamic Flock agent.
            Uses existing non-None fields as input context.
            Returns the hydrated object (self).
            """
            class_name = self.__class__.__name__
            logger.info(f"Starting hydration for instance of {class_name}")

            # Get field information
            all_fields, type_hints = _get_model_fields(self, class_name)
            if all_fields is None or type_hints is None:
                return self  # Return early if field introspection failed

            # Identify existing and missing fields
            existing_data, missing_fields = _identify_fields(self, all_fields)

            if not missing_fields:
                logger.info(f"No fields to hydrate for {class_name} instance.")
                return self

            logger.debug(f"{class_name}: Fields to hydrate: {missing_fields}")
            logger.debug(
                f"{class_name}: Existing data for context: {json.dumps(existing_data, default=str)}"
            )

            # Create agent signatures
            input_str, output_str, input_parts = _build_agent_signatures(
                existing_data,
                missing_fields,
                type_hints,
                all_fields,
                class_name,
            )

            # Create and run agent
            result = await _run_hydration_agent(
                self,
                input_str,
                output_str,
                input_parts,
                existing_data,
                class_name,
            )
            if result is None:
                return self  # Return early if agent run failed

            # Update object fields with results
            _update_fields_with_results(self, result, missing_fields, class_name)

            return self

        # --- Attach the sync hydrate method directly ---
        def hydrate(self) -> T:
            """Synchronous wrapper for the async hydrate method."""
            try:
                # Try to get the current running loop
                loop = asyncio.get_running_loop()

                # If we reach here, there is a running loop
                if loop.is_running():
                    # This runs the coroutine in the existing loop from a different thread
                    import concurrent.futures

                    with concurrent.futures.ThreadPoolExecutor() as executor:
                        future = executor.submit(asyncio.run, hydrate_async(self))
                        return future.result()
                else:
                    # There's a loop but it's not running
                    return loop.run_until_complete(hydrate_async(self))

            except RuntimeError:  # No running loop
                # If no loop is running, create a new one and run our coroutine
                return asyncio.run(hydrate_async(self))

        # Attach the methods to the class
        setattr(cls, "hydrate_async", hydrate_async)
        setattr(cls, "hydrate", hydrate)
        setattr(cls, "hydrate_sync", hydrate)  # Alias for backward compatibility

        logger.debug(f"Attached hydrate methods to class {cls.__name__}")
        return cls

    return decorator


def _get_model_fields(
    obj: BaseModel, class_name: str
) -> tuple[dict | None, dict | None]:
    """Extracts field information from a Pydantic model, handling v1/v2 compatibility."""
    try:
        if hasattr(obj, "model_fields"):  # Pydantic v2
            all_fields = obj.model_fields
            type_hints = {name: field.annotation for name, field in all_fields.items()}
        else:  # Pydantic v1 fallback
            type_hints = get_type_hints(obj.__class__)
            all_fields = getattr(obj, "__fields__", {name: None for name in type_hints})
        return all_fields, type_hints
    except Exception as e:
        logger.error(
            f"Could not get fields/type hints for {class_name}: {e}",
            exc_info=True,
        )
        return None, None


def _identify_fields(
    obj: BaseModel, all_fields: dict
) -> tuple[dict[str, Any], list[str]]:
    """Identifies existing (non-None) and missing fields in the object."""
    existing_data: dict[str, Any] = {}
    missing_fields: list[str] = []

    for field_name in all_fields:
        if hasattr(obj, field_name):  # Check if attribute exists
            value = getattr(obj, field_name)
            if value is not None:
                existing_data[field_name] = value
            else:
                missing_fields.append(field_name)

    return existing_data, missing_fields


def _build_agent_signatures(
    existing_data: dict[str, Any],
    missing_fields: list[str],
    type_hints: dict,
    all_fields: dict,
    class_name: str,
) -> tuple[str, str, list]:
    """Builds input and output signatures for the dynamic agent."""
    # Input signature based on existing data
    input_parts = []
    for name in existing_data:
        field_type = type_hints.get(name, Any)
        type_str = _format_type_to_string(field_type)
        field_info = all_fields.get(name)
        field_desc = getattr(field_info, "description", "")
        if field_desc:
            input_parts.append(f"{name}: {type_str} | {field_desc}")
        else:
            input_parts.append(f"{name}: {type_str}")

    input_str = (
        ", ".join(input_parts)
        if input_parts
        else "context_info: dict | Optional context if no fields have values"
    )

    # Output signature based on missing fields
    output_parts = []
    for name in missing_fields:
        field_type = type_hints.get(name, Any)
        type_str = _format_type_to_string(field_type)
        field_info = all_fields.get(name)
        field_desc = getattr(field_info, "description", "")
        if field_desc:
            output_parts.append(f"{name}: {type_str} | {field_desc}")
        else:
            output_parts.append(f"{name}: {type_str}")

    output_str = ", ".join(output_parts)

    return input_str, output_str, input_parts


async def _run_hydration_agent(
    obj: BaseModel,
    input_str: str,
    output_str: str,
    input_parts: list,
    existing_data: dict[str, Any],
    class_name: str,
) -> dict[str, Any] | None:
    """Creates and runs a dynamic Flock agent to hydrate the object."""
    # Agent configuration
    agent_name = f"hydrator_{class_name}_{id(obj)}"
    description = (
        getattr(obj, "__flock_agent_description__", None)
        or f"Agent that completes missing data for a {class_name} object."
    )
    hydration_model = getattr(obj, "__flock_model__", "openai/gpt-4o")

    logger.debug(f"Creating dynamic agent '{agent_name}' for {class_name}")
    logger.debug(f"  Input Schema: {input_str}")
    logger.debug(f"  Output Schema: {output_str}")

    try:
        # Create agent
        dynamic_agent = FlockFactory.create_default_agent(
            name=agent_name,
            description=description,
            input=input_str,
            output=output_str,
            model=hydration_model,
            no_output=True,
            use_cache=False,
        )

        # Create temporary Flock
        temp_flock = Flock(
            name=f"temp_hydrator_flock_{agent_name}",
            model=hydration_model,
            enable_logging=False,
            show_flock_banner=False,
        )
        temp_flock.add_agent(dynamic_agent)

        # Prepare input data
        agent_input_data = (
            existing_data
            if input_parts
            else {"context_info": {"object_type": class_name}}
        )

        logger.info(f"Running hydration agent '{agent_name}' for {class_name}...")

        # Run agent
        result = await temp_flock.run_async(
            start_agent=agent_name,
            input=agent_input_data,
            box_result=False,
        )
        logger.info(f"Hydration agent returned for {class_name}: {list(result.keys())}")

        return result

    except Exception as e:
        logger.error(
            f"Hydration agent creation or run failed for {class_name}: {e}",
            exc_info=True,
        )
        return None


def _update_fields_with_results(
    obj: BaseModel,
    result: dict[str, Any],
    missing_fields: list[str],
    class_name: str,
) -> None:
    """Updates object fields with results from the hydration agent."""
    updated_count = 0
    for field_name in missing_fields:
        if field_name in result:
            try:
                setattr(obj, field_name, result[field_name])
                logger.debug(
                    f"Hydrated field '{field_name}' in {class_name} with value: {getattr(obj, field_name)}"
                )
                updated_count += 1
            except Exception as e:
                logger.warning(
                    f"Failed to set hydrated value for '{field_name}' in {class_name}: {e}. Value received: {result[field_name]}"
                )
        else:
            logger.warning(
                f"Hydration result missing expected field for {class_name}: '{field_name}'"
            )

    logger.info(
        f"Hydration complete for {class_name}. Updated {updated_count}/{len(missing_fields)} fields."
    )


# Ensure helper functions are available
# from flock.core.serialization.serialization_utils import _format_type_to_string

```

---

### 107. src\flock\core\util\input_resolver.py

- **File ID**: file_106
- **Type**: Code File
- **Line Count**: 180
- **Description**: Utility functions for resolving input keys to their corresponding values.
- **Dependencies**: None
- **Used By**: None

**Content**:
```
"""Utility functions for resolving input keys to their corresponding values."""

from flock.core.context.context import FlockContext


def get_callable_members(obj):
    """Extract all callable (methods/functions) members from a module or class.
    Returns a list of callable objects.
    """
    import inspect

    # Get all members of the object
    members = inspect.getmembers(obj)

    # Filter for callable members that don't start with underscore (to exclude private/special methods)
    callables = [
        member[1]
        for member in members
        if inspect.isroutine(member[1]) and not member[0].startswith("_")
    ]

    return callables


def split_top_level(s: str) -> list[str]:
    """Split a string on commas that are not enclosed within brackets, parentheses, or quotes.

    This function iterates over the string while keeping track of the nesting level. It
    only splits on commas when the nesting level is zero. It also properly handles quoted
    substrings.

    Args:
        s (str): The input string.

    Returns:
        List[str]: A list of substrings split at top-level commas.
    """
    parts = []
    current = []
    level = 0
    in_quote = False
    quote_char = ""

    for char in s:
        # If inside a quote, only exit when the matching quote is found.
        if in_quote:
            current.append(char)
            if char == quote_char:
                in_quote = False
            elif char == "\\":
                # Include escape sequences
                continue
            continue

        # Check for the start of a quote.
        if char in ('"', "'"):
            in_quote = True
            quote_char = char
            current.append(char)
            continue

        # Track nesting.
        if char in "([{":
            level += 1
        elif char in ")]}":
            level -= 1

        # Split on commas if not nested.
        if char == "," and level == 0:
            parts.append("".join(current).strip())
            current = []
        else:
            current.append(char)
    if current:
        parts.append("".join(current).strip())
    return parts


def _parse_keys(keys: list[str]) -> list[str]:
    """Split a comma‐separated string and strip any type annotations.

    For example, "a, b: list[str]" becomes ["a", "b"].
    """
    res_keys = []
    for key in keys:
        if "|" in key:
            key = key.split("|")[0].strip()
        if ":" in key:
            key = key.split(":")[0].strip()
        res_keys.append(key)
    return res_keys


def top_level_to_keys(s: str) -> list[str]:
    """Convert a top-level comma-separated string to a list of keys."""
    top_level_split = split_top_level(s)
    return _parse_keys(top_level_split)


def resolve_inputs(
    input_spec: str, context: FlockContext, previous_agent_name: str
) -> dict:
    """Build a dictionary of inputs based on the input specification string and the provided context.

    The lookup rules are:
      - "context" (case-insensitive): returns the entire context.
      - "context.property": returns an attribute from the context.
      - "def.agent_name": returns the agent definition for the given agent.
      - "agent_name": returns the most up2date record from the given agent's history.
      - "agent_name.property": returns the value of a property from the state variable keyed by "agent_name.property".
      - "property": searches the history for the most recent value of a property.
      - Otherwise, if no matching value is found, fallback to the FLOCK_INITIAL_INPUT.

    -> Recommendations:
        - prefix your agent variables with the agent name or a short handle to avoid conflicts.
        eg. agent name: "idea_agent", variable: "ia_idea" (ia = idea agent)
        - or set hand off mode to strict to avoid conflicts.
        with strict mode, the agent will only accept inputs from the previous agent.

    Args:
        input_spec: Comma-separated input keys (e.g., "query" or "agent_name.property").
        context: A FlockContext instance.

    Returns:
        A dictionary mapping each input key to its resolved value.
    """
    split_input = split_top_level(input_spec)
    keys = _parse_keys(split_input)
    inputs = {}

    for key in keys:
        split_key = key.split(".")

        # Case 1: A single key
        if len(split_key) == 1:
            # Special keyword: "context"
            if key.lower() == "context":
                inputs[key] = context
                continue

            # Try to get a historic record for an agent (if any)
            historic_records = context.get_agent_history(key)
            if historic_records:
                # You may choose to pass the entire record or just its data.
                inputs[key] = historic_records[0].data
                continue

            # Fallback to the most recent value in the state
            historic_value = context.get_most_recent_value(key)
            if historic_value is not None:
                inputs[key] = historic_value
                continue

            # Fallback to the initial input
            var_value = context.get_variable(key)
            if var_value is not None:
                inputs[key] = var_value
                continue

            inputs[key] = context.get_variable("flock." + key)

        # Case 2: A compound key (e.g., "agent_name.property" or "context.property")
        elif len(split_key) == 2:
            entity_name, property_name = split_key

            if entity_name.lower() == "context":
                # Try to fetch the attribute from the context
                inputs[key] = getattr(context, property_name, None)
                continue

            if entity_name.lower() == "def":
                # Return the agent definition for the given property name
                inputs[key] = context.get_agent_definition(property_name)
                continue

            # Otherwise, attempt to look up a state variable with the key "agent_name.property"
            inputs[key] = context.get_variable(f"{entity_name}.{property_name}")
            continue

    return inputs

```

---

### 108. src\flock\core\util\loader.py

- **File ID**: file_107
- **Type**: Code File
- **Line Count**: 59
- **Description**: Provides functionality to load Flock instances from files.
- **Dependencies**: None
- **Used By**: None

**Content**:
```
# src/flock/core/loader.py
"""Provides functionality to load Flock instances from files."""

from pathlib import Path
from typing import TYPE_CHECKING

# Use TYPE_CHECKING to avoid runtime circular import if Flock imports this module indirectly
if TYPE_CHECKING:
    from flock.core.flock import Flock

# Import locally within the function to ensure Serializable methods are available
# from .serialization.serializable import Serializable # Serializable defines the file methods

# Cloudpickle check needs to be top-level
try:
    import cloudpickle

    PICKLE_AVAILABLE = True
except ImportError:
    PICKLE_AVAILABLE = False


def load_flock_from_file(file_path: str) -> "Flock":
    """Load a Flock instance from various file formats (detects type)."""
    # Import Flock locally within the function to avoid circular dependency at module level
    from flock.core.flock import Flock

    p = Path(file_path)
    if not p.exists():
        raise FileNotFoundError(f"Flock file not found: {file_path}")

    try:
        if p.suffix.lower() in [".yaml", ".yml"]:
            return Flock.from_yaml_file(p)
        elif p.suffix.lower() == ".json":
            # Assuming from_json is available via Serializable or directly on Flock
            return Flock.from_json(p.read_text())
        elif p.suffix.lower() == ".msgpack":
            # Assuming from_msgpack_file is available via Serializable or directly on Flock
            return Flock.from_msgpack_file(p)
        elif p.suffix.lower() == ".pkl":
            if PICKLE_AVAILABLE:
                # Assuming from_pickle_file is available via Serializable or directly on Flock
                return Flock.from_pickle_file(p)
            else:
                raise RuntimeError(
                    "Cannot load Pickle file: cloudpickle not installed."
                )
        else:
            raise ValueError(f"Unsupported file extension: {p.suffix}")
    except Exception as e:
        # Add specific error logging if helpful
        from flock.core.logging.logging import get_logger

        logger = get_logger("loader")
        logger.error(
            f"Error loading Flock from {file_path}: {e}", exc_info=True
        )
        raise  # Re-raise the original exception

```

---

### 109. src\flock\evaluators\declarative\declarative_evaluator.py

- **File ID**: file_108
- **Type**: Code File
- **Line Count**: 163
- **Description**: File at src\flock\evaluators\declarative\declarative_evaluator.py
- **Dependencies**: None
- **Used By**: None

**Content**:
```
from collections.abc import Generator
from typing import Any

import dspy
from pydantic import Field, PrivateAttr
from rich.console import Console

from flock.core.flock_agent import FlockAgent
from flock.core.flock_evaluator import FlockEvaluator, FlockEvaluatorConfig
from flock.core.logging.logging import get_logger
from flock.core.mixin.dspy_integration import DSPyIntegrationMixin
from flock.core.mixin.prompt_parser import PromptParserMixin

console = Console()

logger = get_logger("evaluators.declarative")


class DeclarativeEvaluatorConfig(FlockEvaluatorConfig):
    """Configuration for the DeclarativeEvaluator."""

    override_evaluator_type: str | None = None
    model: str | None = "openai/gpt-4o"
    use_cache: bool = True
    temperature: float = 0.0
    max_tokens: int = 4096
    stream: bool = Field(
        default=False,
        description="Enable streaming output from the underlying DSPy program.",
    )
    include_thought_process: bool = Field(
        default=False,
        description="Include the thought process in the output.",
    )
    kwargs: dict[str, Any] = Field(default_factory=dict)


class DeclarativeEvaluator(
    FlockEvaluator, DSPyIntegrationMixin, PromptParserMixin
):
    """Evaluator that uses DSPy for generation."""

    config: DeclarativeEvaluatorConfig = Field(
        default_factory=DeclarativeEvaluatorConfig,
        description="Evaluator configuration",
    )

    _cost: float = PrivateAttr(default=0.0)
    _lm_history: list = PrivateAttr(default_factory=list)

    async def evaluate(
        self, agent: FlockAgent, inputs: dict[str, Any], tools: list[Any]
    ) -> dict[str, Any]:
        """Evaluate using DSPy, with optional asynchronous streaming."""
        # --- Setup Signature and LM ---
        try:
            _dspy_signature = self.create_dspy_signature_class(
                agent.name,
                agent.description,
                f"{agent.input} -> {agent.output}",
            )
            # --- Get output field names ---
            # dspy.Signature holds fields in .output_fields attribute
            output_field_names = list(_dspy_signature.output_fields.keys())
            if not output_field_names:
                logger.warning(
                    f"DSPy signature for agent '{agent.name}' has no defined output fields. Streaming might not produce text."
                )
            # -----------------------------

            self._configure_language_model(
                model=self.config.model or agent.model,
                use_cache=self.config.use_cache,
                temperature=self.config.temperature,
                max_tokens=self.config.max_tokens,
            )
            agent_task = self._select_task(
                _dspy_signature,
                override_evaluator_type=self.config.override_evaluator_type,
                tools=tools,
                kwargs=self.config.kwargs,
            )
        except Exception as setup_error:
            logger.error(
                f"Error setting up DSPy task for agent '{agent.name}': {setup_error}",
                exc_info=True,
            )
            raise RuntimeError(
                f"DSPy task setup failed: {setup_error}"
            ) from setup_error

        # --- Conditional Evaluation (Stream vs No Stream) ---
        if self.config.stream:
            logger.info(
                f"Evaluating agent '{agent.name}' with async streaming."
            )
            if not callable(agent_task):
                logger.error("agent_task is not callable, cannot stream.")
                raise TypeError(
                    "DSPy task could not be created or is not callable."
                )

            streaming_task = dspy.streamify(agent_task)
            stream_generator: Generator = streaming_task(**inputs)
            delta_content = ""

            console.print("\n")
            async for chunk in stream_generator:
                if (
                    hasattr(chunk, "choices")
                    and chunk.choices
                    and hasattr(chunk.choices[0], "delta")
                    and chunk.choices[0].delta
                    and hasattr(chunk.choices[0].delta, "content")
                ):
                    delta_content = chunk.choices[0].delta.content

                if delta_content:
                    console.print(delta_content, end="")

                result_dict, cost, lm_history = self._process_result(
                    chunk, inputs
                )
                self._cost = cost
                self._lm_history = lm_history

            console.print("\n")
            return self.filter_thought_process(
                result_dict, self.config.include_thought_process
            )

        else:  # Non-streaming path
            logger.info(f"Evaluating agent '{agent.name}' without streaming.")
            try:
                # Ensure the call is awaited if the underlying task is async
                result_obj = agent_task(**inputs)
                result_dict, cost, lm_history = self._process_result(
                    result_obj, inputs
                )
                self._cost = cost
                self._lm_history = lm_history
                return self.filter_thought_process(
                    result_dict, self.config.include_thought_process
                )
            except Exception as e:
                logger.error(
                    f"Error during non-streaming evaluation for agent '{agent.name}': {e}",
                    exc_info=True,
                )
                raise RuntimeError(f"Evaluation failed: {e}") from e

    def filter_thought_process(
        self, result_dict: dict[str, Any], include_thought_process: bool
    ) -> dict[str, Any]:
        """Filter out thought process from the result dictionary."""
        if include_thought_process:
            return result_dict
        else:
            return {
                k: v
                for k, v in result_dict.items()
                if not (k.startswith("reasoning") or k.startswith("trajectory"))
            }

```

---

### 110. src\flock\evaluators\memory\azure_search_evaluator.py

- **File ID**: file_109
- **Type**: Code File
- **Line Count**: 0
- **Description**: File at src\flock\evaluators\memory\azure_search_evaluator.py
- **Dependencies**: None
- **Used By**: None

**Content**:
```

```

---

### 111. src\flock\evaluators\memory\memory_evaluator.py

- **File ID**: file_110
- **Type**: Code File
- **Line Count**: 88
- **Description**: File at src\flock\evaluators\memory\memory_evaluator.py
- **Dependencies**: None
- **Used By**: None

**Content**:
```
from typing import Any, Literal

from pydantic import Field

from flock.core.flock_agent import FlockAgent
from flock.core.flock_evaluator import FlockEvaluator, FlockEvaluatorConfig
from flock.core.mixin.dspy_integration import DSPyIntegrationMixin
from flock.core.mixin.prompt_parser import PromptParserMixin
from flock.modules.memory.memory_module import MemoryModule, MemoryModuleConfig


class MemoryEvaluatorConfig(FlockEvaluatorConfig):
    folder_path: str = Field(
        default="concept_memory/",
        description="Directory where memory file and concept graph will be saved",
    )
    concept_graph_file: str = Field(
        default="concept_graph.png",
        description="Base filename for the concept graph image",
    )

    file_path: str | None = Field(
        default="agent_memory.json", description="Path to save memory file"
    )
    memory_mapping: str | None = Field(
        default=None, description="Memory mapping configuration"
    )
    similarity_threshold: float = Field(
        default=0.5, description="Threshold for semantic similarity"
    )
    max_length: int = Field(
        default=1000, description="Max length of memory entry before splitting"
    )
    save_after_update: bool = Field(
        default=True, description="Whether to save memory after each update"
    )
    splitting_mode: Literal["summary", "semantic", "characters", "none"] = (
        Field(default="none", description="Mode to split memory content")
    )
    enable_read_only_mode: bool = Field(
        default=False, description="Whether to enable read only mode"
    )
    number_of_concepts_to_extract: int = Field(
        default=3, description="Number of concepts to extract from the memory"
    )


class MemoryEvaluator(FlockEvaluator, DSPyIntegrationMixin, PromptParserMixin):
    """Evaluator that uses DSPy for generation."""

    config: MemoryEvaluatorConfig = Field(
        default_factory=MemoryEvaluatorConfig,
        description="Evaluator configuration",
    )

    async def evaluate(
        self, agent: FlockAgent, inputs: dict[str, Any], tools: list[Any]
    ) -> dict[str, Any]:
        """Simple evaluator that uses a memory concept graph.

        if inputs contain "query", it searches memory for the query and returns the facts.
        if inputs contain "data", it adds the data to memory
        """
        result = {}
        memory_module = MemoryModule(
            name=self.name,
            config=MemoryModuleConfig(
                folder_path=self.config.folder_path,
                concept_graph_file=self.config.concept_graph_file,
                file_path=self.config.file_path,
                memory_mapping=self.config.memory_mapping,
                similarity_threshold=self.config.similarity_threshold,
                max_length=self.config.max_length,
                save_after_update=self.config.save_after_update,
                splitting_mode=self.config.splitting_mode,
                enable_read_only_mode=self.config.enable_read_only_mode,
                number_of_concepts_to_extract=self.config.number_of_concepts_to_extract,
            ),
        )

        if "query" in inputs:
            facts = await memory_module.search_memory(agent, inputs)
            result = {"facts": facts}

        if "data" in inputs:
            await memory_module.add_to_memory(agent, inputs)
            result = {"message": "Data added to memory"}
        return result

```

---

### 112. src\flock\evaluators\natural_language\natural_language_evaluator.py

- **File ID**: file_111
- **Type**: Code File
- **Line Count**: 66
- **Description**: File at src\flock\evaluators\natural_language\natural_language_evaluator.py
- **Dependencies**: None
- **Used By**: None

**Content**:
```
from typing import Any

from flock.core.flock_evaluator import FlockEvaluator


class NaturalLanguageEvaluator(FlockEvaluator):
    """Evaluator that uses natural language prompting."""

    name: str = "natural_language"
    prompt_template: str = ""
    client: Any = None  # OpenAI client

    async def setup(self, input_schema: str, output_schema: str) -> None:
        """Set up prompt template and client."""
        from openai import AsyncOpenAI

        # Create prompt template
        self.prompt_template = f"""
        You are an AI assistant that processes inputs and generates outputs.
        
        Input Format:
        {input_schema}
        
        Required Output Format:
        {output_schema}
        
        Please process the following input and provide output in the required format:
        {{input}}
        """

        # Set up client
        self.client = AsyncOpenAI()

    async def evaluate(self, inputs: dict[str, Any]) -> dict[str, Any]:
        """Evaluate using natural language."""
        if not self.client:
            raise RuntimeError("Evaluator not set up")

        # Format input for prompt
        input_str = "\n".join(f"{k}: {v}" for k, v in inputs.items())

        # Get completion
        response = await self.client.chat.completions.create(
            model=self.config.model,
            messages=[
                {
                    "role": "user",
                    "content": self.prompt_template.format(input=input_str),
                }
            ],
            temperature=self.config.temperature,
            max_tokens=self.config.max_tokens,
        )

        # Parse response into dictionary
        try:
            import json

            return json.loads(response.choices[0].message.content)
        except json.JSONDecodeError:
            return {"result": response.choices[0].message.content}

    async def cleanup(self) -> None:
        """Close client."""
        if self.client:
            await self.client.close()

```

---

### 113. src\flock\evaluators\test\test_case_evaluator.py

- **File ID**: file_112
- **Type**: Code File
- **Line Count**: 36
- **Description**: File at src\flock\evaluators\test\test_case_evaluator.py
- **Dependencies**: None
- **Used By**: None

**Content**:
```
from typing import Any

from pydantic import Field

from flock.core.flock_agent import FlockAgent
from flock.core.flock_evaluator import FlockEvaluator, FlockEvaluatorConfig
from flock.core.mixin.dspy_integration import DSPyIntegrationMixin


class TestCaseEvaluatorConfig(FlockEvaluatorConfig):
    """Configuration for the TestCaseEvaluator."""

    pass


class TestCaseEvaluator(FlockEvaluator, DSPyIntegrationMixin):
    """Evaluator for test cases."""

    config: TestCaseEvaluatorConfig = Field(
        default_factory=TestCaseEvaluatorConfig,
        description="Evaluator configuration",
    )

    async def evaluate(
        self, agent: FlockAgent, inputs: dict[str, Any], tools: list[Any]
    ) -> dict[str, Any]:
        _dspy_signature = self.create_dspy_signature_class(
            agent.name,
            agent.description,
            f"{agent.input} -> {agent.output}",
        )
        output_field_names = list(_dspy_signature.output_fields.keys())
        result = {}
        for output_field_name in output_field_names:
            result[output_field_name] = "Test Result"
        return result

```

---

### 114. src\flock\evaluators\zep\zep_evaluator.py

- **File ID**: file_113
- **Type**: Code File
- **Line Count**: 57
- **Description**: File at src\flock\evaluators\zep\zep_evaluator.py
- **Dependencies**: None
- **Used By**: None

**Content**:
```
from typing import Any

from pydantic import Field

from flock.core.flock_agent import FlockAgent
from flock.core.flock_evaluator import FlockEvaluator, FlockEvaluatorConfig
from flock.core.mixin.dspy_integration import DSPyIntegrationMixin
from flock.core.mixin.prompt_parser import PromptParserMixin
from flock.modules.zep.zep_module import ZepModule, ZepModuleConfig


class ZepEvaluatorConfig(FlockEvaluatorConfig):
    zep_url: str = "http://localhost:8000"
    zep_api_key: str = "apikey"
    min_fact_rating: float = Field(
        default=0.7, description="Minimum rating for facts to be considered"
    )


class ZepEvaluator(FlockEvaluator, DSPyIntegrationMixin, PromptParserMixin):
    """Evaluator that uses DSPy for generation."""

    config: ZepEvaluatorConfig = Field(
        default_factory=ZepEvaluatorConfig,
        description="Evaluator configuration",
    )

    async def evaluate(
        self, agent: FlockAgent, inputs: dict[str, Any], tools: list[Any]
    ) -> dict[str, Any]:
        """Simple evaluator that uses Zep.

        if inputs contain "query", it searches memory for the query and returns the facts.
        if inputs contain "data", it adds the data to memory
        """
        result = {}
        zep = ZepModule(
            name=self.name,
            config=ZepModuleConfig(
                zep_api_key=self.config.zep_api_key,
                zep_url=self.config.zep_url,
                min_fact_rating=self.config.min_fact_rating,
                enable_read=True,
                enable_write=True,
            ),
        )
        client = zep.get_client()
        if "query" in inputs:
            query = inputs["query"]
            facts = zep.search_memory(query, client)
            result = {"facts": facts}

        if "data" in inputs:
            data = inputs["data"]
            zep.add_to_memory(data, client)
            result = {"message": "Data added to memory"}
        return result

```

---

### 115. src\flock\modules\assertion\assertion_module.py

- **File ID**: file_114
- **Type**: Code File
- **Line Count**: 286
- **Description**: File at src\flock\modules\assertion\assertion_module.py
- **Dependencies**: None
- **Used By**: None

**Content**:
```
# src/flock/modules/asserts/assertion_module.py (New File)

import json
from collections.abc import Callable
from typing import Any, Literal

import dspy  # For potential LLM-based rule checking
from pydantic import BaseModel, Field, PrivateAttr, ValidationError

from flock.core.context.context import FlockContext
from flock.core.flock_agent import FlockAgent
from flock.core.flock_module import FlockModule, FlockModuleConfig

# Need registry access if rules are callables defined elsewhere
from flock.core.flock_registry import flock_component, get_registry
from flock.core.logging.logging import get_logger

logger = get_logger("module.assertion")

# --- Rule Definition ---
# Rules can be defined in several ways:
# 1. Python lambda/function: (result: Dict, inputs: Dict, context: FlockContext) -> bool | Tuple[bool, str]
# 2. String referencing a registered callable: "my_validation_function"
# 3. Natural language rule string: "The summary must contain the keyword 'Flock'." (requires LLM judge)
# 4. Pydantic Model: The output must conform to this Pydantic model.

RuleType = (
    Callable[[dict, dict, FlockContext | None], bool | tuple[bool, str]]
    | str
    | type[BaseModel]
)


class Rule(BaseModel):
    """Container for a single assertion rule."""

    condition: RuleType = Field(
        ...,
        description="""
# --- Rule Definition ---
# Rules can be defined in several ways:
# 1. Python lambda/function: (result: Dict, inputs: Dict, context: FlockContext) -> bool | Tuple[bool, str]
# 2. String referencing a registered callable: "my_validation_function"
# 3. Natural language rule string: "The summary must contain the keyword 'Flock'." (requires LLM judge)
# 4. Pydantic Model: The output must conform to this Pydantic model.
                                """,
    )
    fail_message: str  # Message to provide as feedback on failure
    name: str | None = None  # Optional name for clarity

    def __post_init__(self):
        # Basic validation of fail_message
        if not isinstance(self.fail_message, str) or not self.fail_message:
            raise ValueError("Rule fail_message must be a non-empty string.")


class AssertionModuleConfig(FlockModuleConfig):
    """--- Rule Definition ---
    Rules can be defined in several ways:
    1. Python lambda/function: (result: Dict, inputs: Dict, context: FlockContext) -> bool | Tuple[bool, str]
    2. String referencing a registered callable: "my_validation_function"
    3. Natural language rule string: "The summary must contain the keyword 'Flock'." (requires LLM judge)
    4. Pydantic Model: The output must conform to this Pydantic model.
    """

    rules: list[Rule] = Field(
        default_factory=list,
        description="List of rules to check against the agent's output.",
    )
    # Optional LLM for evaluating natural language rules
    judge_lm_model: str | None = Field(
        None, description="LLM model to use for judging natural language rules."
    )
    # How to handle failure
    on_failure: Literal["add_feedback", "raise_error", "log_warning"] = Field(
        default="add_feedback",
        description="Action on rule failure: 'add_feedback' to context, 'raise_error', 'log_warning'.",
    )
    feedback_context_key: str = Field(
        default="flock.assertion_feedback",
        description="Context key to store failure messages for retry loops.",
    )
    clear_feedback_on_success: bool = Field(
        default=True,
        description="Clear the feedback key from context if all assertions pass.",
    )


@flock_component
class AssertionCheckerModule(FlockModule):
    """Checks the output of an agent against a set of defined rules.

    Can trigger different actions on failure, including adding feedback
    to the context to enable self-correction loops via routing.
    """

    name: str = "assertion_checker"
    config: AssertionModuleConfig = Field(default_factory=AssertionModuleConfig)
    _judge_lm: dspy.LM | None = PrivateAttr(None)  # Initialize lazily

    def _get_judge_lm(self) -> dspy.LM | None:
        """Initializes the judge LM if needed."""
        if self.config.judge_lm_model and self._judge_lm is None:
            try:
                self._judge_lm = dspy.LM(self.config.judge_lm_model)
            except Exception as e:
                logger.error(
                    f"Failed to initialize judge LM '{self.config.judge_lm_model}': {e}"
                )
                # Proceed without judge LM for other rule types
        return self._judge_lm

    async def post_evaluate(
        self,
        agent: FlockAgent,
        inputs: dict[str, Any],
        result: dict[str, Any],
        context: FlockContext | None = None,
    ) -> dict[str, Any]:
        """Checks rules after the main evaluator runs."""
        if not self.config.rules:
            return result  # No rules to check

        logger.debug(f"Running assertion checks for agent '{agent.name}'...")
        all_passed = True
        failed_messages = []
        registry = get_registry()  # Needed for callable lookup

        for i, rule in enumerate(self.config.rules):
            rule_name = rule.name or f"Rule_{i + 1}"
            passed = False
            eval_result = None
            feedback_msg = rule.fail_message

            try:
                condition = rule.condition
                if callable(condition):
                    # Rule is a Python function/lambda
                    logger.debug(f"Checking callable rule: {rule_name}")
                    eval_result = condition(result, inputs, context)
                elif isinstance(condition, str) and registry.contains(
                    condition
                ):
                    # Rule is a string referencing a registered callable
                    logger.debug(
                        f"Checking registered callable rule: '{condition}'"
                    )
                    rule_func = registry.get_callable(condition)
                    eval_result = rule_func(result, inputs, context)
                elif isinstance(condition, str):
                    # Rule is a natural language string (requires judge LLM)
                    logger.debug(
                        f"Checking natural language rule: '{condition}'"
                    )
                    judge_lm = self._get_judge_lm()
                    if judge_lm:
                        # Define a simple judge signature dynamically or use a predefined one
                        class JudgeSignature(dspy.Signature):
                            """Evaluate if the output meets the rule based on input and output."""

                            program_input: str = dspy.InputField(
                                desc="Input provided to the agent."
                            )
                            program_output: str = dspy.InputField(
                                desc="Output generated by the agent."
                            )
                            rule_to_check: str = dspy.InputField(
                                desc="The rule to verify."
                            )
                            is_met: bool = dspy.OutputField(
                                desc="True if the rule is met, False otherwise."
                            )
                            reasoning: str = dspy.OutputField(
                                desc="Brief reasoning for the decision."
                            )

                        judge_predictor = dspy.Predict(
                            JudgeSignature, llm=judge_lm
                        )
                        # Convert complex dicts/lists to strings for the judge prompt
                        input_str = json.dumps(inputs, default=str, indent=2)
                        result_str = json.dumps(result, default=str, indent=2)
                        judge_pred = judge_predictor(
                            program_input=input_str,
                            program_output=result_str,
                            rule_to_check=condition,
                        )
                        passed = judge_pred.is_met
                        feedback_msg = f"{rule.fail_message} (Reason: {judge_pred.reasoning})"
                        logger.debug(
                            f"LLM Judge result for rule '{condition}': {passed} ({judge_pred.reasoning})"
                        )
                    else:
                        logger.warning(
                            f"Cannot evaluate natural language rule '{condition}' - no judge_lm_model configured."
                        )
                        passed = True  # Default to pass if no judge available? Or fail? Let's pass.

                elif isinstance(condition, type) and issubclass(
                    condition, BaseModel
                ):
                    # Rule is a Pydantic model for validation
                    logger.debug(
                        f"Checking Pydantic validation rule: {condition.__name__}"
                    )
                    try:
                        # Assumes the *entire* result dict should match the model
                        # More specific logic might be needed (e.g., validate only a specific key)
                        condition.model_validate(result)
                        passed = True
                    except ValidationError as e:
                        passed = False
                        feedback_msg = (
                            f"{rule.fail_message} (Validation Error: {e})"
                        )
                else:
                    logger.warning(
                        f"Unsupported rule type for rule '{rule_name}': {type(condition)}"
                    )
                    continue  # Skip rule

                # Process result if it was a callable returning bool or (bool, msg)
                if eval_result is not None:
                    if (
                        isinstance(eval_result, tuple)
                        and len(eval_result) == 2
                        and isinstance(eval_result[0], bool)
                    ):
                        passed, custom_msg = eval_result
                        if not passed and custom_msg:
                            feedback_msg = (
                                custom_msg  # Use custom message on failure
                            )
                    elif isinstance(eval_result, bool):
                        passed = eval_result
                    else:
                        logger.warning(
                            f"Rule callable '{rule_name}' returned unexpected type: {type(eval_result)}. Rule skipped."
                        )
                        continue

                # Handle failure
                if not passed:
                    all_passed = False
                    failed_messages.append(feedback_msg)
                    logger.warning(
                        f"Assertion Failed for agent '{agent.name}': {feedback_msg}"
                    )
                    # Optionally break early? For now, check all rules.

            except Exception as e:
                logger.error(
                    f"Error executing rule '{rule_name}' for agent '{agent.name}': {e}",
                    exc_info=True,
                )
                all_passed = False
                failed_messages.append(
                    f"Error checking rule '{rule_name}': {e}"
                )
                # Treat error during check as failure

        # --- Take action based on results ---
        if not all_passed:
            logger.warning(f"Agent '{agent.name}' failed assertion checks.")
            if self.config.on_failure == "add_feedback" and context:
                context.set_variable(
                    self.config.feedback_context_key, "\n".join(failed_messages)
                )
                logger.debug(
                    f"Added assertion feedback to context key '{self.config.feedback_context_key}'"
                )
            elif self.config.on_failure == "raise_error":
                # Maybe wrap in a specific FlockAssertionError
                raise AssertionError(
                    f"Agent '{agent.name}' failed assertions: {'; '.join(failed_messages)}"
                )
            # else "log_warning" is default behavior
        elif context and self.config.clear_feedback_on_success:
            # Clear feedback key if all rules passed and key exists
            if self.config.feedback_context_key in context.state:
                del context.state[self.config.feedback_context_key]
                logger.debug(
                    f"Cleared assertion feedback key '{self.config.feedback_context_key}' on success."
                )

        return result  # Return the original result unmodified

```

---

### 116. src\flock\modules\azure-search\azure_search_module.py

- **File ID**: file_115
- **Type**: Code File
- **Line Count**: 0
- **Description**: File at src\flock\modules\azure-search\azure_search_module.py
- **Dependencies**: None
- **Used By**: None

**Content**:
```

```

---

### 117. src\flock\modules\callback\callback_module.py

- **File ID**: file_116
- **Type**: Code File
- **Line Count**: 89
- **Description**: Callback module for handling agent lifecycle hooks.
- **Dependencies**: None
- **Used By**: None

**Content**:
```
"""Callback module for handling agent lifecycle hooks."""

from collections.abc import Awaitable, Callable
from typing import Any

from pydantic import Field

from flock.core import FlockModule, FlockModuleConfig
from flock.core.context.context import FlockContext


class CallbackModuleConfig(FlockModuleConfig):
    """Configuration for callback module."""

    initialize_callback: (
        Callable[[Any, dict[str, Any]], Awaitable[None]] | None
    ) = Field(
        default=None,
        description="Optional callback function for initialization",
    )
    evaluate_callback: (
        Callable[[Any, dict[str, Any]], Awaitable[dict[str, Any]]] | None
    ) = Field(
        default=None, description="Optional callback function for evaluate"
    )
    terminate_callback: (
        Callable[[Any, dict[str, Any], dict[str, Any]], Awaitable[None]] | None
    ) = Field(
        default=None, description="Optional callback function for termination"
    )
    on_error_callback: (
        Callable[[Any, Exception, dict[str, Any]], Awaitable[None]] | None
    ) = Field(
        default=None,
        description="Optional callback function for error handling",
    )


class CallbackModule(FlockModule):
    """Module that provides callback functionality for agent lifecycle events."""

    name: str = "callbacks"
    config: CallbackModuleConfig = Field(
        default_factory=CallbackModuleConfig,
        description="Callback module configuration",
    )

    async def pre_initialize(
        self,
        agent: Any,
        inputs: dict[str, Any],
        context: FlockContext | None = None,
    ) -> None:
        """Run initialize callback if configured."""
        if self.config.initialize_callback:
            await self.config.initialize_callback(agent, inputs)

    async def pre_evaluate(
        self,
        agent: Any,
        inputs: dict[str, Any],
        context: FlockContext | None = None,
    ) -> dict[str, Any]:
        """Run evaluate callback if configured."""
        if self.config.evaluate_callback:
            return await self.config.evaluate_callback(agent, inputs)
        return inputs

    async def pre_terminate(
        self,
        agent: Any,
        inputs: dict[str, Any],
        result: dict[str, Any],
        context: FlockContext | None = None,
    ) -> None:
        """Run terminate callback if configured."""
        if self.config.terminate_callback:
            await self.config.terminate_callback(agent, inputs, result)

    async def on_error(
        self,
        agent: Any,
        error: Exception,
        inputs: dict[str, Any],
        context: FlockContext | None = None,
    ) -> None:
        """Run error callback if configured."""
        if self.config.on_error_callback:
            await self.config.on_error_callback(agent, error, inputs)

```

---

### 118. src\flock\modules\memory\memory_module.py

- **File ID**: file_117
- **Type**: Code File
- **Line Count**: 407
- **Description**: File at src\flock\modules\memory\memory_module.py
- **Dependencies**: None
- **Used By**: None

**Content**:
```
import json
import uuid
from datetime import datetime
from typing import Any, Literal

from pydantic import Field
from tqdm import tqdm

from flock.core.context.context import FlockContext

# if TYPE_CHECKING:
#     from flock.core import FlockAgent
from flock.core.flock_agent import FlockAgent
from flock.core.flock_module import FlockModule, FlockModuleConfig
from flock.core.logging.logging import get_logger
from flock.modules.memory.memory_parser import MemoryMappingParser
from flock.modules.memory.memory_storage import FlockMemoryStore, MemoryEntry

logger = get_logger("memory")


class MemoryModuleConfig(FlockModuleConfig):
    """Configuration for the MemoryModule.

    This class defines the configuration for the MemoryModule.
    """

    folder_path: str = Field(
        default="concept_memory/",
        description="Directory where memory file and concept graph will be saved",
    )
    concept_graph_file: str = Field(
        default="concept_graph.png",
        description="Base filename for the concept graph image",
    )
    file_path: str | None = Field(
        default="agent_memory.json", description="Path to save memory file"
    )
    memory_mapping: str | None = Field(
        default=None, description="Memory mapping configuration"
    )
    similarity_threshold: float = Field(
        default=0.5, description="Threshold for semantic similarity"
    )
    max_length: int = Field(
        default=1000, description="Max length of memory entry before splitting"
    )
    save_after_update: bool = Field(
        default=True, description="Whether to save memory after each update"
    )
    splitting_mode: Literal["summary", "semantic", "characters", "none"] = (
        Field(default="none", description="Mode to split memory content")
    )
    enable_read_only_mode: bool = Field(
        default=False, description="Whether to enable read only mode"
    )
    number_of_concepts_to_extract: int = Field(
        default=3, description="Number of concepts to extract from the memory"
    )


class MemoryModule(FlockModule):
    """Module that adds memory capabilities to a Flock agent."""

    name: str = "memory"
    config: MemoryModuleConfig = Field(
        default_factory=MemoryModuleConfig,
        description="Memory module configuration",
    )
    memory_store: FlockMemoryStore | None = None
    memory_ops: list[Any] = []

    def __init__(self, name: str, config: MemoryModuleConfig):
        super().__init__(name=name, config=config)
        self.memory_store = FlockMemoryStore.load_from_file(
            self.get_memory_filename(name)
        )
        self.memory_ops = (
            MemoryMappingParser().parse(self.config.memory_mapping)
            if self.config.memory_mapping
            else [{"type": "semantic"}]
        )

    async def initialize(
        self,
        agent: FlockAgent,
        inputs: dict[str, Any],
        context: FlockContext | None = None,
    ) -> None:
        """Initialize memory store if needed."""
        if not self.memory_store:
            self.memory_store = FlockMemoryStore.load_from_file(
                self.get_memory_filename(self.name)
            )
        self.memory_ops = (
            MemoryMappingParser().parse(self.config.memory_mapping)
            if self.config.memory_mapping
            else [{"type": "semantic"}]
        )
        logger.debug(f"Initialized memory module for agent {agent.name}")

    async def pre_evaluate(
        self,
        agent: FlockAgent,
        inputs: dict[str, Any],
        context: FlockContext | None = None,
    ) -> dict[str, Any]:
        """Check memory before evaluation."""
        if not self.memory_store:
            return inputs

        inputs = await self.search_memory(agent, inputs)

        if "context" in inputs:
            agent.input = (
                agent.input + ", context: list | context with more information"
            )

        return inputs

    def get_memory_filename(self, module_name: str) -> str:
        """Generate the full file path for the memory file."""
        folder = self.config.folder_path
        if not folder.endswith(("/", "\\")):
            folder += "/"
        import os

        if not os.path.exists(folder):
            os.makedirs(folder, exist_ok=True)
        # Determine base filename and extension from file_path config
        if self.config.file_path:
            file_name = self.config.file_path.rsplit("/", 1)[-1].rsplit(
                "\\", 1
            )[-1]
            if "." in file_name:
                base, ext = file_name.rsplit(".", 1)
                ext = f".{ext}"
            else:
                base, ext = file_name, ""
        else:
            base, ext = "agent_memory", ".json"
        return f"{folder}{module_name}_{base}{ext}"

    def get_concept_graph_filename(self, module_name: str) -> str:
        """Generate the full file path for the concept graph image."""
        folder = self.config.folder_path
        if not folder.endswith(("/", "\\")):
            folder += "/"
        import os

        if not os.path.exists(folder):
            os.makedirs(folder, exist_ok=True)
        # Use timestamp to create a unique filename
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]
        if self.config.concept_graph_file:
            file_name = self.config.concept_graph_file.rsplit("/", 1)[
                -1
            ].rsplit("\\", 1)[-1]
            if "." in file_name:
                base, ext = file_name.rsplit(".", 1)
                ext = f".{ext}"
            else:
                base, ext = file_name, ""
        else:
            base, ext = "concept_graph", ".png"
        return f"{folder}{module_name}_{base}_{timestamp}{ext}"

    async def search_memory(
        self, agent: FlockAgent, query: dict[str, Any]
    ) -> list[str]:
        """Search memory for the query."""
        if not self.memory_store:
            return []

        try:
            input_text = json.dumps(query)
            query_embedding = self.memory_store.compute_embedding(input_text)
            concepts = await self._extract_concepts(
                agent, input_text, self.config.number_of_concepts_to_extract
            )

            memory_results = []
            for op in self.memory_ops:
                if op["type"] == "semantic":
                    semantic_results = self.memory_store.retrieve(
                        query_embedding,
                        concepts,
                        similarity_threshold=self.config.similarity_threshold,
                    )
                    memory_results.extend(semantic_results)
                elif op["type"] == "exact":
                    exact_results = self.memory_store.exact_match(query)
                    memory_results.extend(exact_results)

            context: list[dict[str, Any]] = []
            if memory_results:
                for result in memory_results:
                    context.append(
                        {"content": result.content, "concepts": result.concepts}
                    )

                logger.debug(
                    f"Found {len(memory_results)} relevant memories",
                    agent=agent.name,
                )
                query["context"] = context

            return query

        except Exception as e:
            logger.warning(f"Memory retrieval failed: {e}", agent=agent.name)
            return query

    async def add_to_memory(
        self, agent: FlockAgent, data: dict[str, Any]
    ) -> None:
        """Add data to memory."""
        if not self.memory_store:
            return

        try:
            chunks = await self._get_chunks(agent, data, None)
            await self._store_chunks(agent, chunks)
        except Exception as e:
            logger.warning(f"Memory storage failed: {e}", agent=agent.name)

    async def post_evaluate(
        self,
        agent: FlockAgent,
        inputs: dict[str, Any],
        result: dict[str, Any],
        context: FlockContext | None = None,
    ) -> dict[str, Any]:
        """Store results in memory after evaluation."""
        if not self.memory_store:
            return result

        try:
            chunks = await self._get_chunks(agent, inputs, result)
            await self._store_chunks(agent, chunks)
        except Exception as e:
            logger.warning(f"Memory storage failed: {e}", agent=agent.name)

        return result

    async def terminate(
        self,
        agent: Any,
        inputs: dict[str, Any],
        result: dict[str, Any],
        context: FlockContext | None = None,
    ) -> None:
        """Save memory store if configured."""
        if self.config.save_after_update and self.memory_store:
            self.save_memory()

    async def _extract_concepts(
        self, agent: FlockAgent, text: str, number_of_concepts: int = 3
    ) -> set[str]:
        """Extract concepts using the agent's LLM capabilities."""
        existing_concepts = set()
        if self.memory_store and self.memory_store.concept_graph:
            existing_concepts = set(
                self.memory_store.concept_graph.graph.nodes()
            )

        input_signature = "text: str | Text to analyze"
        if existing_concepts:
            input_signature += ", existing_concepts: list[str] | Already known concepts that might apply"

        concept_signature = agent.create_dspy_signature_class(
            f"{agent.name}_concept_extractor",
            "Extract key concepts from text",
            f"{input_signature} -> concepts: list[str] | Max {number_of_concepts} key concepts all lower case",
        )

        agent._configure_language_model(agent.model, True, 0.0, 8192)
        predictor = agent._select_task(concept_signature, "Completion")
        result_obj = predictor(
            text=text,
            existing_concepts=list(existing_concepts)
            if existing_concepts
            else None,
        )
        concept_list = getattr(result_obj, "concepts", [])
        return set(concept_list)

    async def _summarize_mode(
        self,
        agent: FlockAgent,
        inputs: dict[str, Any],
        result: dict[str, Any],
    ) -> str:
        """Extract information chunks using summary mode."""
        split_signature = agent.create_dspy_signature_class(
            f"{agent.name}_splitter",
            "Extract a list of potentially needed data and information for future reference",
            """
            content: str | The content to split
            -> chunks: list[str] | List of data and information for future reference
            """,
        )
        agent._configure_language_model(agent.model, True, 0.0, 8192)
        splitter = agent._select_task(split_signature, "Completion")
        full_text = json.dumps(inputs) + json.dumps(result)
        split_result = splitter(content=full_text)
        return "\n".join(split_result.chunks)

    async def _semantic_splitter_mode(
        self,
        agent: FlockAgent,
        inputs: dict[str, Any],
        result: dict[str, Any],
    ) -> str | list[dict[str, str]]:
        """Extract information chunks using semantic mode."""
        split_signature = agent.create_dspy_signature_class(
            f"{self.name}_splitter",
            "Split content into meaningful, self-contained chunks",
            """
            content: str | The content to split
            -> chunks: list[dict[str,str]] | List of chunks as key-value pairs - keys are a short title and values are the chunk content
            """,
        )
        agent._configure_language_model(agent.model, True, 0.0, 8192)
        splitter = agent._select_task(split_signature, "Completion")
        full_text = json.dumps(inputs) + (json.dumps(result) if result else "")
        split_result = splitter(content=full_text)
        return split_result.chunks

    async def _character_splitter_mode(
        self,
        agent: FlockAgent,
        inputs: dict[str, Any],
        result: dict[str, Any],
    ) -> list[str]:
        """Extract information chunks by splitting text into fixed character lengths."""
        full_text = json.dumps(inputs) + (json.dumps(result) if result else "")
        return [
            full_text[i : i + self.config.max_length]
            for i in range(0, len(full_text), self.config.max_length)
        ]

    async def _get_chunks(
        self,
        agent: FlockAgent,
        inputs: dict[str, Any],
        result: dict[str, Any] | None,
    ) -> str | list[str]:
        """Get memory chunks based on the configured splitting mode."""
        mode = self.config.splitting_mode
        if mode == "semantic":
            return await self._semantic_splitter_mode(agent, inputs, result)
        elif mode == "summary":
            return await self._summarize_mode(agent, inputs, result)
        elif mode == "characters":
            return await self._character_splitter_mode(agent, inputs, result)
        elif mode == "none":
            return (
                json.dumps(inputs) + json.dumps(result)
                if result
                else json.dumps(inputs)
            )
        else:
            raise ValueError(f"Unknown splitting mode: {mode}")

    async def _store_chunk(self, agent: FlockAgent, chunk: str) -> None:
        """Store a single chunk in memory."""
        chunk_concepts = await self._extract_concepts(
            agent, chunk, self.config.number_of_concepts_to_extract
        )
        entry = MemoryEntry(
            id=str(uuid.uuid4()),
            content=chunk,
            embedding=self.memory_store.compute_embedding(chunk).tolist(),
            concepts=chunk_concepts,
            timestamp=datetime.now(),
        )
        self.memory_store.add_entry(entry)
        if self.config.save_after_update:
            self.save_memory()
        logger.debug(
            "Stored interaction in memory",
            agent=agent.name,
            entry_id=entry.id,
            concepts=chunk_concepts,
        )

    async def _store_chunks(
        self, agent: FlockAgent, chunks: str | list[str]
    ) -> None:
        """Store chunks (single or multiple) in memory."""
        if isinstance(chunks, str):
            await self._store_chunk(agent, chunks)
        elif isinstance(chunks, list):
            for chunk in tqdm(chunks, desc="Storing chunks in memory"):
                await self._store_chunk(agent, chunk)

    def save_memory(self) -> None:
        """Save memory store to file."""
        if self.memory_store and self.config.file_path:
            json_str = self.memory_store.model_dump_json()
            filename = self.get_memory_filename(self.name)
            with open(filename, "w") as file:
                file.write(json_str)
            self.memory_store.concept_graph.save_as_image(
                self.get_concept_graph_filename(self.name)
            )

```

---

### 119. src\flock\modules\memory\memory_parser.py

- **File ID**: file_118
- **Type**: Code File
- **Line Count**: 125
- **Description**: Parser for memory mapping declarations into executable operations.
- **Dependencies**: None
- **Used By**: None

**Content**:
```
"""Parser for memory mapping declarations into executable operations."""

import re
from typing import Any

from flock.modules.memory.memory_storage import (
    CombineOperation,
    EnrichOperation,
    ExactOperation,
    FilterOperation,
    MemoryOperation,
    MemoryScope,
    SemanticOperation,
    SortOperation,
)


class MemoryMappingParser:
    """Parses memory mapping declarations into executable operations."""

    def parse(self, mapping: str) -> list[MemoryOperation]:
        """Parse a memory mapping string into operations.

        Example mappings:
        "topic -> memory.semantic(threshold=0.9) | memory.exact -> output"
        "query -> memory.semantic(scope='global') | memory.filter(recency='7d') | memory.sort(by='relevance')"
        """
        operations = []
        stages = [s.strip() for s in mapping.split("|")]

        for stage in stages:
            if "->" not in stage:
                continue

            inputs, op_spec = stage.split("->")
            inputs = [i.strip() for i in inputs.split(",")]

            if "memory." in op_spec:
                # Extract operation name and parameters
                match = re.match(r"memory\.(\w+)(?:\((.*)\))?", op_spec.strip())
                if not match:
                    continue

                op_name, params_str = match.groups()
                params = self._parse_params(params_str or "")

                # Create appropriate operation object
                if op_name == "semantic":
                    operation = SemanticOperation(
                        threshold=params.get("threshold", 0.8),
                        scope=params.get("scope", MemoryScope.BOTH),
                        max_results=params.get("max_results", 10),
                    )
                elif op_name == "exact":
                    operation = ExactOperation(
                        keys=inputs, scope=params.get("scope", MemoryScope.BOTH)
                    )
                elif op_name == "enrich":
                    operation = EnrichOperation(
                        tools=params.get("tools", []),
                        strategy=params.get("strategy", "comprehensive"),
                        scope=params.get("scope", MemoryScope.BOTH),
                    )
                elif op_name == "filter":
                    operation = FilterOperation(
                        recency=params.get("recency"),
                        relevance=params.get("relevance"),
                        metadata=params.get("metadata", {}),
                    )
                elif op_name == "sort":
                    operation = SortOperation(
                        by=params.get("by", "relevance"),
                        ascending=params.get("ascending", False),
                    )
                elif op_name == "combine":
                    operation = CombineOperation(
                        weights=params.get(
                            "weights", {"semantic": 0.7, "exact": 0.3}
                        )
                    )

                operations.append(operation)

        return operations

    def _parse_params(self, params_str: str) -> dict[str, Any]:
        """Parse parameters string into a dictionary.

        Handles:
        - Quoted strings: threshold='high'
        - Numbers: threshold=0.9
        - Lists: tools=['web_search', 'extract_numbers']
        - Dictionaries: weights={'semantic': 0.7, 'exact': 0.3}
        """
        if not params_str:
            return {}

        params = {}
        # Split on commas not inside brackets or quotes
        param_pairs = re.findall(
            r"""
            (?:[^,"]|"[^"]*"|'[^']*')+  # Match everything except comma, or quoted strings
        """,
            params_str,
            re.VERBOSE,
        )

        for pair in param_pairs:
            if "=" not in pair:
                continue
            key, value = pair.split("=", 1)
            key = key.strip()
            value = value.strip()

            # Try to evaluate the value (for lists, dicts, numbers)
            try:
                # Safely evaluate the value
                value = eval(value, {"__builtins__": {}}, {})
            except:
                # If evaluation fails, treat as string
                value = value.strip("'\"")

            params[key] = value

        return params

```

---

### 120. src\flock\modules\memory\memory_storage.py

- **File ID**: file_119
- **Type**: Code File
- **Line Count**: 736
- **Description**: Flock memory storage with short-term and long-term memory, concept graph, and clustering....
- **Dependencies**: None
- **Used By**: None

**Content**:
```
"""Flock memory storage with short-term and long-term memory, concept graph, and clustering.

Based on concept graph spreading activation and embedding-based semantic search.
"""

import json
from datetime import datetime
from enum import Enum
from typing import Any, Literal

import networkx as nx
import numpy as np
from networkx.readwrite import json_graph
from opentelemetry import trace
from pydantic import BaseModel, Field, PrivateAttr

# Import SentenceTransformer for production-grade embeddings.
from sentence_transformers import SentenceTransformer

# Import the Flock logger.
from flock.core.logging.logging import get_logger

tracer = trace.get_tracer(__name__)
logger = get_logger("memory")


class MemoryScope(Enum):
    LOCAL = "local"
    GLOBAL = "global"
    BOTH = "both"


class MemoryOperation(BaseModel):
    """Base class for memory operations."""

    type: str
    scope: MemoryScope = MemoryScope.BOTH


class CombineOperation(MemoryOperation):
    """Combine results from multiple operations using weighted scoring."""

    type: Literal["combine"] = "combine"
    weights: dict[str, float] = Field(
        default_factory=lambda: {"semantic": 0.7, "exact": 0.3}
    )


class SemanticOperation(MemoryOperation):
    """Semantic search operation."""

    type: Literal["semantic"] = "semantic"
    threshold: float = 0.5
    max_results: int = 10
    recency_filter: str | None = None  # e.g., "7d", "24h"


class ExactOperation(MemoryOperation):
    """Exact matching operation."""

    type: Literal["exact"] = "exact"
    keys: list[str] = Field(default_factory=list)


class ChunkOperation(MemoryOperation):
    """Operation for handling chunked entries."""

    type: Literal["chunk"] = "chunk"
    reconstruct: bool = True


class EnrichOperation(MemoryOperation):
    """Enrich memory with tool results."""

    type: Literal["enrich"] = "enrich"
    tools: list[str]
    strategy: Literal["comprehensive", "quick", "validated"] = "comprehensive"


class FilterOperation(MemoryOperation):
    """Filter memory results."""

    type: Literal["filter"] = "filter"
    recency: str | None = None
    relevance: float | None = None
    metadata: dict[str, Any] = Field(default_factory=dict)


class SortOperation(MemoryOperation):
    """Sort memory results."""

    type: Literal["sort"] = "sort"
    by: Literal["relevance", "recency", "access_count"] = "relevance"
    ascending: bool = False


class MemoryEntry(BaseModel):
    """A single memory entry."""

    id: str
    content: str
    embedding: list[float] | None = None
    timestamp: datetime = Field(default_factory=datetime.now)
    access_count: int = Field(default=0)
    concepts: set[str] = Field(default_factory=set)
    decay_factor: float = Field(default=1.0)


class MemoryGraph(BaseModel):
    """Graph representation of concept relationships.

    The graph is stored as a JSON string for serialization, while a private attribute holds the actual NetworkX graph.
    """

    # JSON representation using the node-link format with explicit edges="links" to avoid warnings.
    graph_json: str = Field(
        default_factory=lambda: json.dumps(
            json_graph.node_link_data(nx.Graph(), edges="links")
        )
    )
    # Private attribute for the actual NetworkX graph.
    _graph: nx.Graph = PrivateAttr()

    def __init__(self, **data):
        """Initialize the MemoryGraph with a NetworkX graph from JSON data."""
        super().__init__(**data)
        try:
            data_graph = json.loads(self.graph_json)
            self._graph = json_graph.node_link_graph(data_graph, edges="links")
            logger.debug(
                f"MemoryGraph initialized from JSON with {len(self._graph.nodes())} nodes."
            )
        except Exception as e:
            logger.error(f"Failed to load MemoryGraph from JSON: {e}")
            self._graph = nx.Graph()

    @property
    def graph(self) -> nx.Graph:
        """Provides access to the internal NetworkX graph."""
        return self._graph

    def update_graph_json(self) -> None:
        """Update the JSON representation based on the current state of the graph."""
        self.graph_json = json.dumps(
            json_graph.node_link_data(self._graph, edges="links")
        )
        logger.debug("MemoryGraph JSON updated.")

    def add_concepts(self, concepts: set[str]) -> None:
        """Add a set of concepts to the graph and update their associations."""
        concept_list = list(concepts)
        logger.debug(f"Adding concepts: {concept_list}")
        for concept in concepts:
            self._graph.add_node(concept)
        for c1 in concepts:
            for c2 in concepts:
                if c1 != c2:
                    if self._graph.has_edge(c1, c2):
                        self._graph[c1][c2]["weight"] += 1
                    else:
                        self._graph.add_edge(c1, c2, weight=1)
        self.update_graph_json()

    def spread_activation(
        self, initial_concepts: set[str], decay_factor: float = 0.5
    ) -> dict[str, float]:
        """Spread activation through the concept graph.

        Args:
            initial_concepts: The starting set of concepts.
            decay_factor: How much the activation decays at each step.

        Returns:
            A dictionary mapping each concept to its activation level.
        """
        logger.debug(f"Spreading activation from concepts: {initial_concepts}")
        activated = {concept: 1.0 for concept in initial_concepts}
        frontier = list(initial_concepts)

        while frontier:
            current = frontier.pop(0)
            current_activation = activated[current]
            for neighbor in self._graph.neighbors(current):
                weight = self._graph[current][neighbor]["weight"]
                new_activation = current_activation * decay_factor * weight
                if (
                    neighbor not in activated
                    or activated[neighbor] < new_activation
                ):
                    activated[neighbor] = new_activation
                    frontier.append(neighbor)

        logger.debug(f"Activation levels: {activated}")
        return activated

    def save_as_image(self, filename: str = "memory_graph.png") -> None:
        """Visualize the concept graph and save it as a PNG image with improved readability.

        This method uses matplotlib to create a clear and readable visualization by:
        - Using a larger figure size
        - Implementing better node spacing
        - Adding adjustable text labels
        - Using a more visually appealing color scheme
        - Adding edge weight visualization

        Args:
            filename: The path (including .png) where the image will be saved.
        """
        import matplotlib

        matplotlib.use("Agg")
        import matplotlib.pyplot as plt

        logger.info(f"Saving MemoryGraph visualization to '{filename}'")

        if self._graph.number_of_nodes() == 0:
            logger.warning("MemoryGraph is empty; skipping image creation.")
            return

        try:
            # Create a larger figure with higher DPI
            plt.figure(figsize=(16, 12), dpi=100)

            # Use Kamada-Kawai layout for better node distribution
            pos = nx.kamada_kawai_layout(self._graph)

            # Calculate node sizes based on degree
            node_degrees = dict(self._graph.degree())
            node_sizes = [
                2000 * (1 + node_degrees[node] * 0.2)
                for node in self._graph.nodes()
            ]

            # Calculate edge weights for width and transparency
            edge_weights = [
                d["weight"] for (u, v, d) in self._graph.edges(data=True)
            ]
            max_weight = max(edge_weights) if edge_weights else 1
            edge_widths = [1 + (w / max_weight) * 3 for w in edge_weights]
            edge_alphas = [0.2 + (w / max_weight) * 0.8 for w in edge_weights]

            # Draw the network with custom styling
            # Nodes
            nx.draw_networkx_nodes(
                self._graph,
                pos,
                node_size=node_sizes,
                node_color="#5fa4d4",  # Lighter blue
                alpha=0.7,
                edgecolors="white",
            )

            # Edges with varying width and transparency
            for (u, v, d), width, alpha in zip(
                self._graph.edges(data=True), edge_widths, edge_alphas
            ):
                nx.draw_networkx_edges(
                    self._graph,
                    pos,
                    edgelist=[(u, v)],
                    width=width,
                    alpha=alpha,
                    edge_color="#2c3e50",  # Darker blue-grey
                )

            # Add labels with better positioning and background
            labels = nx.get_node_attributes(self._graph, "name") or {
                node: node for node in self._graph.nodes()
            }
            label_pos = {
                node: (x, y + 0.02) for node, (x, y) in pos.items()
            }  # Slightly offset labels

            # Draw labels with white background for better readability
            for node, (x, y) in label_pos.items():
                plt.text(
                    x,
                    y,
                    labels[node],
                    horizontalalignment="center",
                    verticalalignment="center",
                    fontsize=8,
                    fontweight="bold",
                    bbox=dict(
                        facecolor="white", edgecolor="none", alpha=0.7, pad=2.0
                    ),
                )

            # Add edge weight labels for significant weights
            edge_labels = nx.get_edge_attributes(self._graph, "weight")
            significant_edges = {
                (u, v): w
                for (u, v), w in edge_labels.items()
                if w > max_weight * 0.3
            }
            if significant_edges:
                nx.draw_networkx_edge_labels(
                    self._graph,
                    pos,
                    edge_labels=significant_edges,
                    font_size=6,
                    bbox=dict(facecolor="white", edgecolor="none", alpha=0.7),
                )

            # Improve layout
            plt.title("Memory Concept Graph", fontsize=16, pad=20)
            plt.axis("off")

            # Add padding and save
            plt.tight_layout(pad=2.0)
            plt.savefig(filename, bbox_inches="tight", facecolor="white")
            plt.close()

            logger.info(f"MemoryGraph image saved successfully to '{filename}'")

        except Exception as e:
            logger.error(f"Failed to save MemoryGraph image: {e}")
            plt.close()


class FlockMemoryStore(BaseModel):
    """Enhanced Flock memory storage with short-term and long-term memory.

    including embedding-based semantic search, exact matching, and result combination.
    """

    short_term: list[MemoryEntry] = Field(default_factory=list)
    long_term: list[MemoryEntry] = Field(default_factory=list)
    concept_graph: MemoryGraph = Field(default_factory=MemoryGraph)
    clusters: dict[int, list[MemoryEntry]] = Field(default_factory=dict)
    # Instead of np.ndarray, store centroids as lists of floats.
    cluster_centroids: dict[int, list[float]] = Field(default_factory=dict)
    # The embedding model is stored as a private attribute, as it's not serializable.
    _embedding_model: SentenceTransformer | None = PrivateAttr(default=None)

    @classmethod
    def load_from_file(cls, file_path: str | None = None) -> "FlockMemoryStore":
        """Load a memory store from a JSON file.

        Args:
            file_path: Path to the JSON file containing the serialized memory store.
                      If None, returns an empty memory store.

        Returns:
            FlockMemoryStore: A new memory store instance with loaded data.

        Raises:
            FileNotFoundError: If the specified file doesn't exist
            JSONDecodeError: If the file contains invalid JSON
            ValueError: If the JSON structure is invalid
        """
        if file_path is None:
            logger.debug("No file path provided, creating new memory store")
            return cls()

        try:
            logger.info(f"Loading memory store from {file_path}")
            with open(file_path) as f:
                data = json.load(f)

            # Initialize a new store
            store = cls()

            # Load short-term memory entries
            store.short_term = [
                MemoryEntry(
                    id=entry["id"],
                    content=entry["content"],
                    embedding=entry.get("embedding"),
                    timestamp=datetime.fromisoformat(entry["timestamp"]),
                    access_count=entry.get("access_count", 0),
                    concepts=set(entry.get("concepts", [])),
                    decay_factor=entry.get("decay_factor", 1.0),
                )
                for entry in data.get("short_term", [])
            ]

            # Load long-term memory entries
            store.long_term = [
                MemoryEntry(
                    id=entry["id"],
                    content=entry["content"],
                    embedding=entry.get("embedding"),
                    timestamp=datetime.fromisoformat(entry["timestamp"]),
                    access_count=entry.get("access_count", 0),
                    concepts=set(entry.get("concepts", [])),
                    decay_factor=entry.get("decay_factor", 1.0),
                )
                for entry in data.get("long_term", [])
            ]

            # Load concept graph
            if "concept_graph" in data:
                graph_data = json.loads(data["concept_graph"]["graph_json"])
                store.concept_graph = MemoryGraph(
                    graph_json=json.dumps(graph_data)
                )

            # Load clusters
            if "clusters" in data:
                store.clusters = {
                    int(k): [
                        MemoryEntry(
                            id=entry["id"],
                            content=entry["content"],
                            embedding=entry.get("embedding"),
                            timestamp=datetime.fromisoformat(
                                entry["timestamp"]
                            ),
                            access_count=entry.get("access_count", 0),
                            concepts=set(entry.get("concepts", [])),
                            decay_factor=entry.get("decay_factor", 1.0),
                        )
                        for entry in v
                    ]
                    for k, v in data["clusters"].items()
                }

            # Load cluster centroids
            if "cluster_centroids" in data:
                store.cluster_centroids = {
                    int(k): v for k, v in data["cluster_centroids"].items()
                }

            # Initialize the embedding model
            store._embedding_model = None  # Will be lazy-loaded when needed

            logger.info(
                f"Successfully loaded memory store with "
                f"{len(store.short_term)} short-term and "
                f"{len(store.long_term)} long-term entries"
            )
            return store

        except FileNotFoundError:
            logger.warning(
                f"Memory file {file_path} not found, creating new store"
            )
            return cls()
        except json.JSONDecodeError as e:
            logger.error(f"Invalid JSON in memory file: {e}")
            raise
        except Exception as e:
            logger.error(f"Error loading memory store: {e}")
            raise ValueError(f"Failed to load memory store: {e}")

    @classmethod
    def merge_stores(
        cls, stores: list["FlockMemoryStore"]
    ) -> "FlockMemoryStore":
        """Merge multiple memory stores into a single store.

        Args:
            stores: List of FlockMemoryStore instances to merge

        Returns:
            FlockMemoryStore: A new memory store containing merged data
        """
        merged = cls()

        # Merge short-term and long-term memories
        for store in stores:
            merged.short_term.extend(store.short_term)
            merged.long_term.extend(store.long_term)

        # Merge concept graphs
        merged_graph = nx.Graph()
        for store in stores:
            if store.concept_graph and store.concept_graph.graph:
                merged_graph = nx.compose(
                    merged_graph, store.concept_graph.graph
                )

        merged.concept_graph = MemoryGraph(
            graph_json=json.dumps(
                nx.node_link_data(merged_graph, edges="links")
            )
        )

        # Recompute clusters for the merged data
        if merged.short_term:
            merged._update_clusters()

        return merged

    def get_embedding_model(self) -> SentenceTransformer:
        """Initialize and return the SentenceTransformer model.

        Uses "all-MiniLM-L6-v2" as the default model.
        """
        if self._embedding_model is None:
            try:
                logger.debug(
                    "Loading SentenceTransformer model 'all-MiniLM-L6-v2'."
                )
                self._embedding_model = SentenceTransformer("all-MiniLM-L6-v2")
            except Exception as e:
                logger.error(f"Failed to load embedding model: {e}")
                raise RuntimeError(f"Failed to load embedding model: {e}")
        return self._embedding_model

    def compute_embedding(self, text: str) -> np.ndarray:
        """Compute and return the embedding for the provided text as a NumPy array."""
        logger.debug(
            f"Computing embedding for text: {text[:100].replace('{', '{{').replace('}', '}}')}..."
        )  # Log first 30 chars for brevity.
        model = self.get_embedding_model()
        try:
            embedding = model.encode(text, convert_to_numpy=True)
            return embedding
        except Exception as e:
            logger.error(f"Error computing embedding: {e}")
            raise RuntimeError(f"Error computing embedding: {e}")

    def _calculate_similarity(
        self, query_embedding: np.ndarray, entry_embedding: np.ndarray
    ) -> float:
        """Compute the cosine similarity between two embeddings.

        Returns a float between 0 and 1.
        """
        try:
            norm_query = np.linalg.norm(query_embedding)
            norm_entry = np.linalg.norm(entry_embedding)
            if norm_query == 0 or norm_entry == 0:
                return 0.0
            similarity = float(
                np.dot(query_embedding, entry_embedding)
                / (norm_query * norm_entry)
            )
            return similarity
        except Exception as e:
            logger.error(f"Error computing similarity: {e}")
            raise RuntimeError(f"Error computing similarity: {e}")

    def exact_match(self, inputs: dict[str, Any]) -> list[MemoryEntry]:
        """Perform an exact key-based lookup in short-term memory.

        Returns entries where all provided key-value pairs exist in the entry's inputs.
        """
        logger.debug(f"Performing exact match lookup with inputs: {inputs}")
        matches = []
        for entry in self.short_term:
            if all(item in entry.inputs.items() for item in inputs.items()):
                matches.append(entry)
        logger.debug(f"Exact match found {len(matches)} entries.")
        return matches

    def combine_results(
        self, inputs: dict[str, Any], weights: dict[str, float]
    ) -> dict[str, Any]:
        """Combine semantic and exact match results using the provided weights.

        Args:
            inputs: Input dictionary to search memory.
            weights: Dictionary with keys "semantic" and "exact" for weighting.

        Returns:
            A dictionary with "combined_results" as a sorted list of memory entries.
        """
        logger.debug(
            f"Combining results for inputs: {inputs} with weights: {weights}"
        )
        query_text = " ".join(str(value) for value in inputs.values())
        query_embedding = self.compute_embedding(query_text)

        semantic_matches = self.retrieve(
            query_embedding, set(inputs.values()), similarity_threshold=0.8
        )
        exact_matches = self.exact_match(inputs)

        combined: dict[str, dict[str, Any]] = {}
        for entry in semantic_matches:
            if entry.embedding is None:
                continue
            semantic_score = self._calculate_similarity(
                query_embedding, np.array(entry.embedding)
            )
            combined[entry.id] = {
                "entry": entry,
                "semantic_score": semantic_score * weights.get("semantic", 0.7),
                "exact_score": 0.0,
            }
        for entry in exact_matches:
            if entry.id in combined:
                combined[entry.id]["exact_score"] = 1.0 * weights.get(
                    "exact", 0.3
                )
            else:
                combined[entry.id] = {
                    "entry": entry,
                    "semantic_score": 0.0,
                    "exact_score": 1.0 * weights.get("exact", 0.3),
                }
        results: list[tuple[float, MemoryEntry]] = []
        for data in combined.values():
            total_score = data["semantic_score"] + data["exact_score"]
            results.append((total_score, data["entry"]))
        results.sort(key=lambda x: x[0], reverse=True)
        logger.debug(f"Combined results count: {len(results)}")
        return {"combined_results": [entry for score, entry in results]}

    def add_entry(self, entry: MemoryEntry) -> None:
        """Add a new memory entry to short-term memory, update the concept graph and clusters.

        and check for promotion to long-term memory.
        """
        with tracer.start_as_current_span("memory.add_entry") as span:
            logger.info(f"Adding memory entry with id: {entry.id}")
            span.set_attribute("entry.id", entry.id)
            self.short_term.append(entry)
            self.concept_graph.add_concepts(entry.concepts)
            self._update_clusters()
            if entry.access_count > 10:
                self._promote_to_long_term(entry)

    def _promote_to_long_term(self, entry: MemoryEntry) -> None:
        """Promote an entry to long-term memory."""
        logger.info(f"Promoting entry {entry.id} to long-term memory.")
        if entry not in self.long_term:
            self.long_term.append(entry)

    def retrieve(
        self,
        query_embedding: np.ndarray,
        query_concepts: set[str],
        similarity_threshold: float = 0.8,
        exclude_last_n: int = 0,
    ) -> list[MemoryEntry]:
        """Retrieve memory entries using semantic similarity and concept-based activation."""
        with tracer.start_as_current_span("memory.retrieve") as span:
            logger.debug("Retrieving memory entries...")
            results = []
            current_time = datetime.now()
            decay_rate = 0.0001
            norm_query = query_embedding / (
                np.linalg.norm(query_embedding) + 1e-8
            )

            entries = (
                self.short_term[:-exclude_last_n]
                if exclude_last_n > 0
                else self.short_term
            )

            for entry in entries:
                if entry.embedding is None:
                    continue

                # Calculate base similarity
                entry_embedding = np.array(entry.embedding)
                norm_entry = entry_embedding / (
                    np.linalg.norm(entry_embedding) + 1e-8
                )
                similarity = float(np.dot(norm_query, norm_entry))

                # Calculate modifiers
                time_diff = (current_time - entry.timestamp).total_seconds()
                decay = np.exp(-decay_rate * time_diff)
                # Add 1 to base score so new entries aren't zeroed out
                reinforcement = 1.0 + np.log1p(entry.access_count)

                # Calculate final score
                final_score = (
                    similarity * decay * reinforcement * entry.decay_factor
                )

                span.add_event(
                    "memory score",
                    attributes={
                        "entry_id": entry.id,
                        "similarity": similarity,
                        "final_score": final_score,
                    },
                )

                # If base similarity passes threshold, include in results
                if similarity >= similarity_threshold:
                    results.append((final_score, entry))

            # Update access counts and decay for retrieved entries
            for _, entry in results:
                entry.access_count += 1
                self._update_decay_factors(entry)

            # Sort by final score
            results.sort(key=lambda x: x[0], reverse=True)
            logger.debug(f"Retrieved {len(results)} memory entries.")
            return [entry for score, entry in results]

    def _update_decay_factors(self, retrieved_entry: MemoryEntry) -> None:
        """Update decay factors: increase for the retrieved entry and decrease for others."""
        logger.debug(f"Updating decay factor for entry {retrieved_entry.id}")
        retrieved_entry.decay_factor *= 1.1
        for entry in self.short_term:
            if entry != retrieved_entry:
                entry.decay_factor *= 0.9

    def _update_clusters(self) -> None:
        """Update memory clusters using k-means clustering on entry embeddings."""
        logger.debug("Updating memory clusters...")
        if len(self.short_term) < 2:
            logger.debug("Not enough entries for clustering.")
            return

        valid_entries = [
            entry for entry in self.short_term if entry.embedding is not None
        ]
        if not valid_entries:
            logger.debug(
                "No valid entries with embeddings found for clustering."
            )
            return

        embeddings = [np.array(entry.embedding) for entry in valid_entries]
        embeddings_matrix = np.vstack(embeddings)

        from sklearn.cluster import KMeans

        n_clusters = min(10, len(embeddings))
        kmeans = KMeans(n_clusters=n_clusters, random_state=42)
        labels = kmeans.fit_predict(embeddings_matrix)

        self.clusters.clear()
        self.cluster_centroids.clear()

        for i in range(n_clusters):
            cluster_entries = [
                entry
                for entry, label in zip(valid_entries, labels)
                if label == i
            ]
            self.clusters[i] = cluster_entries
            # Convert the centroid (np.ndarray) to a list of floats.
            self.cluster_centroids[i] = kmeans.cluster_centers_[i].tolist()
        logger.debug(f"Clustering complete with {n_clusters} clusters.")

```

---

### 121. src\flock\modules\output\output_module.py

- **File ID**: file_120
- **Type**: Code File
- **Line Count**: 194
- **Description**: Output formatting and display functionality for agents.
- **Dependencies**: None
- **Used By**: None

**Content**:
```
"""Output formatting and display functionality for agents."""

from typing import TYPE_CHECKING, Any

from pydantic import Field

from flock.core.context.context_vars import FLOCK_BATCH_SILENT_MODE

if TYPE_CHECKING:
    from flock.core import FlockAgent

from flock.core.context.context import FlockContext
from flock.core.flock_module import FlockModule, FlockModuleConfig
from flock.core.logging.formatters.themed_formatter import (
    ThemedAgentResultFormatter,
)
from flock.core.logging.formatters.themes import OutputTheme
from flock.core.logging.logging import get_logger

# from flock.core.logging.formatters.themes import OutputTheme
# from flock.core.logging.logging import get_logger
# from flock.core.serialization.json_encoder import FlockJSONEncoder

logger = get_logger("module.output")


class OutputModuleConfig(FlockModuleConfig):
    """Configuration for output formatting and display."""

    theme: OutputTheme = Field(
        default=OutputTheme.afterglow, description="Theme for output formatting"
    )
    render_table: bool = Field(
        default=False, description="Whether to render output as a table"
    )
    max_length: int = Field(
        default=1000, description="Maximum length for displayed output"
    )
    truncate_long_values: bool = Field(
        default=True, description="Whether to truncate long values in display"
    )
    show_metadata: bool = Field(
        default=True, description="Whether to show metadata like timestamps"
    )
    format_code_blocks: bool = Field(
        default=True,
        description="Whether to apply syntax highlighting to code blocks",
    )
    custom_formatters: dict[str, str] = Field(
        default_factory=dict,
        description="Custom formatters for specific output types",
    )
    no_output: bool = Field(
        default=False,
        description="Whether to suppress output",
    )
    print_context: bool = Field(
        default=False,
        description="Whether to print the context",
    )


class OutputModule(FlockModule):
    """Module that handles output formatting and display."""

    name: str = "output"
    config: OutputModuleConfig = Field(
        default_factory=OutputModuleConfig, description="Output configuration"
    )

    def __init__(self, name: str, config: OutputModuleConfig):
        super().__init__(name=name, config=config)
        self._formatter = ThemedAgentResultFormatter(
            theme=self.config.theme,
            max_length=self.config.max_length,
            render_table=self.config.render_table,
        )

    def _format_value(self, value: Any, key: str) -> str:
        """Format a single value based on its type and configuration."""
        # Check for custom formatter
        if key in self.config.custom_formatters:
            formatter_name = self.config.custom_formatters[key]
            if hasattr(self, f"_format_{formatter_name}"):
                return getattr(self, f"_format_{formatter_name}")(value)

        # Default formatting based on type
        if isinstance(value, dict):
            return self._format_dict(value)
        elif isinstance(value, list):
            return self._format_list(value)
        elif isinstance(value, str) and self.config.format_code_blocks:
            return self._format_potential_code(value)
        else:
            return str(value)

    def _format_dict(self, d: dict[str, Any], indent: int = 0) -> str:
        """Format a dictionary with proper indentation."""
        lines = []
        for k, v in d.items():
            formatted_value = self._format_value(v, k)
            if (
                self.config.truncate_long_values
                and len(formatted_value) > self.config.max_length
            ):
                formatted_value = (
                    formatted_value[: self.config.max_length] + "..."
                )
            lines.append(f"{'  ' * indent}{k}: {formatted_value}")
        return "\n".join(lines)

    def _format_list(self, lst: list[Any]) -> str:
        """Format a list with proper indentation."""
        return "\n".join(f"- {self._format_value(item, '')}" for item in lst)

    def _format_potential_code(self, text: str) -> str:
        """Format text that might contain code blocks."""
        import re

        def replace_code_block(match):
            code = match.group(2)
            lang = match.group(1) if match.group(1) else ""
            # Here you could add syntax highlighting
            return f"```{lang}\n{code}\n```"

        # Replace code blocks with formatted versions
        text = re.sub(
            r"```(\w+)?\n(.*?)\n```", replace_code_block, text, flags=re.DOTALL
        )
        return text

    async def post_evaluate(
        self,
        agent: "FlockAgent",
        inputs: dict[str, Any],
        result: dict[str, Any],
        context: FlockContext | None = None,
    ) -> dict[str, Any]:
        """Format and display the output."""
        logger.debug("Formatting and displaying output")

        # Determine if output should be suppressed
        is_silent = self.config.no_output or (
            context and context.get_variable(FLOCK_BATCH_SILENT_MODE, False)
        )

        if is_silent:
            logger.debug("Output suppressed (config or batch silent mode).")
            return result  # Skip console output

        logger.debug("Formatting and displaying output to console.")

        if self.config.print_context and context:
            # Add context snapshot if requested (be careful with large contexts)
            try:
                # Create a copy or select relevant parts to avoid modifying original result dict directly
                display_result = result.copy()
                display_result["context_snapshot"] = (
                    context.to_dict()
                )  # Potential performance hit
            except Exception:
                display_result = result.copy()
                display_result["context_snapshot"] = (
                    "[Error serializing context]"
                )
            result_to_display = display_result
        else:
            result_to_display = result

        if not hasattr(self, "_formatter") or self._formatter is None:
            self._formatter = ThemedAgentResultFormatter(
                theme=self.config.theme,
                max_length=self.config.max_length,
                render_table=self.config.render_table,
                wait_for_input=self.config.wait_for_input,
            )
        self._formatter.display_result(result_to_display, agent.name)

        return result  # Return the original, unmodified result

    def update_theme(self, new_theme: OutputTheme) -> None:
        """Update the output theme."""
        self.config.theme = new_theme
        self._formatter = ThemedAgentResultFormatter(
            theme=self.config.theme,
            max_length=self.config.max_length,
            render_table=self.config.render_table,
            wait_for_input=self.config.wait_for_input,
            write_to_file=self.config.write_to_file,
        )

    def add_custom_formatter(self, key: str, formatter_name: str) -> None:
        """Add a custom formatter for a specific output key."""
        self.config.custom_formatters[key] = formatter_name

```

---

### 122. src\flock\modules\performance\metrics_module.py

- **File ID**: file_121
- **Type**: Code File
- **Line Count**: 492
- **Description**: Performance and metrics tracking for Flock agents.
- **Dependencies**: None
- **Used By**: None

**Content**:
```
"""Performance and metrics tracking for Flock agents."""

import json
import os
import time
from collections import defaultdict
from datetime import datetime
from typing import Any, Literal

import numpy as np
import psutil
from pydantic import BaseModel, Field, validator

from flock.core.context.context import FlockContext
from flock.core.flock_agent import FlockAgent
from flock.core.flock_module import FlockModule, FlockModuleConfig


class MetricPoint(BaseModel):
    """Single metric measurement."""

    timestamp: datetime
    value: int | float | str
    tags: dict[str, str] = {}


class MetricsModuleConfig(FlockModuleConfig):
    """Configuration for performance metrics collection."""

    # Collection settings
    collect_timing: bool = Field(
        default=True, description="Collect timing metrics"
    )
    collect_memory: bool = Field(
        default=True, description="Collect memory usage"
    )
    collect_token_usage: bool = Field(
        default=True, description="Collect token usage stats"
    )
    collect_cpu: bool = Field(default=True, description="Collect CPU usage")

    # Storage settings
    storage_type: Literal["json", "prometheus", "memory"] = Field(
        default="json", description="Where to store metrics"
    )
    metrics_dir: str = Field(
        default="metrics/", description="Directory for metrics storage"
    )

    # Aggregation settings
    aggregation_interval: str = Field(
        default="1h", description="Interval for metric aggregation"
    )
    retention_days: int = Field(default=30, description="Days to keep metrics")

    # Alerting settings
    alert_on_high_latency: bool = Field(
        default=True, description="Alert on high latency"
    )
    latency_threshold_ms: int = Field(
        default=1000, description="Threshold for latency alerts"
    )

    @validator("aggregation_interval")
    def validate_interval(cls, v):
        """Validate time interval format."""
        if v[-1] not in ["s", "m", "h", "d"]:
            raise ValueError("Interval must end with s, m, h, or d")
        return v


class MetricsModule(FlockModule):
    """Module for collecting and analyzing agent performance metrics."""

    name: str = "performance_metrics"
    config: MetricsModuleConfig = Field(
        default_factory=MetricsModuleConfig,
        description="Performance metrics configuration",
    )

    def __init__(self, name, config):
        super().__init__(name=name, config=config)
        self._metrics = defaultdict(list)
        self._start_time: float | None = None
        self._start_memory: int | None = None

        # Set up storage
        if self.config.storage_type == "json":
            os.makedirs(self.config.metrics_dir, exist_ok=True)

        # Set up prometheus if needed
        if self.config.storage_type == "prometheus":
            try:
                from prometheus_client import Counter, Gauge, Histogram

                self._prom_latency = Histogram(
                    "flock_agent_latency_seconds",
                    "Time taken for agent evaluation",
                    ["agent_name"],
                )
                self._prom_memory = Gauge(
                    "flock_agent_memory_bytes",
                    "Memory usage by agent",
                    ["agent_name"],
                )
                self._prom_tokens = Counter(
                    "flock_agent_tokens_total",
                    "Token usage by agent",
                    ["agent_name", "type"],
                )
                self._prom_errors = Counter(
                    "flock_agent_errors_total",
                    "Error count by agent",
                    ["agent_name", "error_type"],
                )
            except ImportError:
                self.config.storage_type = "json"

    """Fixes for metrics summary calculation."""

    def _load_metrics_from_files(
        self, metric_name: str = None
    ) -> dict[str, list[MetricPoint]]:
        """Load metrics from JSON files."""
        metrics = defaultdict(list)

        try:
            # Get all metric files
            files = [
                f
                for f in os.listdir(self.config.metrics_dir)
                if f.endswith(".json") and not f.startswith("summary_")
            ]

            # Filter by metric name if specified
            if metric_name:
                files = [f for f in files if f.startswith(f"{metric_name}_")]

            for filename in files:
                filepath = os.path.join(self.config.metrics_dir, filename)
                with open(filepath) as f:
                    for line in f:
                        try:
                            data = json.loads(line)
                            point = MetricPoint(
                                timestamp=datetime.fromisoformat(
                                    data["timestamp"]
                                ),
                                value=data["value"],
                                tags=data["tags"],
                            )
                            name = filename.split("_")[
                                0
                            ]  # Get metric name from filename
                            metrics[name].append(point)
                        except json.JSONDecodeError:
                            continue

            return dict(metrics)
        except Exception as e:
            print(f"Error loading metrics from files: {e}")
            return {}

    def get_metrics(
        self,
        metric_name: str | None = None,
        start_time: datetime | None = None,
        end_time: datetime | None = None,
    ) -> dict[str, list[MetricPoint]]:
        """Get recorded metrics with optional filtering."""
        # Get metrics from appropriate source
        if self.config.storage_type == "json":
            metrics = self._load_metrics_from_files(metric_name)
        else:
            metrics = self._metrics
            if metric_name:
                metrics = {metric_name: metrics[metric_name]}

        # Apply time filtering if needed
        if start_time or end_time:
            filtered_metrics = defaultdict(list)
            for name, points in metrics.items():
                filtered_points = [
                    p
                    for p in points
                    if (not start_time or p.timestamp >= start_time)
                    and (not end_time or p.timestamp <= end_time)
                ]
                filtered_metrics[name] = filtered_points
            metrics = filtered_metrics

        return dict(metrics)

    def get_statistics(
        self, metric_name: str, percentiles: list[float] = [50, 90, 95, 99]
    ) -> dict[str, float]:
        """Calculate statistics for a metric."""
        # Get all points for this metric
        metrics = self.get_metrics(metric_name=metric_name)
        points = metrics.get(metric_name, [])

        if not points:
            return {}

        values = [p.value for p in points if isinstance(p.value, (int, float))]
        if not values:
            return {}

        stats = {
            "min": min(values),
            "max": max(values),
            "mean": float(
                np.mean(values)
            ),  # Convert to float for JSON serialization
            "std": float(np.std(values)),
            "count": len(values),
            "last_value": values[-1],
        }

        for p in percentiles:
            stats[f"p{p}"] = float(np.percentile(values, p))

        return stats

    async def terminate(
        self,
        agent: FlockAgent,
        inputs: dict[str, Any],
        result: dict[str, Any],
        context: FlockContext | None = None,
    ) -> None:
        """Clean up and final metric recording."""
        if self.config.storage_type == "json":
            # Save aggregated metrics
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            summary_file = os.path.join(
                self.config.metrics_dir,
                f"summary_{agent.name}_{timestamp}.json",
            )

            # Calculate summary for all metrics
            summary = {
                "agent": agent.name,
                "timestamp": timestamp,
                "metrics": {},
            }

            # Get all unique metric names from files
            all_metrics = self._load_metrics_from_files()

            for metric_name in all_metrics.keys():
                stats = self.get_statistics(metric_name)
                if stats:  # Only include metrics that have data
                    summary["metrics"][metric_name] = stats

            with open(summary_file, "w") as f:
                json.dump(summary, f, indent=2)

    def _record_metric(
        self, name: str, value: int | float | str, tags: dict[str, str] = None
    ) -> None:
        """Record a single metric point."""
        point = MetricPoint(
            timestamp=datetime.now(), value=value, tags=tags or {}
        )

        # Store metric
        if self.config.storage_type == "memory":
            self._metrics[name].append(point)

        elif self.config.storage_type == "prometheus":
            if name == "latency":
                self._prom_latency.labels(**tags).observe(value)
            elif name == "memory":
                self._prom_memory.labels(**tags).set(value)
            elif name == "tokens":
                self._prom_tokens.labels(**tags).inc(value)

        elif self.config.storage_type == "json":
            self._save_metric_to_file(name, point)

    def _save_metric_to_file(self, name: str, point: MetricPoint) -> None:
        """Save metric to JSON file."""
        filename = f"{name}_{point.timestamp.strftime('%Y%m')}.json"
        filepath = os.path.join(self.config.metrics_dir, filename)

        data = {
            "timestamp": point.timestamp.isoformat(),
            "value": point.value,
            "tags": point.tags,
        }

        # Append to file
        with open(filepath, "a") as f:
            f.write(json.dumps(data) + "\n")

    def _get_tokenizer(self, model: str):
        """Get the appropriate tokenizer for the model."""
        try:
            import tiktoken

            # Handle different model naming conventions
            if model.startswith("openai/"):
                model = model[7:]  # Strip 'openai/' prefix

            try:
                return tiktoken.encoding_for_model(model)
            except KeyError:
                # Fallback to cl100k_base for unknown models
                return tiktoken.get_encoding("cl100k_base")

        except ImportError:
            return None

    def _calculate_token_usage(self, text: str, model: str = "gpt-4") -> int:
        """Calculate token count using tiktoken when available."""
        tokenizer = self._get_tokenizer(model)

        if tokenizer:
            # Use tiktoken for accurate count
            return len(tokenizer.encode(text))
        else:
            # Fallback to estimation if tiktoken not available
            # Simple estimation - words / 0.75 for average tokens per word
            token_estimate = int(len(text.split()) / 0.75)

            # Log warning about estimation
            print(
                f"Warning: Using estimated token count. Install tiktoken for accurate counting."
            )

    def _should_alert(self, metric: str, value: float) -> bool:
        """Check if metric should trigger alert."""
        if metric == "latency" and self.config.alert_on_high_latency:
            return value * 1000 > self.config.latency_threshold_ms
        return False

    async def initialize(
        self,
        agent: FlockAgent,
        inputs: dict[str, Any],
        context: FlockContext | None = None,
    ) -> None:
        """Initialize metrics collection."""
        self._start_time = time.time()

        if self.config.collect_memory:
            self._start_memory = psutil.Process().memory_info().rss
            self._record_metric(
                "memory",
                self._start_memory,
                {"agent": agent.name, "phase": "start"},
            )

    def _calculate_cost(
        self, text: str, model: str, is_completion: bool = False
    ) -> tuple[int, float]:
        """Calculate both token count and cost."""
        # Get token count
        try:
            from litellm import cost_per_token

            token_count = self._calculate_token_usage(text, model)
            # Calculate total cost
            if is_completion:
                total_cost = token_count * cost_per_token(
                    model, completion_tokens=token_count
                )
            else:
                total_cost = token_count * cost_per_token(
                    model, prompt_tokens=token_count
                )

            return token_count, total_cost
        except Exception:
            token_count = 0
            total_cost = 0.0
            return token_count, total_cost

    async def pre_evaluate(
        self,
        agent: FlockAgent,
        inputs: dict[str, Any],
        context: FlockContext | None = None,
    ) -> dict[str, Any]:
        """Record pre-evaluation metrics."""
        if self.config.collect_token_usage:
            # Calculate input tokens and cost
            total_input_tokens = 0
            total_input_cost = 0.0

            for v in inputs.values():
                tokens, cost = self._calculate_cost(
                    str(v), agent.model, is_completion=False
                )
                total_input_tokens += tokens
                if isinstance(cost, float):
                    total_input_cost += cost
                else:
                    total_input_cost += cost[1]

            self._record_metric(
                "tokens",
                total_input_tokens,
                {"agent": agent.name, "type": "input"},
            )
            self._record_metric(
                "cost", total_input_cost, {"agent": agent.name, "type": "input"}
            )

        if self.config.collect_cpu:
            cpu_percent = psutil.Process().cpu_percent()
            self._record_metric(
                "cpu",
                cpu_percent,
                {"agent": agent.name, "phase": "pre_evaluate"},
            )

        return inputs

    async def post_evaluate(
        self,
        agent: FlockAgent,
        inputs: dict[str, Any],
        result: dict[str, Any],
        context: FlockContext | None = None,
    ) -> dict[str, Any]:
        """Record post-evaluation metrics."""
        if self.config.collect_timing and self._start_time:
            latency = time.time() - self._start_time
            self._record_metric("latency", latency, {"agent": agent.name})

            # Check for alerts
            if self._should_alert("latency", latency):
                # In practice, you'd want to integrate with a proper alerting system
                print(f"ALERT: High latency detected: {latency * 1000:.2f}ms")

        if self.config.collect_token_usage:
            # Calculate output tokens and cost
            total_output_tokens = 0
            total_output_cost = 0.0

            for v in result.values():
                tokens, cost = self._calculate_cost(
                    str(v), agent.model, is_completion=True
                )
                total_output_tokens += tokens
                if isinstance(cost, float):
                    total_output_cost += cost
                else:
                    total_output_cost += cost[1]

            self._record_metric(
                "tokens",
                total_output_tokens,
                {"agent": agent.name, "type": "output"},
            )
            self._record_metric(
                "cost",
                total_output_cost,
                {"agent": agent.name, "type": "output"},
            )

            # Record total cost for this operation
            self._record_metric(
                "total_cost",
                total_output_cost + total_output_cost,
                {"agent": agent.name},
            )

        if self.config.collect_memory and self._start_memory:
            current_memory = psutil.Process().memory_info().rss
            memory_diff = current_memory - self._start_memory
            self._record_metric(
                "memory", memory_diff, {"agent": agent.name, "phase": "end"}
            )

        return result

    async def on_error(
        self,
        agent: FlockAgent,
        error: Exception,
        inputs: dict[str, Any],
        context: FlockContext | None = None,
    ) -> None:
        """Record error metrics."""
        self._record_metric(
            "errors",
            1,
            {"agent": agent.name, "error_type": type(error).__name__},
        )

```

---

### 123. src\flock\modules\zep\zep_module.py

- **File ID**: file_122
- **Type**: Code File
- **Line Count**: 185
- **Description**: File at src\flock\modules\zep\zep_module.py
- **Dependencies**: None
- **Used By**: None

**Content**:
```
import uuid
from typing import Any

from pydantic import Field
from zep_python.client import Zep
from zep_python.types import Message as ZepMessage, SessionSearchResult

from flock.core.context.context import FlockContext
from flock.core.flock_agent import FlockAgent
from flock.core.flock_module import FlockModule, FlockModuleConfig
from flock.core.logging.logging import get_logger

logger = get_logger("module.zep")


class ZepModuleConfig(FlockModuleConfig):
    """Configuration for the Zep module."""

    zep_url: str = "http://localhost:8000"
    zep_api_key: str = "apikey"
    min_fact_rating: float = Field(
        default=0.7, description="Minimum rating for facts to be considered"
    )
    enable_read: bool = True
    enable_write: bool = False


class ZepModule(FlockModule):
    """Module that adds Zep capabilities to a Flock agent."""

    name: str = "zep"
    config: ZepModuleConfig = ZepModuleConfig()
    session_id: str | None = None
    user_id: str | None = None

    def __init__(self, name, config: ZepModuleConfig) -> None:
        """Initialize Zep module."""
        super().__init__(name=name, config=config)
        logger.debug("Initializing Zep module")
        zep_client = Zep(
            base_url=self.config.zep_url, api_key=self.config.zep_api_key
        )
        self.user_id = self.name
        self._setup_user(zep_client)
        self.session_id = str(uuid.uuid4())
        self._setup_session(zep_client)

    def _setup_user(self, zep_client: Zep) -> None:
        """Set up user in Zep."""
        if not zep_client or not self.user_id:
            raise ValueError("Zep service or user_id not initialized")

        try:
            user = zep_client.user.get(user_id=self.user_id)
            if not user:
                zep_client.user.add(user_id=self.user_id)
        except Exception:
            zep_client.user.add(user_id=self.user_id)

    def _setup_session(self, zep_client: Zep) -> None:
        """Set up new session."""
        if not zep_client or not self.user_id or not self.session_id:
            raise ValueError(
                "Zep service, user_id, or session_id not initialized"
            )

        zep_client.memory.add_session(
            user_id=self.user_id,
            session_id=self.session_id,
        )

    def get_client(self) -> Zep:
        """Get Zep client."""
        return Zep(
            base_url=self.config.zep_url, api_key=self.config.zep_api_key
        )

    def get_memory(self, zep_client: Zep) -> str | None:
        """Get memory for the current session."""
        if not zep_client or not self.session_id:
            logger.error("Zep service or session_id not initialized")
            return None

        try:
            memory = zep_client.memory.get(
                self.session_id, min_rating=self.config.min_fact_rating
            )
            if memory:
                return f"{memory.relevant_facts}"
        except Exception as e:
            logger.error(f"Error fetching memory: {e}")
            return None

        return None

    def split_text(
        self, text: str | None, max_length: int = 1000
    ) -> list[ZepMessage]:
        """Split text into smaller chunks."""
        result: list[ZepMessage] = []
        if not text:
            return result
        if len(text) <= max_length:
            return [ZepMessage(role="user", content=text, role_type="user")]
        for i in range(0, len(text), max_length):
            result.append(
                ZepMessage(
                    role="user",
                    content=text[i : i + max_length],
                    role_type="user",
                )
            )
        return result

    def add_to_memory(self, text: str, zep_client: Zep) -> None:
        """Add text to memory."""
        if not zep_client or not self.session_id:
            logger.error("Zep service or session_id not initialized")
            return

        messages = self.split_text(text)
        zep_client.memory.add(session_id=self.session_id, messages=messages)

    def search_memory(
        self, query: str, zep_client: Zep
    ) -> list[SessionSearchResult]:
        """Search memory for a query."""
        if not zep_client or not self.user_id:
            logger.error("Zep service or user_id not initialized")
            return []

        response = zep_client.memory.search_sessions(
            text=query,
            user_id=self.user_id,
            search_scope="facts",
            min_fact_rating=self.config.min_fact_rating,
        )
        if not response.results:
            return []
        return response.results

    async def post_evaluate(
        self,
        agent: FlockAgent,
        inputs: dict[str, Any],
        result: dict[str, Any],
        context: FlockContext | None = None,
    ) -> dict[str, Any]:
        """Format and display the output."""
        if not self.config.enable_write:
            return result
        logger.debug("Saving data to memory")
        zep_client = Zep(
            base_url=self.config.zep_url, api_key=self.config.zep_api_key
        )
        self.add_to_memory(str(result), zep_client)
        return result

    async def pre_evaluate(
        self,
        agent: FlockAgent,
        inputs: dict[str, Any],
        context: FlockContext | None = None,
    ) -> dict[str, Any]:
        """Format and display the output."""
        if not self.config.enable_read:
            return inputs

        zep_client = Zep(
            base_url=self.config.zep_url, api_key=self.config.zep_api_key
        )

        logger.debug("Searching memory")
        facts = self.search_memory(str(inputs), zep_client)

        # Add memory to inputs
        facts_str = ""
        if facts:
            for fact in facts:
                facts_str += fact.fact.fact + "\n"
            logger.debug("Found facts in memory: {}", facts_str)
            agent.input = agent.input + ", memory"
            inputs["memory"] = facts_str

        return inputs

```

---

### 124. src\flock\platform\docker_tools.py

- **File ID**: file_123
- **Type**: Code File
- **Line Count**: 49
- **Description**: File at src\flock\platform\docker_tools.py
- **Dependencies**: None
- **Used By**: None

**Content**:
```
import subprocess
import time


def _check_docker_running():
    """Check if Docker is running by calling 'docker info'."""
    try:
        result = subprocess.run(
            ["docker", "info"],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
        )
        return result.returncode == 0
    except Exception:
        return False


def _start_docker():
    """Attempt to start Docker.
    This example first tries 'systemctl start docker' and then 'service docker start'.
    Adjust as needed for your environment.
    """
    try:
        print("Attempting to start Docker...")
        result = subprocess.run(
            ["sudo", "systemctl", "start", "docker"],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
        )
        if result.returncode != 0:
            result = subprocess.run(
                ["sudo", "service", "docker", "start"],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
            )
        # Give Docker a moment to start.
        time.sleep(3)
        if _check_docker_running():
            print("Docker is now running.")
            return True
        else:
            print("Docker did not start successfully.")
            return False
    except Exception as e:
        print(f"Exception when trying to start Docker: {e}")
        return False

```

---

### 125. src\flock\platform\jaeger_install.py

- **File ID**: file_124
- **Type**: Code File
- **Line Count**: 86
- **Description**: File at src\flock\platform\jaeger_install.py
- **Dependencies**: None
- **Used By**: None

**Content**:
```
import socket
import subprocess
from urllib.parse import urlparse


class JaegerInstaller:
    jaeger_endpoint: str = None
    jaeger_transport: str = "grpc"

    def _check_jaeger_running(self):
        """Check if Jaeger is reachable by attempting a socket connection.
        For HTTP transport, we parse the URL; for gRPC, we expect "host:port".
        """
        try:
            if self.jaeger_transport == "grpc":
                host, port = self.jaeger_endpoint.split(":")
                port = int(port)
            elif self.jaeger_transport == "http":
                parsed = urlparse(self.jaeger_endpoint)
                host = parsed.hostname
                port = parsed.port if parsed.port else 80
            else:
                return False

            # Try connecting to the host and port.
            with socket.create_connection((host, port), timeout=3):
                return True
        except Exception:
            return False

    def _is_jaeger_container_running(self):
        """Check if a Jaeger container (using the official all-in-one image) is running.
        This uses 'docker ps' to filter for containers running the Jaeger image.
        """
        try:
            result = subprocess.run(
                [
                    "docker",
                    "ps",
                    "--filter",
                    "ancestor=jaegertracing/all-in-one:latest",
                    "--format",
                    "{{.ID}}",
                ],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
            )
            return bool(result.stdout.strip())
        except Exception:
            return False

    def _provision_jaeger_container(self):
        """Provision a Jaeger container using Docker."""
        try:
            print("Provisioning Jaeger container using Docker...")
            result = subprocess.run(
                [
                    "docker",
                    "run",
                    "-d",
                    "--name",
                    "jaeger",
                    "-p",
                    "16686:16686",
                    "-p",
                    "14250:14250",
                    "-p",
                    "14268:14268",
                    "jaegertracing/all-in-one:latest",
                ],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
            )
            if result.returncode == 0:
                print("Jaeger container started successfully.")
                return True
            else:
                print(
                    f"Failed to start Jaeger container. Error: {result.stderr}"
                )
                return False
        except Exception as e:
            print(f"Exception when provisioning Jaeger container: {e}")
            return False

```

---

### 126. src\flock\routers\__init__.py

- **File ID**: file_125
- **Type**: Code File
- **Line Count**: 1
- **Description**: Routers for the Flock framework.
- **Dependencies**: None
- **Used By**: None

**Content**:
```
"""Routers for the Flock framework."""

```

---

### 127. src\flock\routers\agent\__init__.py

- **File ID**: file_126
- **Type**: Code File
- **Line Count**: 1
- **Description**: Agent-based router implementation for the Flock framework.
- **Dependencies**: None
- **Used By**: None

**Content**:
```
"""Agent-based router implementation for the Flock framework."""

```

---

### 128. src\flock\routers\agent\agent_router.py

- **File ID**: file_127
- **Type**: Code File
- **Line Count**: 234
- **Description**: Agent-based router implementation for the Flock framework.
- **Dependencies**: None
- **Used By**: None

**Content**:
```
"""Agent-based router implementation for the Flock framework."""

from typing import Any

from flock.core.context.context import FlockContext
from flock.core.flock_agent import FlockAgent
from flock.core.flock_router import (
    FlockRouter,
    FlockRouterConfig,
    HandOffRequest,
)
from flock.core.logging.formatters.themes import OutputTheme
from flock.core.logging.logging import get_logger
from flock.evaluators.declarative.declarative_evaluator import (
    DeclarativeEvaluator,
    DeclarativeEvaluatorConfig,
)
from flock.modules.output.output_module import OutputModule, OutputModuleConfig
from flock.routers.agent.handoff_agent import (
    AgentInfo,
    HandoffAgent,
)

logger = get_logger("agent_router")


class AgentRouterConfig(FlockRouterConfig):
    """Configuration for the agent router.

    This class extends FlockRouterConfig with parameters specific to the agent router.
    """

    with_output: bool = False
    confidence_threshold: float = 0.5  # No additional parameters needed for now


class AgentRouter(FlockRouter):
    """Router that uses a FlockAgent to determine the next agent in a workflow.

    This class is responsible for:
    1. Creating and managing a HandoffAgent
    2. Analyzing available agents in the registry
    3. Using the HandoffAgent to determine the best next agent
    4. Creating a HandOff object with the selected agent
    """

    def __init__(
        self,
        name: str = "agent_router",
        config: AgentRouterConfig | None = None,
    ):
        """Initialize the AgentRouter.

        Args:
            registry: The agent registry containing all available agents
            name: The name of the router
            config: The router configuration
        """
        super().__init__(
            name=name, config=config or AgentRouterConfig(name=name)
        )

    async def route(
        self,
        current_agent: FlockAgent,
        result: dict[str, Any],
        context: FlockContext,
    ) -> HandOffRequest:
        """Determine the next agent to hand off to based on the current agent's output.

        Args:
            current_agent: The agent that just completed execution
            result: The output from the current agent
            context: The global execution context

        Returns:
            A HandOff object containing the next agent and input data
        """
        # Get all available agents from context.agent_definitions
        agent_definitions = context.agent_definitions
        handoff_agent = HandoffAgent(model=current_agent.model)
        handoff_agent.evaluator = DeclarativeEvaluator(
            name="evaluator",
            config=DeclarativeEvaluatorConfig(
                model=current_agent.model,
                use_cache=True,
                max_tokens=1000,
                temperature=0.0,
            ),
        )
        if self.config.with_output:
            handoff_agent.add_module(
                OutputModule(
                    name="output",
                    config=OutputModuleConfig(
                        theme=OutputTheme.abernathy,
                    ),
                )
            )
        available_agents = self._get_available_agents(
            agent_definitions, current_agent.name
        )

        if not available_agents:
            logger.warning("No available agents for agent-based routing")
            return HandOffRequest(
                next_agent="",
                output_to_input_merge_strategy="add",
                override_next_agent=None,
                override_context=None,
            )

        # Prepare input for the handoff agent
        handoff_input = {
            "current_agent_name": current_agent.name,
            "current_agent_description": current_agent.description,
            "current_agent_input": current_agent.input,
            "current_agent_output": current_agent.output,
            "current_result": result,
            "available_agents": available_agents,
        }

        try:
            # Run the handoff agent to determine the next agent
            handoff_result = await handoff_agent.run_async(handoff_input)

            # Extract the decision
            next_agent_name = handoff_result.get("agent_name")
            confidence = handoff_result.get("confidence")
            reasoning = handoff_result.get("reasoning")
            logger.info(
                f"Agent router selected agent '{next_agent_name}' with confidence {confidence} and reasoning: {reasoning}"
            )

            if confidence < self.config.confidence_threshold:
                logger.info(
                    f"No suitable next agent found (best score: {confidence})"
                )
                return HandOffRequest(
                    next_agent="",
                    output_to_input_merge_strategy="add",
                    override_next_agent=None,
                    override_context=None,
                )

            next_agent = agent_definitions.get(next_agent_name)
            if not next_agent:
                logger.error(
                    f"Selected agent '{next_agent_name}' not found in agent definitions"
                )
                return HandOffRequest(
                    next_agent="",
                    output_to_input_merge_strategy="add",
                    override_next_agent=None,
                    override_context=None,
                )

            logger.info(
                f"Agent router selected agent '{next_agent_name}' with confidence {confidence}"
            )
            return HandOffRequest(
                next_agent=next_agent_name,
                output_to_input_merge_strategy="add",
                override_next_agent=None,
                override_context=None,
            )

        except Exception as e:
            logger.error(f"Error in agent-based routing: {e}")
            return HandOffRequest(
                next_agent="",
                output_to_input_merge_strategy="add",
                override_next_agent=None,
                override_context=None,
            )

    def _get_available_agents(
        self, agent_definitions: dict[str, Any], current_agent_name: str
    ) -> list[AgentInfo]:
        """Get all available agents except the current one and the handoff agent.

        Args:
            agent_definitions: Dictionary of available agents
            current_agent_name: Name of the current agent to exclude

        Returns:
            List of available agents as AgentInfo objects
        """
        agents = []
        for agent_name in agent_definitions:
            if agent_name != current_agent_name:
                agent = agent_definitions[agent_name]
                agent_info = AgentInfo(
                    name=agent_name,
                    description=agent.agent_data["description"]
                    if agent.agent_data["description"]
                    else "",
                    input_schema=agent.agent_data["input"],
                    output_schema=agent.agent_data["output"],
                )
                agents.append(agent_info)
        return agents

    def _get_schema_from_agent(
        self, agent: Any, schema_type: str
    ) -> dict[str, Any]:
        """Extract input or output schema from an agent.

        Args:
            agent: The agent to extract schema from
            schema_type: Either "input" or "output"

        Returns:
            Dictionary representation of the schema
        """
        schema = {}
        schema_str = agent.agent_data.get(schema_type, "")

        # Parse the schema string to extract field names, types, and descriptions
        if schema_str:
            fields = schema_str.split(",")
            for field in fields:
                field = field.strip()
                if ":" in field:
                    name, rest = field.split(":", 1)
                    name = name.strip()
                    schema[name] = rest.strip()
                else:
                    schema[field] = "Any"

        return schema

    # The _create_next_input method is no longer needed since we're using hand_off_mode="add"
    # instead of manually preparing inputs for the next agent

```

---

### 129. src\flock\routers\agent\handoff_agent.py

- **File ID**: file_128
- **Type**: Code File
- **Line Count**: 58
- **Description**: Handoff agent for the agent-based router.
- **Dependencies**: None
- **Used By**: None

**Content**:
```
"""Handoff agent for the agent-based router."""

from pydantic import BaseModel

from flock.core.flock_agent import FlockAgent


class AgentInfo(BaseModel):
    """Information about an agent for handoff decisions."""

    name: str
    description: str = ""
    input_schema: str = ""
    output_schema: str = ""


class HandoffDecision(BaseModel):
    """Decision about which agent to hand off to."""

    agent_name: str
    confidence: float
    reasoning: str


class HandoffAgent(FlockAgent):
    """Agent that decides which agent to hand off to next.

    This agent analyzes the current agent's output and available agents
    to determine the best next agent in the workflow.
    """

    def __init__(
        self,
        name: str = "handoff_agent",
        model: str | None = None,
        description: str = "Decides which agent to hand off to next",
    ):
        """Initialize the HandoffAgent.

        Args:
            name: The name of the agent
            model: The model to use (e.g., 'openai/gpt-4o')
            description: A human-readable description of the agent
        """
        super().__init__(
            name=name,
            model=model,
            description=description,
            input=(
                "current_agent_name: str | Name of the current agent, "
                "current_agent_description: str | Description of the current agent, "
                "current_agent_input: str | Input schema of the current agent, "
                "current_agent_output: str | Output schema of the current agent, "
                "current_result: dict | Output from the current agent, "
                "available_agents: list[AgentInfo] | List of available agents"
            ),
            output="agent_name: str | Name of the agent to hand off to, confidence: float | Confidence in the decision, reasoning: str | Reasoning for the decision",
        )

```

---

### 130. src\flock\routers\conditional\conditional_router.py

- **File ID**: file_129
- **Type**: Code File
- **Line Count**: 482
- **Description**: File at src\flock\routers\conditional\conditional_router.py
- **Dependencies**: None
- **Used By**: None

**Content**:
```
# src/flock/routers/conditional/conditional_router.py

import re
from collections.abc import Callable
from typing import Any, Literal

from pydantic import Field, model_validator

from flock.core.context.context import FlockContext
from flock.core.flock_agent import FlockAgent
from flock.core.flock_registry import flock_component, get_registry
from flock.core.flock_router import (
    FlockRouter,
    FlockRouterConfig,
    HandOffRequest,
)
from flock.core.logging.logging import get_logger

logger = get_logger("router.conditional")


class ConditionalRouterConfig(FlockRouterConfig):
    """Configuration for the ConditionalRouter."""

    condition_context_key: str = Field(
        default="flock.condition",
        description="Context key containing the value to evaluate the condition against.",
    )

    # --- Define ONE type of condition check ---
    condition_callable: (
        str | Callable[[Any], tuple[bool, str | None]] | None
    ) = Field(
        default=None,
        description="A callable (or registered name) that takes the context value and returns a tuple containing: (bool: True if condition passed, False otherwise, Optional[str]: Feedback message if condition failed).",
    )
    # String Checks
    expected_string: str | None = Field(
        default=None, description="String value to compare against."
    )
    string_mode: Literal[
        "equals",
        "contains",
        "regex",
        "startswith",
        "endswith",
        "not_equals",
        "not_contains",
    ] = Field(default="equals", description="How to compare strings.")
    ignore_case: bool = Field(
        default=True, description="Ignore case during string comparison."
    )
    # Length Checks (String or List)
    min_length: int | None = Field(
        default=None,
        description="Minimum length for strings or items for lists.",
    )
    max_length: int | None = Field(
        default=None,
        description="Maximum length for strings or items for lists.",
    )
    # Number Checks
    expected_number: int | float | None = Field(
        default=None, description="Number to compare against."
    )
    number_mode: Literal["<", "<=", "==", "!=", ">=", ">"] = Field(
        default="==", description="How to compare numbers."
    )
    # List Checks
    min_items: int | None = Field(
        default=None, description="Minimum number of items in a list."
    )
    max_items: int | None = Field(
        default=None, description="Maximum number of items in a list."
    )
    # Type Check
    expected_type_name: str | None = Field(
        default=None,
        description="Registered name of the expected Python type (e.g., 'str', 'list', 'MyCustomType').",
    )
    # Boolean Check
    expected_bool: bool | None = Field(
        default=None, description="Expected boolean value (True or False)."
    )
    # Existence Check
    check_exists: bool | None = Field(
        default=None,
        description="If True, succeeds if key exists; if False, succeeds if key *doesn't* exist. Ignores value.",
    )

    # --- Routing Targets ---
    success_agent: str | None = Field(
        default=None,
        description="Agent name to route to if the condition evaluates to True.",
    )
    failure_agent: str | None = Field(
        default=None,
        description="Agent name to route to if the condition evaluates to False (after retries, if enabled).",
    )
    retry_agent: str | None = Field(
        default=None,
        description="Agent name to route to if the condition evaluates to False (during retries, if enabled).",
    )

    # --- Optional Retry Logic (for Failure Path) ---
    retry_on_failure: bool = Field(
        default=False,
        description="If True, route back to the retry_agent on failure before going to failure_agent.",
    )
    max_retries: int = Field(
        default=1,
        description="Maximum number of times to retry the current agent on failure.",
    )
    feedback_context_key: str | None = Field(
        default="flock.assertion_feedback",  # Useful if paired with AssertionCheckerModule
        description="Optional context key containing feedback message to potentially include when retrying.",
    )
    retry_count_context_key_prefix: str = Field(
        default="flock.conditional_retry_count_",
        description="Internal prefix for context key storing retry attempts per agent.",
    )

    # --- Validator to ensure only one condition type is set ---
    @model_validator(mode="after")
    def check_exclusive_condition(self) -> "ConditionalRouterConfig":
        conditions_set = [
            self.condition_callable is not None,
            self.expected_string is not None
            or self.min_length is not None
            or self.max_length is not None,  # String/Length group
            self.expected_number is not None,  # Number group
            self.min_items is not None
            or self.max_items is not None,  # List size group
            self.expected_type_name is not None,  # Type group
            self.expected_bool is not None,  # Bool group
            self.check_exists is not None,  # Existence group
        ]
        if sum(conditions_set) > 1:
            raise ValueError(
                "Only one type of condition (callable, string/length, number, list size, type, boolean, exists) can be configured per ConditionalRouter."
            )
        if sum(conditions_set) == 0:
            raise ValueError(
                "At least one condition type must be configured for ConditionalRouter."
            )
        return self


@flock_component
class ConditionalRouter(FlockRouter):
    """Routes workflow based on evaluating a condition against a value in the FlockContext.
    Supports various built-in checks (string, number, list, type, bool, existence)
    or a custom callable. Can optionally retry the current agent on failure.
    """

    name: str = "conditional_router"
    config: ConditionalRouterConfig = Field(
        default_factory=ConditionalRouterConfig
    )

    def _evaluate_condition(self, value: Any) -> tuple[bool, str | None]:
        """Evaluates the condition based on the router's configuration.

        Returns:
            Tuple[bool, Optional[str]]: A tuple containing:
                - bool: True if the condition passed, False otherwise.
                - Optional[str]: A feedback message if the condition failed, otherwise None.
        """
        cfg = self.config
        condition_passed = False
        feedback = cfg.feedback_on_failure  # Default feedback
        condition_type = "unknown"

        try:
            # 0. Check Existence first (simplest)
            if cfg.check_exists is not None:
                condition_type = "existence"
                value_exists = value is not None
                condition_passed = (
                    value_exists if cfg.check_exists else not value_exists
                )
                if not condition_passed:
                    feedback = f"Existence check failed: Expected key '{cfg.condition_context_key}' to {'exist' if cfg.check_exists else 'not exist or be None'}, but it was {'found' if value_exists else 'missing/None'}."

            # 1. Custom Callable
            elif cfg.condition_callable:
                condition_type = "callable"
                callable_func = cfg.condition_callable
                if isinstance(callable_func, str):  # Lookup registered callable
                    registry = get_registry()
                    try:
                        callable_func = registry.get_callable(callable_func)
                    except KeyError:
                        feedback = f"Condition callable '{cfg.condition_callable}' not found in registry."
                        logger.error(feedback)
                        return False, feedback  # Treat as failure

                if callable(callable_func):
                    eval_result = callable_func(value)
                    if (
                        isinstance(eval_result, tuple)
                        and len(eval_result) == 2
                        and isinstance(eval_result[0], bool)
                    ):
                        condition_passed, custom_feedback = eval_result
                        if not condition_passed and isinstance(
                            custom_feedback, str
                        ):
                            feedback = custom_feedback
                    elif isinstance(eval_result, bool):
                        condition_passed = eval_result
                        if not condition_passed:
                            feedback = f"Callable condition '{getattr(callable_func, '__name__', 'anonymous')}' returned False."
                    else:
                        feedback = f"Condition callable '{getattr(callable_func, '__name__', 'anonymous')}' returned unexpected type: {type(eval_result)}."
                        logger.warning(feedback)
                        return False, feedback  # Treat as failure
                else:
                    feedback = f"Configured condition_callable '{cfg.condition_callable}' is not callable."
                    logger.error(feedback)
                    return False, feedback

            # 2. String / Length Checks
            elif (
                cfg.expected_string is not None
                or cfg.min_length is not None
                or cfg.max_length is not None
            ):
                condition_type = "string/length"
                if not isinstance(value, str):
                    feedback = f"Cannot perform string/length check on non-string value: {type(value)}."
                    logger.warning(feedback)
                    return False, feedback
                s_value = value
                val_len = len(s_value)
                length_passed = True
                length_feedback = []
                if cfg.min_length is not None and val_len < cfg.min_length:
                    length_passed = False
                    length_feedback.append(
                        f"length {val_len} is less than minimum {cfg.min_length}"
                    )
                if cfg.max_length is not None and val_len > cfg.max_length:
                    length_passed = False
                    length_feedback.append(
                        f"length {val_len} is greater than maximum {cfg.max_length}"
                    )

                content_passed = True
                content_feedback = ""
                if cfg.expected_string is not None:
                    expected = cfg.expected_string
                    s1 = s_value if not cfg.ignore_case else s_value.lower()
                    s2 = expected if not cfg.ignore_case else expected.lower()
                    mode = cfg.string_mode
                    if mode == "equals":
                        content_passed = s1 == s2
                    elif mode == "contains":
                        content_passed = s2 in s1
                    elif mode == "startswith":
                        content_passed = s1.startswith(s2)
                    elif mode == "endswith":
                        content_passed = s1.endswith(s2)
                    elif mode == "not_equals":
                        content_passed = s1 != s2
                    elif mode == "not_contains":
                        content_passed = s2 not in s1
                    elif mode == "regex":
                        content_passed = bool(re.search(expected, value))
                    else:
                        content_passed = False
                    if not content_passed:
                        content_feedback = f"String content check '{mode}' failed against expected '{expected}' (ignore_case={cfg.ignore_case})."

                condition_passed = length_passed and content_passed
                if not condition_passed:
                    feedback_parts = length_feedback + (
                        [content_feedback] if content_feedback else []
                    )
                    feedback = (
                        "; ".join(feedback_parts)
                        if feedback_parts
                        else "String/length condition failed."
                    )

            # 3. Number Check
            elif cfg.expected_number is not None:
                condition_type = "number"
                if not isinstance(value, (int, float)):
                    feedback = f"Cannot perform number check on non-numeric value: {type(value)}."
                    logger.warning(feedback)
                    return False, feedback
                num_value = value
                expected = cfg.expected_number
                mode = cfg.number_mode
                op_map = {
                    "<": lambda a, b: a < b,
                    "<=": lambda a, b: a <= b,
                    "==": lambda a, b: a == b,
                    "!=": lambda a, b: a != b,
                    ">=": lambda a, b: a >= b,
                    ">": lambda a, b: a > b,
                }
                if mode in op_map:
                    condition_passed = op_map[mode](num_value, expected)
                    if not condition_passed:
                        feedback = f"Number check failed: {num_value} {mode} {expected} is false."
                else:
                    condition_passed = False
                    feedback = f"Invalid number comparison mode: {mode}"

            # 4. List Size Check
            elif cfg.min_items is not None or cfg.max_items is not None:
                condition_type = "list size"
                if not isinstance(value, list):
                    feedback = f"Cannot perform list size check on non-list value: {type(value)}."
                    logger.warning(feedback)
                    return False, feedback
                list_len = len(value)
                size_passed = True
                size_feedback = []
                if cfg.min_items is not None and list_len < cfg.min_items:
                    size_passed = False
                    size_feedback.append(
                        f"list size {list_len} is less than minimum {cfg.min_items}"
                    )
                if cfg.max_items is not None and list_len > cfg.max_items:
                    size_passed = False
                    size_feedback.append(
                        f"list size {list_len} is greater than maximum {cfg.max_items}"
                    )
                condition_passed = size_passed
                if not condition_passed:
                    feedback = "; ".join(size_feedback)

            # 5. Type Check
            elif cfg.expected_type_name is not None:
                condition_type = "type"
                registry = get_registry()
                try:
                    expected_type = registry.get_type(cfg.expected_type_name)
                    condition_passed = isinstance(value, expected_type)
                    if not condition_passed:
                        feedback = f"Type check failed: Value type '{type(value).__name__}' is not instance of expected '{cfg.expected_type_name}'."
                except KeyError:
                    feedback = f"Expected type '{cfg.expected_type_name}' not found in registry."
                    logger.error(feedback)
                    return False, feedback

            # 6. Boolean Check
            elif cfg.expected_bool is not None:
                condition_type = "boolean"
                if not isinstance(value, bool):
                    feedback = f"Cannot perform boolean check on non-bool value: {type(value)}."
                    logger.warning(feedback)
                    return False, feedback
                condition_passed = value == cfg.expected_bool
                if not condition_passed:
                    feedback = f"Boolean check failed: Value '{value}' is not expected '{cfg.expected_bool}'."

            logger.debug(
                f"Condition check '{condition_type}' result: {condition_passed}"
            )
            return condition_passed, feedback if not condition_passed else None

        except Exception as e:
            feedback = (
                f"Error evaluating condition type '{condition_type}': {e}"
            )
            logger.error(feedback, exc_info=True)
            return (
                False,
                feedback,
            )  # Treat evaluation errors as condition failure

    async def route(
        self,
        current_agent: FlockAgent,
        result: dict[str, Any],
        context: FlockContext,
    ) -> HandOffRequest:
        cfg = self.config
        condition_value = context.get_variable(cfg.condition_context_key, None)
        feedback_value = context.get_variable(cfg.feedback_context_key, None)

        logger.debug(
            f"Routing based on condition key '{cfg.condition_context_key}', value: {str(condition_value)[:100]}..."
        )

        # Evaluate the condition and get feedback on failure
        condition_passed, feedback_msg = self._evaluate_condition(
            condition_value
        )

        if condition_passed:
            # --- Success Path ---
            logger.info(
                f"Condition PASSED for agent '{current_agent.name}'. Routing to success path."
            )
            # Reset retry count if applicable
            if cfg.retry_on_failure:
                retry_key = (
                    f"{cfg.retry_count_context_key_prefix}{current_agent.name}"
                )
                if retry_key in context.state:
                    del context.state[retry_key]
                    logger.debug(
                        f"Reset retry count for agent '{current_agent.name}'."
                    )

            # Clear feedback from context on success
            if (
                cfg.feedback_context_key
                and cfg.feedback_context_key in context.state
            ):
                del context.state[cfg.feedback_context_key]
                logger.debug(
                    f"Cleared feedback key '{cfg.feedback_context_key}' on success."
                )

            next_agent = cfg.success_agent or ""  # Stop chain if None
            logger.debug(f"Success route target: '{next_agent}'")
            return HandOffRequest(next_agent=next_agent)

        else:
            # --- Failure Path ---
            logger.warning(
                f"Condition FAILED for agent '{current_agent.name}'. Reason: {feedback_msg}"
            )

            if cfg.retry_on_failure:
                # --- Retry Logic ---
                retry_key = (
                    f"{cfg.retry_count_context_key_prefix}{current_agent.name}"
                )
                retry_count = context.get_variable(retry_key, 0)

                if retry_count < cfg.max_retries:
                    next_retry_count = retry_count + 1
                    context.set_variable(retry_key, next_retry_count)
                    logger.info(
                        f"Routing back to agent '{current_agent.name}' for retry #{next_retry_count}/{cfg.max_retries}."
                    )

                    # Add specific feedback to context if retry is enabled
                    if cfg.feedback_context_key:
                        context.set_variable(
                            cfg.feedback_context_key,
                            feedback_msg or cfg.feedback_on_failure,
                        )
                        logger.debug(
                            f"Set feedback key '{cfg.feedback_context_key}': {feedback_msg or cfg.feedback_on_failure}"
                        )

                    return HandOffRequest(
                        next_agent=current_agent.name,  # Route back to self
                        output_to_input_merge_strategy="add",  # Make feedback available
                    )
                else:
                    # --- Max Retries Exceeded ---
                    logger.error(
                        f"Max retries ({cfg.max_retries}) exceeded for agent '{current_agent.name}'."
                    )
                    if retry_key in context.state:
                        del context.state[retry_key]  # Reset count
                    # Clear feedback before final failure route? Optional.
                    # if cfg.feedback_context_key in context.state: del context.state[cfg.feedback_context_key]
                    next_agent = cfg.failure_agent or ""
                    logger.debug(
                        f"Failure route target (after retries): '{next_agent}'"
                    )
                    return HandOffRequest(next_agent=next_agent)
            else:
                # --- No Retry Logic ---
                next_agent = (
                    cfg.failure_agent or ""
                )  # Use failure agent or stop
                logger.debug(f"Failure route target (no retry): '{next_agent}'")
                # Optionally add feedback even if not retrying?
                # if cfg.feedback_context_key:
                #     context.set_variable(cfg.feedback_context_key, feedback_msg or cfg.feedback_on_failure)
                return HandOffRequest(next_agent=next_agent)

```

---

### 131. src\flock\routers\default\__init__.py

- **File ID**: file_130
- **Type**: Code File
- **Line Count**: 1
- **Description**: Default router implementation for the Flock framework.
- **Dependencies**: None
- **Used By**: None

**Content**:
```
"""Default router implementation for the Flock framework."""

```

---

### 132. src\flock\routers\default\default_router.py

- **File ID**: file_131
- **Type**: Code File
- **Line Count**: 78
- **Description**: Default router implementation for the Flock framework.
- **Dependencies**: None
- **Used By**: None

**Content**:
```
"""Default router implementation for the Flock framework."""

from collections.abc import Callable
from typing import Any

from pydantic import Field

from flock.core.context.context import FlockContext
from flock.core.flock_agent import FlockAgent
from flock.core.flock_router import (
    FlockRouter,
    FlockRouterConfig,
    HandOffRequest,
)
from flock.core.logging.logging import get_logger

logger = get_logger("default_router")


class DefaultRouterConfig(FlockRouterConfig):
    """Configuration for the default router."""

    hand_off: str | HandOffRequest | Callable[..., HandOffRequest] = Field(
        default="", description="Next agent to hand off to"
    )


class DefaultRouter(FlockRouter):
    """Default router implementation.

    This router simply uses the agent's hand_off property to determine the next agent.
    It does not perform any dynamic routing.
    """

    name: str = "default_router"
    config: DefaultRouterConfig = Field(
        default_factory=DefaultRouterConfig, description="Output configuration"
    )

    def __init__(
        self,
        name: str = "default_router",
        config: DefaultRouterConfig | None = None,
    ):
        """Initialize the DefaultRouter.

        Args:
            name: The name of the router
            config: The router configuration
        """
        super().__init__(
            name=name, config=config or DefaultRouterConfig(name=name)
        )

    async def route(
        self,
        current_agent: FlockAgent,
        result: dict[str, Any],
        context: FlockContext,
    ) -> HandOffRequest:
        """Determine the next agent to hand off to based on the current agent's output.

        Args:
            current_agent: The agent that just completed execution
            result: The output from the current agent
            context: The global execution context

        Returns:
            A HandOff object containing the next agent and input data
        """
        handoff = self.config.hand_off
        if callable(handoff):
            handoff = handoff(context, result)
        if isinstance(handoff, str):
            handoff = HandOffRequest(
                next_agent=handoff, output_to_input_merge_strategy="match"
            )
        return handoff

```

---

### 133. src\flock\routers\feedback\feedback_router.py

- **File ID**: file_132
- **Type**: Code File
- **Line Count**: 114
- **Description**: File at src\flock\routers\feedback\feedback_router.py
- **Dependencies**: None
- **Used By**: None

**Content**:
```
# src/flock/routers/correction/correction_router.py (New File)

from typing import Any

from pydantic import Field

from flock.core.context.context import FlockContext
from flock.core.flock_agent import FlockAgent
from flock.core.flock_registry import flock_component
from flock.core.flock_router import (
    FlockRouter,
    FlockRouterConfig,
    HandOffRequest,
)
from flock.core.logging.logging import get_logger

logger = get_logger("router.correction")


class FeedbackRetryRouterConfig(FlockRouterConfig):
    max_retries: int = Field(
        default=1,
        description="Maximum number of times to retry the same agent on failure.",
    )
    feedback_context_key: str = Field(
        default="flock.assertion_feedback",
        description="Context key containing feedback from AssertionCheckerModule.",
    )
    retry_count_context_key_prefix: str = Field(
        default="flock.retry_count_",
        description="Prefix for context key storing retry attempts per agent.",
    )
    fallback_agent: str | None = Field(
        None, description="Agent to route to if max_retries is exceeded."
    )


@flock_component
class FeedbackRetryRouter(FlockRouter):
    """Routes based on assertion feedback in the context.

    If feedback exists for the current agent and retries are not exhausted,
    it routes back to the same agent, adding the feedback to its input.
    Otherwise, it can route to a fallback agent or stop the chain.
    """

    name: str = "feedback_retry_router"
    config: FeedbackRetryRouterConfig = Field(
        default_factory=FeedbackRetryRouterConfig
    )

    async def route(
        self,
        current_agent: FlockAgent,
        result: dict[str, Any],
        context: FlockContext,
    ) -> HandOffRequest:
        feedback = context.get_variable(self.config.feedback_context_key)

        if feedback:
            logger.warning(
                f"Assertion feedback detected for agent '{current_agent.name}'. Attempting retry."
            )

            retry_key = f"{self.config.retry_count_context_key_prefix}{current_agent.name}"
            retry_count = context.get_variable(retry_key, 0)
            logger.warning(f"Feedback: {feedback} - Retry Count {retry_count}")

            if retry_count < self.config.max_retries:
                logger.info(
                    f"Routing back to agent '{current_agent.name}' for retry #{retry_count + 1}"
                )
                context.set_variable(retry_key, retry_count + 1)
                context.set_variable(
                    f"{current_agent.name}_prev_result", result
                )
                # Add feedback to the *next* agent's input (which is the same agent)
                # Requires the agent's signature to potentially accept a 'feedback' input field.
                return HandOffRequest(
                    next_agent=current_agent.name,
                    output_to_input_merge_strategy="match",  # Add feedback to existing context/previous results
                    add_input_fields=[
                        f"{self.config.feedback_context_key} | Feedback for prev result",
                        f"{current_agent.name}_prev_result | Previous Result",
                    ],
                    add_description=f"Try to fix the previous result based on the feedback.",
                    override_context=None,  # Context already updated with feedback and retry count
                )
            else:
                logger.error(
                    f"Max retries ({self.config.max_retries}) exceeded for agent '{current_agent.name}'."
                )
                # Max retries exceeded, route to fallback or stop
                if self.config.fallback_agent:
                    logger.info(
                        f"Routing to fallback agent '{self.config.fallback_agent}'"
                    )
                    # Clear feedback before going to fallback? Optional.
                    if self.config.feedback_context_key in context.state:
                        del context.state[self.config.feedback_context_key]
                    return HandOffRequest(next_agent=self.config.fallback_agent)
                else:
                    logger.info("No fallback agent defined. Stopping workflow.")
                    return HandOffRequest(next_agent="")  # Stop the chain

        else:
            # No feedback, assertions passed or module not configured for feedback
            logger.debug(
                f"No assertion feedback for agent '{current_agent.name}'. Proceeding normally."
            )
            # Default behavior: Stop the chain if no other routing is defined
            # In a real system, you might chain this with another router (e.g., LLMRouter)
            # to decide the *next different* agent if assertions passed.
            return HandOffRequest(next_agent="")  # Stop or pass to next router

```

---

### 134. src\flock\routers\list_generator\list_generator_router.py

- **File ID**: file_133
- **Type**: Code File
- **Line Count**: 166
- **Description**: File at src\flock\routers\list_generator\list_generator_router.py
- **Dependencies**: None
- **Used By**: None

**Content**:
```
# src/flock/routers/list_generator/iterative_list_router.py (New File)

from typing import Any

from pydantic import Field

from flock.core.context.context import FlockContext
from flock.core.flock_agent import FlockAgent
from flock.core.flock_registry import flock_component
from flock.core.flock_router import (
    FlockRouter,
    FlockRouterConfig,
    HandOffRequest,
)
from flock.core.logging.logging import get_logger

# Need signature utils

logger = get_logger("router.list_generator")


class IterativeListGeneratorRouterConfig(FlockRouterConfig):
    target_list_field: str = Field(
        ...,
        description="Name of the final list output field (e.g., 'chapters').",
    )
    item_output_field: str = Field(
        ...,
        description="Name of the single item output field for each iteration (e.g., 'chapter').",
    )
    context_input_field: str = Field(
        default="previous_items",
        description="Input field name for passing back generated items (e.g., 'existing_chapters').",
    )
    max_iterations: int = Field(
        default=10, description="Maximum number of items to generate."
    )
    # More advanced: termination_condition: Optional[Callable] = None
    # Store iteration state in context under this prefix
    context_state_prefix: str = Field(
        default="flock.iterator_state_",
        description="Prefix for context keys storing iteration state.",
    )

    # Field to extract item type from target_list_field signature
    # This might require parsing the original agent's output signature
    # item_type_str: Optional[str] = None # e.g., 'dict[str, str]' or 'MyChapterType'


@flock_component
class IterativeListGeneratorRouter(FlockRouter):
    name: str = "iterative_list_generator"
    config: IterativeListGeneratorRouterConfig = Field(
        default_factory=IterativeListGeneratorRouterConfig
    )

    # Helper to get state keys
    def _get_state_keys(self, agent_name: str) -> tuple[str, str]:
        prefix = self.config.context_state_prefix
        list_key = f"{prefix}{agent_name}_{self.config.target_list_field}"
        count_key = f"{prefix}{agent_name}_iteration_count"
        return list_key, count_key

    async def route(
        self,
        current_agent: FlockAgent,
        result: dict[str, Any],
        context: FlockContext,
    ) -> HandOffRequest:
        list_key, count_key = self._get_state_keys(current_agent.name)

        # --- State Initialization (First Run) ---
        if count_key not in context.state:
            logger.debug(
                f"Initializing iterative list generation for '{self.config.target_list_field}' in agent '{current_agent.name}'."
            )
            context.set_variable(count_key, 0)
            context.set_variable(list_key, [])
            # Modify agent signature for the *first* iteration (remove context_input_field, use item_output_field)
            # This requires modifying the agent's internal state or creating a temporary one.
            # Let's try modifying the context passed to the *next* run instead.
            context.set_variable(
                f"{current_agent.name}.next_run_output_field",
                self.config.item_output_field,
            )
            context.set_variable(
                f"{current_agent.name}.next_run_input_fields_to_exclude",
                {self.config.context_input_field},
            )

        # --- Process Result of Previous Iteration ---
        iteration_count = context.get_variable(count_key, 0)
        generated_items = context.get_variable(list_key, [])

        # Get the single item generated in the *last* run
        # The result dict should contain the 'item_output_field' if it wasn't the very first run
        new_item = result.get(self.config.item_output_field)

        if (
            new_item is not None and iteration_count > 0
        ):  # Add item from previous run (not the init run)
            generated_items.append(new_item)
            context.set_variable(list_key, generated_items)  # Update context
            logger.info(
                f"Added item #{iteration_count} to list '{self.config.target_list_field}' for agent '{current_agent.name}'."
            )
        elif iteration_count > 0:
            logger.warning(
                f"Iteration {iteration_count} for agent '{current_agent.name}' did not produce expected output field '{self.config.item_output_field}'."
            )
            # Decide how to handle: stop, retry, continue? Let's continue for now.

        # Increment iteration count *after* processing the result of the previous one
        current_iteration = iteration_count + 1
        context.set_variable(count_key, current_iteration)

        # --- Termination Check ---
        if current_iteration > self.config.max_iterations:
            logger.info(
                f"Max iterations ({self.config.max_iterations}) reached for '{self.config.target_list_field}' in agent '{current_agent.name}'. Finalizing."
            )
            # Clean up state
            del context.state[count_key]
            # Final result should be the list itself under the target_list_field key
            final_result = {self.config.target_list_field: generated_items}
            # Handoff with empty next_agent to stop, but potentially override the *result*
            # This is tricky. Routers usually decide the *next agent*, not the *final output*.
            # Maybe the router should just signal termination, and the Flock run loop handles assembling the final output?
            # Let's assume the router signals termination by returning next_agent=""
            # The final list is already in the context under list_key.
            # A final "AssemblerAgent" could read this context variable.
            # OR we modify the HandOffRequest:
            return HandOffRequest(
                next_agent="", final_output_override=final_result
            )  # Needs HandOffRequest modification

        # --- Prepare for Next Iteration ---
        logger.info(
            f"Routing back to agent '{current_agent.name}' for item #{current_iteration} of '{self.config.target_list_field}'."
        )

        # The agent needs the context (previously generated items) and the original inputs again.
        # We will pass the generated items via the context_input_field.
        # The original inputs (like story_outline) should still be in the context.
        next_input_override = {
            self.config.context_input_field: generated_items  # Pass the list back
        }

        # Modify agent signature for the *next* iteration (add context_input_field, use item_output_field)
        # This is the trickiest part - how to modify the agent's perceived signature for the next run?
        # Option 1: Pass overrides via HandOffRequest (cleanest)
        next_signature_input = f"{current_agent.input}, {self.config.context_input_field}: list | Previously generated items"  # Needs smarter joining
        next_signature_output = (
            self.config.item_output_field
        )  # Only ask for one item

        # This requires HandOffRequest and Flock execution loop to support signature overrides
        return HandOffRequest(
            next_agent=current_agent.name,
            output_to_input_merge_strategy="add",  # Add the context_input_field to existing context
            input_override=next_input_override,  # Provide the actual list data
            # --- Hypothetical Overrides ---
            next_run_input_signature_override=next_signature_input,
            next_run_output_signature_override=next_signature_output,
            # -----------------------------
        )

```

---

### 135. src\flock\routers\llm\__init__.py

- **File ID**: file_134
- **Type**: Code File
- **Line Count**: 1
- **Description**: LLM-based router implementation for the Flock framework.
- **Dependencies**: None
- **Used By**: None

**Content**:
```
"""LLM-based router implementation for the Flock framework."""

```

---

### 136. src\flock\routers\llm\llm_router.py

- **File ID**: file_135
- **Type**: Code File
- **Line Count**: 363
- **Description**: LLM-based router implementation for the Flock framework.
- **Dependencies**: None
- **Used By**: None

**Content**:
```
"""LLM-based router implementation for the Flock framework."""

import json
from typing import Any

import litellm

from flock.core.context.context import FlockContext
from flock.core.flock_agent import FlockAgent
from flock.core.flock_router import (
    FlockRouter,
    FlockRouterConfig,
    HandOffRequest,
)
from flock.core.logging.logging import get_logger

logger = get_logger("llm_router")


class LLMRouterConfig(FlockRouterConfig):
    """Configuration for the LLM router.

    This class extends FlockRouterConfig with parameters specific to the LLM router.
    """

    temperature: float = 0.2
    max_tokens: int = 500
    confidence_threshold: float = 0.5
    prompt: str = ""


class LLMRouter(FlockRouter):
    """Router that uses an LLM to determine the next agent in a workflow.

    This class is responsible for:
    1. Analyzing available agents in the registry
    2. Using an LLM to score each agent's suitability as the next step
    3. Selecting the highest-scoring agent
    4. Creating a HandOff object with the selected agent
    """

    def __init__(
        self,
        name: str = "llm_router",
        config: LLMRouterConfig | None = None,
    ):
        """Initialize the LLMRouter.

        Args:
            registry: The agent registry containing all available agents
            name: The name of the router
            config: The router configuration
        """
        logger.info(f"Initializing LLM Router '{name}'")
        super().__init__(name=name, config=config or LLMRouterConfig(name=name))
        logger.debug(
            "LLM Router configuration",
            temperature=self.config.temperature,
            max_tokens=self.config.max_tokens,
        )

    async def route(
        self,
        current_agent: FlockAgent,
        result: dict[str, Any],
        context: FlockContext,
    ) -> HandOffRequest:
        """Determine the next agent to hand off to based on the current agent's output.

        Args:
            current_agent: The agent that just completed execution
            result: The output from the current agent
            context: The global execution context

        Returns:
            A HandOff object containing the next agent and input data
        """
        logger.info(
            f"Routing from agent '{current_agent.name}'",
            current_agent=current_agent.name,
        )
        logger.debug("Current agent result", result=result)

        agent_definitions = context.agent_definitions
        # Get all available agents from the registry
        available_agents = self._get_available_agents(
            agent_definitions, current_agent.name
        )
        logger.debug(
            "Available agents for routing",
            count=len(available_agents),
            agents=[a.agent_data["name"] for a in available_agents],
        )

        if not available_agents:
            logger.warning(
                "No available agents for routing",
                current_agent=current_agent.name,
            )
            return HandOffRequest(
                next_agent="", override_next_agent={}, override_context=None
            )

        # Use LLM to determine the best next agent
        next_agent_name, score = await self._select_next_agent(
            current_agent, result, available_agents
        )
        logger.info(
            "Agent selection result",
            next_agent=next_agent_name,
            score=score,
        )

        if not next_agent_name or score < self.config.confidence_threshold:
            logger.warning(
                "No suitable next agent found",
                best_score=score,
            )
            return HandOffRequest(
                next_agent="", override_next_agent={}, override_context=None
            )

        # Get the next agent from the registry
        next_agent = agent_definitions.get(next_agent_name)
        if not next_agent:
            logger.error(
                "Selected agent not found in registry",
                agent_name=next_agent_name,
            )
            return HandOffRequest(
                next_agent="", override_next_agent={}, override_context=None
            )

        # Create input for the next agent

        logger.success(
            f"Successfully routed to agent '{next_agent_name}'",
            score=score,
            from_agent=current_agent.name,
        )
        return HandOffRequest(
            next_agent=next_agent_name,
            output_to_input_merge_strategy="add",
            override_next_agent=None,
            override_context=None,
        )

    def _get_available_agents(
        self, agent_definitions: dict[str, Any], current_agent_name: str
    ) -> list[FlockAgent]:
        """Get all available agents except the current one.

        Args:
            current_agent_name: Name of the current agent to exclude

        Returns:
            List of available agents
        """
        logger.debug(
            "Getting available agents",
            total_agents=len(agent_definitions),
            current_agent=current_agent_name,
        )
        agents = []
        for agent in agent_definitions:
            if agent != current_agent_name:
                agents.append(agent_definitions.get(agent))
        return agents

    async def _select_next_agent(
        self,
        current_agent: FlockAgent,
        result: dict[str, Any],
        available_agents: list[FlockAgent],
    ) -> tuple[str, float]:
        """Use an LLM to select the best next agent.

        Args:
            current_agent: The agent that just completed execution
            result: The output from the current agent
            available_agents: List of available agents to choose from

        Returns:
            Tuple of (selected_agent_name, confidence_score)
        """
        logger.debug(
            "Selecting next agent",
            current_agent=current_agent.name,
            available_count=len(available_agents),
        )

        # Prepare the prompt for the LLM
        prompt = self._create_selection_prompt(
            current_agent, result, available_agents
        )
        logger.debug("Generated selection prompt", prompt_length=len(prompt))

        try:
            logger.info(
                "Calling LLM for agent selection",
                model=current_agent.model,
                temperature=self.config.temperature,
            )
            # Call the LLM to get the next agent
            response = await litellm.acompletion(
                model=current_agent.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=self.config.temperature
                if isinstance(self.config, LLMRouterConfig)
                else 0.2,
                max_tokens=self.config.max_tokens
                if isinstance(self.config, LLMRouterConfig)
                else 500,
            )

            content = response.choices[0].message.content
            # Parse the response to get the agent name and score
            try:
                # extract the json object from the response
                content = content.split("```json")[1].split("```")[0]
                data = json.loads(content)
                next_agent = data.get("next_agent", "")
                score = float(data.get("score", 0))
                reasoning = data.get("reasoning", "")
                logger.info(
                    "Successfully parsed LLM response",
                    next_agent=next_agent,
                    score=score,
                    reasoning=reasoning,
                )
                return next_agent, score
            except (json.JSONDecodeError, ValueError) as e:
                logger.error(
                    "Failed to parse LLM response",
                    error=str(e),
                    raw_response=content,
                )
                logger.debug("Attempting fallback parsing")

                # Fallback: try to extract the agent name from the text
                for agent in available_agents:
                    if agent.agent_data["name"] in content:
                        logger.info(
                            "Found agent name in response using fallback",
                            agent=agent.agent_data["name"],
                        )
                        return agent.agent_data[
                            "name"
                        ], 0.6  # Default score for fallback

                return "", 0.0

        except Exception as e:
            logger.error(
                "Error calling LLM for agent selection",
                error=str(e),
                current_agent=current_agent.name,
            )
            return "", 0.0

    def _create_selection_prompt(
        self,
        current_agent: FlockAgent,
        result: dict[str, Any],
        available_agents: list[FlockAgent],
    ) -> str:
        """Create a prompt for the LLM to select the next agent.

        Args:
            current_agent: The agent that just completed execution
            result: The output from the current agent
            available_agents: List of available agents to choose from

        Returns:
            Prompt string for the LLM
        """
        # Format the current agent's output
        result_str = json.dumps(result, indent=2)

        # Format the available agents' information
        agents_info = []
        for agent in available_agents:
            agent_info = {
                "name": agent.agent_data["name"],
                "description": agent.agent_data["description"]
                if agent.agent_data["description"]
                else "",
                "input": agent.agent_data["input"],
                "output": agent.agent_data["output"],
            }
            agents_info.append(agent_info)

        agents_str = json.dumps(agents_info, indent=2)

        # Create the prompt
        if self.config.prompt:
            prompt = self.config.prompt
        else:
            prompt = f"""
You are a workflow router that determines the next agent to execute in a multi-agent system.

CURRENT AGENT:
Name: {current_agent.name}
Description: {current_agent.description}
Input: {current_agent.input}
Output: {current_agent.output}

CURRENT AGENT'S OUTPUT:
{result_str}

AVAILABLE AGENTS:
{agents_str}

Based on the current agent's output and the available agents, determine which agent should be executed next.
Consider the following:
1. Which agent's input requirements best match the current agent's output?
2. Which agent's purpose and description make it the most logical next step?
3. Which agent would provide the most value in continuing the workflow?

Respond with a JSON object containing:
1. "next_agent": The name of the selected agent
2. "score": A confidence score between 0 and 1 indicating how suitable this agent is
3. "reasoning": A brief explanation of why this agent was selected

If no agent is suitable, set "next_agent" to an empty string and "score" to 0.

JSON Response:
"""
        return prompt

    def _create_next_input(
        self,
        current_agent: FlockAgent,
        result: dict[str, Any],
        next_agent: FlockAgent,
    ) -> dict[str, Any]:
        """Create the input for the next agent, including the previous agent's output.

        Args:
            current_agent: The agent that just completed execution
            result: The output from the current agent
            next_agent: The next agent to execute

        Returns:
            Input dictionary for the next agent
        """
        # Start with an empty input
        next_input = {}

        # Add a special field for the previous agent's output
        next_input["previous_agent_output"] = {
            "agent_name": current_agent.name,
            "result": result,
        }

        # Try to map the current agent's output to the next agent's input
        # This is a simple implementation that could be enhanced with more sophisticated mapping
        for key in result:
            # If the next agent expects this key, add it directly
            if key in next_agent.input:
                next_input[key] = result[key]

        return next_input

```

---

### 137. src\flock\workflow\__init__.py

- **File ID**: file_136
- **Type**: Code File
- **Line Count**: 0
- **Description**: File at src\flock\workflow\__init__.py
- **Dependencies**: None
- **Used By**: None

**Content**:
```

```

---

### 138. src\flock\workflow\activities.py

- **File ID**: file_137
- **Type**: Code File
- **Line Count**: 220
- **Description**: Defines Temporal activities for running a chain of agents with logging and tracing.
- **Dependencies**: None
- **Used By**: None

**Content**:
```
"""Defines Temporal activities for running a chain of agents with logging and tracing."""

from datetime import datetime

from opentelemetry import trace
from temporalio import activity

from flock.core.context.context import FlockContext
from flock.core.context.context_vars import FLOCK_CURRENT_AGENT, FLOCK_MODEL
from flock.core.flock_agent import FlockAgent
from flock.core.flock_registry import get_registry
from flock.core.flock_router import HandOffRequest
from flock.core.logging.logging import get_logger
from flock.core.util.input_resolver import resolve_inputs

logger = get_logger("activities")
tracer = trace.get_tracer(__name__)


@activity.defn
async def run_agent(context: FlockContext) -> dict:
    """Runs a chain of agents using the provided context.

    The context contains state, history, and agent definitions.
    After each agent run, its output is merged into the context.
    """
    # Start a top-level span for the entire run_agent activity.
    with tracer.start_as_current_span("run_agent") as span:
        registry = get_registry()
        previous_agent_name = ""
        if isinstance(context, dict):
            context = FlockContext.from_dict(context)
        current_agent_name = context.get_variable(FLOCK_CURRENT_AGENT)
        span.set_attribute("initial.agent", current_agent_name)
        logger.info("Starting agent chain", initial_agent=current_agent_name)

        agent = registry.get_agent(current_agent_name)
        if agent.model is None or agent.evaluator.config.model is None:
            agent.set_model(context.get_variable(FLOCK_MODEL))
        agent.resolve_callables(context=context)
        if not agent:
            logger.error("Agent not found", agent=current_agent_name)
            span.record_exception(
                Exception(f"Agent '{current_agent_name}' not found")
            )
            return {"error": f"Agent '{current_agent_name}' not found."}

        # Loop over agents in the chain.
        while agent:
            # Create a nested span for this iteration.
            with tracer.start_as_current_span("agent_iteration") as iter_span:
                iter_span.set_attribute("agent.name", agent.name)
                agent.context = context
                # Resolve inputs for the agent.
                agent_inputs = resolve_inputs(
                    agent.input, context, previous_agent_name
                )
                iter_span.add_event(
                    "resolved inputs", attributes={"inputs": str(agent_inputs)}
                )

                # Execute the agent with its own span.
                with tracer.start_as_current_span("execute_agent") as exec_span:
                    logger.info("Executing agent", agent=agent.name)
                    try:
                        result = await agent.run_async(agent_inputs)
                        exec_span.set_attribute("result", str(result))
                        logger.debug(
                            "Agent execution completed", agent=agent.name
                        )
                    except Exception as e:
                        logger.error(
                            "Agent execution failed",
                            agent=agent.name,
                            error=str(e),
                        )
                        exec_span.record_exception(e)
                        raise

                # Determine the next agent using the handoff router if available
                handoff_data = HandOffRequest()

                if agent.handoff_router:
                    logger.info(
                        f"Using handoff router: {agent.handoff_router.__class__.__name__}",
                        agent=agent.name,
                    )
                    try:
                        # Route to the next agent
                        handoff_data = await agent.handoff_router.route(
                            agent, result, context
                        )

                        if callable(handoff_data):
                            logger.debug(
                                "Executing handoff function", agent=agent.name
                            )
                            try:
                                handoff_data = handoff_data(context, result)
                                if isinstance(
                                    handoff_data.next_agent, FlockAgent
                                ):
                                    handoff_data.next_agent = (
                                        handoff_data.next_agent.name
                                    )
                            except Exception as e:
                                logger.error(
                                    "Handoff function error {} {}",
                                    agent=agent.name,
                                    error=str(e),
                                )
                                iter_span.record_exception(e)
                                return {"error": f"Handoff function error: {e}"}
                        elif isinstance(handoff_data.next_agent, FlockAgent):
                            handoff_data.next_agent = (
                                handoff_data.next_agent.name
                            )

                        if not handoff_data.next_agent:
                            logger.info(
                                "Router found no suitable next agent",
                                agent=agent.name,
                            )
                            context.record(
                                agent.name,
                                result,
                                timestamp=datetime.now().isoformat(),
                                hand_off=None,
                                called_from=previous_agent_name,
                            )
                            logger.info("Completing chain", agent=agent.name)
                            iter_span.add_event("chain completed")
                            return result
                    except Exception as e:
                        logger.error(
                            "Router error {} {}",
                            agent.name,
                            str(e),
                        )
                        iter_span.record_exception(e)
                        return {"error": f"Router error: {e}"}
                else:
                    # No router, so no handoff
                    logger.info(
                        "No handoff router defined, completing chain",
                        agent=agent.name,
                    )
                    context.record(
                        agent.name,
                        result,
                        timestamp=datetime.now().isoformat(),
                        hand_off=None,
                        called_from=previous_agent_name,
                    )
                    iter_span.add_event("chain completed")
                    return result

                # Record the agent run in the context.
                context.record(
                    agent.name,
                    result,
                    timestamp=datetime.now().isoformat(),
                    hand_off=handoff_data.model_dump(),
                    called_from=previous_agent_name,
                )
                previous_agent_name = agent.name
                previous_agent_output = agent.output
                if handoff_data.override_context:
                    context.update(handoff_data.override_context)

                # Prepare the next agent.
                try:
                    agent = registry.get_agent(handoff_data.next_agent)
                    if handoff_data.output_to_input_merge_strategy == "add":
                        agent.input = previous_agent_output + ", " + agent.input

                    if handoff_data.add_input_fields:
                        for field in handoff_data.add_input_fields:
                            agent.input = field + ", " + agent.input

                    if handoff_data.add_output_fields:
                        for field in handoff_data.add_output_fields:
                            agent.output = field + ", " + agent.output

                    if handoff_data.add_description:
                        if agent.description:
                            agent.description = (
                                agent.description
                                + "\n"
                                + handoff_data.add_description
                            )
                        else:
                            agent.description = handoff_data.add_description

                    agent.resolve_callables(context=context)
                    if not agent:
                        logger.error(
                            "Next agent not found",
                            agent=handoff_data.next_agent,
                        )
                        iter_span.record_exception(
                            Exception(
                                f"Next agent '{handoff_data.next_agent}' not found"
                            )
                        )
                        return {
                            "error": f"Next agent '{handoff_data.next_agent}' not found."
                        }

                    context.set_variable(FLOCK_CURRENT_AGENT, agent.name)

                    logger.info("Handing off to next agent", next=agent.name)
                    iter_span.set_attribute("next.agent", agent.name)
                except Exception as e:
                    logger.error("Error during handoff", error=str(e))
                    iter_span.record_exception(e)
                    return {"error": f"Error during handoff: {e}"}

        # If the loop exits unexpectedly, return the initial input.
        return context.get_variable("init_input")

```

---

### 139. src\flock\workflow\agent_activities.py

- **File ID**: file_138
- **Type**: Code File
- **Line Count**: 24
- **Description**: File at src\flock\workflow\agent_activities.py
- **Dependencies**: None
- **Used By**: None

**Content**:
```
from temporalio import activity

from flock.core.context.context import FlockContext
from flock.core.flock_agent import FlockAgent


@activity.defn
async def run_declarative_agent_activity(params: dict) -> dict:
    """Temporal activity to run a declarative (or batch) agent.

    Expects a dictionary with:
      - "agent_data": a dict representation of the agent (as produced by .dict()),
      - "context_data": a dict containing the FlockContext state and optionally other fields.

    The activity reconstructs the agent and a FlockContext, then calls the agent’s _evaluate() method.
    """
    agent_data = params.get("agent_data")
    context_data = params.get("context_data", {})
    # Reconstruct the agent from its serialized representation.
    agent = FlockAgent.from_dict(agent_data)
    # Reconstruct the FlockContext from the state.
    context = FlockContext.from_dict(context_data)
    result = await agent.evaluate(context)
    return result

```

---

### 140. src\flock\workflow\temporal_setup.py

- **File ID**: file_139
- **Type**: Code File
- **Line Count**: 38
- **Description**: File at src\flock\workflow\temporal_setup.py
- **Dependencies**: None
- **Used By**: None

**Content**:
```
import asyncio
import uuid

from temporalio.client import Client
from temporalio.worker import Worker


async def create_temporal_client() -> Client:
    client = await Client.connect("localhost:7233")
    return client


async def setup_worker(workflow, activity) -> Client:
    worker_client = await create_temporal_client()
    worker = Worker(worker_client, task_queue="flock-queue", workflows=[workflow], activities=[activity])
    asyncio.create_task(worker.run())
    await asyncio.sleep(1)


async def run_worker(client: Client, task_queue: str, workflows, activities):
    worker = Worker(client, task_queue=task_queue, workflows=workflows, activities=activities)
    await worker.run()


async def run_activity(client: Client, name: str, func, param):
    run_id = f"{name}_{uuid.uuid4().hex[:4]}"

    try:
        result = await client.execute_activity(
            func,
            param,
            id=run_id,
            task_queue="flock-queue",
            start_to_close_timeout=300,  # e.g., 5 minutes
        )
        return result
    except Exception:
        raise

```

---

### 141. src\flock\workflow\workflow.py

- **File ID**: file_140
- **Type**: Code File
- **Line Count**: 58
- **Description**: File at src\flock\workflow\workflow.py
- **Dependencies**: None
- **Used By**: None

**Content**:
```
from datetime import timedelta

from temporalio import workflow

from flock.core.context.context import FlockContext
from flock.core.logging.logging import get_logger
from flock.workflow.activities import run_agent

# Import activity, passing it through the sandbox without reloading the module


logger = get_logger("workflow")


@workflow.defn
class FlockWorkflow:
    def __init__(self) -> None:
        self.context = None

    @workflow.run
    async def run(self, context_dict: dict) -> dict:
        self.context = FlockContext.from_dict(context_dict)
        self.context.workflow_id = workflow.info().workflow_id
        self.context.workflow_timestamp = workflow.info().start_time.strftime("%Y-%m-%d %H:%M:%S")

        try:
            logger.info(
                "Starting workflow execution",
                timestamp=self.context.workflow_timestamp,
            )

            result = await workflow.execute_activity(
                run_agent,
                self.context,
                start_to_close_timeout=timedelta(minutes=5),
            )

            self.context.set_variable(
                "flock.result",
                {
                    "result": result,
                    "success": True,
                },
            )

            logger.success("Workflow completed successfully")
            return result

        except Exception as e:
            logger.exception("Workflow execution failed", error=str(e))
            self.context.set_variable(
                "flock.result",
                {
                    "result": f"Failed: {e}",
                    "success": False,
                },
            )
            return self.context

```

---

### 142. tests\__init__.py

- **File ID**: file_141
- **Type**: Code File
- **Line Count**: 1
- **Description**: Test package for Flock.
- **Dependencies**: None
- **Used By**: None

**Content**:
```
"""Test package for Flock.""" 
```

---

### 143. tests\core\test_flock_batch.py

- **File ID**: file_142
- **Type**: Code File
- **Line Count**: 75
- **Description**: File at tests\core\test_flock_batch.py
- **Dependencies**: None
- **Used By**: None

**Content**:
```


import os
import pytest
import pandas as pd
from flock.core import Flock, FlockAgent, FlockFactory
from flock.core.flock_registry import get_registry
from flock.evaluators.test.test_case_evaluator import TestCaseEvaluator, TestCaseEvaluatorConfig



@pytest.fixture
def basic_flock() -> Flock:
    """Fixture for a basic Flock instance."""
    return Flock(name="test_basic_flock", model="test-model", enable_logging=False, show_flock_banner=False)

@pytest.fixture
def simple_agent() -> FlockAgent:
    """Fixture for a simple agent instance."""
    agent = FlockFactory.create_default_agent(name="agent1", input="query", output="col1,col2,col3,col4")
    agent.evaluator = TestCaseEvaluator(name="test_case_evaluator", config=TestCaseEvaluatorConfig())
    return agent


@pytest.fixture(autouse=True)
def clear_registry():
    """Fixture to ensure a clean registry for each test."""
    registry = get_registry()
    registry._initialize() # Reset internal dictionaries
    yield # Run the test
    registry._initialize() # Clean up after test  


@pytest.mark.asyncio
async def test_batch_execution_with_dataframe_input(basic_flock: Flock, simple_agent: FlockAgent):
    """Test batch execution with CSV input."""
    batch_inputs = pd.DataFrame({
        "query": ["test1", "test2", "test3"],
    })
    results = await basic_flock.run_batch_async(
        start_agent=simple_agent,
        batch_inputs=batch_inputs,
        input_mapping={"query": "query"},
        parallel=True,
    )
    assert len(results) == 3
    assert results[0]["col1"] == "Test Result"
    assert results[1]["col2"] == "Test Result"
    assert results[2]["col3"] == "Test Result"
    assert results[2]["col4"] == "Test Result"
    
    
@pytest.mark.asyncio
async def test_batch_execution_with_dataframe_input_and_csv_output(basic_flock: Flock, simple_agent: FlockAgent):
    """Test batch execution with CSV input."""
    batch_inputs = pd.DataFrame({
        "query": ["test1", "test2", "test3"],
    })
    results = await basic_flock.run_batch_async(
        start_agent=simple_agent,
        batch_inputs=batch_inputs,
        input_mapping={"query": "query"},
        parallel=True,
        write_to_csv="test_output.csv",
        hide_columns=["col1", "col2"],
        delimiter="-",
    )
    assert os.path.exists("test_output.csv")
    df = pd.read_csv("test_output.csv", delimiter="-")
    assert len(df) == 3
    assert df.columns.tolist() == ["col3", "col4"]
    assert df["col3"][0] == "Test Result"
    assert df["col4"][1] == "Test Result"
    os.remove("test_output.csv")


```

---

### 144. tests\core\test_flock_core.py

- **File ID**: file_143
- **Type**: Code File
- **Line Count**: 289
- **Description**: File at tests\core\test_flock_core.py
- **Dependencies**: None
- **Used By**: None

**Content**:
```
# tests/core/test_flock_core.py
import asyncio
import pytest
from unittest.mock import MagicMock, patch, AsyncMock

from pydantic import BaseModel

from flock.core.flock import Flock
from flock.core.flock_agent import FlockAgent
from flock.core.context.context import FlockContext
from flock.core.flock_registry import get_registry, FlockRegistry

# Simple mock agent for testing addition
class SimpleAgent(FlockAgent):
    async def evaluate(self, inputs: dict) -> dict:
        return {"result": "simple"}

@pytest.fixture(autouse=True)
def clear_registry():
    """Fixture to ensure a clean registry for each test."""
    registry = get_registry()
    registry._initialize() # Reset internal dictionaries
    yield # Run the test
    registry._initialize() # Clean up after test


@pytest.fixture
def basic_flock() -> Flock:
    """Fixture for a basic Flock instance."""
    return Flock(name="test_basic_flock", model="test-model", enable_logging=False, show_flock_banner=False)

@pytest.fixture
def simple_agent() -> SimpleAgent:
    """Fixture for a simple agent instance."""
    return SimpleAgent(name="agent1", input="query", output="result")


# --- Initialization Tests ---

def test_flock_init_defaults():
    """Test Flock initialization with default values."""
    flock = Flock(enable_logging=False, show_flock_banner=False)
    assert flock.name.startswith("flock_")
    assert flock.model == "openai/gpt-4o" # Default from config
    assert flock.description is None
    assert not flock.enable_temporal
    assert not flock.enable_logging
    assert not flock.show_flock_banner
    assert flock._agents == {}

def test_flock_init_custom(mocker):
    """Test Flock initialization with custom values."""
    mock_configure_logging = mocker.patch.object(Flock, '_configure_logging')
    mock_set_temporal = mocker.patch.object(Flock, '_set_temporal_debug_flag')
    mock_ensure_session = mocker.patch.object(Flock, '_ensure_session_id')

    flock = Flock(
        name="custom_flock",
        model="custom_model",
        description="My custom flock",
        enable_temporal=True,
        enable_logging=["flock", "agent"],
        show_flock_banner=False
    )
    assert flock.name == "custom_flock"
    assert flock.model == "custom_model"
    assert flock.description == "My custom flock"
    assert flock.enable_temporal
    assert flock.enable_logging == ["flock", "agent"]
    assert not flock.show_flock_banner

    mock_configure_logging.assert_called_once_with(["flock", "agent"])
    mock_set_temporal.assert_called_once()
    mock_ensure_session.assert_called_once()


def test_flock_init_with_agents(simple_agent):
    """Test Flock initialization with agents passed in the constructor."""
    flock = Flock(agents=[simple_agent], enable_logging=False, show_flock_banner=False)
    assert "agent1" in flock.agents
    assert flock.agents["agent1"] is simple_agent

# --- Agent Management Tests ---

def test_add_agent(basic_flock, simple_agent):
    """Test adding an agent to the Flock."""
    basic_flock.add_agent(simple_agent)
    assert "agent1" in basic_flock._agents
    assert basic_flock._agents["agent1"] is simple_agent
    # Check if agent was registered globally too
    assert get_registry().get_agent("agent1") is simple_agent

def test_add_agent_sets_default_model(basic_flock):
    """Test that adding an agent without a model assigns the Flock's default."""
    agent_no_model = SimpleAgent(name="agent_no_model", model=None, input="in", output="out")
    basic_flock.model = "flock-default-model"
    basic_flock.add_agent(agent_no_model)
    assert agent_no_model.model == "flock-default-model"

def test_add_agent_duplicate(basic_flock, simple_agent, caplog):
    """Test adding an agent with a name that already exists."""
    basic_flock.add_agent(simple_agent)
    new_agent_same_name = SimpleAgent(name="agent1", input="query2", output="result2")
    basic_flock.add_agent(new_agent_same_name)

    assert "already exists. Overwriting" in caplog.text
    assert basic_flock.agents["agent1"] is new_agent_same_name # Should be overwritten

def test_add_agent_invalid_type(basic_flock):
    """Test adding something that is not a FlockAgent."""
    with pytest.raises(TypeError):
        basic_flock.add_agent({"not": "an agent"})

def test_agents_property(basic_flock, simple_agent):
    """Test the agents property."""
    assert basic_flock.agents == {}
    basic_flock.add_agent(simple_agent)
    assert basic_flock.agents == {"agent1": simple_agent}


# --- Execution Tests ---

@pytest.mark.asyncio
async def test_run_async_local_delegation(basic_flock, simple_agent, mocker):
    """Test run_async delegates to local executor when enable_temporal is False."""
    basic_flock.enable_temporal = False
    basic_flock.add_agent(simple_agent)
    mock_local_exec = mocker.patch('flock.core.flock.run_local_workflow', new_callable=AsyncMock)
    mock_temporal_exec = mocker.patch('flock.core.flock.run_temporal_workflow', new_callable=AsyncMock)
    mock_init_context = mocker.patch('flock.core.flock.initialize_context')
    mock_result = {"final_output": "local_result"}
    mock_local_exec.return_value = mock_result

    input_data = {"query": "test"}
    result = await basic_flock.run_async(start_agent="agent1", input=input_data, box_result=False)

    assert result == mock_result
    mock_init_context.assert_called_once()
    mock_local_exec.assert_awaited_once()
    mock_temporal_exec.assert_not_awaited()
    # Check context passed to initialize_context
    call_args, _ = mock_init_context.call_args
    context_arg = call_args[0]
    assert isinstance(context_arg, FlockContext)
    assert call_args[1] == "agent1" # start_agent_name
    assert call_args[2] == input_data # run_input
    assert call_args[4] is True # local_debug flag for initialize_context
    assert call_args[5] == basic_flock.model # model


@pytest.mark.asyncio
async def test_run_async_temporal_delegation(basic_flock, simple_agent, mocker):
    """Test run_async delegates to temporal executor when enable_temporal is True."""
    basic_flock.enable_temporal = True
    basic_flock.add_agent(simple_agent)
    mock_local_exec = mocker.patch('flock.core.flock.run_local_workflow', new_callable=AsyncMock)
    mock_temporal_exec = mocker.patch('flock.core.flock.run_temporal_workflow', new_callable=AsyncMock)
    mock_init_context = mocker.patch('flock.core.flock.initialize_context')
    mock_result = {"final_output": "temporal_result"}
    mock_temporal_exec.return_value = mock_result

    input_data = {"query": "test"}
    result = await basic_flock.run_async(start_agent="agent1", input=input_data, box_result=False)

    assert result == mock_result
    mock_init_context.assert_called_once()
    mock_temporal_exec.assert_awaited_once()
    mock_local_exec.assert_not_awaited()
    # Check context passed to initialize_context
    call_args, _ = mock_init_context.call_args
    context_arg = call_args[0]
    assert isinstance(context_arg, FlockContext)
    assert call_args[4] is False # local_debug flag for initialize_context


@pytest.mark.asyncio
async def test_run_async_no_start_agent_single_agent(basic_flock, simple_agent):
    """Test run_async uses the only agent if none is specified."""
    basic_flock.add_agent(simple_agent)
    basic_flock.enable_temporal = False # Use local for simplicity
    with patch('flock.core.flock.run_local_workflow', new_callable=AsyncMock) as mock_exec:
        mock_exec.return_value = {"result": "ok"}
        await basic_flock.run_async(input={"query": "test"})
        # Check context passed to execute call
        call_args, _ = mock_exec.call_args
        context_arg = call_args[0]
        assert context_arg.get_variable("flock.current_agent") == "agent1"

@pytest.mark.asyncio
async def test_run_async_no_start_agent_multiple_agents(basic_flock, simple_agent):
    """Test run_async raises error if no start agent specified with multiple agents."""
    agent2 = SimpleAgent(name="agent2", input="in", output="out")
    basic_flock.add_agent(simple_agent)
    basic_flock.add_agent(agent2)
    with pytest.raises(ValueError, match="No start_agent specified"):
        await basic_flock.run_async(input={"query": "test"})

@pytest.mark.asyncio
async def test_run_async_agent_not_found(basic_flock):
    """Test run_async raises error if start agent doesn't exist."""
    with pytest.raises(ValueError, match="Start agent 'non_existent_agent' not found"):
        await basic_flock.run_async(start_agent="non_existent_agent", input={"query": "test"})

@pytest.mark.asyncio
async def test_run_async_result_boxing(basic_flock, simple_agent, mocker):
    """Test the box_result parameter."""
    basic_flock.enable_temporal = False
    basic_flock.add_agent(simple_agent)
    mock_exec = mocker.patch('flock.core.flock.run_local_workflow', new_callable=AsyncMock)
    raw_result = {"final_output": "local_result"}
    mock_exec.return_value = raw_result

    # Test with boxing (default)
    boxed_result = await basic_flock.run_async(start_agent="agent1", input={}, box_result=True)
    from box import Box # Import locally for test
    assert isinstance(boxed_result, Box)
    assert boxed_result.final_output == "local_result"

    # Test without boxing
    dict_result = await basic_flock.run_async(start_agent="agent1", input={}, box_result=False)
    assert isinstance(dict_result, dict)
    assert not isinstance(dict_result, Box)
    assert dict_result == raw_result

def test_run_sync_wrapper(basic_flock, mocker):
    """Test that the synchronous run method correctly calls run_async."""
    # Mock run_async to avoid actual async execution
    mock_run_async = mocker.patch.object(basic_flock, 'run_async', new_callable=AsyncMock)
    mock_run_async.return_value = {"result": "async_ran"}

    # Mock asyncio loop handling
    mock_loop = MagicMock()
    mock_get_loop = mocker.patch('asyncio.get_running_loop', return_value=mock_loop)
    mock_loop.run_until_complete.return_value = {"result": "async_ran"}

    # Call the synchronous run method
    sync_result = basic_flock.run(start_agent="agent1", input={"query": "sync_test"})

    # Assert run_async was called
    mock_run_async.assert_awaited_once_with(
        start_agent="agent1",
        input={"query": "sync_test"},
        context=None,
        run_id="",
        box_result=True,
        agents=None,
    )
    # Assert the result from run_until_complete is returned
    assert sync_result == {"result": "async_ran"}
    mock_loop.run_until_complete.assert_called_once()


# --- Serialization Delegation Tests ---

def test_to_dict_delegates_to_serializer(basic_flock, mocker):
    """Verify Flock.to_dict calls FlockSerializer.serialize."""
    mock_serializer_serialize = mocker.patch('flock.core.serialization.flock_serializer.FlockSerializer.serialize')
    mock_serializer_serialize.return_value = {"serialized": "data"}

    result = basic_flock.to_dict(path_type="relative")

    mock_serializer_serialize.assert_called_once_with(basic_flock, path_type="relative")
    assert result == {"serialized": "data"}

def test_from_dict_delegates_to_serializer(mocker):
    """Verify Flock.from_dict calls FlockSerializer.deserialize."""
    mock_flock_instance = Flock(name="deserialized", enable_logging=False, show_flock_banner=False)
    mock_serializer_deserialize = mocker.patch('flock.core.serialization.flock_serializer.FlockSerializer.deserialize')
    mock_serializer_deserialize.return_value = mock_flock_instance
    input_data = {"name": "deserialized", "agents": {}}

    result = Flock.from_dict(input_data)

    mock_serializer_deserialize.assert_called_once_with(Flock, input_data)
    assert result is mock_flock_instance

# --- Static Loader Delegation Test ---

def test_load_from_file_delegates_to_loader(mocker):
    """Verify Flock.load_from_file calls the loader function."""
    mock_loader_func = mocker.patch('flock.core.loader.load_flock_from_file')
    mock_flock_instance = Flock(name="loaded_from_file", enable_logging=False, show_flock_banner=False)
    mock_loader_func.return_value = mock_flock_instance
    file_path = "dummy/path/flock.yaml"

    result = Flock.load_from_file(file_path)

    mock_loader_func.assert_called_once_with(file_path)
    assert result is mock_flock_instance
```

---

### 145. tests\serialization\__init__.py

- **File ID**: file_144
- **Type**: Code File
- **Line Count**: 1
- **Description**: Test package for Flock.
- **Dependencies**: None
- **Used By**: None

**Content**:
```
"""Test package for Flock.""" 
```

---

### 146. tests\serialization\test_enhanced_serialization.py

- **File ID**: file_145
- **Type**: Code File
- **Line Count**: 135
- **Description**: Tests for enhanced serialization with type and component definitions.
- **Dependencies**: None
- **Used By**: None

**Content**:
```
"""Tests for enhanced serialization with type and component definitions."""
import os
import tempfile
from pathlib import Path
from typing import Literal

from pydantic import BaseModel

from flock.core import Flock, FlockFactory
from flock.core.flock_registry import flock_type


# Define a custom type for testing
@flock_type
class TestPerson(BaseModel):
    """Test person model for serialization tests."""
    name: str
    age: int
    role: Literal["admin", "user", "guest"]
    bio: str = ""

@flock_type
class TestCompany(BaseModel):
    """Test company model for serialization tests."""
    name: str
    industry: str
    employees: int


def test_serialization_with_custom_type():
    """Test serialization with custom type definitions."""
    # Create a test Flock with an agent using the custom type
    flock = Flock(name="test_flock")
    
    # Create an agent that uses TestPerson in its output
    agent = FlockFactory.create_default_agent(
        name="person_agent",
        input="query: str",
        output="result: TestPerson",
    )
    flock.add_agent(agent)
    
    # Create a temporary file for the YAML
    with tempfile.NamedTemporaryFile(suffix=".yaml", delete=False) as temp_file:
        yaml_path = temp_file.name
    
    try:
        # Serialize the Flock to YAML
        flock.to_yaml_file(yaml_path)
        
        # Read the YAML to verify it includes type definitions
        yaml_content = Path(yaml_path).read_text()
        
        # Check that type definitions are included
        assert "types:" in yaml_content
        assert "TestPerson:" in yaml_content
        
        # Check that component definitions are included
        assert "components:" in yaml_content
        assert "DeclarativeEvaluator:" in yaml_content
        
        # Load the Flock back from YAML
        loaded_flock = Flock.load_from_file(yaml_path)
        
        # Verify the loaded Flock structure
        assert loaded_flock.name == "test_flock"
        assert "person_agent" in loaded_flock.agents
        assert loaded_flock.agents["person_agent"].output == "result: TestPerson"
        
        # Try running the loaded Flock (this will validate the TestPerson type is available)
        # Just a basic smoke test - not testing actual output
        result = loaded_flock.run(
            start_agent="person_agent",
            input={"query": "Get a test person"},
        )
        
        # Verify result contains a TestPerson
        assert hasattr(result, "result")
        
    finally:
        # Clean up the temporary file
        if os.path.exists(yaml_path):
            os.unlink(yaml_path)


def test_serialization_with_multiple_types():
    """Test serialization with multiple custom types."""
    # Define another custom type for testing
    
        
    # Create a test Flock with agents using both custom types
    flock = Flock(name="multi_type_flock")
    
    # Create agents that use both custom types
    person_agent = FlockFactory.create_default_agent(
        name="person_agent",
        input="query: str",
        output="result: TestPerson",
    )
    flock.add_agent(person_agent)
    
    company_agent = FlockFactory.create_default_agent(
        name="company_agent",
        input="industry: str",
        output="result: TestCompany",
    )
    flock.add_agent(company_agent)
    
    # Create a temporary file for the YAML
    with tempfile.NamedTemporaryFile(suffix=".yaml", delete=False) as temp_file:
        yaml_path = temp_file.name
    
    try:
        # Serialize the Flock to YAML
        flock.to_yaml_file(yaml_path)
        
        # Read the YAML to verify it includes both type definitions
        yaml_content = Path(yaml_path).read_text()
        print(yaml_content)
        
        # Check that both type definitions are included
        assert "TestPerson:" in yaml_content
        assert "TestCompany:" in yaml_content
        
        # Load the Flock back from YAML
        loaded_flock = Flock.load_from_file(yaml_path)
        
        # Verify both agents are present
        assert "person_agent" in loaded_flock.agents
        assert "company_agent" in loaded_flock.agents
        
    finally:
        # Clean up the temporary file
        if os.path.exists(yaml_path):
            os.unlink(yaml_path) 
```

---

### 147. tests\serialization\test_file_path_serialization.py

- **File ID**: file_146
- **Type**: Code File
- **Line Count**: 107
- **Description**: Tests for file path serialization and component loading.
- **Dependencies**: None
- **Used By**: None

**Content**:
```
"""Tests for file path serialization and component loading."""
import os
import tempfile
from pathlib import Path

from flock.core import Flock, FlockFactory
from flock.core.flock_registry import flock_component
from pydantic import BaseModel, Field


def test_component_file_path_serialization():
    """Test that component file paths are serialized correctly."""
    # Create a test Flock
    flock = Flock(name="file_path_test_flock")
    
    # Create an agent with default components
    agent = FlockFactory.create_default_agent(
        name="test_agent",
        input="query: str",
        output="result: str",
    )
    flock.add_agent(agent)
    
    # Create a temporary file for the YAML
    with tempfile.NamedTemporaryFile(suffix=".yaml", delete=False) as temp_file:
        yaml_path = temp_file.name
    
    try:
        # Serialize the Flock to YAML
        flock.to_yaml_file(yaml_path)
        
        # Read the YAML to verify it includes file paths
        yaml_content = Path(yaml_path).read_text()
        
        # Check that file paths are included for components
        assert "file_path:" in yaml_content
        
        # Load the Flock back from YAML
        loaded_flock = Flock.load_from_file(yaml_path)
        
        # Verify the loaded Flock structure
        assert loaded_flock.name == "file_path_test_flock"
        assert "test_agent" in loaded_flock.agents
        
        # Basic smoke test - can we run the agent?
        result = loaded_flock.run(
            start_agent="test_agent",
            input={"query": "Test query"},
        )
        
        # Verify we got a result
        assert hasattr(result, "result")
        
    finally:
        # Clean up the temporary file
        if os.path.exists(yaml_path):
            os.unlink(yaml_path)


def test_loading_component_from_file_path():
    """Test loading a component using its file path when module import fails."""
    # This test is more complex and would require creating a real file on disk
    # with a component definition that we can load by path.
    # For simplicity, we'll check if the serialized YAML contains the file path.
    
    # Create a test Flock
    flock = Flock(name="file_path_load_test")
    
    # Create an agent with default components
    agent = FlockFactory.create_default_agent(
        name="path_agent",
        input="query: str",
        output="result: str",
    )
    flock.add_agent(agent)
    
    # Create a temporary file for the YAML
    with tempfile.NamedTemporaryFile(suffix=".yaml", delete=False) as temp_file:
        yaml_path = temp_file.name
    
    try:
        # Serialize the Flock to YAML
        flock.to_yaml_file(yaml_path, path_type="absolute")
        
        # Read the YAML to extract a component's file path
        yaml_content = Path(yaml_path).read_text()
        
        # Check that file paths exist and seem reasonable
        assert "file_path:" in yaml_content
        
        # Verify the paths in the YAML are absolute and pointing to real files
        lines = yaml_content.split("\n")
        file_path_lines = [line.strip() for line in lines if "file_path:" in line]
        
        for line in file_path_lines:
            if "null" not in line:  # Skip null file paths
                # Extract the path using a simple split (this is a test, so it's fine)
                path = line.split("file_path:", 1)[1].strip()
                if path:
                    # Check that this is an absolute path that exists
                    assert path.startswith("/")
                    assert os.path.exists(path), f"Path does not exist: {path}"
        
    finally:
        # Clean up the temporary file
        if os.path.exists(yaml_path):
            os.unlink(yaml_path) 
```

---

### 148. tests\serialization\test_flock_serializer.py

- **File ID**: file_147
- **Type**: Code File
- **Line Count**: 318
- **Description**: File at tests\serialization\test_flock_serializer.py
- **Dependencies**: None
- **Used By**: None

**Content**:
```
# tests/serialization/test_flock_serializer.py
import os
import random
from unittest.mock import patch
import pytest
from typing import List, Literal
from dataclasses import dataclass

from pydantic import BaseModel, Field

from flock.core.flock import Flock
from flock.core.flock_agent import FlockAgent
from flock.core.flock_evaluator import FlockEvaluator, FlockEvaluatorConfig
from flock.core.flock_module import FlockModule, FlockModuleConfig
from flock.core.flock_router import FlockRouter, FlockRouterConfig, HandOffRequest
from flock.core.serialization.flock_serializer import FlockSerializer
from flock.core.flock_registry import FlockRegistry, get_registry, flock_component, flock_tool, flock_type
from flock.core.serialization.serializable import Serializable # Needed for mocks if they inherit

# --- Mock Components for Testing ---
# Ensure these are registered before tests that need them run

@pytest.fixture(autouse=True)
def setup_registry():
    """Fixture to ensure mocks are registered before each test."""
    registry = get_registry()
    registry._initialize() # Start fresh

    # Register Mocks
    registry.register_component(MockEvaluator)
    registry.register_component(MockModule)
    registry.register_component(MockRouter)
    registry.register_callable(sample_tool) # Register by function object
    registry.register_callable(print)       # Register built-in
    registry.register_type(MyCustomData)

    yield # Run test

    registry._initialize() # Clean up


class MockEvalConfig(FlockEvaluatorConfig):
    mock_eval_param: str = "eval_default"

@flock_component # Auto-register is fine too
class MockEvaluator(FlockEvaluator, Serializable):
    config: MockEvalConfig = MockEvalConfig()
    # Add evaluate if needed by agent.run, but not strictly required for serialization test
    async def evaluate(self, agent, inputs, tools): return {"mock_result": "mock"}
    # Need serialization methods if not relying solely on Pydantic model_dump in agent
    def to_dict(self): return {"name": self.name, "config": self.config.model_dump(), "type": "MockEvaluator"}
    @classmethod
    def from_dict(cls, data): return cls(name=data.get("name", "default_eval_name"), config=MockEvalConfig(**data.get("config",{})))


class MockModuleConfig(FlockModuleConfig):
    mock_module_param: bool = True

@flock_component
class MockModule(FlockModule, Serializable):
    config: MockModuleConfig = MockModuleConfig()
    def to_dict(self): return {"name": self.name, "config": self.config.model_dump(), "type": "MockModule"}
    @classmethod
    def from_dict(cls, data): return cls(name=data.get("name", "default_mod_name"), config=MockModuleConfig(**data.get("config",{})))


class MockRouterConfig(FlockRouterConfig):
    next_agent_name: str = "default_next"

@flock_component
class MockRouter(FlockRouter, Serializable):
    config: MockRouterConfig = MockRouterConfig()
    async def route(self, current_agent, result, context): return HandOffRequest(next_agent=self.config.next_agent_name)
    def to_dict(self): return {"name": self.name, "config": self.config.model_dump(), "type": "MockRouter"}
    @classmethod
    def from_dict(cls, data): return cls(name=data.get("name", "default_router_name"), config=MockRouterConfig(**data.get("config",{})))


@flock_tool
def sample_tool(text: str) -> str:
    """A sample registered tool."""
    return text.upper()

@flock_type
class MyCustomData(BaseModel):
    id: int = Field(default_factory=lambda: random.randint(1, 1000000))
    value: str = Field(default_factory=lambda: "random_value")

# --- Fixtures ---
@pytest.fixture
def basic_flock():
    return Flock(name="serial_flock", model="serial_model", description="Serialization test flock", enable_logging=False, show_flock_banner=False)

@pytest.fixture
def flock_with_agents(basic_flock):
    agent1 = FlockAgent(name="a1", input="in", output="out1")
    agent2 = FlockAgent(
        name="a2",
        input="out1", output="data: MyCustomData", # Use custom type
        evaluator=MockEvaluator(name="a2_eval", config=MockEvalConfig(mock_eval_param="a2_eval")),
        modules={"m1": MockModule(name="m1", config=MockModuleConfig(mock_module_param=False))},
        tools=[sample_tool, print],
        handoff_router=MockRouter(name="a2_router", config=MockRouterConfig(next_agent_name="a1"))
    )
    basic_flock.add_agent(agent1)
    basic_flock.add_agent(agent2)
    return basic_flock


# --- Serialization Tests ---

def test_serialize_basic_flock(basic_flock):
    """Test serializing a Flock with no agents or complex components."""
    serialized_data = FlockSerializer.serialize(basic_flock)

    assert serialized_data["name"] == "serial_flock"
    assert serialized_data["model"] == "serial_model"
    assert serialized_data["description"] == "Serialization test flock"
    assert "agents" in serialized_data and serialized_data["agents"] == {}
    assert "types" not in serialized_data # No custom types used
    assert "components" not in serialized_data # No components used
    assert "metadata" in serialized_data
    assert serialized_data["metadata"]["path_type"] == "relative" # Default for serialize

def test_serialize_flock_with_agents(flock_with_agents):
    """Test serializing a Flock with agents and their components."""
    serialized_data = FlockSerializer.serialize(flock_with_agents)

    assert "agents" in serialized_data
    assert "a1" in serialized_data["agents"]
    assert "a2" in serialized_data["agents"]

    # Check agent a2 serialization details
    agent2_data = serialized_data["agents"]["a2"]
    assert agent2_data["name"] == "a2"
    assert agent2_data["input"] == "out1"
    assert agent2_data["output"] == "data: MyCustomData"

    # Check components attached to agent a2
    assert "evaluator" in agent2_data
    assert agent2_data["evaluator"]["type"] == "MockEvaluator"
    assert agent2_data["evaluator"]["config"]["mock_eval_param"] == "a2_eval"

    assert "modules" in agent2_data
    assert "m1" in agent2_data["modules"]
    assert agent2_data["modules"]["m1"]["type"] == "MockModule"
    assert agent2_data["modules"]["m1"]["config"]["mock_module_param"] is False

    assert "tools" in agent2_data
    assert agent2_data["tools"] == ["sample_tool", "print"] # Just names

    assert "handoff_router" in agent2_data
    assert agent2_data["handoff_router"]["type"] == "MockRouter"
    assert agent2_data["handoff_router"]["config"]["next_agent_name"] == "a1"

def test_serialize_includes_types(flock_with_agents):
    """Test that custom types used in agent signatures are included."""
    serialized_data = FlockSerializer.serialize(flock_with_agents)

    assert "types" in serialized_data
    assert "MyCustomData" in serialized_data["types"]
    type_def = serialized_data["types"]["MyCustomData"]
    assert type_def["type"] == "pydantic.BaseModel" # Assuming @flock_type registers it as such
    assert "module_path" in type_def
    assert "schema" in type_def
    assert "id" in type_def["schema"]["properties"]
    assert type_def["schema"]["properties"]["id"]["type"] == "integer"

def test_serialize_includes_components(flock_with_agents):
    """Test that components (evaluators, modules, routers, tools) are included."""
    serialized_data = FlockSerializer.serialize(flock_with_agents)

    assert "components" in serialized_data
    components = serialized_data["components"]

    # Check components from agent a2
    assert "MockEvaluator" in components
    assert components["MockEvaluator"]["type"] == "flock_component"
    assert "file_path" in components["MockEvaluator"] # Path should be present

    assert "MockModule" in components
    assert components["MockModule"]["type"] == "flock_component"

    assert "MockRouter" in components
    assert components["MockRouter"]["type"] == "flock_component"

    # Check tools (callables)
    assert "sample_tool" in components
    assert components["sample_tool"]["type"] == "flock_callable"
    assert "module_path" in components["sample_tool"]
    assert "file_path" in components["sample_tool"]
    assert "description" in components["sample_tool"]

    assert "print" in components # Built-in
    assert components["print"]["type"] == "flock_callable"
    assert components["print"]["module_path"] == "builtins"


def test_serialize_path_type_absolute(flock_with_agents, mocker):
    """Test serialization with absolute path type."""
    # Mock os.path.abspath to ensure we can check its call
    mock_relpath = mocker.patch('os.path.relpath', side_effect=lambda x: f"relative/path/to/{os.path.basename(x)}")
    mocker.patch('inspect.getfile', side_effect=lambda x: f"/abs/path/{x.__name__}.py") # Mock getfile

    serialized_data = FlockSerializer.serialize(flock_with_agents, path_type="absolute")
    components = serialized_data["components"]

    assert "MockEvaluator" in components
    assert components["MockEvaluator"]["file_path"] == "/abs/path/MockEvaluator.py" # Check absolute

    assert "sample_tool" in components
    assert components["sample_tool"]["file_path"] == "/abs/path/sample_tool.py" # Check absolute

    assert mock_relpath.call_count == 0 # Ensure abspath was involved

def test_serialize_path_type_relative(flock_with_agents, mocker):
    """Test serialization with relative path type."""
    # Mock os.path.relpath
    mock_relpath = mocker.patch('os.path.relpath', side_effect=lambda x: f"relative/path/to/{os.path.basename(x)}")
    mocker.patch('inspect.getfile', side_effect=lambda x: f"/abs/path/{x.__name__}.py") # Mock getfile

    serialized_data = FlockSerializer.serialize(flock_with_agents, path_type="relative")
    components = serialized_data["components"]

    assert "MockEvaluator" in components
    assert components["MockEvaluator"]["file_path"] == "relative/path/to/MockEvaluator.py"

    assert "sample_tool" in components
    assert components["sample_tool"]["file_path"] == "relative/path/to/sample_tool.py"

    assert mock_relpath.call_count > 0 # Ensure relpath was involved


# --- Deserialization Tests ---

def test_deserialize_basic_flock(basic_flock):
    """Test deserializing a basic Flock from its dictionary representation."""
    serialized_data = FlockSerializer.serialize(basic_flock)
    loaded_flock = FlockSerializer.deserialize(Flock, serialized_data)

    assert isinstance(loaded_flock, Flock)
    assert loaded_flock.name == basic_flock.name
    assert loaded_flock.model == basic_flock.model
    assert loaded_flock.agents == {}

def test_deserialize_flock_with_agents_and_components(flock_with_agents):
    """Test deserializing a complex Flock with agents and components."""
    serialized_data = FlockSerializer.serialize(flock_with_agents)
    loaded_flock = FlockSerializer.deserialize(Flock, serialized_data)

    assert isinstance(loaded_flock, Flock)
    assert len(loaded_flock.agents) == 2
    assert "a1" in loaded_flock.agents
    assert "a2" in loaded_flock.agents

    # Check agent a2 structure after loading
    agent2 = loaded_flock.agents["a2"]
    assert isinstance(agent2, FlockAgent)
    assert isinstance(agent2.evaluator, MockEvaluator)
    assert agent2.evaluator.config.mock_eval_param == "a2_eval"
    assert isinstance(agent2.modules["m1"], MockModule)
    assert agent2.modules["m1"].config.mock_module_param is False
    assert isinstance(agent2.handoff_router, MockRouter)
    assert agent2.handoff_router.config.next_agent_name == "a1"
    assert len(agent2.tools) == 2
    assert agent2.tools[0] is sample_tool
    assert agent2.tools[1] is print

def test_deserialize_registers_components_and_types(flock_with_agents):
    """Verify that deserialization registers the necessary components and types."""
    serialized_data = FlockSerializer.serialize(flock_with_agents)

    # Create a new clean registry to simulate loading in a fresh environment
    new_registry = FlockRegistry()
    new_registry._initialize()

    with patch('flock.core.flock_registry.get_registry', return_value=new_registry):
        # Ensure mocks are NOT initially registered in the new registry
        with pytest.raises(KeyError):
            new_registry.get_type("MockEvaluator")
        with pytest.raises(KeyError):
            new_registry.get_callable("sample_tool")

        # Deserialize using the new registry
        loaded_flock = FlockSerializer.deserialize(Flock, serialized_data)

        # Assert that components and types ARE NOW registered
        assert new_registry.get_component("MockEvaluator") is MockEvaluator
        assert new_registry.get_callable("sample_tool") is sample_tool
        assert new_registry.get_type("MyCustomData") is MyCustomData
        assert new_registry.get_callable("print") is print # Built-in should be handled

def test_deserialize_missing_component_definition(basic_flock, caplog):
    """Test deserialization when a component's definition is missing."""
    # Create agent data referencing a non-existent component type
    agent_data = {
        "name": "missing_comp_agent",
        "input": "in",
        "output": "out",
        "evaluator": {
            "type": "NonExistentEvaluator", # This type is not registered
            "name": "bad_eval",
            "config": {}
        }
    }
    flock_data = FlockSerializer.serialize(basic_flock)
    flock_data["agents"] = {"missing_comp_agent": agent_data}
    # Manually remove component definition if it snuck in somehow (it shouldn't)
    flock_data.pop("components", None)

    # Act
    loaded_flock = FlockSerializer.deserialize(Flock, flock_data)

    # Assert
    assert "missing_comp_agent" in loaded_flock.agents
    agent = loaded_flock.agents["missing_comp_agent"]
    # The agent should be created, but the component should be None or raise during use
    assert agent.evaluator is None

```

---

### 149. tests\serialization\test_nested_serialization.py

- **File ID**: file_148
- **Type**: Code File
- **Line Count**: 95
- **Description**: Tests for serialization with nested type structures.
- **Dependencies**: None
- **Used By**: None

**Content**:
```
"""Tests for serialization with nested type structures."""
import os
import tempfile
from pathlib import Path
from typing import Dict, List

from pydantic import BaseModel

from flock.core import Flock, FlockFactory
from flock.core.flock_registry import flock_type


# Define nested custom types for testing
@flock_type
class Address(BaseModel):
    """Address model for nested serialization test."""
    street: str
    city: str
    zip_code: str


@flock_type
class Contact(BaseModel):
    """Contact information for nested serialization test."""
    email: str
    phone: str
    address: Address  # Nested model


@flock_type
class Company(BaseModel):
    """Company model with nested types for serialization test."""
    name: str
    industry: str
    headquarters: Address  # Nested model
    contacts: List[Contact]  # List of nested models
    departments: Dict[str, List[str]]  # Dictionary with nested list


def test_nested_type_serialization():
    """Test serialization with nested type structures."""
    # Create a test Flock with an agent using nested types
    flock = Flock(name="nested_types_flock")
    
    # Create an agent that uses Company in its output (which contains nested types)
    agent = FlockFactory.create_default_agent(
        name="company_agent",
        input="query: str",
        output="result: Company",
    )
    flock.add_agent(agent)
    
    # Create a temporary file for the YAML
    with tempfile.NamedTemporaryFile(suffix=".yaml", delete=False) as temp_file:
        yaml_path = temp_file.name
    
    try:
        # Serialize the Flock to YAML
        flock.to_yaml_file(yaml_path)
        
        # Read the YAML to verify it includes nested type definitions
        yaml_content = Path(yaml_path).read_text()
        
        # Check that all type definitions are included
        assert "Company:" in yaml_content
        assert "Address:" in yaml_content  
        assert "Contact:" in yaml_content
        
        # Load the Flock back from YAML
        loaded_flock = Flock.load_from_file(yaml_path)
        
        # Verify the loaded Flock can be used
        assert "company_agent" in loaded_flock.agents
        assert loaded_flock.agents["company_agent"].output == "result: Company"
        
        # Try running the loaded Flock (this will validate all nested types are available)
        result = loaded_flock.run(
            start_agent="company_agent",
            input={"query": "Get a test company"},
        )
        
        # Basic validation that the result has the expected structure
        company = result.result
        assert hasattr(company, "name")
        assert hasattr(company, "headquarters")
        assert hasattr(company.headquarters, "street")
        assert isinstance(company.contacts, list)
        if company.contacts:
            assert hasattr(company.contacts[0], "address")
            assert hasattr(company.contacts[0].address, "city")
        
    finally:
        # Clean up the temporary file
        if os.path.exists(yaml_path):
            os.unlink(yaml_path) 
```

---

### 150. tests\serialization\test_yaml_serialization.py

- **File ID**: file_149
- **Type**: Code File
- **Line Count**: 286
- **Description**: File at tests\serialization\test_yaml_serialization.py
- **Dependencies**: None
- **Used By**: None

**Content**:
```
# tests/serialization/test_yaml_serialization.py

import os
import tempfile
from pathlib import Path
from typing import Any, Dict, List, Optional, Callable
from dataclasses import dataclass

import pytest
import yaml

# --- Core Flock Imports ---
# Assume these are correctly implemented and importable
from flock.core.flock import Flock
from flock.core.flock_agent import FlockAgent
from flock.core.flock_evaluator import FlockEvaluator, FlockEvaluatorConfig
from flock.core.flock_module import FlockModule, FlockModuleConfig
from flock.core.flock_router import FlockRouter, FlockRouterConfig, HandOffRequest
from flock.core.context.context import FlockContext
from flock.core.serialization.serializable import Serializable

# --- Registry and Decorators ---
from flock.core.flock_registry import (
    flock_component,
    flock_tool,
    flock_type,
    get_registry,
)

# Get registry instance
FlockRegistry = get_registry()

# --- Mock Components for Testing ---

class MockEvalConfig(FlockEvaluatorConfig):
    mock_eval_param: str = "eval_default"

@flock_component # Register this component class
class MockEvaluator(FlockEvaluator, Serializable): # Inherit Serializable
    name: str = "mock_evaluator"
    config: MockEvalConfig = MockEvalConfig()

    # Needed for serialization if not just using Pydantic dump
    def to_dict(self) -> Dict[str, Any]:
        return {"name": self.name, "config": self.config.model_dump(), "type": self.__class__.__name__}

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "MockEvaluator":
        config = MockEvalConfig(**data.get("config", {}))
        return cls(name=data.get("name", "mock_evaluator"), config=config)

    async def evaluate(self, agent: Any, inputs: Dict[str, Any], tools: List[Any]) -> Dict[str, Any]:
        return {"mock_result": f"evaluated {inputs.get('test_input', '')} with {self.config.mock_eval_param}"}

class MockModuleConfig(FlockModuleConfig):
    mock_module_param: bool = True

@flock_component # Register this component class
class MockModule(FlockModule, Serializable): # Inherit Serializable
    config: MockModuleConfig = MockModuleConfig()

    def to_dict(self) -> Dict[str, Any]:
        return {"name": self.name, "config": self.config.model_dump(), "type": self.__class__.__name__}

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "MockModule":
        config = MockModuleConfig(**data.get("config", {}))
        return cls(name=data.get("name", "mock_module"), config=config)

    # Mock lifecycle methods if needed for testing interactions
    async def post_evaluate(self, agent: FlockAgent, inputs: dict[str, Any], result: dict[str, Any], context: FlockContext | None = None) -> dict[str, Any]:
        result["mock_module_added"] = self.config.mock_module_param
        return result

class MockRouterConfig(FlockRouterConfig):
    next_agent_name: str = "default_next"

@flock_component # Register this component class
class MockRouter(FlockRouter, Serializable): # Inherit Serializable
    config: MockRouterConfig = MockRouterConfig()

    def to_dict(self) -> Dict[str, Any]:
        return {"name": self.name, "config": self.config.model_dump(), "type": self.__class__.__name__}

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "MockRouter":
        config = MockRouterConfig(**data.get("config", {}))
        return cls(name=data.get("name", "mock_router"), config=config)

    async def route(self, current_agent: Any, result: Dict[str, Any], context: FlockContext) -> HandOffRequest:
        return HandOffRequest(next_agent=self.config.next_agent_name)

# --- Sample Tool Function ---
@flock_tool # Register this tool function
def sample_tool(text: str, capitalize: bool = True) -> str:
    """A sample registered tool."""
    return text.upper() if capitalize else text.lower()

# --- Sample Custom Type ---
@flock_type # Register this custom type
@dataclass
class MyCustomData:
    id: int
    value: str
    tags: List[str] | None = None

# --- Test Class ---

class TestFlockYAMLSerialization:

    def test_agent_serialization_basic(self, tmp_path):
        """Test serializing and deserializing a basic FlockAgent."""
        agent = FlockAgent(
            name="basic_agent",
            model="test_model",
            description="A basic agent",
            input="query: str",
            output="answer: str"
        )
        file_path = tmp_path / "basic_agent.yaml"

        # Act
        agent.to_yaml_file(file_path)
        loaded_agent = FlockAgent.from_yaml_file(file_path)

        # Assert
        assert isinstance(loaded_agent, FlockAgent)
        assert loaded_agent.name == agent.name
        assert loaded_agent.model == agent.model
        assert loaded_agent.description == agent.description
        assert loaded_agent.input == agent.input
        assert loaded_agent.output == agent.output
        assert loaded_agent.evaluator is None # Default should be None before factory
        assert loaded_agent.modules == {}
        assert loaded_agent.handoff_router is None
        assert loaded_agent.tools == []

    def test_agent_serialization_with_components(self, tmp_path):
        """Test agent serialization with evaluator, module, and router."""
        evaluator = MockEvaluator(name="mock_evaluator", config=MockEvalConfig(mock_eval_param="test_eval"))
        router = MockRouter(name="mock_router", config=MockRouterConfig(next_agent_name="agent_two"))
        module = MockModule(name="extra_module", config=MockModuleConfig(mock_module_param=False))

        agent = FlockAgent(
            name="component_agent",
            model="test_model_comp",
            evaluator=evaluator,
            handoff_router=router,
            modules={"extra_module": module}
        )
        file_path = tmp_path / "component_agent.yaml"

        # Act
        agent.to_yaml_file(file_path)
        loaded_agent = FlockAgent.from_yaml_file(file_path)

        # Assert
        assert loaded_agent.name == "component_agent"
        assert isinstance(loaded_agent.evaluator, MockEvaluator)
        assert loaded_agent.evaluator.name == "mock_evaluator" # Default name from mock
        assert loaded_agent.evaluator.config.mock_eval_param == "test_eval"

        assert isinstance(loaded_agent.handoff_router, MockRouter)
        assert loaded_agent.handoff_router.name == "mock_router" # Default name from mock
        assert loaded_agent.handoff_router.config.next_agent_name == "agent_two"

        assert "extra_module" in loaded_agent.modules
        assert isinstance(loaded_agent.modules["extra_module"], MockModule)
        assert loaded_agent.modules["extra_module"].config.mock_module_param is False

    def test_agent_serialization_with_tools(self, tmp_path):
        """Test agent serialization with callable tools."""
        agent = FlockAgent(
            name="tool_agent",
            model="tool_model",
            tools=[sample_tool, print] # Include a built-in for testing path gen
        )
        file_path = tmp_path / "tool_agent.yaml"

        # Act
        agent.to_yaml_file(file_path)
        # Optional: Inspect YAML for callable refs
        yaml_content = file_path.read_text()
        assert "sample_tool" in yaml_content
        assert "print" in yaml_content # Check built-in

        loaded_agent = FlockAgent.from_yaml_file(file_path)

        # Assert
        assert loaded_agent.name == "tool_agent"
        assert loaded_agent.tools is not None
        assert len(loaded_agent.tools) == 2
        assert loaded_agent.tools[0] is sample_tool # Check identity after registry lookup
        assert loaded_agent.tools[1] is print       # Check identity for built-in
        # Test calling the loaded tool
        assert loaded_agent.tools[0]("hello") == "HELLO"

    def test_agent_serialization_with_custom_type(self, tmp_path):
        """Test agent serialization where signature uses a registered custom type."""
        agent = FlockAgent(
            name="custom_type_agent",
            model="custom_model",
            input="data: MyCustomData", # Use the registered custom type
            output="result_tags: list[str]"
        )
        file_path = tmp_path / "custom_type_agent.yaml"

        # Act
        agent.to_yaml_file(file_path)
        loaded_agent = FlockAgent.from_yaml_file(file_path)

        # Assert - Primarily check that loading worked and fields are correct
        assert loaded_agent.name == "custom_type_agent"
        assert loaded_agent.input == "data: MyCustomData"
        assert loaded_agent.output == "result_tags: list[str]"
        # We don't directly test type resolution here, but successful loading implies
        # the type string was stored correctly. Resolution is tested implicitly by DSPy mixin tests.

    def test_flock_serialization_basic(self, tmp_path):
        """Test serializing and deserializing a basic Flock instance."""
        flock = Flock(
            model="global_model",
            description="Test Flock Instance"
        )
        file_path = tmp_path / "basic_flock.yaml"

        # Act
        flock.to_yaml_file(file_path)
        loaded_flock = Flock.from_yaml_file(file_path)

        # Assert
        assert isinstance(loaded_flock, Flock)
        assert loaded_flock.model == flock.model
        assert loaded_flock.description == flock.description
        assert loaded_flock.agents == {} # No agents added

    def test_flock_serialization_with_agents(self, tmp_path):
        """Test Flock serialization with multiple agents."""
        flock = Flock(model="flock_model")
        agent1 = FlockAgent(name="agent_one", input="in1", output="out1")
        agent2 = FlockAgent(
            name="agent_two",
            input="out1", output="out2",
            evaluator=MockEvaluator(config=MockEvalConfig(mock_eval_param="agent2_eval"))
        )
        flock.add_agent(agent1)
        flock.add_agent(agent2)

        file_path = tmp_path / "flock_with_agents.yaml"

        # Act
        flock.to_yaml_file(file_path)
        loaded_flock = Flock.from_yaml_file(file_path)

        # Assert
        assert loaded_flock.model == "flock_model"
        assert len(loaded_flock.agents) == 2
        assert "agent_one" in loaded_flock.agents
        assert "agent_two" in loaded_flock.agents

        loaded_a1 = loaded_flock.agents["agent_one"]
        loaded_a2 = loaded_flock.agents["agent_two"]

        assert isinstance(loaded_a1, FlockAgent)
        assert loaded_a1.name == "agent_one"
        assert loaded_a1.input == "in1"

        assert isinstance(loaded_a2, FlockAgent)
        assert loaded_a2.name == "agent_two"
        assert loaded_a2.input == "out1"
        assert isinstance(loaded_a2.evaluator, MockEvaluator)
        assert loaded_a2.evaluator.config.mock_eval_param == "agent2_eval"


    def test_yaml_dump_options(self, tmp_path):
        """Verify that options can be passed to yaml.dump."""
        agent = FlockAgent(name="dump_options_test")
        file_path = tmp_path / "dump_options.yaml"

        # Act - Use sort_keys=True
        agent.to_yaml_file(file_path, sort_keys=True)
        content = file_path.read_text()

        # Assert - Check if keys are roughly sorted (basic check)
        # Note: Exact order depends on implementation details, this is a basic check
        assert content.startswith("description:") or content.startswith("evaluator:") # description might come first alphabetically
```

---

### 151. tests\tools\test_zendesk_tools.py

- **File ID**: file_150
- **Type**: Code File
- **Line Count**: 29
- **Description**: File at tests\tools\test_zendesk_tools.py
- **Dependencies**: None
- **Used By**: None

**Content**:
```


from flock.core.tools.zendesk_tools import get_comments_by_ticket_id, get_ticket_by_id, get_tickets


def test_get_tickets():
    tickets = get_tickets()
    assert len(tickets) > 0
    assert tickets[0]["id"] is not None
    assert tickets[0]["subject"] is not None
    
    
def test_get_ticket_by_id():
    ticket = get_ticket_by_id("366354")
    assert ticket["id"] is not None
    assert ticket["subject"] is not None
    

def test_get_comments_by_ticket_id():
    comments = get_comments_by_ticket_id("366354")
    assert len(comments) > 0
    assert comments[0]["id"] is not None
    assert comments[0]["body"] is not None
    
    





```

---

## Summary

- **Total Files**: 151
- **Code Files**: 123
- **Regular Files**: 28
- **Total Lines of Code**: 26362
