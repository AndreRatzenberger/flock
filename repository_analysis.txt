# Code Repository Analysis
Generated on 2025-05-26 17:59:54.216109

## Repository Summary

- **Extension analyzed**: `.py`
- **Number of files**: 256
- **Root folder**: `.`
- **Total lines of code**: 46637

## Project Structure

```
└── ./
    ├── docs/
    │   └── create_doc_boilerplate.py
    ├── examples/
    │   ├── 01-getting-started/
    │   │   ├── 01-hello-flock.py
    │   │   ├── 02-inputs-and-outputs.py
    │   │   ├── 03-using-a-tool.py
    │   │   └── 04-flock-architecture.py
    │   ├── 02-core-concepts/
    │   │   ├── tools/
    │   │   │   ├── pet_diary.py
    │   │   │   └── pet_tools.py
    │   │   ├── 01-pydantic-types.py
    │   │   ├── 02-declarative-vs-prompting.py
    │   │   ├── 03-simple-chaining.py
    │   │   ├── 04-tools-in-action.py
    │   │   ├── 05-intro-to-modules.py
    │   │   └── 06-context-basics.py
    │   ├── 03-intermediate-guides/
    │   │   ├── to_do/
    │   │   │   ├── 01-building-custom-tools.py
    │   │   │   ├── 01-memory.py
    │   │   │   ├── 01-memory_internal.py
    │   │   │   ├── 02-dynamic-routing.py
    │   │   │   ├── 02-memory-graph.py
    │   │   │   ├── 03-basic-rag-agent.py
    │   │   │   └── 04-knowledge-graph-memory.py
    │   │   ├── 01-saving-loading-flocks.py
    │   │   ├── 02-providing-rest-api-ui-chat.py
    │   │   └── 03-custom-endpoints.py
    │   ├── 04-advanced-features/
    │   │   ├── 01-batch-processing-creativity.py
    │   │   ├── 02-agent-evaluation-benchmark.py
    │   │   ├── 03-streaming-generation.py
    │   │   ├── 04-hierarchical-memory.py
    │   │   ├── 05-api-interaction.py
    │   │   ├── 06-cli-interaction.py
    │   │   ├── 07-hydrator.py
    │   │   └── 08-temporal.py
    │   ├── 05-full-projects/
    │   │   ├── project-codebase-documenter/
    │   │   │   ├── agents.py
    │   │   │   └── main.py
    │   │   ├── project-dynamic-website-gen/
    │   │   │   ├── agents.py
    │   │   │   └── main.py
    │   │   ├── project-roguelike-ai/
    │   │   │   ├── game_logic.py
    │   │   │   └── npc_agents.py
    │   │   ├── project-story-engine/
    │   │   │   ├── agents.py
    │   │   │   ├── main.py
    │   │   │   └── types.py
    │   │   └── project-textual-project-planner/
    │   │       └── src/
    │   │           └── flock_flightplan/
    │   │               ├── agents/
    │   │               │   ├── flocky.py
    │   │               │   └── planning.py
    │   │               ├── cli/
    │   │               │   └── utils.py
    │   │               ├── collector/
    │   │               │   ├── __init__.py
    │   │               │   ├── analysis.py
    │   │               │   ├── config.py
    │   │               │   ├── discovery.py
    │   │               │   ├── metrics.py
    │   │               │   └── reporting.py
    │   │               ├── __init__.py
    │   │               ├── app.py
    │   │               ├── model.py
    │   │               ├── project_tree.py
    │   │               └── utils.py
    │   └── cookbook/
    │       ├── _old/
    │       │   ├── agentic_chat.py
    │       │   ├── custom-evaluator-example.py
    │       │   └── working_with_images_2.py
    │       ├── defining-complex-pydantic-types.py
    │       ├── github-initializer.py
    │       ├── long_form_web_search.py
    │       ├── overriding_agent_hooks.py
    │       ├── simple-error-handling.py
    │       ├── using-specific-llm-provider.py
    │       └── working-with-images.py
    ├── scripts/
    │   ├── code_collector.py
    │   ├── create_docs.py
    │   └── ensure_uv.py
    ├── src/
    │   └── flock/
    │       ├── adapter/
    │       │   ├── __init__.py
    │       │   ├── azure_adapter.py
    │       │   ├── chroma_adapter.py
    │       │   ├── faiss_adapter.py
    │       │   ├── pinecone_adapter.py
    │       │   └── vector_base.py
    │       ├── cli/
    │       │   ├── config.py
    │       │   ├── constants.py
    │       │   ├── create_agent.py
    │       │   ├── create_flock.py
    │       │   ├── execute_flock.py
    │       │   ├── load_agent.py
    │       │   ├── load_examples.py
    │       │   ├── load_flock.py
    │       │   ├── load_release_notes.py
    │       │   ├── loaded_flock_cli.py
    │       │   ├── manage_agents.py
    │       │   ├── registry_management.py
    │       │   ├── runner.py
    │       │   ├── settings.py
    │       │   ├── utils.py
    │       │   ├── view_results.py
    │       │   └── yaml_editor.py
    │       ├── core/
    │       │   ├── api/
    │       │   │   ├── __init__.py
    │       │   │   ├── custom_endpoint.py
    │       │   │   ├── endpoints.py
    │       │   │   ├── main.py
    │       │   │   ├── models.py
    │       │   │   ├── run_store.py
    │       │   │   ├── runner.py
    │       │   │   └── service.py
    │       │   ├── context/
    │       │   │   ├── context.py
    │       │   │   ├── context_manager.py
    │       │   │   └── context_vars.py
    │       │   ├── evaluation/
    │       │   │   └── utils.py
    │       │   ├── execution/
    │       │   │   ├── batch_executor.py
    │       │   │   ├── evaluation_executor.py
    │       │   │   ├── local_executor.py
    │       │   │   └── temporal_executor.py
    │       │   ├── interpreter/
    │       │   │   └── python_interpreter.py
    │       │   ├── logging/
    │       │   │   ├── formatters/
    │       │   │   │   ├── enum_builder.py
    │       │   │   │   ├── theme_builder.py
    │       │   │   │   ├── themed_formatter.py
    │       │   │   │   └── themes.py
    │       │   │   ├── span_middleware/
    │       │   │   │   └── baggage_span_processor.py
    │       │   │   ├── telemetry_exporter/
    │       │   │   │   ├── base_exporter.py
    │       │   │   │   ├── file_exporter.py
    │       │   │   │   └── sqlite_exporter.py
    │       │   │   ├── __init__.py
    │       │   │   ├── logging.py
    │       │   │   ├── telemetry.py
    │       │   │   └── trace_and_logged.py
    │       │   ├── mcp/
    │       │   │   ├── types/
    │       │   │   │   ├── __init__.py
    │       │   │   │   ├── callbacks.py
    │       │   │   │   ├── factories.py
    │       │   │   │   ├── handlers.py
    │       │   │   │   └── types.py
    │       │   │   ├── util/
    │       │   │   │   ├── __init__.py
    │       │   │   │   └── helpers.py
    │       │   │   ├── __init__.py
    │       │   │   ├── flock_mcp_server.py
    │       │   │   ├── flock_mcp_tool_base.py
    │       │   │   ├── mcp_client.py
    │       │   │   ├── mcp_client_manager.py
    │       │   │   └── mcp_config.py
    │       │   ├── mixin/
    │       │   │   ├── dspy_integration.py
    │       │   │   └── prompt_parser.py
    │       │   ├── serialization/
    │       │   │   ├── __init__.py
    │       │   │   ├── callable_registry.py
    │       │   │   ├── flock_serializer.py
    │       │   │   ├── json_encoder.py
    │       │   │   ├── secure_serializer.py
    │       │   │   ├── serializable.py
    │       │   │   └── serialization_utils.py
    │       │   ├── util/
    │       │   │   ├── cli_helper.py
    │       │   │   ├── file_path_utils.py
    │       │   │   ├── hydrator.py
    │       │   │   ├── input_resolver.py
    │       │   │   ├── loader.py
    │       │   │   └── spliter.py
    │       │   ├── __init__.py
    │       │   ├── flock.py
    │       │   ├── flock_agent.py
    │       │   ├── flock_evaluator.py
    │       │   ├── flock_factory.py
    │       │   ├── flock_module.py
    │       │   ├── flock_registry.py
    │       │   ├── flock_router.py
    │       │   └── flock_server_manager.py
    │       ├── evaluators/
    │       │   ├── declarative/
    │       │   │   ├── __init__.py
    │       │   │   └── declarative_evaluator.py
    │       │   ├── memory/
    │       │   │   └── memory_evaluator.py
    │       │   ├── test/
    │       │   │   └── test_case_evaluator.py
    │       │   ├── zep/
    │       │   │   └── zep_evaluator.py
    │       │   └── __init__.py
    │       ├── mcp/
    │       │   └── servers/
    │       │       ├── sse/
    │       │       │   ├── __init__.py
    │       │       │   └── flock_sse_server.py
    │       │       ├── stdio/
    │       │       │   ├── __init__.py
    │       │       │   └── flock_stdio_server.py
    │       │       └── websockets/
    │       │           ├── __init__.py
    │       │           └── flock_websocket_server.py
    │       ├── modules/
    │       │   ├── assertion/
    │       │   │   ├── __init__.py
    │       │   │   └── assertion_module.py
    │       │   ├── callback/
    │       │   │   ├── __init__.py
    │       │   │   └── callback_module.py
    │       │   ├── enterprise_memory/
    │       │   │   └── enterprise_memory_module.py
    │       │   ├── mem0/
    │       │   │   ├── __init__.py
    │       │   │   └── mem0_module.py
    │       │   ├── mem0_async/
    │       │   │   ├── __init__.py
    │       │   │   └── async_mem0_module.py
    │       │   ├── memory/
    │       │   │   ├── __init__.py
    │       │   │   ├── memory_module.py
    │       │   │   ├── memory_parser.py
    │       │   │   └── memory_storage.py
    │       │   ├── output/
    │       │   │   ├── __init__.py
    │       │   │   └── output_module.py
    │       │   ├── performance/
    │       │   │   ├── __init__.py
    │       │   │   └── metrics_module.py
    │       │   ├── zep/
    │       │   │   ├── __init__.py
    │       │   │   └── zep_module.py
    │       │   └── __init__.py
    │       ├── platform/
    │       │   ├── docker_tools.py
    │       │   └── jaeger_install.py
    │       ├── routers/
    │       │   ├── agent/
    │       │   │   ├── __init__.py
    │       │   │   ├── agent_router.py
    │       │   │   └── handoff_agent.py
    │       │   ├── conditional/
    │       │   │   └── conditional_router.py
    │       │   ├── default/
    │       │   │   ├── __init__.py
    │       │   │   └── default_router.py
    │       │   ├── feedback/
    │       │   │   └── feedback_router.py
    │       │   ├── list_generator/
    │       │   │   └── list_generator_router.py
    │       │   ├── llm/
    │       │   │   ├── __init__.py
    │       │   │   └── llm_router.py
    │       │   └── __init__.py
    │       ├── tools/
    │       │   ├── __init__.py
    │       │   ├── azure_tools.py
    │       │   ├── code_tools.py
    │       │   ├── file_tools.py
    │       │   ├── github_tools.py
    │       │   ├── markdown_tools.py
    │       │   ├── system_tools.py
    │       │   ├── text_tools.py
    │       │   ├── web_tools.py
    │       │   └── zendesk_tools.py
    │       ├── webapp/
    │       │   ├── app/
    │       │   │   ├── api/
    │       │   │   │   ├── __init__.py
    │       │   │   │   ├── agent_management.py
    │       │   │   │   ├── execution.py
    │       │   │   │   ├── flock_management.py
    │       │   │   │   └── registry_viewer.py
    │       │   │   ├── services/
    │       │   │   │   ├── __init__.py
    │       │   │   │   ├── flock_service.py
    │       │   │   │   ├── sharing_models.py
    │       │   │   │   └── sharing_store.py
    │       │   │   ├── __init__.py
    │       │   │   ├── chat.py
    │       │   │   ├── config.py
    │       │   │   ├── dependencies.py
    │       │   │   ├── main.py
    │       │   │   ├── models_ui.py
    │       │   │   ├── theme_mapper.py
    │       │   │   └── utils.py
    │       │   ├── __init__.py
    │       │   └── run.py
    │       ├── workflow/
    │       │   ├── __init__.py
    │       │   ├── activities.py
    │       │   ├── agent_activities.py
    │       │   ├── agent_execution_activity.py
    │       │   ├── flock_workflow.py
    │       │   ├── temporal_config.py
    │       │   └── temporal_setup.py
    │       ├── __init__.py
    │       ├── config.py
    │       └── di.py
    ├── tests/
    │   ├── api/
    │   │   └── test_custom_endpoints.py
    │   ├── core/
    │   │   ├── test_flock_batch.py
    │   │   └── test_flock_core.py
    │   ├── integration/
    │   │   ├── __init__.py
    │   │   └── test_shared_link_store.py
    │   ├── modules/
    │   │   ├── test_enterprise_memory_store.py
    │   │   └── test_vector_adapters.py
    │   ├── serialization/
    │   │   ├── __init__.py
    │   │   ├── test_enhanced_serialization.py
    │   │   ├── test_file_path_serialization.py
    │   │   ├── test_flock_serializer.py
    │   │   ├── test_nested_serialization.py
    │   │   └── test_yaml_serialization.py
    │   ├── tools/
    │   │   └── test_zendesk_tools.py
    │   └── __init__.py
    └── test_store_integration.py
```

## Key Files

These files appear to be central to the codebase based on dependencies and naming conventions:

### src\flock\core\flock.py

- **Lines**: 974
- **Last modified**: 2025-05-25 23:17:06
- **Used by**: 157 files

**Description**: High-level orchestrator for managing and executing agents within the Flock framework.

**Classes**:
- `Flock`: 19 methods

**Content**:
```py
# src/flock/core/flock.py
"""High-level orchestrator for managing and executing agents within the Flock framework."""

from __future__ import annotations  # Ensure forward references work

import asyncio
import contextvars
import os
import uuid
from collections.abc import Awaitable, Callable, Sequence
from concurrent.futures import ThreadPoolExecutor
from pathlib import Path
from typing import (
    TYPE_CHECKING,
    Any,
    Literal,
    TypeVar,
)

# Third-party imports
from box import Box
from temporalio import workflow

from flock.core.flock_server_manager import FlockServerManager
from flock.core.mcp.flock_mcp_server import FlockMCPServerBase

with workflow.unsafe.imports_passed_through():
    from datasets import Dataset  # type: ignore

    # Assuming run_local_workflow is correctly placed and importable
    from flock.core.execution.local_executor import (
        run_local_workflow,
    )

import opik
from opentelemetry import trace
from opentelemetry.baggage import get_baggage, set_baggage
from opik.integrations.dspy.callback import OpikCallback
from pandas import DataFrame  # type: ignore
from pydantic import BaseModel, Field

# Flock core components & utilities
from flock.config import DEFAULT_MODEL, TELEMETRY
from flock.core.api.custom_endpoint import (
    FlockEndpoint,  # Keep for type hinting custom_endpoints
)
from flock.core.context.context import FlockContext
from flock.core.context.context_manager import initialize_context

# Assuming run_temporal_workflow is correctly placed and importable
from flock.core.execution.temporal_executor import run_temporal_workflow
from flock.core.flock_evaluator import FlockEvaluator  # For type hint
from flock.core.logging.logging import get_logger
from flock.core.serialization.serializable import Serializable
from flock.core.util.cli_helper import init_console
from flock.workflow.temporal_config import TemporalWorkflowConfig

# Import FlockAgent using TYPE_CHECKING to avoid circular import at runtime
if TYPE_CHECKING:
    # These imports are only for type hints
    from flock.core.flock_agent import FlockAgent


# Registry
from flock.core.flock_registry import get_registry

try:
    import pandas as pd  # type: ignore

    PANDAS_AVAILABLE = True
except ImportError:
    pd = None  # type: ignore
    PANDAS_AVAILABLE = False

logger = get_logger("flock.api")
TELEMETRY.setup_tracing()  # Setup OpenTelemetry
tracer = trace.get_tracer(__name__)
FlockRegistry = get_registry()  # Get the registry instance

# Define TypeVar for generic class methods like from_dict
T = TypeVar("T", bound="Flock")
_R = TypeVar("_R")


class Flock(BaseModel, Serializable):
    """Orchestrator for managing and executing agent systems.

    Manages agent definitions, context, and execution flow (local or Temporal).
    Relies on FlockSerializer for serialization/deserialization logic.
    Inherits from Pydantic BaseModel and Serializable.
    """

    name: str | None = Field(
        default_factory=lambda: f"flock_{uuid.uuid4().hex[:8]}",
        description="A unique identifier for this Flock instance.",
    )
    model: str | None = Field(
        default=DEFAULT_MODEL,
        description="Default model identifier for agents if not specified otherwise.",
    )
    description: str | None = Field(
        default=None,
        description="A brief description of the purpose of this Flock configuration.",
    )
    enable_temporal: bool = Field(
        default=False,
        description="If True, execute workflows via Temporal; otherwise, run locally.",
    )
    enable_opik: bool = Field(
        default=False,
        description="If True, enable Opik for cost tracking and model management.",
    )
    show_flock_banner: bool = Field(
        default=True,
        description="If True, show the Flock banner on console interactions.",
    )
    # --- Temporal Configuration (Optional) ---
    temporal_config: TemporalWorkflowConfig | None = Field(
        default=None,
        description="Optional Temporal settings specific to the workflow execution for this Flock.",
    )
    # --- Temporal Dev/Test Setting ---
    temporal_start_in_process_worker: bool = Field(
        default=True,
        description="If True (default) and enable_temporal=True, start a temporary in-process worker for development/testing convenience. Set to False when using dedicated workers.",
    )

    benchmark_agent_name: str | None = Field(
        default=None,
        description="The name of the agent to use for the benchmark.",
    )
    benchmark_eval_field: str | None = Field(
        default=None,
        description="The output field to use for the benchmark.",
    )
    benchmark_input_field: str | None = Field(
        default=None,
        description="The input field to use for the benchmark.",
    )
    # Internal agent storage - not part of the Pydantic model for direct serialization
    # Marked with underscore to indicate it's managed internally and accessed via property
    _agents: dict[str, FlockAgent]
    _start_agent_name: str | None = None  # For potential pre-configuration
    _start_input: dict = {}  # For potential pre-configuration

    # Internal server storage - not part of the Pydantic model for direct serialization
    _servers: dict[str, FlockMCPServerBase]

    # Async context-manager for startup and teardown of servers
    # Not part of the pydantic model
    _mgr: FlockServerManager

    # Pydantic v2 model config
    model_config = {
        "arbitrary_types_allowed": True,
        # Assuming FlockRegistry type might not be serializable by default
        "ignored_types": (type(FlockRegistry),),
    }

    def _run_sync(self, coro: Awaitable[_R]) -> _R:
        """Execute *coro* synchronously.

        * If no loop is running → ``asyncio.run``.
        * Otherwise run ``asyncio.run`` inside a fresh thread **with**
          context-vars propagation.
        """
        try:
            asyncio.get_running_loop()
        except RuntimeError:  # no loop → simple
            return asyncio.run(coro)

        # A loop is already running – Jupyter / ASGI / etc.
        ctx = contextvars.copy_context()  # propagate baggage
        with ThreadPoolExecutor(max_workers=1) as pool:
            future = pool.submit(ctx.run, asyncio.run, coro)
            try:
                return future.result()
            finally:
                if not future.done():
                    future.cancel()

    def __init__(
        self,
        name: str | None = None,
        model: str | None = DEFAULT_MODEL,
        description: str | None = None,
        show_flock_banner: bool = True,
        enable_temporal: bool = False,
        enable_opik: bool = False,
        agents: list[FlockAgent] | None = None,
        servers: list[FlockMCPServerBase] | None = None,
        temporal_config: TemporalWorkflowConfig | None = None,
        temporal_start_in_process_worker: bool = True,
        **kwargs,
    ):
        """Initialize the Flock orchestrator."""
        # Use provided name or generate default BEFORE super init if needed elsewhere
        effective_name = name or f"flock_{uuid.uuid4().hex[:8]}"

        # Initialize Pydantic fields
        super().__init__(
            name=effective_name,
            model=model,
            description=description,
            enable_temporal=enable_temporal,
            enable_opik=enable_opik,
            show_flock_banner=show_flock_banner,
            temporal_config=temporal_config,
            temporal_start_in_process_worker=temporal_start_in_process_worker,
            **kwargs,
        )

        # Initialize runtime attributes AFTER super().__init__()
        self._agents = {}
        self._servers = {}
        self._start_agent_name = None
        self._start_input = {}
        self._mgr = FlockServerManager()

        # Register passed servers
        # (need to be registered first so that agents can retrieve them from the registry)
        # This will also add them to the managed list of self._mgr
        if servers:
            from flock.core.mcp.flock_mcp_server import (
                FlockMCPServerBase as ConcreteFlockMCPServer,
            )

            for server in servers:
                if isinstance(server, ConcreteFlockMCPServer):
                    self.add_server(server)
                else:
                    logger.warning(
                        f"Item provided in 'servers' list is not a FlockMCPServer: {type(server)}"
                    )

        # Register passed agents
        if agents:
            from flock.core.flock_agent import (
                FlockAgent as ConcreteFlockAgent,  # Local import
            )

            for agent in agents:
                if isinstance(agent, ConcreteFlockAgent):
                    self.add_agent(agent)
                else:
                    logger.warning(
                        f"Item provided in 'agents' list is not a FlockAgent: {type(agent)}"
                    )

        # Initialize console if needed for banner
        if self.show_flock_banner:  # Check instance attribute
            init_console(clear_screen=True, show_banner=self.show_flock_banner)

        # Set Temporal debug environment variable
        self._set_temporal_debug_flag()

        # Ensure session ID exists in baggage
        self._ensure_session_id()

        FlockRegistry.discover_and_register_components()

        if self.enable_opik:
            import dspy

            opik.configure(use_local=True, automatic_approvals=True)
            opik_callback = OpikCallback(project_name=self.name, log_graph=True)
            dspy.settings.configure(
                callbacks=[opik_callback],
            )

        logger.info(
            "Flock instance initialized",
            name=self.name,
            model=self.model,
            enable_temporal=self.enable_temporal,
        )

    def prepare_benchmark(
        self,
        agent: FlockAgent | str | None = None,
        input_field: str | None = None,
        eval_field: str | None = None,
    ):
        """Prepare a benchmark for the Flock instance."""
        from flock.core.flock_agent import FlockAgent as ConcreteFlockAgent

        logger.info(
            f"Preparing benchmark for Flock instance '{self.name}' with agent '{agent}'."
        )

        name = agent.name if isinstance(agent, ConcreteFlockAgent) else agent

        if self._agents.get(name) is None:
            raise ValueError(
                f"Agent '{name}' not found in Flock instance '{self.name}'."
            )

        self.benchmark_agent_name = name
        self.benchmark_eval_field = eval_field
        self.benchmark_input_field = input_field

    def inspect(self):
        """Inspect the Flock instance."""
        logger.info(
            f"Inspecting Flock instance '{self.name}' with start agent '{self.benchmark_agent_name}' and input '{input}'."
        )

        async def run(input: dict[str, Any]) -> dict[str, Any]:
            """Inspect the Flock instance."""
            logger.info(
                f"Inspecting Flock instance '{self.name}' with start agent '{self.benchmark_agent_name}' and input '{input}'."
            )
            msg_content = input.get("messages")[0].get("content")

            agent_input = {self.benchmark_input_field: msg_content}

            result = await self.run_async(
                start_agent=self.benchmark_agent_name,
                input=agent_input,
                box_result=False,
            )

            agent_output = result.get(
                self.benchmark_eval_field, "No answer found"
            )

            return {
                "output": agent_output,
            }

        return run

    def _set_temporal_debug_flag(self):
        """Set or remove LOCAL_DEBUG env var based on enable_temporal."""
        if not self.enable_temporal:
            if "LOCAL_DEBUG" not in os.environ:
                os.environ["LOCAL_DEBUG"] = "1"
                logger.debug(
                    "Set LOCAL_DEBUG environment variable for local execution."
                )
        elif "LOCAL_DEBUG" in os.environ:
            del os.environ["LOCAL_DEBUG"]
            logger.debug(
                "Removed LOCAL_DEBUG environment variable for Temporal execution."
            )

    def _ensure_session_id(self):
        """Ensure a session_id exists in the OpenTelemetry baggage."""
        session_id = get_baggage("session_id")
        if not session_id:
            session_id = str(uuid.uuid4())
            set_baggage("session_id", session_id)
            logger.debug(f"Generated new session_id: {session_id}")

    def add_server(self, server: FlockMCPServerBase) -> FlockMCPServerBase:
        """Adds a server instance to this Flock configuration and registry as well as set it up to be managed by self._mgr."""
        from flock.core.mcp.flock_mcp_server import (
            FlockMCPServerBase as ConcreteFlockMCPServer,
        )

        if not isinstance(server, ConcreteFlockMCPServer):
            raise TypeError("Provided object is not a FlockMCPServer instance.")
        if not server.config.name:
            raise ValueError("Server must have a name.")

        if server.config.name in self.servers:
            raise ValueError(
                f"Server with this name already exists. Name: '{server.config.name}'"
            )

        self._servers[server.config.name] = server
        FlockRegistry.register_server(server)  # Register globally.

        # Make sure that the server is also added to
        # the server_list managed by FlockServerManager
        if not self._mgr:
            self._mgr = FlockServerManager()

        # Prepare server to be managed by the FlockServerManager
        logger.info(f"Adding server '{server.config.name}' to managed list.")
        self._mgr.add_server_sync(server=server)
        logger.info(f"Server '{server.config.name}' is now on managed list.")

        logger.info(
            f"Server '{server.config.name}' added to Flock '{self.name}'"
        )
        return server

    def add_agent(self, agent: FlockAgent) -> FlockAgent:
        """Adds an agent instance to this Flock configuration and registry.

        This also registers all servers attached to the agent, if they have not been registered
        beforehand.
        """
        from flock.core.flock_agent import FlockAgent as ConcreteFlockAgent

        if not isinstance(agent, ConcreteFlockAgent):
            raise TypeError("Provided object is not a FlockAgent instance.")
        if not agent.name:
            raise ValueError("Agent must have a name.")

        if agent.name in self._agents:
            # Allow re-adding the same instance, but raise error for different instance with same name
            if self._agents[agent.name] is not agent:
                raise ValueError(
                    f"Agent with name '{agent.name}' already exists with a different instance."
                )
            else:
                logger.debug(
                    f"Agent '{agent.name}' is already added. Skipping."
                )
                return agent  # Return existing agent

        self._agents[agent.name] = agent
        FlockRegistry.register_agent(agent)  # Register globally

        # Set default model if agent doesn't have one
        if agent.model is None:
            if self.model:
                agent.set_model(self.model)
                logger.debug(
                    f"Agent '{agent.name}' using Flock default model: {self.model}"
                )
            else:
                logger.warning(
                    f"Agent '{agent.name}' has no model and Flock default model is not set."
                )

        logger.info(f"Agent '{agent.name}' added to Flock '{self.name}'.")
        return agent

    @property
    def agents(self) -> dict[str, FlockAgent]:
        """Returns the dictionary of agents managed by this Flock instance."""
        return self._agents

    @property
    def servers(self) -> dict[str, FlockMCPServerBase]:
        """Returns the dictionary of servers managed by this Flock instance."""
        return self._servers

    def run(
        self,
        start_agent: FlockAgent | str | None = None,
        input: dict | None = None,
        context: FlockContext | None = None,
        run_id: str = "",
        box_result: bool = True,
        agents: list[FlockAgent] | None = None,
        servers: list[FlockMCPServerBase] | None = None,
        memo: dict[str, Any] | None = None,
    ) -> Box | dict:
        return self._run_sync(
            self.run_async(
                start_agent=start_agent,
                input=input,
                context=context,
                run_id=run_id,
                box_result=box_result,
                agents=agents,
                servers=servers,
                memo=memo,
            )
        )

    async def run_async(
        self,
        start_agent: FlockAgent | str | None = None,
        input: dict | None = None,
        context: FlockContext | None = None,
        run_id: str = "",
        box_result: bool = True,
        agents: list[FlockAgent] | None = None,
        servers: list[FlockMCPServerBase] | None = None,
        memo: dict[str, Any] | None = None,
    ) -> Box | dict:
        """Entry point for running an agent system asynchronously."""
        # Import here to allow forward reference resolution
        from flock.core.flock_agent import FlockAgent as ConcreteFlockAgent
        from flock.core.mcp.flock_mcp_server import (
            FlockMCPServerBase as ConcreteFlockServer,
        )

        with tracer.start_as_current_span("flock.run_async") as span:
            # Add passed servers so that agents have access to them.
            if servers:
                for server_obj in servers:
                    if isinstance(server_obj, ConcreteFlockServer):
                        self.add_server(server=server_obj)
                    else:
                        logger.warning(
                            f"Item in 'servers' list is not a FlockMCPServer: {type(server_obj)}"
                        )

            # Add passed agents
            if agents:
                for agent_obj in agents:
                    if isinstance(agent_obj, ConcreteFlockAgent):
                        self.add_agent(agent_obj)
                    else:
                        logger.warning(
                            f"Item in 'agents' list is not a FlockAgent: {type(agent_obj)}"
                        )

            # Determine starting agent name
            start_agent_name: str | None = None
            if isinstance(start_agent, ConcreteFlockAgent):
                start_agent_name = start_agent.name
                if (
                    start_agent_name not in self._agents
                ):  # Add if not already present
                    self.add_agent(start_agent)
            elif isinstance(start_agent, str):
                start_agent_name = start_agent
            else:  # start_agent is None
                start_agent_name = self._start_agent_name

            # Default to first agent if only one exists and none specified
            if not start_agent_name and len(self._agents) == 1:
                start_agent_name = next(iter(self._agents.keys()))
            elif not start_agent_name:
                raise ValueError(
                    "No start_agent specified and multiple/no agents exist in the Flock instance."
                )

            # Check if start_agent is in agents
            if start_agent_name not in self._agents:
                # Try loading from registry if not found locally yet
                reg_agent = FlockRegistry.get_agent(start_agent_name)
                if reg_agent:
                    self.add_agent(reg_agent)
                    logger.info(
                        f"Loaded start agent '{start_agent_name}' from registry."
                    )
                else:
                    raise ValueError(
                        f"Start agent '{start_agent_name}' not found locally or in registry."
                    )

            run_input = input if input is not None else self._start_input
            effective_run_id = run_id or f"flockrun_{uuid.uuid4().hex[:8]}"

            span.set_attribute("start_agent", start_agent_name)
            span.set_attribute("input", str(run_input))
            span.set_attribute("run_id", effective_run_id)
            span.set_attribute("enable_temporal", self.enable_temporal)
            logger.info(
                f"Initiating Flock run '{self.name}'. Start Agent: '{start_agent_name}'. Temporal: {self.enable_temporal}."
            )

            try:
                resolved_start_agent = self._agents.get(start_agent_name)
                if not resolved_start_agent:  # Should have been handled by now
                    raise ValueError(
                        f"Start agent '{start_agent_name}' not found after checks."
                    )

                run_context = context if context else FlockContext()
                set_baggage("run_id", effective_run_id)  # Set for OpenTelemetry

                initialize_context(
                    run_context,
                    start_agent_name,
                    run_input,
                    effective_run_id,
                    not self.enable_temporal,  # local_debug is inverse of enable_temporal
                    self.model or resolved_start_agent.model or DEFAULT_MODEL,
                )
                # Add agent definitions to context for routing/serialization within workflow
                for agent_name_iter, agent_instance_iter in self.agents.items():
                    agent_dict_repr = (
                        agent_instance_iter.to_dict()
                    )  # Agents handle their own serialization
                    run_context.add_agent_definition(
                        agent_type=type(agent_instance_iter),
                        agent_name=agent_name_iter,
                        agent_data=agent_dict_repr,
                    )

                # Add temporal config to context if enabled
                if self.enable_temporal and self.temporal_config:
                    run_context.set_variable(
                        "flock.temporal_workflow_config",
                        self.temporal_config.model_dump(mode="json"),
                    )

                # At this point, initial setup is done
                # and flock is ready to execute it's agent_workflow.
                # Befor that happens, the ServerManager needs to
                # get the Servers up and running (Populate pools, build connections, start scripts, etc.)
                async with self._mgr:
                    # Enter the manager's async context,
                    # running it's __aenter__ method and starting all registered servers
                    # after this block ends, self._mgr's __aexit__ will be called
                    # all servers will be torn down.
                    logger.info(
                        f"Entering managed server context. Servers starting up."
                    )

                    logger.info(
                        "Starting agent execution",
                        agent=start_agent_name,
                        enable_temporal=self.enable_temporal,
                    )

                    # Execute workflow
                    if not self.enable_temporal:
                        result = await run_local_workflow(
                            run_context,
                            box_result=False,  # Boxing handled below
                        )
                    else:
                        result = await run_temporal_workflow(
                            self,  # Pass the Flock instance
                            run_context,
                            box_result=False,  # Boxing handled below
                            memo=memo,
                        )

                    span.set_attribute("result.type", str(type(result)))
                    result_str = str(result)
                    span.set_attribute(
                        "result.preview",
                        result_str[:1000]
                        + ("..." if len(result_str) > 1000 else ""),
                    )

                    if box_result:
                        try:
                            logger.debug("Boxing final result.")
                            return Box(result)
                        except ImportError:
                            logger.warning(
                                "Box library not installed, returning raw dict."
                            )
                            return result
                    else:
                        return result

                        # The context of self._mgr ends here, meaning, that servers will
                        # be cleaned up and shut down.

            except Exception as e:
                logger.error(
                    f"Flock run '{self.name}' failed: {e}", exc_info=True
                )
                span.record_exception(e)
                span.set_status(trace.Status(trace.StatusCode.ERROR, str(e)))
                # Return a consistent error structure
                error_output = {
                    "error": str(e),
                    "details": f"Flock run '{self.name}' failed.",
                    "run_id": effective_run_id,
                    "start_agent": start_agent_name,
                }
                return Box(error_output) if box_result else error_output

    # --- Batch Processing (Delegation) ---
    async def run_batch_async(
        self,
        start_agent: FlockAgent | str,
        batch_inputs: list[dict[str, Any]] | DataFrame | str,
        input_mapping: dict[str, str] | None = None,
        static_inputs: dict[str, Any] | None = None,
        parallel: bool = True,
        max_workers: int = 5,
        use_temporal: bool | None = None,
        box_results: bool = True,
        return_errors: bool = False,
        silent_mode: bool = False,
        write_to_csv: str | None = None,
        hide_columns: list[str] | None = None,
        delimiter: str = ",",
    ) -> list[Box | dict | None | Exception]:
        """Runs the specified agent/workflow for each item in a batch asynchronously (delegated)."""
        # Import processor locally
        from flock.core.execution.batch_executor import BatchProcessor

        processor = BatchProcessor(self)  # Pass self
        return await processor.run_batch_async(
            start_agent=start_agent,
            batch_inputs=batch_inputs,
            input_mapping=input_mapping,
            static_inputs=static_inputs,
            parallel=parallel,
            max_workers=max_workers,
            use_temporal=use_temporal,
            box_results=box_results,
            return_errors=return_errors,
            silent_mode=silent_mode,
            write_to_csv=write_to_csv,
            hide_columns=hide_columns,
            delimiter=delimiter,
        )

    def run_batch(
        self,
        start_agent: FlockAgent | str,
        batch_inputs: list[dict[str, Any]] | DataFrame | str,
        input_mapping: dict[str, str] | None = None,
        static_inputs: dict[str, Any] | None = None,
        parallel: bool = True,
        max_workers: int = 5,
        use_temporal: bool | None = None,
        box_results: bool = True,
        return_errors: bool = False,
        silent_mode: bool = False,
        write_to_csv: str | None = None,
        hide_columns: list[str] | None = None,
        delimiter: str = ",",
    ) -> list[Box | dict | None | Exception]:
        return self._run_sync(
            self.run_batch_async(
                start_agent=start_agent,
                batch_inputs=batch_inputs,
                input_mapping=input_mapping,
                static_inputs=static_inputs,
                parallel=parallel,
                max_workers=max_workers,
                use_temporal=use_temporal,
                box_results=box_results,
                return_errors=return_errors,
                silent_mode=silent_mode,
                write_to_csv=write_to_csv,
                hide_columns=hide_columns,
                delimiter=delimiter,
            )
        )

    # --- Evaluation (Delegation) ---
    async def evaluate_async(
        self,
        dataset: str | Path | list[dict[str, Any]] | DataFrame | Dataset,  # type: ignore
        start_agent: FlockAgent | str,
        input_mapping: dict[str, str],
        answer_mapping: dict[str, str],
        metrics: list[
            str
            | Callable[[Any, Any], bool | float | dict[str, Any]]
            | FlockAgent  # Type hint only
            | FlockEvaluator  # Type hint only
        ],
        metric_configs: dict[str, dict[str, Any]] | None = None,
        static_inputs: dict[str, Any] | None = None,
        parallel: bool = True,
        max_workers: int = 5,
        use_temporal: bool | None = None,
        error_handling: Literal["raise", "skip", "log"] = "log",
        output_file: str | Path | None = None,
        return_dataframe: bool = True,
        silent_mode: bool = False,
        metadata_columns: list[str] | None = None,
    ) -> DataFrame | list[dict[str, Any]]:  # type: ignore
        """Evaluates the Flock's performance against a dataset (delegated)."""
        # Import processor locally
        from flock.core.execution.evaluation_executor import (
            EvaluationExecutor,
        )

        processor = EvaluationExecutor(self)  # Pass self
        return await processor.evaluate_async(
            dataset=dataset,
            start_agent=start_agent,
            input_mapping=input_mapping,
            answer_mapping=answer_mapping,
            metrics=metrics,
            metric_configs=metric_configs,
            static_inputs=static_inputs,
            parallel=parallel,
            max_workers=max_workers,
            use_temporal=use_temporal,
            error_handling=error_handling,
            output_file=output_file,
            return_dataframe=return_dataframe,
            silent_mode=silent_mode,
            metadata_columns=metadata_columns,
        )

    def evaluate(
        self,
        dataset: str | Path | list[dict[str, Any]] | DataFrame | Dataset,  # type: ignore
        start_agent: FlockAgent | str,
        input_mapping: dict[str, str],
        answer_mapping: dict[str, str],
        metrics: list[
            str
            | Callable[[Any, Any], bool | float | dict[str, Any]]
            | FlockAgent  # Type hint only
            | FlockEvaluator  # Type hint only
        ],
        metric_configs: dict[str, dict[str, Any]] | None = None,
        static_inputs: dict[str, Any] | None = None,
        parallel: bool = True,
        max_workers: int = 5,
        use_temporal: bool | None = None,
        error_handling: Literal["raise", "skip", "log"] = "log",
        output_file: str | Path | None = None,
        return_dataframe: bool = True,
        silent_mode: bool = False,
        metadata_columns: list[str] | None = None,
    ) -> DataFrame | list[dict[str, Any]]:  # type: ignore
        return self._run_sync(
            self.evaluate_async(
                dataset=dataset,
                start_agent=start_agent,
                input_mapping=input_mapping,
                answer_mapping=answer_mapping,
                metrics=metrics,
                metric_configs=metric_configs,
                static_inputs=static_inputs,
                parallel=parallel,
                max_workers=max_workers,
                use_temporal=use_temporal,
                error_handling=error_handling,
                output_file=output_file,
                return_dataframe=return_dataframe,
                silent_mode=silent_mode,
                metadata_columns=metadata_columns,
            )
        )

    # --- Server & CLI Starters (Delegation) ---
    def start_api(
        self,
        host: str = "127.0.0.1",
        port: int = 8344,
        server_name: str = "Flock Server",
        create_ui: bool = True,  # Default to True for the integrated experience
        ui_theme: str | None = None,
        custom_endpoints: Sequence[FlockEndpoint]
        | dict[tuple[str, list[str] | None], Callable[..., Any]]
        | None = None,
    ) -> None:
        """Starts a unified REST API server and/or Web UI for this Flock instance."""
        import warnings

        warnings.warn(
            "start_api() is deprecated and will be removed in a future release. "
            "Use serve() instead.",
            DeprecationWarning,
            stacklevel=2,
        )
        # Delegate to the new serve() method (create_ui maps to ui)
        return self.serve(
            host=host,
            port=port,
            server_name=server_name,
            ui=create_ui,
            ui_theme=ui_theme,
            custom_endpoints=custom_endpoints,
        )

    # ------------------------------------------------------------------
    # New preferred method name
    # ------------------------------------------------------------------

    def serve(
        self,
        host: str = "127.0.0.1",
        port: int = 8344,
        server_name: str = "Flock Server",
        ui: bool = True,
        chat: bool = False,
        chat_agent: str | None = None,  # Reserved for future real agent chat
        chat_message_key: str = "message",
        chat_history_key: str = "history",
        chat_response_key: str = "response",
        ui_theme: str | None = None,
        custom_endpoints: Sequence[FlockEndpoint]
        | dict[tuple[str, list[str] | None], Callable[..., Any]]
        | None = None,
    ) -> None:
        """Launch an HTTP server that exposes the core REST API and, optionally, the
        browser-based UI.

        Args:
            host: Bind address for the server (default "127.0.0.1").
            port: TCP port to listen on (default 8344).
            server_name: Title shown in the OpenAPI docs / logs.
            ui: If True (default) the Pico/HTMX web UI routes are included. If False
                 only the JSON API groups (core & custom) are served.
            chat: If True, enable chat routes.
            chat_agent: Name of the agent to use for chat.
            chat_message_key: Key for chat message in input.
            chat_history_key: Key for chat history in input.
            chat_response_key: Key for chat response in output.
            ui_theme: Optional UI theme name or "random".
            custom_endpoints: Additional API routes to add, either as a list of
                 FlockEndpoint objects or the legacy dict format.
        """
        try:
            from flock.webapp.run import start_unified_server
        except ImportError:
            logger.error(
                "Web application components not found (flock.webapp.run). "
                "Cannot start HTTP server. Ensure webapp dependencies are installed."
            )
            return

        logger.info(
            f"Attempting to start server for Flock '{self.name}' on {host}:{port}. UI enabled: {ui}"
        )

        start_unified_server(
            flock_instance=self,
            host=host,
            port=port,
            server_title=server_name,
            enable_ui_routes=ui,
            enable_chat_routes=chat,
            ui_theme=ui_theme,
            custom_endpoints=custom_endpoints,
        )

    def start_cli(
        self,
        start_agent: FlockAgent
        | str
        | None = None,  # Added start_agent to match method signature in file_26
        server_name: str = "Flock CLI",
        show_results: bool = False,
        edit_mode: bool = False,
    ) -> None:
        """Starts an interactive CLI for this Flock instance."""
        # Import runner locally
        try:
            from flock.cli.runner import start_flock_cli
        except ImportError:
            logger.error(
                "CLI components not found. Cannot start CLI. "
                "Ensure CLI dependencies are installed."
            )
            return

        # The start_flock_cli function in file_50 doesn't take start_agent
        # but the original docs for start_cli did.
        # For now, I'll pass it through, assuming start_flock_cli will be updated or ignore it.
        # If start_agent is crucial here, start_flock_cli needs to handle it.
        logger.info(f"Starting CLI for Flock '{self.name}'...")
        start_flock_cli(
            flock=self,  # Pass the Flock instance
            # start_agent=start_agent, # This argument is not in the definition of start_flock_cli in file_50
            server_name=server_name,
            show_results=show_results,
            edit_mode=edit_mode,
        )

    # --- Serialization Delegation Methods ---
    def to_dict(self, path_type: str = "relative") -> dict[str, Any]:
        """Serialize Flock instance to dictionary using FlockSerializer."""
        from flock.core.serialization.flock_serializer import FlockSerializer

        return FlockSerializer.serialize(self, path_type=path_type)

    @classmethod
    def from_dict(cls: type[T], data: dict[str, Any]) -> T:
        """Deserialize Flock instance from dictionary using FlockSerializer."""
        from flock.core.serialization.flock_serializer import FlockSerializer

        return FlockSerializer.deserialize(cls, data)

    # --- Static Method Loader (Delegates to loader module) ---
    @staticmethod
    def load_from_file(file_path: str) -> Flock:  # Ensure return type is Flock
        """Load a Flock instance from various file formats (delegates to loader)."""
        from flock.core.util.loader import load_flock_from_file

        loaded_flock = load_flock_from_file(file_path)
        # Ensure the loaded object is indeed a Flock instance
        if not isinstance(loaded_flock, Flock):
            raise TypeError(
                f"Loaded object from {file_path} is not a Flock instance, but {type(loaded_flock)}"
            )
        return loaded_flock
```

### src\flock\core\logging\logging.py

- **Lines**: 552
- **Last modified**: 2025-05-24 17:10:34
- **Used by**: 3 files

**Description**: A unified logging module for Flock that works both in local/worker contexts and inside Temporal workflows.

Key points:
  - We always have Temporal imported, so we cannot decide based on import.
  - Instead, we dynamically check if we're in a workflow context by trying
    to call `workflow.info()`.
  - In a workflow, we use Temporal's built-in logger and skip debug/info/warning
    logs during replay.
  - Outside workflows, we use Loguru with rich formatting.

**Classes**:
- `ImmediateFlushSink`: 3 methods
- `PrintAndFlushSink`: 2 methods
- `DummyLogger`: 6 methods
- `FlockLogger`: 9 methods

**Functions**:
- `in_workflow_context()`
- `get_current_trace_id()`
- `color_for_category(category)`
- `custom_format(record)`
- `get_default_severity(level)`
- `configure_logging(flock_level, external_level, specific_levels)`
- `get_logger(name)`
- `get_module_loggers()`
- `truncate_for_logging(obj, max_item_length, max_items)`

**Content**:
```py
# File: src/flock/core/logging.py
"""A unified logging module for Flock that works both in local/worker contexts and inside Temporal workflows.

Key points:
  - We always have Temporal imported, so we cannot decide based on import.
  - Instead, we dynamically check if we're in a workflow context by trying
    to call `workflow.info()`.
  - In a workflow, we use Temporal's built-in logger and skip debug/info/warning
    logs during replay.
  - Outside workflows, we use Loguru with rich formatting.
"""

import logging
import sys
from typing import Literal

from opentelemetry import trace

# Always import Temporal workflow (since it's part of the project)
from temporalio import workflow

with workflow.unsafe.imports_passed_through():
    from loguru import logger as loguru_logger

# ENABLED_FLOCK_LOGGER_LEVELS constant removed

# Mapping from level names to numeric values
LOG_LEVELS: dict[str, int] = {
    "CRITICAL": logging.CRITICAL,
    "ERROR": logging.ERROR,
    "WARNING": logging.WARNING,
    "INFO": logging.INFO,
    "DEBUG": logging.DEBUG,
    "SUCCESS": 35,  # Custom success level
    "NO_LOGS": 100,  # Special level to disable logging
}


def in_workflow_context() -> bool:
    """Returns True if this code is running inside a Temporal workflow context.

    It does this by attempting to call workflow.info() and returning True
    if successful. Otherwise, it returns False.
    """
    try:
        workflow.logger.debug("Checking if in workflow context...")
        # loguru_logger.debug("Checking if in workflow context...")
        # This call will succeed only if we're in a workflow context.
        return bool(hasattr(workflow.info(), "is_replaying"))
    except Exception:
        return False


def get_current_trace_id() -> str:
    """Fetch the current trace ID from OpenTelemetry, if available."""
    current_span = trace.get_current_span()
    span_context = current_span.get_span_context()
    # Format the trace_id as hex (if valid)
    if span_context.is_valid:
        return format(span_context.trace_id, "032x")
    return "no-trace"


COLOR_MAP = {
    # Core & Orchestration
    "flock": "magenta",  # Color only
    "agent": "blue",  # Color only
    "workflow": "cyan",  # Color only
    "activities": "cyan",
    "context": "green",
    # Components & Mechanisms
    "registry": "yellow",  # Color only
    "serialization": "yellow",
    "serialization.utils": "light-yellow",
    "evaluator": "light-blue",
    "module": "light-green",
    "router": "light-magenta",
    "mixin.dspy": "yellow",
    # Specific Modules (Examples)
    "memory": "yellow",
    "module.output": "green",
    "module.metrics": "blue",
    "module.zep": "red",
    "module.hierarchical": "light-green",
    # Tools & Execution
    "tools": "light-black",
    "interpreter": "light-yellow",
    # API Components
    "api": "white",  # Color only
    "api.main": "white",
    "api.endpoints": "light-black",
    "api.run_store": "light-black",
    "api.ui": "light-blue",  # Color only
    "api.ui.routes": "light-blue",
    "api.ui.utils": "cyan",
    # Default/Unknown
    "evaluators.declarative": "light-green",
    "unknown": "light-black",
}

LOGGERS = [
    "flock",  # Core Flock orchestration
    "flock.api", # Flock API specific logs
    "agent",  # General agent operations
    "context",  # Context management
    "registry",  # Unified registry operations (new)
    "serialization",  # General serialization (new - can be base for others)
    "serialization.utils",  # Serialization helpers (new, more specific)
    "evaluator",  # Base evaluator category (new/optional)
    "evaluators.declarative",  # Declarative evaluator specifics
    "module",  # Base module category (new/optional)
    "router",  # Base router category (new/optional)
    "mixin.dspy",  # DSPy integration specifics (new)
    "memory",  # Memory module specifics
    "module.output",  # Output module specifics (example specific module)
    "module.metrics",  # Metrics module specifics (example specific module)
    "module.zep",  # Zep module specifics (example specific module)
    "module.hierarchical",  # Hierarchical memory specifics (example specific module)
    "interpreter",  # Code interpreter (if still used)
    "activities",  # Temporal activities
    "workflow",  # Temporal workflow logic
    "tools",  # Tool execution/registration
    "api",  # General API server (new)
    "api.main",  # API main setup (new)
    "api.endpoints",  # API endpoints (new)
    "api.run_store",  # API run state management (new)
    "api.ui",  # UI general (new)
    "api.ui.routes",  # UI routes (new)
    "api.ui.utils",  # UI utils (new)
]

BOLD_CATEGORIES = [
    "flock",
    "agent",
    "workflow",
    "registry",
    "api",
    "api.ui",
]


def color_for_category(category: str) -> str:
    """Return the Rich markup color code name for the given category."""
    # Handle potentially nested names like 'serialization.utils'
    # Try exact match first, then go up the hierarchy
    if category in COLOR_MAP:
        return COLOR_MAP[category]
    parts = category.split(".")
    for i in range(len(parts) - 1, 0, -1):
        parent_category = ".".join(parts[:i])
        if parent_category in COLOR_MAP:
            return COLOR_MAP[parent_category]
    # Fallback to default 'unknown' color
    return COLOR_MAP.get("unknown", "light-black")  # Final fallback


def custom_format(record):
    """A formatter that applies truncation and sequential styling tags."""
    t = record["time"].strftime("%Y-%m-%d %H:%M:%S")
    level_name = record["level"].name
    category = record["extra"].get("category", "unknown")
    trace_id = record["extra"].get("trace_id", "no-trace")
    color_tag = color_for_category(category)  # Get the color tag name (e.g., "yellow")

    message = record["message"]
    message = message.replace("{", "{{").replace("}", "}}")

    # MAX_LENGTH = 500 # Example value
    if len(message) > MAX_LENGTH:
        truncated_chars = len(message) - MAX_LENGTH
        message = (
            message[:MAX_LENGTH] + f"<yellow>...+({truncated_chars} chars)</yellow>"
        )

    # Determine if category needs bolding (can refine this logic)
    needs_bold = category in BOLD_CATEGORIES

    # Apply tags sequentially
    category_styled = f"[{category}]"  # Start with the plain category name
    category_styled = f"<{color_tag}>{category_styled}</{color_tag}>"  # Wrap with color
    if needs_bold:
        category_styled = f"<bold>{category_styled}</bold>"  # Wrap with bold if needed

    # Final format string using sequential tags for category
    return (
        f"<green>{t}</green> | <level>{level_name: <8}</level> | "
        f"<cyan>[trace_id: {trace_id}]</cyan> | "
        f"{category_styled} | {message}\n"  # Apply the sequentially styled category
    )


class ImmediateFlushSink:
    """A custom Loguru sink that writes to a stream and flushes immediately after each message.

    This ensures that logs appear in real time.
    """

    def __init__(self, stream=None):
        """Initialize the ImmediateFlushSink.

        Args:
            stream (Stream, optional): The stream to write to. Defaults to sys.stderr.
        """
        self._stream = stream if stream else sys.stderr

    def write(self, message):
        """Write a message to the stream and flush immediately.

        Args:
            message (str): The message to write.
        """
        self._stream.write(message)
        self._stream.flush()

    def flush(self):
        """Flush the stream."""
        self._stream.flush()


class PrintAndFlushSink:
    """A Loguru sink.

    forcibly prints each log record and flushes immediately,
    mimicking print(..., flush=True).
    """

    def write(self, message: str):
        """Write a message to the stream and flush immediately.

        Args:
            message (str): The message to write.
        """
        # message already ends with a newline
        print(message, end="", flush=True)

    def flush(self):
        """Flush the stream.

        Already flushed on every write call.
        """
        pass


# Configure Loguru for non-workflow (local/worker) contexts.
# Note that in workflow code, we will use Temporal's workflow.logger instead.
loguru_logger.remove()
loguru_logger.add(
    PrintAndFlushSink(),
    level="DEBUG",
    colorize=True,
    format=custom_format,
)
logging.basicConfig(level=LOG_LEVELS["ERROR"])  # Default to ERROR level for fallback
# Optionally add a file handler, e.g.:
# loguru_logger.add("logs/flock.log", rotation="100 MB", retention="30 days", level="DEBUG")


def get_default_severity(level: Literal["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL", "NO_LOGS", "SUCCESS"] | int) -> int:
    """Get the default severity for a given level."""
    if isinstance(level, str):
        level_str = level.upper()
        return LOG_LEVELS.get(level_str, LOG_LEVELS["ERROR"])
    return level


def configure_logging(flock_level: Literal["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL", "NO_LOGS", "SUCCESS"] | int,
                      external_level: Literal["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL", "NO_LOGS", "SUCCESS"] | int,
                      specific_levels: dict[str, Literal["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL", "NO_LOGS", "SUCCESS"] | int] | None = None) -> None:
    """Configure both external and internal Flock logging systems.

    Args:
        flock_level (str | int): The default logging level (e.g., "INFO", "ERROR", "DEBUG") or numeric level for Flock logging.
        external_level (str | int): The default logging level (e.g., "INFO", "ERROR", "DEBUG") or numeric level for external logging.
        specific_levels (dict[str, str | int] | None, optional): A dictionary mapping
            logger names to their specific logging levels. Defaults to None.
    """
    # Get default severity

    external_severity = get_default_severity(external_level)
    logging.basicConfig(level=external_severity)


    flock_severity = get_default_severity(flock_level)

    specific_severities = {}
    if specific_levels:
        for name, logger_level in specific_levels.items():
            severity = get_default_severity(logger_level)
            specific_severities[name] = severity

    # Apply to all cached loggers
    for logger_name, log_instance in _LOGGER_CACHE.items():
        target_severity = flock_severity
        if logger_name in specific_severities:
            target_severity = specific_severities[logger_name]

        log_instance.min_level_severity = target_severity




# Define a dummy logger that does nothing
class DummyLogger:
    """A dummy logger that does nothing when called."""

    def debug(self, *args, **kwargs):  # noqa: D102
        pass

    def info(self, *args, **kwargs):  # noqa: D102
        pass

    def warning(self, *args, **kwargs):  # noqa: D102
        pass

    def error(self, *args, **kwargs):  # noqa: D102
        pass

    def exception(self, *args, **kwargs):  # noqa: D102
        pass

    def success(self, *args, **kwargs):  # noqa: D102
        pass


dummy_logger = DummyLogger()


# Maximum length for log messages before truncation
MAX_LENGTH = 500


class FlockLogger:
    """A unified logger that selects the appropriate logging mechanism based on context.

    - If running in a workflow context, it uses Temporal's built-in logger.
      Additionally, if workflow.info().is_replaying is True, it suppresses debug/info/warning logs.
    - Otherwise, it uses Loguru.
    """

    def __init__(self, name: str, initial_min_level_severity: int):
        """Initialize the FlockLogger.

        Args:
            name (str): The name of the logger.
            initial_min_level_severity (int): The minimum severity level for messages to be logged.
        """
        self.name = name
        self.min_level_severity = initial_min_level_severity

    def _get_logger(self):
        if in_workflow_context():
            # Use Temporal's workflow.logger inside a workflow context.
            return workflow.logger
        # Bind our logger with category and trace_id
        return loguru_logger.bind(
            name=self.name,
            category=self.name,  # Customize this per module (e.g., "flock", "agent", "context")
            trace_id=get_current_trace_id(),
        )

    def _truncate_message(self, message: str, max_length: int) -> str:
        """Truncate a message if it exceeds max_length and add truncation indicator."""
        if len(message) > max_length:
            truncated_chars = len(message) - max_length
            return (
                message[:max_length] + f"...<yellow>+({truncated_chars} chars)</yellow>"
            )
        return message

    def debug(
        self,
        message: str,
        *args,
        flush: bool = False,
        max_length: int = MAX_LENGTH,
        **kwargs,
    ) -> None:
        current_method_severity = LOG_LEVELS["DEBUG"]
        if self.min_level_severity == LOG_LEVELS["NO_LOGS"] or \
           current_method_severity < self.min_level_severity:
            return
        """Debug a message.

        Args:
            message (str): The message to debug.
            flush (bool, optional): Whether to flush the message. Defaults to False.
            max_length (int, optional): The maximum length of the message. Defaults to MAX_LENGTH.
        """
        message = self._truncate_message(message, max_length)
        self._get_logger().debug(message, *args, **kwargs)

    def info(
        self,
        message: str,
        *args,
        flush: bool = False,
        max_length: int = MAX_LENGTH,
        **kwargs,
    ) -> None:
        current_method_severity = LOG_LEVELS["INFO"]
        if self.min_level_severity == LOG_LEVELS["NO_LOGS"] or \
           current_method_severity < self.min_level_severity:
            return
        """Info a message.

        Args:
            message (str): The message to info.
            flush (bool, optional): Whether to flush the message. Defaults to False.
            max_length (int, optional): The maximum length of the message. Defaults to MAX_LENGTH.
        """
        message = self._truncate_message(message, max_length)
        self._get_logger().info(message, *args, **kwargs)

    def warning(
        self,
        message: str,
        *args,
        flush: bool = False,
        max_length: int = MAX_LENGTH,
        **kwargs,
    ) -> None:
        current_method_severity = LOG_LEVELS["WARNING"]
        if self.min_level_severity == LOG_LEVELS["NO_LOGS"] or \
           current_method_severity < self.min_level_severity:
            return
        """Warning a message.

        Args:
            message (str): The message to warning.
            flush (bool, optional): Whether to flush the message. Defaults to False.
            max_length (int, optional): The maximum length of the message. Defaults to MAX_LENGTH.
        """
        message = self._truncate_message(message, max_length)
        self._get_logger().warning(message, *args, **kwargs)

    def error(
        self,
        message: str,
        *args,
        flush: bool = False,
        max_length: int = MAX_LENGTH,
        **kwargs,
    ) -> None:
        current_method_severity = LOG_LEVELS["ERROR"]
        if self.min_level_severity == LOG_LEVELS["NO_LOGS"] or \
           current_method_severity < self.min_level_severity:
            return
        """Error a message.

        Args:
            message (str): The message to error.
            flush (bool, optional): Whether to flush the message. Defaults to False.
            max_length (int, optional): The maximum length of the message. Defaults to MAX_LENGTH.
        """
        message = self._truncate_message(message, max_length)
        self._get_logger().error(message, *args, **kwargs)

    def exception(
        self,
        message: str,
        *args,
        flush: bool = False,
        max_length: int = MAX_LENGTH,
        **kwargs,
    ) -> None:
        current_method_severity = LOG_LEVELS["ERROR"] # Exception implies ERROR level
        if self.min_level_severity == LOG_LEVELS["NO_LOGS"] or \
           current_method_severity < self.min_level_severity:
            return
        """Exception a message.

        Args:
            message (str): The message to exception.
            flush (bool, optional): Whether to flush the message. Defaults to False.
            max_length (int, optional): The maximum length of the message. Defaults to MAX_LENGTH.
        """
        message = self._truncate_message(message, max_length)
        self._get_logger().exception(message, *args, **kwargs)

    def success(
        self,
        message: str,
        *args,
        flush: bool = False,
        max_length: int = MAX_LENGTH,
        **kwargs,
    ) -> None:
        current_method_severity = LOG_LEVELS["SUCCESS"]
        if self.min_level_severity == LOG_LEVELS["NO_LOGS"] or \
           current_method_severity < self.min_level_severity:
            return
        """Success a message.

        Args:
            message (str): The message to success.
            flush (bool, optional): Whether to flush the message. Defaults to False.
            max_length (int, optional): The maximum length of the message. Defaults to MAX_LENGTH.
        """
        message = self._truncate_message(message, max_length)
        self._get_logger().success(message, *args, **kwargs)


_LOGGER_CACHE: dict[str, FlockLogger] = {}


def get_logger(name: str = "flock") -> FlockLogger:
    """Return a cached FlockLogger instance for the given name.

    If the logger doesn't exist, it is created with 'enable_logging' set to False
    by default (i.e., errors-only mode). Its state can then be changed by calling
    the `configure_logging()` function.
    If a logger with the given name already exists in the cache, its 'min_level_severity'
    state is NOT modified by this function; it's simply returned.
    """
    if name not in _LOGGER_CACHE:
        # New loggers default to errors-only (min_level_severity = ERROR_SEVERITY)
        # until explicitly configured by configure_logging()
        _LOGGER_CACHE[name] = FlockLogger(name, LOG_LEVELS["ERROR"])
    # The min_level_severity state of existing or newly created loggers
    # should be managed by the configure_logging() function.
    return _LOGGER_CACHE[name]


def get_module_loggers() -> list[FlockLogger]:
    """Return a cached FlockLogger instance for the given module name."""
    result = []
    for kvp in _LOGGER_CACHE:
        if kvp.startswith("module."):
            result.append(_LOGGER_CACHE[kvp])

    return result


def truncate_for_logging(obj, max_item_length=100, max_items=10):
    """Truncate large data structures for logging purposes."""
    if isinstance(obj, str) and len(obj) > max_item_length:
        return obj[:max_item_length] + f"... ({len(obj) - max_item_length} more chars)"
    elif isinstance(obj, dict):
        if len(obj) > max_items:
            return {
                k: truncate_for_logging(v)
                for i, (k, v) in enumerate(obj.items())
                if i < max_items
            }
        return {k: truncate_for_logging(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        if len(obj) > max_items:
            return [truncate_for_logging(item) for item in obj[:max_items]] + [
                f"... ({len(obj) - max_items} more items)"
            ]
        return [truncate_for_logging(item) for item in obj]
    return obj
```

### src\flock\core\mcp\mcp_client.py

- **Lines**: 658
- **Last modified**: 2025-05-26 13:19:27

**Description**: Wrapper Class for a mcp ClientSession Object.

**Classes**:
- `FlockMCPClientBase`: 2 methods

**Content**:
```py
"""Wrapper Class for a mcp ClientSession Object."""

import asyncio
import random
from abc import ABC, abstractmethod
from asyncio import Lock
from contextlib import (
    AbstractAsyncContextManager,
    AsyncExitStack,
)
from datetime import timedelta
from typing import (
    Any,
)

import httpx
from anyio import ClosedResourceError
from anyio.streams.memory import (
    MemoryObjectReceiveStream,
    MemoryObjectSendStream,
)
from cachetools import TTLCache, cached
from mcp import (
    ClientSession,
    InitializeResult,
    ListToolsResult,
    McpError,
    ServerCapabilities,
)
from mcp.types import CallToolResult, JSONRPCMessage
from opentelemetry import trace
from pydantic import (
    BaseModel,
    ConfigDict,
    Field,
)

from flock.core.logging.logging import get_logger
from flock.core.mcp.flock_mcp_tool_base import FlockMCPToolBase
from flock.core.mcp.mcp_config import FlockMCPConfigurationBase
from flock.core.mcp.types.factories import (
    default_flock_mcp_list_roots_callback_factory,
    default_flock_mcp_logging_callback_factory,
    default_flock_mcp_message_handler_callback_factory,
    default_flock_mcp_sampling_callback_factory,
)
from flock.core.mcp.types.types import (
    FlockListRootsMCPCallback,
    FlockLoggingMCPCallback,
    FlockMessageHandlerMCPCallback,
    FlockSamplingMCPCallback,
    MCPRoot,
    ServerParameters,
)
from flock.core.mcp.util.helpers import cache_key_generator

logger = get_logger("core.mcp.client_base")
tracer = trace.get_tracer(__name__)


class FlockMCPClientBase(BaseModel, ABC):
    """Wrapper for mcp ClientSession.

    Class will attempt to re-establish connection if possible.
    If connection establishment fails after max_retries, then
    `has_error` will be set to true and `error_message` will
    contain the details of the exception.
    """

    # --- Properties ---
    config: FlockMCPConfigurationBase = Field(
        ..., description="The config for this client instance."
    )

    tool_cache: TTLCache | None = Field(
        default=None,
        exclude=True,
        description="Cache for tools. Excluded from Serialization.",
    )

    tool_result_cache: TTLCache | None = Field(
        default=None,
        exclude=True,
        description="Cache for the result of tool call. Excluded from Serialization.",
    )

    resource_contents_cache: TTLCache | None = Field(
        default=None,
        exclude=True,
        description="Cache for resource contents. Excluded from Serialization.",
    )

    resource_list_cache: TTLCache | None = Field(
        default=None,
        exclude=True,
        description="Cache for Resource Lists. Excluded from Serialization.",
    )

    client_session: ClientSession | None = Field(
        default=None, exclude=True, description="ClientSession Reference."
    )

    connected_server_capabilities: ServerCapabilities | None = Field(
        default=None,
        exclude=True,
        description="Capabilities of the connected server.",
    )

    current_roots: list[MCPRoot] | None = Field(
        default=None, description="Currently used roots of the client."
    )

    lock: Lock = Field(
        default_factory=Lock,
        exclude=True,
        description="Global lock for the client.",
    )

    session_stack: AsyncExitStack = Field(
        default_factory=AsyncExitStack,
        exclude=True,
        description="Internal AsyncExitStack for session.",
    )

    sampling_callback: FlockSamplingMCPCallback | None = Field(
        default=None, description="Sampling Callback."
    )

    list_roots_callback: FlockListRootsMCPCallback | None = Field(
        default=None, description="List Roots Callback."
    )

    logging_callback: FlockLoggingMCPCallback | None = Field(
        default=None, description="Logging Callback."
    )

    message_handler: FlockMessageHandlerMCPCallback | None = Field(
        default=None, description="MessageHandler Callback."
    )

    additional_params: dict[str, Any] | None = Field(
        default=None,
        description="Additional Parameters for connection. Can be modified using server modules.",
    )

    # Auto-reconnect proxy
    class _SessionProxy:
        def __init__(self, client: Any):
            self._client = client

        def __getattr__(self, name: str):
            # return an async function that auto-reconnects, then calls through.
            async def _method(*args, **kwargs):
                with tracer.start_as_current_span(
                    "session_proxy.__getattr__"
                ) as span:
                    client = self._client
                    cfg = client.config
                    max_tries = cfg.connection_config.max_retries or 1
                    base_delay = 0.1
                    span.set_attribute("client.name", client.config.name)

                    for attempt in range(1, max_tries + 2):
                        span.set_attribute(
                            "max_tries", max_tries
                        )  # TODO: shift outside of loop
                        span.set_attribute("base_delay", base_delay)
                        span.set_attribute("attempt", attempt)
                        await client._ensure_connected()
                        try:
                            # delegate the real session
                            return await getattr(client.client_session, name)(
                                *args, **kwargs
                            )
                        except McpError as e:
                            # only retry on a transport timeout
                            if e.error.code == httpx.codes.REQUEST_TIMEOUT:
                                kind = "timeout"
                            else:
                                # application-level MCP error -> give up immediately
                                logger.error(
                                    f"MCP error in session.{name}: {e.error}"
                                )
                                return None
                        except (BrokenPipeError, ClosedResourceError) as e:
                            kind = type(e).__name__
                            span.record_exception(e)
                        except Exception as e:
                            # anything else is treated as transport failure
                            span.record_exception(e)
                            kind = type(e).__name__

                        # no more retries
                        if attempt > max_tries:
                            logger.error(
                                f"Session.{name} failed after {max_tries} retries ({kind}); giving up."
                            )
                            try:
                                await client.disconnect()
                            except Exception as e:
                                logger.warning(
                                    f"Error tearing down stale session: {e}"
                                )
                                span.record_exception(e)
                            return None

                        # otherwise log + tear down + back off
                        logger.warning(
                            f"Session.{name} attempt {attempt}/{max_tries} failed. ({kind}). Reconnecting."
                        )
                        try:
                            await client.disconnect()
                            await client._connect()
                        except Exception as e:
                            logger.error(f"Reconnect failed: {e}")
                            span.record_exception(e)

                        # Exponential backoff + 10% jitter
                        delay = base_delay ** (2 ** (attempt - 1))
                        delay += random.uniform(0, delay * 0.1)
                        await asyncio.sleep(delay)

            return _method

    def __init__(
        self,
        config: FlockMCPConfigurationBase,
        lock: Lock | None = None,
        tool_cache: TTLCache | None = None,
        tool_result_cache: TTLCache | None = None,
        resource_contents_cache: TTLCache | None = None,
        resource_list_cache: TTLCache | None = None,
        client_session: ClientSession | None = None,
        connected_server_capabilities: ServerCapabilities | None = None,
        session_stack: AsyncExitStack = AsyncExitStack(),
        sampling_callback: FlockSamplingMCPCallback | None = None,
        list_roots_callback: FlockListRootsMCPCallback | None = None,
        logging_callback: FlockLoggingMCPCallback | None = None,
        message_handler: FlockMessageHandlerMCPCallback | None = None,
        current_roots: list[MCPRoot] | None = None,
        **kwargs,
    ):
        """Init function."""
        lock = lock or Lock()
        super().__init__(
            config=config,
            lock=lock,
            tool_cache=tool_cache,
            tool_result_cache=tool_result_cache,
            resource_contents_cache=resource_contents_cache,
            resource_list_cache=resource_list_cache,
            client_session=client_session,
            connected_server_capabilities=connected_server_capabilities,
            session_stack=session_stack,
            sampling_callback=sampling_callback,
            list_roots_callback=list_roots_callback,
            logging_callback=logging_callback,
            message_handler=message_handler,
            current_roots=current_roots,
            **kwargs,
        )

        # Check if roots are specified in the config:
        if (
            not self.current_roots
            and self.config.connection_config.mount_points
        ):
            # That means that the roots are set in the config
            self.current_roots = self.config.connection_config.mount_points

        if not self.tool_cache:
            self.tool_cache = TTLCache(
                maxsize=self.config.caching_config.tool_cache_max_size,
                ttl=self.config.caching_config.tool_cache_max_ttl,
            )

        # set up the caches
        if not self.tool_result_cache:
            self.tool_result_cache = TTLCache(
                maxsize=self.config.caching_config.tool_result_cache_max_size,
                ttl=self.config.caching_config.tool_result_cache_max_ttl,
            )

        if not self.resource_contents_cache:
            self.resource_contents_cache = TTLCache(
                maxsize=self.config.caching_config.resource_contents_cache_max_size,
                ttl=self.config.caching_config.resource_contents_cache_max_ttl,
            )
        if not self.resource_list_cache:
            self.resource_list_cache = TTLCache(
                maxsize=self.config.caching_config.resource_list_cache_max_size,
                ttl=self.config.caching_config.resource_list_cache_max_ttl,
            )

        # set up callbacks
        if not self.logging_callback:
            if not self.config.callback_config.logging_callback:
                self.logging_callback = (
                    default_flock_mcp_logging_callback_factory(
                        associated_client=self,
                        logger=logger,
                    )
                )
            else:
                self.logging_callback = (
                    self.config.callback_config.logging_callback
                )

        if not self.message_handler:
            if not self.config.callback_config.message_handler:
                self.message_handler = (
                    default_flock_mcp_message_handler_callback_factory(
                        associated_client=self,
                        logger=logger,
                    )
                )
            else:
                self.message_handler = (
                    self.config.callback_config.message_handler
                )

        if not self.list_roots_callback:
            if not self.config.callback_config.list_roots_callback:
                self.list_roots_callback = (
                    default_flock_mcp_list_roots_callback_factory(
                        associated_client=self,
                        logger=logger,
                    )
                )
            else:
                self.list_roots_callback = (
                    self.config.callback_config.list_roots_callback
                )

        if not self.sampling_callback:
            if not self.config.callback_config.sampling_callback:
                self.sampling_callback = (
                    default_flock_mcp_sampling_callback_factory(
                        associated_client=self,
                        logger=logger,
                    )
                )
            else:
                self.sampling_callback = (
                    self.config.callback_config.sampling_callback
                )

    @property
    def session(self) -> _SessionProxy:
        """Always-connected proxy for client_session methods.

        Usage: await self.client_session.call_tool(...), await self.client_session.list_tools(...)
        """
        return self._SessionProxy(self)

    model_config = ConfigDict(
        arbitrary_types_allowed=True,
        extra="allow",
    )

    # --- Abstract methods / class methods ---
    @abstractmethod
    async def create_transport(
        self,
        params: ServerParameters,
        additional_params: dict[str, Any] | None = None,
    ) -> AbstractAsyncContextManager[
        tuple[
            MemoryObjectReceiveStream[JSONRPCMessage | Exception],
            MemoryObjectSendStream[JSONRPCMessage],
        ]
    ]:
        """Given your custom ServerParameters, return an async-contextmgr whose __aenter yields (read_stream, write_stream)."""
        ...

    # --- Public methods ---
    async def get_tools(
        self,
        agent_id: str,
        run_id: str,
    ) -> list[FlockMCPToolBase]:
        """Gets a list of available tools from the server."""

        @cached(cache=self.tool_cache, key=cache_key_generator)
        async def _get_tools_cached(
            agent_id: str,
            run_id: str,
        ) -> list[FlockMCPToolBase]:
            if not self.config.feature_config.tools_enabled:
                return []

            async def _get_tools_internal() -> list[FlockMCPToolBase]:
                response: ListToolsResult = await self.session.list_tools()
                flock_tools = []

                for tool in response.tools:
                    converted_tool = FlockMCPToolBase.from_mcp_tool(
                        tool,
                        agent_id=agent_id,
                        run_id=run_id,
                    )
                    if converted_tool:
                        flock_tools.append(converted_tool)
                return flock_tools

            return await _get_tools_internal()

        return await _get_tools_cached(agent_id=agent_id, run_id=run_id)

    async def call_tool(
        self, agent_id: str, run_id: str, name: str, arguments: dict[str, Any]
    ) -> CallToolResult:
        """Call a tool via the MCP Protocol on the client's server."""

        @cached(cache=self.tool_result_cache, key=cache_key_generator)
        async def _call_tool_cached(
            agent_id: str, run_id: str, name: str, arguments: dict[str, Any]
        ) -> CallToolResult:
            async def _call_tool_internal(
                name: str, arguments: dict[str, Any]
            ) -> CallToolResult:
                logger.debug(
                    f"Calling tool '{name}' with arguments {arguments}"
                )
                return await self.session.call_tool(
                    name=name,
                    arguments=arguments,
                )

            return await _call_tool_internal(name=name, arguments=arguments)

        return await _call_tool_cached(
            agent_id=agent_id, run_id=run_id, name=name, arguments=arguments
        )

    async def get_server_name(self) -> str:
        """Return the server_name.

        Uses a lock under the hood.
        """
        async with self.lock:
            return self.config.name

    async def get_roots(self) -> list[MCPRoot] | None:
        """Get the currently set roots of the client.

        Locks under the hood.
        """
        async with self.lock:
            return self.current_roots

    async def set_roots(self, new_roots: list[MCPRoot]) -> None:
        """Set the current roots of the client.

        Locks under the hood.
        """
        async with self.lock:
            self.current_roots = new_roots
            if self.session:
                try:
                    await self.client_session.send_roots_list_changed()
                except McpError as e:
                    logger.warning(f"Send roots list changed: {e}")

    async def invalidate_tool_cache(self) -> None:
        """Invalidate the entries in the tool cache."""
        logger.debug(f"Invalidating tool_cache for server '{self.config.name}'")
        async with self.lock:
            if self.tool_cache:
                self.tool_cache.clear()
                logger.debug(
                    f"Invalidated tool_cache for server '{self.config.name}'"
                )

    async def invalidate_resource_list_cache(self) -> None:
        """Invalidate the entries in the resource list cache."""
        logger.debug(
            f"Invalidating resource_list_cache for server '{self.config.name}'"
        )
        async with self.lock:
            if self.resource_list_cache:
                self.resource_list_cache.clear()
                logger.debug(
                    f"Invalidated resource_list_cache for server '{self.config.name}'"
                )

    async def invalidate_resource_contents_cache(self) -> None:
        """Invalidate the entries in the resource contents cache."""
        logger.debug(
            f"Invalidating resource_contents_cache for server '{self.config.name}'."
        )
        async with self.lock:
            if self.resource_contents_cache:
                self.resource_contents_cache.clear()
                logger.debug(
                    f"Invalidated resource_contents_cache for server '{self.config.name}'"
                )

    async def invalidate_resource_contents_cache_entry(self, key: str) -> None:
        """Invalidate a single entry in the resource contents cache."""
        logger.debug(
            f"Attempting to clear entry with key: {key} from resource_contents_cache for server '{self.config.name}'"
        )
        async with self.lock:
            if self.resource_contents_cache:
                try:
                    self.resource_contents_cache.pop(key, None)
                    logger.debug(
                        f"Cleared entry with key {key} from resource_contents_cache for server '{self.config.name}'"
                    )
                except Exception as e:
                    logger.debug(
                        f"No entry for key {key} found in resource_contents_cache for server '{self.config.name}'. Ignoring. (Exception was: {e})"
                    )
                    return  # do nothing

    async def disconnect(self) -> None:
        """If previously connected via `self._connect()`, tear it down."""
        async with self.lock:
            if self.session_stack:
                # manually __aexit__
                await self.session_stack.aclose()
                self.session_stack = None
                self.client_session = None

    # --- Private Methods ---
    async def _create_session(self) -> None:
        """Create and hol onto a single ClientSession + ExitStack."""
        logger.debug(f"Creating Client Session for server '{self.config.name}'")
        stack = AsyncExitStack()
        await stack.__aenter__()

        server_params = self.config.connection_config.connection_parameters

        # Single Hook
        transport_ctx = await self.create_transport(
            server_params, self.additional_params
        )
        read, write = await stack.enter_async_context(transport_ctx)
        read_timeout = self.config.connection_config.read_timeout_seconds

        if (
            self.additional_params
            and "read_timeout_seconds" in self.additional_params
        ):
            read_timeout = self.additional_params.get(
                "read_timeout_seconds", read_timeout
            )

        timeout_seconds = (
            read_timeout
            if isinstance(read_timeout, timedelta)
            else timedelta(seconds=float(read_timeout))
        )

        session = await stack.enter_async_context(
            ClientSession(
                read_stream=read,
                write_stream=write,
                read_timeout_seconds=timeout_seconds,
                list_roots_callback=self.list_roots_callback,
                message_handler=self.message_handler,
                sampling_callback=self.sampling_callback,
                logging_callback=self.logging_callback,
            )
        )
        logger.debug(f"Created Client Session for server '{self.config.name}'")
        # store for reuse
        self.session_stack = stack
        self.client_session = session

    async def _connect(self, retries: int | None = None) -> ClientSession:
        """Connect to an MCP Server and set self.client_session to ClientSession.

        Establish the transport and keep it open.
        """
        async with self.lock:
            # if already connected, return it
            if self.client_session:
                logger.debug(
                    f"Client Session for Server '{self.config.name}' exists and is healthy."
                )
                return self.client_session

            else:
                logger.debug(
                    f"Client Session for Server '{self.config.name}' does not exist yet. Connecting..."
                )
                await self._create_session()

            if not self.connected_server_capabilities:
                # This means we never asked the server to initialize the connection.
                await self._perform_initial_handshake()
        return self.client_session

    async def _perform_initial_handshake(self) -> None:
        """Tell the server who we are, what capabilities we have, and what roots we're interested in."""
        # 1) do the LSP-style initialize handshake
        logger.debug(
            f"Performing intialize handshake with server '{self.config.name}'"
        )
        init: InitializeResult = await self.client_session.initialize()

        self.connected_server_capabilities = init

        init_report = f"""
            Server Init Handshake completed Server '{self.config.name}'
            Lists the following Capabilities:

            - Protocol Version: {init.protocolVersion}
            - Instructions: {init.instructions or "No specific Instructions"}
            - MCP Implementation:
                - Name: {init.serverInfo.name}
                - Version: {init.serverInfo.version}
            - Capabilities:
                {init.capabilities}
            """

        logger.debug(init_report)

        # 2) if we already know our current roots, notify the server
        #    so that it will follow up with a ListRootsRequest
        if self.current_roots and self.config.feature_config.roots_enabled:
            await self.client_session.send_roots_list_changed()

        # 3) Tell the server, what logging level we would like to use
        try:
            await self.client_session.set_logging_level(
                level=self.config.connection_config.server_logging_level
            )
        except McpError as e:
            logger.warning(
                f"Trying to set logging level for server '{self.config.name}' resulted in Exception: {e}"
            )

    async def _ensure_connected(self) -> None:
        # if we've never connected, then connect.
        if not self.client_session:
            await self._connect()
            return

        # otherwise, ping and reconnect on error
        try:
            await self.client_session.send_ping()
        except Exception as e:
            logger.warning(
                f"Session to '{self.config.name}' died, reconnecting. Exception was: {e}"
            )
            await self.disconnect()
            await self._connect()

    async def _get_client_session(self) -> ClientSession | None:
        """Lazily start one session and reuse it forever (until closed)."""
        async with self.lock:
            if self.client_session is None:
                await self._create_session()

        return self.client_session
```

### src\flock\core\mcp\mcp_client_manager.py

- **Lines**: 201
- **Last modified**: 2025-05-22 21:27:37

**Description**: Manages a pool of connections for a particular server.

**Classes**:
- `FlockMCPClientManagerBase`: 0 methods

**Content**:
```py
"""Manages a pool of connections for a particular server."""

import copy
from abc import ABC, abstractmethod
from asyncio import Lock
from typing import Any, Generic, TypeVar

from opentelemetry import trace
from pydantic import (
    BaseModel,
    ConfigDict,
    Field,
)

from flock.core.logging.logging import get_logger
from flock.core.mcp.flock_mcp_tool_base import FlockMCPToolBase
from flock.core.mcp.mcp_client import (
    FlockMCPClientBase,
)
from flock.core.mcp.mcp_config import FlockMCPConfigurationBase

logger = get_logger("core.mcp.connection_manager_base")
tracer = trace.get_tracer(__name__)

TClient = TypeVar("TClient", bound="FlockMCPClientBase")


class FlockMCPClientManagerBase(BaseModel, ABC, Generic[TClient]):
    """Handles a Pool of MCPClients of type TClient."""

    client_config: FlockMCPConfigurationBase = Field(
        ..., description="Configuration for clients."
    )

    lock: Lock = Field(
        default_factory=Lock,
        description="Lock for mutex access.",
        exclude=True,
    )

    clients: dict[str, dict[str, FlockMCPClientBase]] = Field(
        default_factory=dict,
        exclude=True,
        description="Internal Store for the clients.",
    )

    # --- Pydantic v2 Configuratioin ---
    model_config = ConfigDict(
        arbitrary_types_allowed=True,
    )

    @abstractmethod
    async def make_client(
        self,
        additional_params: dict[str, Any] | None = None,
    ) -> type[TClient]:
        """Instantiate-but don't connect yet-a fresh client of the concrete subtype."""
        # default implementation
        pass

    async def get_client(
        self,
        agent_id: str,
        run_id: str,
        additional_params: dict[str, Any] | None = None,
    ) -> type[TClient]:
        """Provides a client from the pool."""
        # Attempt to get a client from the client store.
        # clients are stored like this: agent_id -> run_id -> client
        with tracer.start_as_current_span("client_manager.get_client") as span:
            span.set_attribute("agent_id", agent_id)
            span.set_attribute("run_id", run_id)
            async with self.lock:
                try:
                    logger.debug(
                        f"Attempting to get client for server '{self.client_config.name}'"
                    )
                    refresh = False
                    if additional_params:
                        refresh = bool(
                            additional_params.get("refresh_client", False)
                        )
                    client = None
                    run_clients = self.clients.get(agent_id, None)
                    if run_clients is None or refresh:
                        # This means, that across all runs, no agent has ever needed a client.
                        # This also means that we need to create a client.
                        client = await self.make_client(
                            additional_params=copy.deepcopy(additional_params)
                        )
                        # Insert the freshly created client
                        self.clients[agent_id] = {}
                        self.clients[agent_id][run_id] = client

                    else:
                        # This means there is at least one entry for the agent_id available
                        # Now, all we need to do is check if the run_id matches the entrie's run_id
                        client = run_clients.get(run_id, None)
                        if client is None or refresh:
                            # Means no client here with the respective run_id
                            client = await self.make_client(
                                additional_params=copy.deepcopy(
                                    additional_params
                                )
                            )
                            # Insert the freshly created client.
                            self.clients[agent_id][run_id] = client

                    return client
                except Exception as e:
                    # Log the exception and raise it so it becomes visible downstream
                    logger.error(
                        f"Unexpected Exception ocurred while trying to get client for server '{self.client_config.name}' with agent_id: {agent_id} and run_id: {run_id}: {e}"
                    )
                    span.record_exception(e)
                    raise e

    async def call_tool(
        self,
        agent_id: str,
        run_id: str,
        name: str,
        arguments: dict[str, Any],
        additional_params: dict[str, Any] | None = None,
    ) -> Any:
        """Call a tool."""
        with tracer.start_as_current_span("client_manager.call_tool") as span:
            span.set_attribute("agent_id", agent_id)
            span.set_attribute("run_id", run_id)
            span.set_attribute("tool_name", name)
            span.set_attribute("arguments", str(arguments))
            try:
                client = await self.get_client(
                    agent_id=agent_id,
                    run_id=run_id,
                    additional_params=additional_params,
                )
                result = await client.call_tool(
                    agent_id=agent_id,
                    run_id=run_id,
                    name=name,
                    arguments=arguments,
                )
                return result
            except Exception as e:
                logger.error(
                    f"Exception occurred while trying to call tool {name} on server '{self.client_config.name}': {e}"
                )
                span.record_exception(e)
                return None

    async def get_tools(
        self,
        agent_id: str,
        run_id: str,
        additional_params: dict[str, Any] | None = None,
    ) -> list[FlockMCPToolBase]:
        """Retrieves a list of tools for the agents to act on."""
        with tracer.start_as_current_span("client_manager.get_tools") as span:
            span.set_attribute("agent_id", agent_id)
            span.set_attribute("run_id", run_id)
            try:
                client = await self.get_client(
                    agent_id=agent_id,
                    run_id=run_id,
                    additional_params=additional_params,
                )
                tools: list[FlockMCPToolBase] = await client.get_tools(
                    agent_id=agent_id, run_id=run_id
                )
                return tools
            except Exception as e:
                logger.error(
                    f"Exception occurred while trying to retrieve Tools for server '{self.client_config.name}' with agent_id: {agent_id} and run_id: {run_id}: {e}"
                )
                span.record_exception(e)
                return []

    async def close_all(self) -> None:
        """Closes all connections in the pool and cancels background tasks."""
        with tracer.start_as_current_span("client_manager.close_all") as span:
            async with self.lock:
                for agent_id, run_dict in self.clients.items():
                    logger.debug(
                        f"Shutting down all clients for agent_id: {agent_id}"
                    )
                    for run_id, client in run_dict.items():
                        logger.debug(
                            f"Shutting down client for agent_id {agent_id} and run_id {run_id}"
                        )
                        try:
                            await client.disconnect()
                        except Exception as e:
                            logger.error(
                                f"Error when trying to disconnect client for server '{self.client_config.name}': {e}"
                            )
                            span.record_exception(e)
                self.clients = {}  # Let the GC take care of the rest.
                logger.info(
                    f"All clients disconnected for server '{self.client_config.name}'"
                )
```

### tests\core\test_flock_core.py

- **Lines**: 476
- **Last modified**: 2025-05-22 21:27:37

**Classes**:
- `SimpleAgent`: 0 methods

**Functions**:
- `clear_registry()`
- `basic_flock()`
- `simple_agent()`
- `test_flock_init_defaults()`
- `test_flock_init_custom(mocker)`
- `test_flock_init_with_agents(simple_agent)`
- `test_add_agent(basic_flock, simple_agent)`
- `test_add_agent_sets_default_model(basic_flock)`
- `test_add_agent_duplicate(basic_flock, simple_agent)`
- `test_add_agent_invalid_type(basic_flock)`
- `test_agents_property(basic_flock, simple_agent)`
- `test_run_sync_wrapper(basic_flock, mocker)`
- `test_to_dict_delegates_to_serializer(basic_flock, mocker)`
- `test_from_dict_delegates_to_serializer(mocker)`
- `test_load_from_file_delegates_to_loader(mocker)`

**Content**:
```py
# tests/core/test_flock_core.py
import asyncio
import pytest
from unittest.mock import MagicMock, patch, AsyncMock
from datetime import timedelta  # Import timedelta

from pydantic import BaseModel

from flock.core.flock import Flock
from flock.core.flock_agent import FlockAgent
from flock.core.context.context import FlockContext
from flock.core.flock_registry import get_registry, FlockRegistry
from flock.workflow.temporal_config import TemporalRetryPolicyConfig, TemporalWorkflowConfig

# Import Temporal Config models


# Simple mock agent for testing addition
class SimpleAgent(FlockAgent):
    async def evaluate(self, inputs: dict) -> dict:
        return {"result": "simple"}


@pytest.fixture(autouse=True)
def clear_registry():
    """Fixture to ensure a clean registry for each test."""
    registry = get_registry()
    registry._initialize()  # Reset internal dictionaries
    yield  # Run the test
    registry._initialize()  # Clean up after test


@pytest.fixture
def basic_flock() -> Flock:
    """Fixture for a basic Flock instance."""
    return Flock(name="test_basic_flock", model="test-model", show_flock_banner=False)


@pytest.fixture
def simple_agent() -> SimpleAgent:
    """Fixture for a simple agent instance."""
    return SimpleAgent(name="agent1", input="query", output="result")


# --- Initialization Tests ---

def test_flock_init_defaults():
    """Test Flock initialization with default values."""
    flock = Flock(show_flock_banner=False)
    assert flock.name.startswith("flock_")
    assert flock.model == "openai/gpt-4o"  # Default from config
    assert flock.description is None
    assert not flock.enable_temporal
    assert not flock.show_flock_banner
    assert flock._agents == {}


def test_flock_init_custom(mocker):
    """Test Flock initialization with custom values."""
    mock_set_temporal = mocker.patch.object(Flock, '_set_temporal_debug_flag')
    mock_ensure_session = mocker.patch.object(Flock, '_ensure_session_id')

    flock = Flock(
        name="custom_flock",
        model="custom_model",
        description="My custom flock",
        enable_temporal=True,
        show_flock_banner=False
    )
    assert flock.name == "custom_flock"
    assert flock.model == "custom_model"
    assert flock.description == "My custom flock"
    assert flock.enable_temporal
    assert not flock.show_flock_banner

    mock_set_temporal.assert_called_once()
    mock_ensure_session.assert_called_once()


def test_flock_init_with_agents(simple_agent):
    """Test Flock initialization with agents passed in the constructor."""
    flock = Flock(agents=[simple_agent], show_flock_banner=False)
    assert "agent1" in flock.agents
    assert flock.agents["agent1"] is simple_agent

# --- Agent Management Tests ---


def test_add_agent(basic_flock, simple_agent):
    """Test adding an agent to the Flock."""
    basic_flock.add_agent(simple_agent)
    assert "agent1" in basic_flock._agents
    assert basic_flock._agents["agent1"] is simple_agent
    # Check if agent was registered globally too
    assert get_registry().get_agent("agent1") is simple_agent


def test_add_agent_sets_default_model(basic_flock):
    """Test that adding an agent without a model assigns the Flock's default."""
    agent_no_model = SimpleAgent(
        name="agent_no_model", model=None, input="in", output="out")
    basic_flock.model = "flock-default-model"
    basic_flock.add_agent(agent_no_model)
    assert agent_no_model.model == "flock-default-model"


def test_add_agent_duplicate(basic_flock, simple_agent):
    """Test adding an agent with a name that already exists raises ValueError."""
    basic_flock.add_agent(simple_agent)
    new_agent_same_name = SimpleAgent(
        name="agent1", input="query2", output="result2")

    with pytest.raises(ValueError, match="Agent with this name already exists"):
        basic_flock.add_agent(new_agent_same_name)


def test_add_agent_invalid_type(basic_flock):
    """Test adding something that is not a FlockAgent."""
    with pytest.raises(TypeError):
        basic_flock.add_agent({"not": "an agent"})


def test_agents_property(basic_flock, simple_agent):
    """Test the agents property."""
    assert basic_flock.agents == {}
    basic_flock.add_agent(simple_agent)
    assert basic_flock.agents == {"agent1": simple_agent}


# --- Execution Tests ---

@pytest.mark.asyncio
async def test_run_async_local_delegation(basic_flock, simple_agent, mocker):
    """Test run_async delegates to local executor when enable_temporal is False."""
    basic_flock.enable_temporal = False
    basic_flock.add_agent(simple_agent)
    mock_local_exec = mocker.patch(
        'flock.core.flock.run_local_workflow', new_callable=AsyncMock)
    mock_temporal_exec = mocker.patch(
        'flock.core.flock.run_temporal_workflow', new_callable=AsyncMock)
    mock_init_context = mocker.patch('flock.core.flock.initialize_context')
    mock_result = {"final_output": "local_result"}
    mock_local_exec.return_value = mock_result

    input_data = {"query": "test"}
    result = await basic_flock.run_async(start_agent="agent1", input=input_data, box_result=False)

    assert result == mock_result
    mock_init_context.assert_called_once()
    mock_local_exec.assert_awaited_once()
    mock_temporal_exec.assert_not_awaited()
    # Check context passed to initialize_context
    call_args, _ = mock_init_context.call_args
    context_arg = call_args[0]
    assert isinstance(context_arg, FlockContext)
    assert call_args[1] == "agent1"  # start_agent_name
    assert call_args[2] == input_data  # run_input
    assert call_args[4] is True  # local_debug flag for initialize_context
    assert call_args[5] == basic_flock.model  # model


@pytest.mark.asyncio
async def test_run_async_temporal_delegation(basic_flock, simple_agent, mocker):
    """Test run_async delegates to temporal executor when enable_temporal is True."""
    basic_flock.enable_temporal = True
    basic_flock.add_agent(simple_agent)
    mock_local_exec = mocker.patch(
        'flock.core.flock.run_local_workflow', new_callable=AsyncMock)
    mock_temporal_exec = mocker.patch(
        'flock.core.flock.run_temporal_workflow', new_callable=AsyncMock)
    mock_init_context = mocker.patch('flock.core.flock.initialize_context')
    mock_result = {"final_output": "temporal_result"}
    mock_temporal_exec.return_value = mock_result

    input_data = {"query": "test"}
    result = await basic_flock.run_async(start_agent="agent1", input=input_data, box_result=False)

    assert result == mock_result
    mock_init_context.assert_called_once()
    mock_temporal_exec.assert_awaited_once()
    mock_local_exec.assert_not_awaited()
    # Check context passed to initialize_context
    call_args, _ = mock_init_context.call_args
    context_arg = call_args[0]
    assert isinstance(context_arg, FlockContext)
    assert call_args[4] is False  # local_debug flag for initialize_context


@pytest.mark.asyncio
async def test_run_async_no_start_agent_single_agent(basic_flock, simple_agent):
    """Test run_async uses the only agent if none is specified."""
    basic_flock.add_agent(simple_agent)
    basic_flock.enable_temporal = False  # Use local for simplicity
    with patch('flock.core.flock.run_local_workflow', new_callable=AsyncMock) as mock_exec:
        mock_exec.return_value = {"result": "ok"}
        await basic_flock.run_async(input={"query": "test"})
        # Check context passed to execute call
        call_args, _ = mock_exec.call_args
        context_arg = call_args[0]
        assert context_arg.get_variable("flock.current_agent") == "agent1"


@pytest.mark.asyncio
async def test_run_async_no_start_agent_multiple_agents(basic_flock, simple_agent):
    """Test run_async raises error if no start agent specified with multiple agents."""
    agent2 = SimpleAgent(name="agent2", input="in", output="out")
    basic_flock.add_agent(simple_agent)
    basic_flock.add_agent(agent2)
    with pytest.raises(ValueError, match="No start_agent specified"):
        await basic_flock.run_async(input={"query": "test"})


@pytest.mark.asyncio
async def test_run_async_agent_not_found_in_registry(basic_flock, mocker):
    """Test run_async raises error if start agent not found locally or in registry."""
    mocker.patch.object(FlockRegistry, 'get_agent', return_value=None)
    with pytest.raises(ValueError, match="Start agent 'non_existent_agent' not found locally or in registry"):
        await basic_flock.run_async(start_agent="non_existent_agent", input={"query": "test"})


@pytest.mark.asyncio
async def test_run_async_result_boxing(basic_flock, simple_agent, mocker):
    """Test the box_result parameter."""
    basic_flock.enable_temporal = False
    basic_flock.add_agent(simple_agent)
    mock_exec = mocker.patch(
        'flock.core.flock.run_local_workflow', new_callable=AsyncMock)
    raw_result = {"final_output": "local_result"}
    mock_exec.return_value = raw_result

    # Test with boxing (default)
    boxed_result = await basic_flock.run_async(start_agent="agent1", input={}, box_result=True)
    from box import Box  # Import locally for test
    assert isinstance(boxed_result, Box)
    assert boxed_result.final_output == "local_result"

    # Test without boxing
    dict_result = await basic_flock.run_async(start_agent="agent1", input={}, box_result=False)
    assert isinstance(dict_result, dict)
    assert not isinstance(dict_result, Box)
    assert dict_result == raw_result


def test_run_sync_wrapper(basic_flock, mocker):
    """Test that the synchronous run method correctly calls run_async."""
    # Mock run_async to avoid actual async execution
    mock_run_async = mocker.patch(
        'flock.core.flock.Flock.run_async', new_callable=AsyncMock)
    mock_run_async.return_value = {"result": "async_ran"}

    # Mock the asyncio module functions
    mock_loop = MagicMock()
    mock_get_loop = mocker.patch(
        'asyncio.get_running_loop', side_effect=RuntimeError)
    mock_new_loop = mocker.patch(
        'asyncio.new_event_loop', return_value=mock_loop)
    mock_set_loop = mocker.patch('asyncio.set_event_loop')
    mock_get_event_loop = mocker.patch(
        'asyncio.get_event_loop', return_value=mock_loop)

    # Configure the loop for the "not running" branch
    mock_loop.is_running.return_value = False
    mock_loop.run_until_complete.return_value = {"result": "async_ran"}

    # Call the synchronous run method
    sync_result = basic_flock.run(
        start_agent="agent1", input={"query": "sync_test"})

    # Assert everything was called correctly
    mock_get_loop.assert_called_once()
    mock_new_loop.assert_called_once()
    mock_set_loop.assert_called_once_with(mock_loop)
    mock_get_event_loop.assert_called_once()
    mock_loop.is_running.assert_called_once()
    mock_loop.run_until_complete.assert_called_once()

    # Assert the result is correct
    assert sync_result == {"result": "async_ran"}


# --- Serialization Delegation Tests ---

def test_to_dict_delegates_to_serializer(basic_flock, mocker):
    """Verify Flock.to_dict calls FlockSerializer.serialize."""
    mock_serializer_serialize = mocker.patch(
        'flock.core.serialization.flock_serializer.FlockSerializer.serialize')
    mock_serializer_serialize.return_value = {"serialized": "data"}

    result = basic_flock.to_dict(path_type="relative")

    mock_serializer_serialize.assert_called_once_with(
        basic_flock, path_type="relative")
    assert result == {"serialized": "data"}


def test_from_dict_delegates_to_serializer(mocker):
    """Verify Flock.from_dict calls FlockSerializer.deserialize."""
    mock_flock_instance = Flock(
        name="deserialized", show_flock_banner=False)
    mock_serializer_deserialize = mocker.patch(
        'flock.core.serialization.flock_serializer.FlockSerializer.deserialize')
    mock_serializer_deserialize.return_value = mock_flock_instance
    input_data = {"name": "deserialized", "agents": {}}

    result = Flock.from_dict(input_data)

    mock_serializer_deserialize.assert_called_once_with(Flock, input_data)
    assert result is mock_flock_instance

# --- Static Loader Delegation Test ---


def test_load_from_file_delegates_to_loader(mocker):
    """Verify Flock.load_from_file calls the loader function."""
    mock_loader_func = mocker.patch(
        'flock.core.util.loader.load_flock_from_file')
    mock_flock_instance = Flock(
        name="loaded_from_file", show_flock_banner=False)
    mock_loader_func.return_value = mock_flock_instance
    file_path = "dummy/path/flock.yaml"

    result = Flock.load_from_file(file_path)

    mock_loader_func.assert_called_once_with(file_path)
    assert result is mock_flock_instance

# --- Temporal Config Tests (Mocking run_temporal_workflow) ---


@pytest.mark.asyncio
async def test_run_async_temporal_uses_workflow_config(simple_agent, mocker):
    """Verify run_async passes correct workflow config options to the temporal executor."""
    mock_temporal_exec = mocker.patch(
        'flock.core.flock.run_temporal_workflow', new_callable=AsyncMock)
    mock_init_context = mocker.patch('flock.core.flock.initialize_context')
    mock_temporal_exec.return_value = {"result": "temporal_config_test"}

    # 1. Define specific Temporal Workflow Config
    retry_config = TemporalRetryPolicyConfig(maximum_attempts=5)
    workflow_config = TemporalWorkflowConfig(
        task_queue="config-test-queue",
        workflow_execution_timeout=timedelta(minutes=15),
        workflow_run_timeout=timedelta(minutes=5),
        default_activity_retry_policy=retry_config
    )

    # 2. Create Flock with this config
    flock_with_config = Flock(
        name="flock_with_temporal_cfg",
        enable_temporal=True,
        temporal_config=workflow_config,
        agents=[simple_agent],  # Add agent directly
        show_flock_banner=False
    )

    input_data = {"query": "test"}
    # 3. Run the flock
    await flock_with_config.run_async(start_agent="agent1", input=input_data, box_result=False)

    # 4. Assert run_temporal_workflow was called correctly
    mock_temporal_exec.assert_awaited_once()
    call_args, call_kwargs = mock_temporal_exec.call_args

    # Check positional arguments passed to run_temporal_workflow
    assert len(call_args) == 2  # Should be (flock_instance, context)
    assert call_args[0] is flock_with_config  # First arg is the Flock instance
    assert isinstance(call_args[1], FlockContext)  # Second arg is the context

    # Check keyword arguments passed to run_temporal_workflow
    # Assert that box_result is False and memo is its default (None)
    assert call_kwargs == {'box_result': False, 'memo': None}

    # We need to mock deeper (start_workflow) to check queue/timeouts,
    # but we *can* check that the config is correctly stored on the Flock instance passed.
    passed_flock_instance = call_args[0]
    assert passed_flock_instance.temporal_config is workflow_config


@pytest.mark.asyncio
async def test_run_async_temporal_passes_memo(basic_flock, simple_agent, mocker):
    """Verify run_async passes the memo argument to the temporal executor."""
    basic_flock.enable_temporal = True
    basic_flock.add_agent(simple_agent)
    # Patch where it's called from (in the flock module)
    mock_temporal_exec = mocker.patch(
        'flock.core.flock.run_temporal_workflow', new_callable=AsyncMock)
    mock_init_context = mocker.patch(
        'flock.core.flock.initialize_context')  # Keep this mock
    mock_temporal_exec.return_value = {"result": "memo_test"}

    memo_data = {"user": "test_user", "run": 123}
    input_data = {"query": "test"}

    # Call run_async only ONCE
    await basic_flock.run_async(
        start_agent="agent1",
        input=input_data,
        memo=memo_data,  # Pass memo here
        box_result=False
    )

    # Assert the single mock was called correctly
    mock_temporal_exec.assert_awaited_once_with(
        basic_flock,
        mocker.ANY,  # Context object (check type separately if needed)
        box_result=False,
        memo=memo_data  # Assert memo is passed as kwarg
    )
    # Verify context was initialized
    mock_init_context.assert_called_once()


@pytest.mark.asyncio
async def test_run_async_temporal_no_in_process_worker(basic_flock, simple_agent, mocker):
    """Test that the in-process worker is NOT started if flag is False."""
    # Mock the executor and the setup_worker function it calls
    mock_temporal_exec = mocker.patch(
        'flock.core.flock.run_temporal_workflow', new_callable=AsyncMock)
    # Patch setup_worker in executor
    mock_setup_worker = mocker.patch(
        'flock.core.execution.temporal_executor.setup_worker')
    # To check if worker.run() task is created
    mock_create_task = mocker.patch('asyncio.create_task')

    # Configure Flock to disable in-process worker
    basic_flock.enable_temporal = True
    basic_flock.temporal_start_in_process_worker = False
    basic_flock.add_agent(simple_agent)

    # Mock the return value of the executor itself (since setup_worker won't be called to run it)
    # Need to actually call the real executor but mock deeper dependencies

    # --- Let's adjust the mocking strategy ---
    # We need to call the *real* run_temporal_workflow, but mock its internal call to setup_worker
    # So, don't mock run_temporal_workflow itself. Patch setup_worker and start_workflow.

    # Use wraps if needed, but let's patch deeper
    mocker.patch('flock.core.flock.run_temporal_workflow',
                 wraps=run_temporal_workflow)

    mock_setup_worker = mocker.patch(
        'flock.core.execution.temporal_executor.setup_worker', new_callable=AsyncMock)
    # Mock the client and start_workflow to prevent actual Temporal calls
    mock_client = MagicMock()
    mock_handle = MagicMock()
    mock_handle.result = AsyncMock(return_value={"result": "no_worker_test"})
    mock_client.start_workflow = AsyncMock(return_value=mock_handle)
    mocker.patch('flock.core.execution.temporal_executor.create_temporal_client',
                 new_callable=AsyncMock, return_value=mock_client)

    # Reconfigure Flock
    flock_no_worker = Flock(
        name="flock_no_worker",
        enable_temporal=True,
        temporal_start_in_process_worker=False,  # Key setting
        agents=[simple_agent],
        show_flock_banner=False
    )

    # Run
    await flock_no_worker.run_async(start_agent="agent1", input={"query": "test"}, box_result=False)

    # Assert: setup_worker should NOT have been called
    mock_setup_worker.assert_not_awaited()
    # Assert: start_workflow WAS called (the workflow execution should still be attempted)
    mock_client.start_workflow.assert_awaited_once()

# --- Serialization/Dict Tests (Placeholders - Add to serialization tests) ---

# TODO: Add tests in tests/serialization/ to verify TemporalWorkflowConfig
#       and TemporalActivityConfig are correctly handled by Flock.to_dict() / from_dict()
#       and FlockAgent.to_dict() / from_dict().
# Tests added to tests/serialization/test_flock_serializer.py


# --- Other Tests (Keep existing tests like boxing, sync wrapper etc.) ---
```

### src\flock\adapter\vector_base.py

- **Lines**: 47
- **Last modified**: 2025-05-21 19:51:15
- **Used by**: 5 files

**Classes**:
- `VectorHit`: 0 methods
- `VectorAdapter`: 4 methods

**Content**:
```py
from __future__ import annotations

from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import Any


@dataclass
class VectorHit:
    """Result object returned from vector search."""

    id: str
    content: str | None
    metadata: dict[str, Any]
    score: float  # similarity score (higher = more similar)


class VectorAdapter(ABC):
    """Protocol for vector-store adapters."""

    def __init__(self, **kwargs):
        """Store-specific kwargs are passed through subclass constructor."""
        super().__init__()

    # ----------------------
    # CRUD operations
    # ----------------------
    @abstractmethod
    def add(
        self,
        *,
        id: str,
        content: str,
        embedding: list[float],
        metadata: dict[str, Any] | None = None,
    ) -> None:  # pragma: no cover – interface
        """Insert or upsert a single document."""

    @abstractmethod
    def query(
        self, *, embedding: list[float], k: int
    ) -> list[VectorHit]:  # pragma: no cover – interface
        """Return top-k most similar hits."""

    def close(self) -> None:  # Optional override
        """Free resources / flush buffers."""
        return
```

### src\flock\core\api\main.py

- **Lines**: 162
- **Last modified**: 2025-05-21 19:51:15

**Description**: This module defines the FlockAPI class, which is now primarily responsible for
managing and adding user-defined custom API endpoints to a main FastAPI application.

**Classes**:
- `FlockAPI`: 2 methods

**Content**:
```py
# src/flock/core/api/main.py
"""This module defines the FlockAPI class, which is now primarily responsible for
managing and adding user-defined custom API endpoints to a main FastAPI application.
"""

import inspect
from collections.abc import Callable, Sequence
from typing import TYPE_CHECKING, Any

from fastapi import (  # Ensure Request is aliased
    Body,
    Depends,
    FastAPI,
    Request as FastAPIRequest,
)

from flock.core.logging.logging import get_logger

from .custom_endpoint import FlockEndpoint

if TYPE_CHECKING:
    from flock.core.flock import Flock

logger = get_logger("core.api.custom_setup")


class FlockAPI:
    """A helper class to manage the addition of user-defined custom API endpoints
    to an existing FastAPI application, in the context of a Flock instance.
    """

    def __init__(
        self,
        flock_instance: "Flock",
        custom_endpoints: Sequence[FlockEndpoint] | dict[tuple[str, list[str] | None], Callable[..., Any]] | None = None,
    ):
        self.flock = flock_instance
        self.processed_custom_endpoints: list[FlockEndpoint] = []
        if custom_endpoints:
            if isinstance(custom_endpoints, dict):
                logger.warning("Received custom_endpoints as dict, converting. Prefer Sequence[FlockEndpoint].")
                for (path, methods), cb in custom_endpoints.items():
                    self.processed_custom_endpoints.append(
                        FlockEndpoint(path=path, methods=list(methods) if methods else ["GET"], callback=cb)
                    )
            elif isinstance(custom_endpoints, Sequence):
                for ep_item in custom_endpoints: # Renamed loop variable
                    if isinstance(ep_item, FlockEndpoint):
                        self.processed_custom_endpoints.append(ep_item)
                    else:
                        logger.warning(f"Skipping non-FlockEndpoint item in custom_endpoints sequence: {type(ep_item)}")
            else:
                logger.warning(f"Unsupported type for custom_endpoints: {type(custom_endpoints)}")
        logger.info(
            f"FlockAPI helper initialized for Flock: '{self.flock.name}'. "
            f"Prepared {len(self.processed_custom_endpoints)} custom endpoints."
        )

    def add_custom_routes_to_app(self, app: FastAPI):
        if not self.processed_custom_endpoints:
            logger.debug("No custom endpoints to add to the FastAPI app.")
            return

        logger.info(f"Adding {len(self.processed_custom_endpoints)} custom endpoints to the FastAPI app instance.")

        for current_ep_def in self.processed_custom_endpoints: # Use current_ep_def to avoid closure issues

            # This factory now takes current_ep_def to ensure it uses the correct endpoint's details
            def _create_handler_factory(
                # Capture the specific endpoint definition for this factory instance
                specific_ep: FlockEndpoint
            ):
                # This inner function prepares the payload and calls the user's callback
                async def _invoke_user_callback(
                    request_param: FastAPIRequest, # Parameter for FastAPI's Request object
                    body_param: Any,      # Will be populated by the _route_handler
                    query_param: Any      # Will be populated by the _route_handler
                ):
                    payload_to_user: dict[str, Any] = {"flock": self.flock} # self here refers to FlockAPI instance

                    if request_param: # Ensure request_param is not None
                        payload_to_user.update(request_param.path_params)
                        # query_param is already the parsed Pydantic model or None
                        if specific_ep.query_model and query_param is not None:
                            payload_to_user["query"] = query_param
                        # Fallback for raw query if callback expects 'query' but no query_model was set
                        elif 'query' in inspect.signature(specific_ep.callback).parameters and not specific_ep.query_model:
                             if request_param.query_params:
                                payload_to_user["query"] = dict(request_param.query_params)

                        # body_param is already the parsed Pydantic model or None
                        if specific_ep.request_model and body_param is not None:
                            payload_to_user["body"] = body_param
                        # Fallback for raw body if callback expects 'body' but no request_model was set
                        elif 'body' in inspect.signature(specific_ep.callback).parameters and \
                             not specific_ep.request_model and \
                             request_param.method in {"POST", "PUT", "PATCH"}:
                            try: payload_to_user["body"] = await request_param.json()
                            except Exception: payload_to_user["body"] = await request_param.body()

                        # If user callback explicitly asks for 'request'
                        if 'request' in inspect.signature(specific_ep.callback).parameters:
                            payload_to_user['request'] = request_param


                    user_callback_sig = inspect.signature(specific_ep.callback)
                    final_kwargs = {
                        k: v for k, v in payload_to_user.items() if k in user_callback_sig.parameters
                    }

                    if inspect.iscoroutinefunction(specific_ep.callback):
                        return await specific_ep.callback(**final_kwargs)
                    return specific_ep.callback(**final_kwargs)

                # --- Select the correct handler signature based on specific_ep's models ---
                if specific_ep.request_model and specific_ep.query_model:
                    async def _route_handler_body_query(
                        request: FastAPIRequest, # Correct alias for FastAPI Request
                        body: specific_ep.request_model = Body(...),  # type: ignore
                        query: specific_ep.query_model = Depends(specific_ep.query_model)  # type: ignore
                    ):
                        return await _invoke_user_callback(request, body, query)
                    return _route_handler_body_query
                elif specific_ep.request_model and not specific_ep.query_model:
                    async def _route_handler_body_only(
                        request: FastAPIRequest, # Correct alias
                        body: specific_ep.request_model = Body(...)  # type: ignore
                    ):
                        return await _invoke_user_callback(request, body, None)
                    return _route_handler_body_only
                elif not specific_ep.request_model and specific_ep.query_model:
                    async def _route_handler_query_only(
                        request: FastAPIRequest, # Correct alias
                        query: specific_ep.query_model = Depends(specific_ep.query_model)  # type: ignore
                    ):
                        return await _invoke_user_callback(request, None, query)
                    return _route_handler_query_only
                else: # Neither request_model nor query_model
                    async def _route_handler_request_only(
                        request: FastAPIRequest # Correct alias
                    ):
                        return await _invoke_user_callback(request, None, None)
                    return _route_handler_request_only

            # Create the handler for the current_ep_def
            selected_handler = _create_handler_factory(current_ep_def) # Pass current_ep_def
            selected_handler.__name__ = f"handler_for_{current_ep_def.path.replace('/', '_').lstrip('_')}_{current_ep_def.methods[0]}"


            app.add_api_route(
                current_ep_def.path,
                selected_handler,
                methods=current_ep_def.methods or ["GET"],
                name=current_ep_def.name or f"custom:{current_ep_def.path.replace('/', '_').lstrip('_')}",
                include_in_schema=current_ep_def.include_in_schema,
                response_model=current_ep_def.response_model,
                summary=current_ep_def.summary,
                description=current_ep_def.description,
                dependencies=current_ep_def.dependencies,
                tags=["Flock API Custom Endpoints"],
            )
            logger.debug(f"Added custom route to app: {current_ep_def.methods} {current_ep_def.path} (Handler: {selected_handler.__name__}, Summary: {current_ep_def.summary})")
```

### src\flock\core\api\models.py

- **Lines**: 97
- **Last modified**: 2025-04-16 00:11:15
- **Used by**: 3 files

**Description**: Pydantic models for the Flock API.

**Classes**:
- `FlockAPIRequest`: 0 methods
- `FlockAPIResponse`: 0 methods
- `FlockBatchRequest`: 0 methods
- `FlockBatchResponse`: 0 methods

**Content**:
```py
# src/flock/core/api/models.py
"""Pydantic models for the Flock API."""

from datetime import datetime
from typing import Any

from pydantic import BaseModel, Field


class FlockAPIRequest(BaseModel):
    """Request model for running an agent via JSON API."""

    agent_name: str = Field(..., description="Name of the agent to run")
    inputs: dict[str, Any] = Field(
        default_factory=dict, description="Input data for the agent"
    )
    async_run: bool = Field(
        default=False, description="Whether to run asynchronously"
    )


class FlockAPIResponse(BaseModel):
    """Response model for API run requests."""

    run_id: str = Field(..., description="Unique ID for this run")
    status: str = Field(..., description="Status of the run")
    result: dict[str, Any] | None = Field(
        None, description="Run result if completed"
    )
    started_at: datetime = Field(..., description="When the run started")
    completed_at: datetime | None = Field(
        None, description="When the run completed"
    )
    error: str | None = Field(None, description="Error message if failed")


class FlockBatchRequest(BaseModel):
    """Request model for batch processing via JSON API."""

    agent_name: str = Field(..., description="Name of the agent to run")
    batch_inputs: list[dict[str, Any]] | str = Field(
        ..., description="List of input dictionaries or path to CSV file"
    )
    input_mapping: dict[str, str] | None = Field(
        None, description="Maps DataFrame/CSV column names to agent input keys"
    )
    static_inputs: dict[str, Any] | None = Field(
        None, description="Inputs constant across all batch runs"
    )
    parallel: bool = Field(
        default=True, description="Whether to run jobs in parallel"
    )
    max_workers: int = Field(
        default=5, description="Max concurrent workers for parallel runs"
    )
    use_temporal: bool | None = Field(
        None, description="Override Flock's enable_temporal setting"
    )
    box_results: bool = Field(
        default=True, description="Wrap results in Box objects"
    )
    return_errors: bool = Field(
        default=False, description="Return Exception objects for failed runs"
    )
    silent_mode: bool = Field(
        default=True, description="Suppress output and show progress bar"
    )
    write_to_csv: str | None = Field(
        None, description="Path to save results as CSV file"
    )


class FlockBatchResponse(BaseModel):
    """Response model for batch processing requests."""

    batch_id: str = Field(..., description="Unique ID for this batch run")
    status: str = Field(..., description="Status of the batch run")
    results: list[Any] = Field(
        default_factory=list,
        description="List of results from batch processing",
    )
    started_at: datetime = Field(..., description="When the batch run started")
    completed_at: datetime | None = Field(
        None, description="When the batch run completed"
    )
    error: str | None = Field(None, description="Error message if failed")

    # Additional fields for batch progress tracking
    total_items: int = Field(
        0, description="Total number of items in the batch"
    )
    completed_items: int = Field(
        0, description="Number of completed items in the batch"
    )
    progress_percentage: float = Field(
        0.0, description="Percentage of completion (0-100)"
    )
```

### src\flock\core\util\cli_helper.py

- **Lines**: 90
- **Last modified**: 2025-05-22 21:27:37

**Functions**:
- `display_hummingbird()`
- `init_console(clear_screen, show_banner)`
- `display_banner_no_version()`

**Content**:
```py
from importlib.metadata import PackageNotFoundError, version

try:
    __version__ = version("flock-core")
except PackageNotFoundError:
    __version__ = "0.2.0"


def display_hummingbird():
    """Display the hummingbird."""
    print("""
\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m
\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m
\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m
\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m
\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m
\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;24;23;22m▀\033[0m\033[38;2;0;0;0;48;2;47;44;40m▀\033[0m\033[38;2;0;0;0;48;2;30;28;27m▀\033[0m\033[38;2;0;0;0;48;2;1;1;1m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m
\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;14;14;13m▀\033[0m\033[38;2;2;2;2;48;2;173;161;143m▀\033[0m\033[38;2;97;92;83;48;2;243;226;198m▀\033[0m\033[38;2;204;190;168;48;2;245;226;197m▀\033[0m\033[38;2;243;225;197;48;2;245;225;195m▀\033[0m\033[38;2;243;226;198;48;2;181;168;147m▀\033[0m\033[38;2;243;226;199;48;2;193;179;158m▀\033[0m\033[38;2;213;198;176;48;2;245;226;198m▀\033[0m\033[38;2;110;102;89;48;2;245;226;197m▀\033[0m\033[38;2;7;7;6;48;2;217;202;178m▀\033[0m\033[38;2;0;0;0;48;2;119;111;99m▀\033[0m\033[38;2;0;0;0;48;2;56;54;50m▀\033[0m\033[38;2;0;0;0;48;2;22;22;20m▀\033[0m\033[38;2;0;0;0;48;2;6;6;6m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m
\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;11;11;10;48;2;141;131;119m▀\033[0m\033[38;2;192;179;159;48;2;243;225;197m▀\033[0m\033[38;2;243;225;197;48;2;245;226;197m▀\033[0m\033[38;2;245;226;197;48;2;245;226;195m▀\033[0m\033[38;2;246;226;197;48;2;245;226;197m▀\033[0m\033[38;2;222;205;179;48;2;239;220;193m▀\033[0m\033[38;2;2;2;2;48;2;64;59;52m▀\033[0m\033[38;2;17;17;15;48;2;91;85;77m▀\033[0m\033[38;2;243;226;199;48;2;243;226;198m▀\033[0m\033[38;2;245;226;198;48;2;238;221;193m▀\033[0m\033[38;2;245;226;197;48;2;170;158;139m▀\033[0m\033[38;2;243;225;197;48;2;99;92;81m▀\033[0m\033[38;2;243;224;197;48;2;50;47;43m▀\033[0m\033[38;2;242;224;198;48;2;14;13;12m▀\033[0m\033[38;2;227;213;191;48;2;0;0;0m▀\033[0m\033[38;2;204;192;172;48;2;0;0;0m▀\033[0m\033[38;2;187;175;158;48;2;0;0;0m▀\033[0m\033[38;2;168;159;143;48;2;0;0;0m▀\033[0m\033[38;2;147;140;128;48;2;0;0;0m▀\033[0m\033[38;2;128;120;111;48;2;0;0;0m▀\033[0m\033[38;2;99;93;85;48;2;1;1;1m▀\033[0m\033[38;2;58;55;51;48;2;9;9;8m▀\033[0m\033[38;2;6;6;6;48;2;21;21;21m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m
\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;32;31;29;48;2;121;113;102m▀\033[0m\033[38;2;240;223;195;48;2;243;226;195m▀\033[0m\033[38;2;245;226;197;48;2;245;226;195m▀\033[0m\033[38;2;245;226;197;48;2;245;226;197m▀\033[0m\033[38;2;246;226;197;48;2;246;226;197m▀\033[0m\033[38;2;245;225;195;48;2;245;226;197m▀\033[0m\033[38;2;245;226;195;48;2;245;226;195m▀\033[0m\033[38;2;243;225;198;48;2;239;221;195m▀\033[0m\033[38;2;240;223;199;48;2;71;65;60m▀\033[0m\033[38;2;136;127;111;48;2;0;0;0m▀\033[0m\033[38;2;21;20;19;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m
\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;186;173;153;48;2;220;204;179m▀\033[0m\033[38;2;243;225;197;48;2;245;226;198m▀\033[0m\033[38;2;245;226;197;48;2;245;225;197m▀\033[0m\033[38;2;245;226;197;48;2;245;225;195m▀\033[0m\033[38;2;245;226;197;48;2;245;225;195m▀\033[0m\033[38;2;245;226;197;48;2;245;225;197m▀\033[0m\033[38;2;245;225;195;48;2;227;211;187m▀\033[0m\033[38;2;112;104;93;48;2;6;6;5m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m
\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;219;204;179;48;2;188;175;156m▀\033[0m\033[38;2;245;226;197;48;2;245;226;198m▀\033[0m\033[38;2;245;226;195;48;2;245;226;197m▀\033[0m\033[38;2;245;226;195;48;2;245;225;195m▀\033[0m\033[38;2;245;225;195;48;2;245;225;195m▀\033[0m\033[38;2;245;225;195;48;2;245;225;195m▀\033[0m\033[38;2;238;220;192;48;2;245;226;195m▀\033[0m\033[38;2;149;137;121;48;2;243;225;197m▀\033[0m\033[38;2;16;14;13;48;2;197;183;161m▀\033[0m\033[38;2;0;0;0;48;2;15;14;13m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m
\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;20;19;17m▀\033[0m\033[38;2;0;0;0;48;2;146;136;121m▀\033[0m\033[38;2;16;15;14;48;2;235;219;194m▀\033[0m\033[38;2;161;149;134;48;2;243;226;198m▀\033[0m\033[38;2;245;226;198;48;2;243;226;197m▀\033[0m\033[38;2;245;226;195;48;2;245;226;197m▀\033[0m\033[38;2;245;226;195;48;2;245;225;195m▀\033[0m\033[38;2;245;226;195;48;2;245;225;197m▀\033[0m\033[38;2;245;225;195;48;2;243;225;197m▀\033[0m\033[38;2;243;225;198;48;2;192;178;158m▀\033[0m\033[38;2;235;218;190;48;2;152;141;125m▀\033[0m\033[38;2;245;226;197;48;2;245;226;197m▀\033[0m\033[38;2;163;151;134;48;2;243;225;198m▀\033[0m\033[38;2;0;0;0;48;2;46;43;39m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m
\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;40;38;35m▀\033[0m\033[38;2;56;52;48;48;2;232;216;193m▀\033[0m\033[38;2;220;204;182;48;2;207;193;170m▀\033[0m\033[38;2;243;227;199;48;2;72;67;60m▀\033[0m\033[38;2;211;195;175;48;2;1;1;1m▀\033[0m\033[38;2;151;140;125;48;2;0;0;0m▀\033[0m\033[38;2;130;120;108;48;2;0;0;0m▀\033[0m\033[38;2;138;128;113;48;2;0;0;0m▀\033[0m\033[38;2;163;151;133;48;2;0;0;0m▀\033[0m\033[38;2;170;158;140;48;2;0;0;0m▀\033[0m\033[38;2;121;113;101;48;2;0;0;0m▀\033[0m\033[38;2;16;15;14;48;2;39;36;32m▀\033[0m\033[38;2;155;145;129;48;2;231;215;191m▀\033[0m\033[38;2;245;226;197;48;2;245;226;198m▀\033[0m\033[38;2;245;226;197;48;2;246;226;198m▀\033[0m\033[38;2;118;109;97;48;2;151;140;124m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m
\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;1;1;1;48;2;75;71;65m▀\033[0m\033[38;2;183;170;153;48;2;211;197;177m▀\033[0m\033[38;2;182;170;151;48;2;18;16;15m▀\033[0m\033[38;2;12;12;11;48;2;112;104;95m▀\033[0m\033[38;2;12;11;10;48;2;227;211;187m▀\033[0m\033[38;2;86;81;73;48;2;234;217;193m▀\033[0m\033[38;2;127;119;107;48;2;192;179;160m▀\033[0m\033[38;2;138;129;116;48;2;174;162;144m▀\033[0m\033[38;2;126;118;105;48;2;172;160;142m▀\033[0m\033[38;2;105;97;88;48;2;170;158;140m▀\033[0m\033[38;2;97;92;83;48;2;159;147;131m▀\033[0m\033[38;2;141;131;119;48;2;121;113;101m▀\033[0m\033[38;2;227;211;191;48;2;47;44;40m▀\033[0m\033[38;2;194;181;161;48;2;139;130;116m▀\033[0m\033[38;2;245;226;198;48;2;245;226;198m▀\033[0m\033[38;2;245;226;197;48;2;245;226;197m▀\033[0m\033[38;2;149;138;123;48;2;113;105;94m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m
\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;163;154;139;48;2;219;205;182m▀\033[0m\033[38;2;100;94;85;48;2;102;96;87m▀\033[0m\033[38;2;165;156;141;48;2;210;195;175m▀\033[0m\033[38;2;223;207;184;48;2;29;28;24m▀\033[0m\033[38;2;97;92;81;48;2;52;48;44m▀\033[0m\033[38;2;9;9;8;48;2;110;103;93m▀\033[0m\033[38;2;0;0;0;48;2;140;131;118m▀\033[0m\033[38;2;0;0;0;48;2;142;131;119m▀\033[0m\033[38;2;0;0;0;48;2;121;113;102m▀\033[0m\033[38;2;0;0;0;48;2;109;102;93m▀\033[0m\033[38;2;0;0;0;48;2;128;120;107m▀\033[0m\033[38;2;1;1;0;48;2;195;181;162m▀\033[0m\033[38;2;91;86;79;48;2;243;225;198m▀\033[0m\033[38;2;241;222;195;48;2;245;225;197m▀\033[0m\033[38;2;245;226;197;48;2;245;226;198m▀\033[0m\033[38;2;243;225;197;48;2;190;176;157m▀\033[0m\033[38;2;47;44;40;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m
\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;9;9;8;48;2;21;20;19m▀\033[0m\033[38;2;197;184;163;48;2;192;179;162m▀\033[0m\033[38;2;190;177;159;48;2;236;220;195m▀\033[0m\033[38;2;101;95;84;48;2;235;217;193m▀\033[0m\033[38;2;197;184;163;48;2;135;125;111m▀\033[0m\033[38;2;243;226;199;48;2;40;39;36m▀\033[0m\033[38;2;231;215;190;48;2;2;1;1m▀\033[0m\033[38;2;194;179;160;48;2;0;0;0m▀\033[0m\033[38;2;179;166;147;48;2;10;9;9m▀\033[0m\033[38;2;179;166;147;48;2;64;60;55m▀\033[0m\033[38;2;182;170;152;48;2;169;158;143m▀\033[0m\033[38;2;240;224;195;48;2;245;226;199m▀\033[0m\033[38;2;245;226;197;48;2;245;226;198m▀\033[0m\033[38;2;245;225;197;48;2;245;226;197m▀\033[0m\033[38;2;245;225;197;48;2;243;225;199m▀\033[0m\033[38;2;243;224;195;48;2;117;110;97m▀\033[0m\033[38;2;56;52;48;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m
\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;15;14;13;48;2;6;6;5m▀\033[0m\033[38;2;235;220;195;48;2;242;225;201m▀\033[0m\033[38;2;243;225;199;48;2;243;225;199m▀\033[0m\033[38;2;80;75;67;48;2;226;210;187m▀\033[0m\033[38;2;65;60;56;48;2;243;226;199m▀\033[0m\033[38;2;126;118;105;48;2;195;182;162m▀\033[0m\033[38;2;173;163;146;48;2;136;127;113m▀\033[0m\033[38;2;210;195;177;48;2;89;84;77m▀\033[0m\033[38;2;238;222;198;48;2;52;50;47m▀\033[0m\033[38;2;234;219;195;48;2;92;87;80m▀\033[0m\033[38;2;186;174;157;48;2;209;195;172m▀\033[0m\033[38;2;236;218;192;48;2;243;226;198m▀\033[0m\033[38;2;245;226;198;48;2;221;205;181m▀\033[0m\033[38;2;243;225;198;48;2;81;76;68m▀\033[0m\033[38;2;131;124;111;48;2;0;0;0m▀\033[0m\033[38;2;1;1;1;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m
\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;2;2;2;48;2;0;0;0m▀\033[0m\033[38;2;238;222;197;48;2;233;218;194m▀\033[0m\033[38;2;243;225;198;48;2;169;157;141m▀\033[0m\033[38;2;177;163;147;48;2;13;12;11m▀\033[0m\033[38;2;42;39;36;48;2;116;108;99m▀\033[0m\033[38;2;4;4;4;48;2;219;204;182m▀\033[0m\033[38;2;68;64;58;48;2;243;226;199m▀\033[0m\033[38;2;153;142;128;48;2;225;210;186m▀\033[0m\033[38;2;224;209;187;48;2;147;137;121m▀\033[0m\033[38;2;243;225;199;48;2;69;65;59m▀\033[0m\033[38;2;222;206;182;48;2;6;6;5m▀\033[0m\033[38;2;118;110;99;48;2;0;0;0m▀\033[0m\033[38;2;14;13;12;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m
\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;230;216;193;48;2;192;181;163m▀\033[0m\033[38;2;59;55;52;48;2;189;176;157m▀\033[0m\033[38;2;202;188;168;48;2;242;224;198m▀\033[0m\033[38;2;245;227;200;48;2;133;123;110m▀\033[0m\033[38;2;216;199;176;48;2;10;9;8m▀\033[0m\033[38;2;95;89;79;48;2;0;0;0m▀\033[0m\033[38;2;6;5;5;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m
\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;222;207;186;48;2;225;210;189m▀\033[0m\033[38;2;240;222;195;48;2;105;100;89m▀\033[0m\033[38;2;81;75;67;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m
\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;161;149;136;48;2;40;36;36m▀\033[0m\033[38;2;1;1;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m
\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m
\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m
\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m
\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m\033[38;2;0;0;0;48;2;0;0;0m▀\033[0m

""")


def init_console(clear_screen: bool = True, show_banner: bool = True):
    """Display the Flock banner."""
    from rich.console import Console
    from rich.syntax import Text

    console = Console()
    banner_text = Text(
        f"""
🦆    🐓     🐤     🐧
╭━━━━━━━━━━━━━━━━━━━━━━━━╮
│ ▒█▀▀▀ █░░ █▀▀█ █▀▀ █░█ │
│ ▒█▀▀▀ █░░ █░░█ █░░ █▀▄ │
│ ▒█░░░ ▀▀▀ ▀▀▀▀ ▀▀▀ ▀░▀ │
╰━━━━━━━━v{__version__}━━━━━━━━╯
🦆     🐤    🐧     🐓
""",
        justify="center",
        style="bold orange3",
    )
    if clear_screen:
        console.clear()

    if show_banner:
        console.print(banner_text)
        console.print(
            "[italic]'Magpie'[/] milestone - [bold]white duck GmbH[/] - [cyan]https://whiteduck.de[/]\n"
        )


def display_banner_no_version():
    """Display the Flock banner."""
    from rich.console import Console
    from rich.syntax import Text

    console = Console()
    banner_text = Text(
        """
🦆    🐓     🐤     🐧
╭━━━━━━━━━━━━━━━━━━━━━━━━╮
│ ▒█▀▀▀ █░░ █▀▀█ █▀▀ █░█ │
│ ▒█▀▀▀ █░░ █░░█ █░░ █▀▄ │
│ ▒█░░░ ▀▀▀ ▀▀▀▀ ▀▀▀ ▀░▀ │
╰━━━━━━━━━━━━━━━━━━━━━━━━╯
🦆     🐤    🐧     🐓
""",
        justify="center",
        style="bold orange3",
    )
    console.print(banner_text)
    console.print("[bold]white duck GmbH[/] - [cyan]https://whiteduck.de[/]\n")
```

### examples\05-full-projects\project-textual-project-planner\src\flock_flightplan\app.py

- **Lines**: 501
- **Last modified**: 2025-05-22 18:59:04

**Classes**:
- `StyledTextArea`: 3 methods
- `ContentView`: 6 methods
- `FlightPlanApp`: 9 methods

**Content**:
```py
# src/flock_flightplan/app.py
import asyncio
import json
from pathlib import Path
import httpx
from textual.app import App, ComposeResult
from textual.widgets import Header, Footer, Input, Button, Static, Markdown, Tree, TabbedContent, TabPane, TextArea, Log, RichLog # Import Tree
from textual.containers import Container, Horizontal
from textual.widget import Widget, Strip
from rich.text import Text
from rich.panel import Panel
from rich.markdown import Markdown as RichMarkdown

from flock_flightplan import collector, model
from flock_flightplan.agents.flocky import generate_flocky_response
from flock_flightplan.agents.planning import generate_planning_structure
from flock_flightplan.cli import utils
from flock_flightplan.utils import cli_init # Import Widget base class
from flock_flightplan.collector import config
from .project_tree import ProjectTree

cli_header = Text(
        """
                                                                                                                         
    _/_/_/_/       _/       _/                     _/          _/                         _/                             
   _/             _/                  _/_/_/      _/_/_/    _/_/_/_/        _/_/_/       _/        _/_/_/      _/_/_/    
  _/_/_/         _/       _/       _/    _/      _/    _/    _/            _/    _/     _/      _/    _/      _/    _/   
 _/             _/       _/       _/    _/      _/    _/    _/            _/    _/     _/      _/    _/      _/    _/    
_/             _/       _/         _/_/_/      _/    _/      _/_/        _/_/_/       _/        _/_/_/      _/    _/     
                                      _/                                _/                                               
                                 _/_/                                  _/                                                

""",
        style="bold green"
    )

cli_subtitle = (Text("plan anything - implement everything - ｐｏｗｅｒｅｄ ｂｙ ＦＬＯＣＫ\n", style="bold white"))

class StyledTextArea(TextArea):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.highlights = {}  # Store line/column ranges and their styles
        
    def add_highlight(self, start_line, start_col, end_line, end_col, style_class):
        """Add a highlighted region with a specific style class."""
        key = (start_line, start_col, end_line, end_col)
        self.highlights[key] = style_class
        self.refresh()
    
    def render_line(self, y: int) -> Strip:
        """Override to add styling to specific parts of text."""
        strip = super().render_line(y)
        
        # Apply highlights for this line
        for (start_line, start_col, end_line, end_col), style in self.highlights.items():
            if y >= start_line and y <= end_line:
                # Calculate the affected column range for this line
                if y == start_line and y == end_line:
                    # Highlight is contained within this line
                    col_start, col_end = start_col, end_col
                elif y == start_line:
                    # This is the first line of a multi-line highlight
                    col_start, col_end = start_col, len(strip.text)
                elif y == end_line:
                    # This is the last line of a multi-line highlight
                    col_start, col_end = 0, end_col
                else:
                    # This is a middle line of a multi-line highlight
                    col_start, col_end = 0, len(strip.text)
                
                # Apply styling
                if style == "red":
                    strip.stylize(f"red", col_start, col_end)
                elif style == "blue":
                    strip.stylize(f"blue", col_start, col_end)
                # Add more color options as needed
        
        return strip

class ContentView(Container):
    """Content view for displaying templates."""

    def __init__(self):
        super().__init__(id="content-view")
        self.template_map = {}
        self._current_content_widget: Widget | None = None # Track the current widget (Markdown or Static)

    def on_mount(self) -> None:
        """Load templates when the view is mounted."""
        self._load_templates()

    def _load_templates(self):
        """Load templates from templates.json."""
        try:
            # Ensure the path is correct relative to this file (app.py)
            templates_path = Path(__file__).parent.parent.parent / "templates.json"
            self.app.log(f"Attempting to load templates from: {templates_path}") # Debug log path
            if not templates_path.exists():
                 raise FileNotFoundError(f"Templates file not found at {templates_path}")

            with open(templates_path) as f:
                templates = json.load(f)
                # Ensure it's a list of lists/tuples with 2 elements each
                if isinstance(templates, list) and all(isinstance(item, (list, tuple)) and len(item) == 2 for item in templates):
                    self.template_map = {item[0]: item[1] for item in templates}
                    self.app.log(f"Successfully loaded {len(self.template_map)} templates.")
                else:
                    raise ValueError("templates.json is not in the expected format (list of [key, value] pairs).")

        except FileNotFoundError as e:
             self.app.log.error(f"Template loading error: {e}")
             self._display_message(f"Error: templates.json not found.")
        except json.JSONDecodeError as e:
             self.app.log.error(f"Template parsing error: {e}")
             self._display_message(f"Error: Could not parse templates.json. Invalid JSON.")
        except Exception as e:
             # Catch other potential errors (permissions, format issues)
             self.app.log.error(f"Unexpected error loading templates: {e}", exc_info=True)
             self._display_message(f"Error loading templates: {str(e)}")

    def _clear_content(self):
        """Safely remove the currently displayed widget."""
        if self._current_content_widget:
            try:
                self._current_content_widget.remove()
            except Exception as e:
                # Log if removal fails for some reason (shouldn't normally happen)
                self.app.log.warning(f"Could not remove previous content widget: {e}")
            self._current_content_widget = None

    def _display_message(self, message: str):
        """Helper to display a Static message."""
        self._clear_content()
        self._current_content_widget = Static(message)
        self.mount(self._current_content_widget)

    def show_template(self, node_type: str | None):
        """Show template for the given node type, or clear if None."""
        self._clear_content() # Clear previous content first

        if node_type is None:
            # Optionally show a default message when root or unmapped node is clicked
            # self._display_message("Select a node in the tree to see details.")
            return # Or do nothing

        if node_type in self.template_map:
            markdown_content = self.template_map[node_type]
            # Use a dynamic ID maybe? Or just rely on the parent container
            self._current_content_widget = Markdown(markdown_content) # id=f"md-{node_type}"
            self.mount(self._current_content_widget)
            # self.app.log(f"Displayed template for: {node_type}") # Debug log
        else:
            # This case should ideally only happen if template loading failed
            # or if a node_type exists in data.json but not templates.json
            self.app.log.warning(f"No template found for node type: {node_type}")
            self._display_message(f"No template available for {node_type}")


class FlightPlanApp(App):
    """A Textual app for visualizing project structure as a tree."""

    # *** Reverted CSS back to inline definition ***
    CSS = """
    #app-grid {
        layout: grid;
        grid-size: 2;
        grid-columns: 1fr 3fr;
        height: 1fr;
    }

    #tree-container {
        height: 100%;
        overflow: auto;
        padding: 1;
    }

    #content-container {
        height: 100%;
        overflow: auto;
        padding: 1;
    }
    
    #cli-grid {
        border: solid grey;
        padding: 1;
    }

    #content-view {
        height: auto;
        width: 100%;
    }
    


    #input-container {
        dock: bottom;
        height: auto;
        margin-bottom: 1;  /* Add margin to prevent overlap with footer */
        padding: 1;
    }

    Input {
        width: 1fr;
    }

    Button {
        width: 1;
    }

    /* Ensure footer is docked properly */
    Footer {
        dock: bottom;
    }
    """

    BINDINGS = [
        ("q", "quit", "Quit"),
        ("r", "refresh", "Refresh Tree"),
    ]

    def __init__(self):
        super().__init__()
        self._tree_widget: ProjectTree | None = None
        self._data: dict | None = None
        self._content_view: ContentView | None = None # Will be assigned in compose

    def compose(self) -> ComposeResult:
        """Create child widgets."""
        yield Header(show_clock=True)
        
        with TabbedContent(id="views"):
            
            with TabPane("CLI View", id="cli-tab"):
    
                # Main app content
                with Container(id="cli-grid"):
                    with Container(id="cli-container"):
                        yield RichLog(id="cli-output", markup=True, highlight=True, wrap=True, auto_scroll=True)
                        
                        
            with TabPane("Schema", id="schema-tab"):  # Remove visible parameter
    
                # Main app content
                with Container(id="app-grid"):
                    with Container(id="tree-container"):
                        # Tree will be added later after plan generation
                        pass
                    with Container(id="content-container"):
                        # Create the content view here
                        self._content_view = ContentView()
                        yield self._content_view
                        
            with TabPane("Files", id="files-tab"):
    
                # Main app content
                with Container(id="app-grid"):
                    with Container(id="file-tree-container"):
                        # Tree will be added later in on_mount or update_tree
                        yield Tree("Files")
                    with Container(id="file-editor-container"):
                        # Create the content view here
                        yield TextArea()
                        
            

        
        # Input container - add it BEFORE the footer
        with Horizontal(id="input-container"):
            yield Input(placeholder="Enter command or message... /help for help", id="msg-input")
            yield Button(">", id="fetch-button")
        
        # Footer comes last
        yield Footer()
        

    def on_mount(self) -> None:
        """Load the initial data when the app is mounted."""
        # Don't load data on startup anymore
        # self.load_local_data()
        
        # Hide the schema tab on startup
        # tabbed_content = self.query_one("#views", TabbedContent)
        # schema_tab = self.query_one("#schema-tab", TabPane)
        # tabbed_content.remove_pane(schema_tab)
        
        log = self.query_one("#cli-output", RichLog)
        collector.set_app_log(log)
        
        log.write(cli_header,animate=True)
        log.write(cli_subtitle)
        log.write(Text("\n\nFlocky > Hello, I'm Flocky! How can I help you today? Please enter your command or message below.\n", style="bold white"))

    def on_input_submitted(self, event: Input.Submitted) -> None:
        """Handle submit event when Enter is pressed in the input field."""
        if event.input.id == "msg-input":
            msg = event.value.strip()
            log = self.query_one("#cli-output", RichLog)
            log.write(Text(f"\n{msg}\n", style="bold green"))
            if msg == "/collect":
                repository = collector.run_collection(
                include_args=['py:.'],
                exclude_args=[],
                calculate_metrics=False,
                )
                # Display repository using rich rendering methods
                repository.render_summary()
                repository.render_files()
                repository.save_to_json("repository.json")
                repository.save_to_markdown("repository.md")
                utils.print_success("Repository object saved to repository.json and repository.md")
                event.input.value = ""
                return
            if msg:
                
                
                # Temporarily disable input to prevent multiple submissions
                event.input.disabled = True
                
                # Run the async fetch task in the background
                self.run_worker(self.fetch_data(msg), exclusive=False, thread=True)
                #self.run_worker(self.fetch_data(msg), exclusive=False)
                
                # Clear the input field
                event.input.value = ""
            else:
                self.notify("Please enter a valid message", severity="warning")

    def on_button_pressed(self, event: Button.Pressed) -> None:
        """Handle button press event."""
        if event.button.id == "fetch-button":
            msg_input = self.query_one("#msg-input", Input)
            msg = msg_input.value.strip()
            log = self.query_one("#cli-output", RichLog)
            log.write(Text(f"\n{msg}\n", style="bold green"))
            if msg == "/collect":
                repository = collector.run_collection(
                include_args=['py:.'],
                exclude_args=[],
                calculate_metrics=False,
                )
                # Display repository using rich rendering methods
                repository.render_summary()
                repository.render_files()
                repository.save_to_json("repository.json")
                repository.save_to_markdown("repository.md")
                utils.print_success("Repository object generated successfully")
                msg_input.value = ""
                return
            
            
            if msg:
                # Temporarily disable input to prevent multiple submissions
                msg_input.disabled = True
                
                # Run the async fetch task in the background
                self.run_worker(self.fetch_data(msg), exclusive=False, thread=True)
                
                # Clear the input field
                msg_input.value = ""
            else:
                self.notify("Please enter a valid message", severity="warning")
                msg_input.focus()

    def load_local_data(self):
        """Load data from local file data.json."""
        data_path = Path(__file__).parent.parent.parent / "data.json"
        self.log(f"Attempting to load data from: {data_path}") # Debug log path
        try:
            if not data_path.exists():
                 raise FileNotFoundError(f"Data file not found at {data_path}")
            with open(data_path) as f:
                self._data = json.load(f)
            self.log("Data loaded successfully. Updating tree.")
            self.update_tree() # Build tree with loaded data
        except FileNotFoundError as e:
             self.notify(f"Error: data.json not found.", severity="error", timeout=10)
             self.log.error(f"Data loading error: {e}")
        except json.JSONDecodeError as e:
             self.notify(f"Error: Could not parse data.json. Invalid JSON.", severity="error", timeout=10)
             self.log.error(f"Data parsing error: {e}")
        except Exception as e:
             self.notify(f"Error loading local data: {str(e)}", severity="error", timeout=10)
             self.log.error(f"Unexpected error loading data: {e}", exc_info=True)


    def update_tree(self):
        """Update or create the tree with the current data."""
        if not self._data:
            self.notify("No data available to build the tree.", severity="warning")
            return

        tree_container = self.query_one("#tree-container")

        # If tree exists, remove it first
        if self._tree_widget:
            try:
                self._tree_widget.remove()
            except Exception as e:
                 self.log.warning(f"Could not remove previous tree widget: {e}")
            self._tree_widget = None

        # Create and mount the new tree
        try:
            self._tree_widget = ProjectTree(self._data)
            # Assign the handler method from *this* app instance
            self._tree_widget.node_selected_handler = self.on_tree_node_selected
            tree_container.mount(self._tree_widget)

            self.log("Project tree updated and mounted.")
            # Optional: Clear content view when tree refreshes
            if self._content_view:
                 self._content_view.show_template(None)

        except Exception as e:
            self.notify("Failed to build project tree.", severity="error")
            self.log.error(f"Error creating/mounting ProjectTree: {e}", exc_info=True)


    def on_tree_node_selected(self, node_type: str | None):
        """Handle tree node selection, called by ProjectTree."""
        if self._content_view:
             # self.log(f"App received node selection: {node_type}") # Debug log
             self._content_view.show_template(node_type)

    async def fetch_data(self, msg):
        """Fetch data from the provided URL."""
        self.notify(f"Processing your request...", timeout=3)
        
        # Enable the input field immediately to allow continued interaction
        msg_input = self.query_one("#msg-input", Input)
        msg_input.disabled = False
        
        # Show loading indicator in the CLI output
        log = self.query_one("#cli-output", RichLog)
        log.write(Text("Flocky > Thinking... 🤔", style="bold blue"))
        
        try:
            # Run API calls in background worker
            result_planning = await asyncio.wait_for(generate_planning_structure(msg), timeout=30)
            
            # Update loading message to show progress
            log.write(Text("Flocky > Planning structure generated, creating detailed response... ⏳", style="bold blue"))
            
            result_flocky = await asyncio.wait_for(generate_flocky_response(msg, result_planning), timeout=30)
            
            def process_results():
                log = self.query_one("#cli-output", RichLog) # Get log ref inside the thread-safe call
                log.write(Text(f"\nFlocky > {result_flocky.answer}\n", style="bold white"))

                # Update app state
                self._data = result_flocky.project_plan
                templates = getattr(result_flocky.project_plan, 'markdown_templates', None) # Safely get templates
                if isinstance(templates, list) and all(isinstance(item, (list, tuple)) and len(item) == 2 for item in templates):
                     if self._content_view: # Ensure content view exists
                         self._content_view.template_map = {item[0]: item[1] for item in templates}
                         self.log(f"Successfully loaded {len(self._content_view.template_map)} templates from response.")
                elif templates is not None: # Log only if templates existed but were wrong format
                    self.log.warning(f"Received templates in unexpected format: {type(templates)}")

                self.notify("Data fetched successfully!", title="Success")
                self.update_tree() # Rebuild tree with new data
            self.call_from_thread(process_results)
            return result_flocky
            
        except httpx.HTTPStatusError as e:
            log = self.query_one("#cli-output", RichLog)
            log.write(Text(f"Flocky > Sorry, I encountered an HTTP error: {e.response.status_code}\n", style="bold red"))
            
            self.notify(f"HTTP Error {e.response.status_code}", severity="error", timeout=10)
            self.log.error(f"HTTP Error: {e}")
            
        except httpx.RequestError as e:
            log = self.query_one("#cli-output", RichLog)
            log.write(Text(f"Flocky > Sorry, I encountered a network error. Please check your connection and try again.\n", style="bold red"))
            
            self.notify(f"Network Error", severity="error", timeout=10)
            self.log.error(f"Request Error: {e}")
            
        except json.JSONDecodeError as e:
            log = self.query_one("#cli-output", RichLog)
            log.write(Text(f"Flocky > Sorry, I received an invalid response format. Please try again.\n", style="bold red"))
            
            self.notify(f"Invalid response format", severity="error", timeout=10)
            self.log.error(f"JSON Decode Error: {e}")
            
        except Exception as e:
            log = self.query_one("#cli-output", RichLog)
            log.write(Text(f"Flocky > Sorry, an unexpected error occurred: {str(e)}\n", style="bold red"))
            
            self.notify(f"Error processing request", severity="error", timeout=10)
            self.log.error(f"Unexpected fetch error: {e}", exc_info=True)


    def action_refresh(self) -> None:
        """Refresh the tree using existing data."""
        if self._data:
            self.log("Refreshing tree view...")
            self.update_tree()
            self.notify("Tree refreshed")
        else:
            self.notify("No data loaded to refresh from. Try loading local data or fetching.", severity="warning")
```

### src\flock\__init__.py

- **Lines**: 221
- **Last modified**: 2025-05-21 19:51:15

**Description**: Flock package initialization.

**Functions**:
- `main()`

**Content**:
```py
"""Flock package initialization."""

import argparse
import os
import sys


def main():
    """Main function."""
    # Parse command line arguments
    parser = argparse.ArgumentParser(
        description="Flock - Declarative LLM Orchestration at Scale"
    )
    parser.add_argument(
        "--web",
        action="store_true",
        help="Start the web interface instead of the CLI",
    )
    parser.add_argument(
        "--chat",
        action="store_true",
        help="Start a chat interface. If --web is also used, enables chat within the web app; otherwise, starts standalone chat.",
    )
    parser.add_argument(
        "--theme",
        type=str,
        default=None,
        help="Specify the theme name for the web interface (if --web is used).",
    )
    args = parser.parse_args()

    # If --web flag is provided, start the web server
    if args.web:
        try:
            # Set environment variable for theme if provided
            if args.theme:
                print(
                    f"INFO: Setting FLOCK_WEB_THEME environment variable to: {args.theme}"
                )
                os.environ["FLOCK_WEB_THEME"] = args.theme
            else:
                # Ensure it's not set if no theme arg is passed
                if "FLOCK_WEB_THEME" in os.environ:
                    del os.environ["FLOCK_WEB_THEME"]

            if args.chat: # --web --chat
                print("INFO: Starting web application with chat feature enabled.")
                os.environ["FLOCK_CHAT_ENABLED"] = "true"
            else: # Just --web
                print("INFO: Starting web application.")
                if "FLOCK_CHAT_ENABLED" in os.environ:
                    del os.environ["FLOCK_CHAT_ENABLED"]

            # Ensure standalone chat mode is not active
            if "FLOCK_START_MODE" in os.environ:
                 del os.environ["FLOCK_START_MODE"]

            # Import and run the standalone webapp main function
            from flock.webapp.run import main as run_webapp_main
            run_webapp_main()

        except ImportError:
            print(
                "Error: Could not import webapp components. Ensure web dependencies are installed.",
                file=sys.stderr,
            )
            sys.exit(1)
        except Exception as e:
            print(f"Error starting webapp: {e}", file=sys.stderr)
            sys.exit(1)
        return

    elif args.chat: # Standalone chat mode (args.web is false)
        try:
            print("INFO: Starting standalone chat application.")
            os.environ["FLOCK_START_MODE"] = "chat"

            # Clear web-specific env vars that might conflict or not apply
            if "FLOCK_WEB_THEME" in os.environ:
                del os.environ["FLOCK_WEB_THEME"]
            if "FLOCK_CHAT_ENABLED" in os.environ:
                del os.environ["FLOCK_CHAT_ENABLED"]

            # Handle --theme if passed with --chat only
            if args.theme:
                 print(f"INFO: Standalone chat mode started with --theme '{args.theme}'. FLOCK_WEB_THEME will be set.")
                 os.environ["FLOCK_WEB_THEME"] = args.theme

            from flock.webapp.run import main as run_webapp_main
            run_webapp_main() # The webapp main needs to interpret FLOCK_START_MODE="chat"

        except ImportError:
            print("Error: Could not import webapp components for chat. Ensure web dependencies are installed.", file=sys.stderr)
            sys.exit(1)
        except Exception as e:
            print(f"Error starting standalone chat application: {e}", file=sys.stderr)
            sys.exit(1)
        return

    # Otherwise, run the CLI interface
    import questionary
    from rich.console import Console
    from rich.panel import Panel

    from flock.cli.config import init_config_file, load_config_file
    from flock.cli.constants import (
        CLI_CFG_FILE,
        CLI_CREATE_AGENT,
        CLI_CREATE_FLOCK,
        CLI_EXIT,
        CLI_LOAD_AGENT,
        CLI_LOAD_EXAMPLE,
        CLI_LOAD_FLOCK,
        CLI_NOTES,
        CLI_REGISTRY_MANAGEMENT,
        CLI_SETTINGS,
        CLI_START_WEB_SERVER,
        CLI_THEME_BUILDER,
    )
    from flock.cli.load_flock import load_flock
    from flock.cli.load_release_notes import load_release_notes
    from flock.cli.settings import settings_editor
    from flock.core.logging.formatters.theme_builder import theme_builder
    from flock.core.util.cli_helper import init_console

    console = Console()

    # Show a welcome message on first run with the new tool serialization format
    cfg_file = os.path.expanduser(f"~/.flock/{CLI_CFG_FILE}")
    if not os.path.exists(cfg_file):
        # Create the directory if it doesn't exist
        os.makedirs(os.path.dirname(cfg_file), exist_ok=True)

        init_config_file()
    else:
        # Load the config file
        load_config_file()

    feature_flag_file = os.path.expanduser("~/.flock/tool_serialization_notice")
    if not os.path.exists(feature_flag_file):
        # Create the directory if it doesn't exist
        os.makedirs(os.path.dirname(feature_flag_file), exist_ok=True)

        # Show the notice about the new tool serialization
        console.print(
            Panel(
                "[bold green]Flock 0.4.0b 'Magpie'- Flock CLI Management Console BETA[/]\n\n"
                "Flock now offers a tool for managing your flock:\n"
                "- Serialization and deserialization of Flock instances\n"
                "- Execution of Flock instances\n"
                "- Managing components, types, and agents via the registry management\n"
                "- Starting a web server to interact with your flock\n"
                "plannes featues: deployment on docker, kubernetes, and more!"
            ),
            justify="center",
        )
        console.line()

        # Create the flag file to prevent showing this notice again
        with open(feature_flag_file, "w") as f:
            f.write("Tool serialization notice shown")

        input("Press Enter to continue to the main menu...")
        console.clear()

    while True:
        init_console()

        # console.print("Flock Management Console\n", style="bold green")
        console.print(
            Panel("[bold green]Flock Management Console[/]"), justify="center"
        )
        console.line()

        result = questionary.select(
            "What do you want to do?",
            choices=[
                questionary.Separator(line=" "),
                # CLI_CREATE_AGENT,
                CLI_CREATE_FLOCK,
                # CLI_LOAD_AGENT,
                CLI_LOAD_FLOCK,
                # CLI_LOAD_EXAMPLE,
                questionary.Separator(),
                CLI_REGISTRY_MANAGEMENT,
                questionary.Separator(),
                CLI_THEME_BUILDER,
                CLI_SETTINGS,
                questionary.Separator(),
                CLI_NOTES,
                CLI_EXIT,
            ],
        ).ask()

        if result == CLI_LOAD_FLOCK:
            load_flock()
        elif result == CLI_CREATE_FLOCK:
            # This will be implemented in a separate create_flock.py
            from flock.cli.create_flock import create_flock

            create_flock()
        elif result == CLI_THEME_BUILDER:
            theme_builder()
        elif result == CLI_REGISTRY_MANAGEMENT:
            # Import registry management when needed
            from flock.cli.registry_management import manage_registry

            manage_registry()
        elif result == CLI_SETTINGS:
            settings_editor()
        elif result == CLI_NOTES:
            load_release_notes()
        elif result == CLI_EXIT:
            break
        input("\nPress Enter to continue...\n\n")

        console.clear()


if __name__ == "__main__":
    main()
```

### src\flock\cli\loaded_flock_cli.py

- **Lines**: 254
- **Last modified**: 2025-05-21 19:51:15

**Description**: CLI interface for working with a loaded Flock instance.

This module provides a CLI interface for a Flock instance that has already been loaded,
allowing users to execute, edit, or manage agents from the existing configuration.

**Functions**:
- `start_loaded_flock_cli(flock, server_name, show_results, edit_mode)`
- `_start_web_server(flock, create_ui)`

**Content**:
```py
"""CLI interface for working with a loaded Flock instance.

This module provides a CLI interface for a Flock instance that has already been loaded,
allowing users to execute, edit, or manage agents from the existing configuration.
"""

import questionary
from rich.console import Console
from rich.panel import Panel

from flock.cli.constants import (
    CLI_REGISTRY_MANAGEMENT,
    CLI_SETTINGS,
)
from flock.core.flock import Flock
from flock.core.logging.logging import get_logger
from flock.core.util.cli_helper import init_console

# Import future modules we'll create
# These will be implemented later
try:
    from flock.cli.yaml_editor import yaml_editor

    yaml_editor_available = True
except ImportError:
    yaml_editor_available = False

try:
    from flock.cli.manage_agents import manage_agents

    manage_agents_available = True
except ImportError:
    manage_agents_available = False

try:
    from flock.cli.execute_flock import execute_flock, execute_flock_batch

    execute_flock_available = True
except ImportError:
    execute_flock_available = False

try:
    from flock.cli.view_results import view_results

    view_results_available = True
except ImportError:
    view_results_available = False

# Create console instance
console = Console()
logger = get_logger("cli.loaded_flock")


def start_loaded_flock_cli(
    flock: Flock,
    server_name: str = "Flock CLI",
    show_results: bool = False,
    edit_mode: bool = False,
) -> None:
    """Start a CLI interface with a loaded Flock instance.

    Args:
        flock: The loaded Flock instance
        server_name: Optional name for the CLI interface
        show_results: Whether to initially show results of previous runs
        edit_mode: Whether to open directly in edit mode
    """
    if not flock:
        console.print("[bold red]Error: No Flock instance provided.[/]")
        return

    agent_names = list(flock._agents.keys())

    # Directly go to specific modes if requested
    if edit_mode and yaml_editor_available:
        yaml_editor(flock)
        return

    if show_results and view_results_available:
        view_results(flock)
        return

    # Main CLI loop
    while True:
        # Initialize console for each loop iteration
        init_console()

        # Display header with Flock information
        console.print(Panel(f"[bold green]{server_name}[/]"), justify="center")
        console.print(
            f"Flock loaded with [bold cyan]{len(agent_names)}[/] agents: {', '.join(agent_names)}"
        )
        console.line()

        # Main menu choices
        choices = [
            questionary.Separator(line=" "),
            "Execute Flock",
            "Execute Flock - Batch Mode",
            "Start Web Server",
            "Start Web Server with UI",
            "Manage Agents",
            "View Results of Past Runs",
        ]

        # Add YAML Editor option if available
        if yaml_editor_available:
            choices.append("Edit YAML Configurations")

        # Add remaining options
        choices.extend([questionary.Separator(), CLI_REGISTRY_MANAGEMENT])
        choices.extend(
            [
                questionary.Separator(),
                CLI_SETTINGS,
                questionary.Separator(),
                "Exit",
            ]
        )

        # Display menu and get choice
        choice = questionary.select(
            "What would you like to do?",
            choices=choices,
        ).ask()

        # Handle menu selection
        if choice == "Execute Flock":
            if execute_flock_available:
                execute_flock(flock)
            else:
                console.print(
                    "[yellow]Execute Flock functionality not yet implemented.[/]"
                )
                input("\nPress Enter to continue...")

        elif choice == "Execute Flock - Batch Mode":
            if execute_flock_available:
                execute_flock_batch(flock)
            else:
                console.print(
                    "[yellow]Batch execution functionality not yet implemented.[/]"
                )
                input("\nPress Enter to continue...")

        elif choice == "Start Web Server":
            _start_web_server(flock, create_ui=False)

        elif choice == "Start Web Server with UI":
            _start_web_server(flock, create_ui=True)

        elif choice == "Manage Agents":
            if manage_agents_available:
                manage_agents(flock)
            else:
                console.print(
                    "[yellow]Manage Agents functionality not yet implemented.[/]"
                )
                input("\nPress Enter to continue...")

        elif choice == "View Results of Past Runs":
            if view_results_available:
                view_results(flock)
            else:
                console.print(
                    "[yellow]View Results functionality not yet implemented.[/]"
                )
                input("\nPress Enter to continue...")

        elif choice == CLI_REGISTRY_MANAGEMENT:
            from flock.cli.registry_management import manage_registry

            manage_registry()

        elif choice == "Edit YAML Configurations" and yaml_editor_available:
            yaml_editor(flock)

        elif choice == CLI_SETTINGS:
            from flock.cli.settings import settings_editor

            settings_editor()

        elif choice == "Exit":
            break

        # Pause after each action unless we're exiting
        if choice != "Exit" and not choice.startswith("Start Web Server"):
            input("\nPress Enter to continue...")


def _start_web_server(flock: Flock, create_ui: bool = False) -> None:
    """Start a web server with the loaded Flock instance."""
    host = "127.0.0.1"
    port = 8344
    # server_name = flock.name + " API" # Use flock name by default for server_name

    console.print("\n[bold]Web Server Configuration[/]")

    host_input = questionary.text(
        "Host (default: 127.0.0.1):", default=host
    ).ask()
    if host_input:
        host = host_input

    port_input = questionary.text(
        "Port (default: 8344):", default=str(port)
    ).ask()
    if port_input and port_input.isdigit():
        port = int(port_input)

    server_name_input = questionary.text(
        "Server name (default: FlockName API):", default=f"{flock.name or 'Flock'} API"
    ).ask()
    # if server_name_input: # server_name will be set by flock.start_api if not passed, or use its default
    # server_name = server_name_input

    ui_theme_to_pass = None
    if create_ui:
        try:
            from flock.webapp.app.config import (
                DEFAULT_THEME_NAME,
                list_available_themes,
            )
            available_themes = list_available_themes()
            theme_choices = ["default (current environment setting)", "random"] + sorted(available_themes)

            selected_theme_choice = questionary.select(
                "Select UI theme:",
                choices=theme_choices,
                default="default (current environment setting)"
            ).ask()

            if selected_theme_choice == "random":
                ui_theme_to_pass = "random"
            elif selected_theme_choice and selected_theme_choice != "default (current environment setting)":
                ui_theme_to_pass = selected_theme_choice
            # If "default" or None, ui_theme_to_pass remains None, Flock.start_api will use its logic

        except ImportError:
            logger.warning("Could not import webapp theme configuration for CLI selection. Theme will use default.")
            ui_theme_to_pass = None # Fallback if webapp components not there

    console.print(
        f"\nStarting web server on {host}:{port} {'with UI' if create_ui else 'without UI'}{' (Theme: ' + (ui_theme_to_pass if ui_theme_to_pass else 'default') + ')' if create_ui else ''}..."
    )

    # Call the Flock instance's own start_api method directly
    flock.start_api(
        host=host,
        port=port,
        server_name=server_name_input, # Pass the chosen server name
        create_ui=create_ui,
        ui_theme=ui_theme_to_pass,
    )
```

### src\flock\core\mcp\mcp_config.py

- **Lines**: 237
- **Last modified**: 2025-05-26 13:19:27

**Description**: Base Config for MCP Clients.

**Classes**:
- `FlockMCPCachingConfigurationBase`: 1 methods
- `FlockMCPCallbackConfigurationBase`: 1 methods
- `FlockMCPConnectionConfigurationBase`: 1 methods
- `FlockMCPFeatureConfigurationBase`: 1 methods
- `FlockMCPConfigurationBase`: 1 methods

**Content**:
```py
"""Base Config for MCP Clients."""

from typing import Literal, TypeVar

from pydantic import BaseModel, ConfigDict, Field, create_model

from flock.core.mcp.types.types import (
    FlockListRootsMCPCallback,
    FlockLoggingMCPCallback,
    FlockMessageHandlerMCPCallback,
    FlockSamplingMCPCallback,
    MCPRoot,
    ServerParameters,
)

LoggingLevel = Literal[
    "debug",
    "info",
    "notice",
    "warning",
    "error",
    "critical",
    "alert",
    "emergency",
]


A = TypeVar("A", bound="FlockMCPCallbackConfigurationBase")
B = TypeVar("B", bound="FlockMCPConnectionConfigurationBase")
C = TypeVar("C", bound="FlockMCPConfigurationBase")
D = TypeVar("D", bound="FlockMCPCachingConfigurationBase")
E = TypeVar("E", bound="FlockMCPFeatureConfigurationBase")


class FlockMCPCachingConfigurationBase(BaseModel):
    """Configuration for Caching in Clients."""

    tool_cache_max_size: float = Field(
        default=100, description="Maximum number of items in the Tool Cache."
    )

    tool_cache_max_ttl: float = Field(
        default=60,
        description="Max TTL for items in the tool cache in seconds.",
    )

    resource_contents_cache_max_size: float = Field(
        default=10,
        description="Maximum number of entries in the Resource Contents cache.",
    )

    resource_contents_cache_max_ttl: float = Field(
        default=60 * 5,
        description="Maximum number of items in the Resource Contents cache.",
    )

    resource_list_cache_max_size: float = Field(
        default=10,
        description="Maximum number of entries in the Resource List Cache.",
    )

    resource_list_cache_max_ttl: float = Field(
        default=100,
        description="Maximum TTL for entries in the Resource List Cache.",
    )

    tool_result_cache_max_size: float = Field(
        default=1000,
        description="Maximum number of entries in the Tool Result Cache.",
    )

    tool_result_cache_max_ttl: float = Field(
        default=20,
        description="Maximum TTL in seconds for entries in the Tool Result Cache.",
    )

    model_config = ConfigDict(
        arbitrary_types_allowed=True,
        extra="allow",
    )

    @classmethod
    def with_fields(cls: type[D], **field_definitions) -> type[D]:
        """Create a new config class with additional fields."""
        return create_model(
            f"Dynamic{cls.__name__}", __base__=cls, **field_definitions
        )


class FlockMCPCallbackConfigurationBase(BaseModel):
    """Base Configuration Class for Callbacks for Clients."""

    sampling_callback: FlockSamplingMCPCallback | None = Field(
        default=None,
        description="Callback for handling sampling requests from an external server.",
    )

    list_roots_callback: FlockListRootsMCPCallback | None = Field(
        default=None, description="Callback for handling list roots requests."
    )

    logging_callback: FlockLoggingMCPCallback | None = Field(
        default=None,
        description="Callback for handling logging messages from an external server.",
    )

    message_handler: FlockMessageHandlerMCPCallback | None = Field(
        default=None,
        description="Callback for handling messages not covered by other callbacks.",
    )

    model_config = ConfigDict(arbitrary_types_allowed=True, extra="allow")

    @classmethod
    def with_fields(cls: type[A], **field_definitions) -> type[A]:
        """Create a new config class with additional fields."""
        return create_model(
            f"Dynamic{cls.__name__}", __base__=cls, **field_definitions
        )


class FlockMCPConnectionConfigurationBase(BaseModel):
    """Base Configuration Class for Connection Parameters for a client."""

    max_retries: int = Field(
        default=3,
        description="How many times to attempt to establish the connection before giving up.",
    )

    connection_parameters: ServerParameters = Field(
        ..., description="Connection parameters for the server."
    )

    transport_type: Literal["stdio", "websockets", "sse", "custom"] = Field(
        ..., description="Type of transport to use."
    )

    mount_points: list[MCPRoot] | None = Field(
        default=None, description="Initial Mountpoints to operate under."
    )

    read_timeout_seconds: float | int = Field(
        default=60 * 5, description="Read Timeout."
    )

    server_logging_level: LoggingLevel = Field(
        default="error",
        description="The logging level for logging events from the remote server.",
    )

    model_config = ConfigDict(arbitrary_types_allowed=True, extra="allow")

    @classmethod
    def with_fields(cls: type[B], **field_definitions) -> type[B]:
        """Create a new config class with additional fields."""
        return create_model(
            f"Dynamic{cls.__name__}", __base__=cls, **field_definitions
        )


class FlockMCPFeatureConfigurationBase(BaseModel):
    """Base Configuration Class for switching MCP Features on and off."""

    roots_enabled: bool = Field(
        default=False,
        description="Whether or not the Roots feature is enabled for this client.",
    )

    sampling_enabled: bool = Field(
        default=False,
        description="Whether or not the Sampling feature is enabled for this client.",
    )

    tools_enabled: bool = Field(
        default=False,
        description="Whether or not the Tools feature is enabled for this client.",
    )

    prompts_enabled: bool = Field(
        default=False,
        description="Whether or not the Prompts feature is enabled for this client.",
    )

    model_config = ConfigDict(
        arbitrary_types_allowed=True,
        extra="allow",
    )

    @classmethod
    def with_fields(cls: type[E], **field_definitions) -> type[E]:
        """Create a new config class with additional fields."""
        return create_model(
            f"Dynamic{cls.__name__}", __base__=cls, **field_definitions
        )


class FlockMCPConfigurationBase(BaseModel):
    """Base Configuration Class for MCP Clients.

    Each Client should implement their own config
    model by inheriting from this class.
    """

    name: str = Field(
        ..., description="Name of the server the client connects to."
    )

    connection_config: FlockMCPConnectionConfigurationBase = Field(
        ..., description="MCP Connection Configuration for a client."
    )

    caching_config: FlockMCPCachingConfigurationBase = Field(
        default_factory=FlockMCPCachingConfigurationBase,
        description="Configuration for the internal caches of the client.",
    )

    callback_config: FlockMCPCallbackConfigurationBase = Field(
        default_factory=FlockMCPCallbackConfigurationBase,
        description="Callback configuration for the client.",
    )

    feature_config: FlockMCPFeatureConfigurationBase = Field(
        default_factory=FlockMCPFeatureConfigurationBase,
        description="Feature configuration for the client.",
    )

    model_config = ConfigDict(
        arbitrary_types_allowed=True,
        extra="allow",
    )

    @classmethod
    def with_fields(cls: type[C], **field_definitions) -> type[C]:
        """Create a new config class with additional fields."""
        return create_model(
            f"Dynamic{cls.__name__}", __base__=cls, **field_definitions
        )
```

### src\flock\webapp\app\main.py

- **Lines**: 986
- **Last modified**: 2025-05-26 17:33:51

**Classes**:
- `CreateShareLinkRequest`: 0 methods
- `CreateShareLinkResponse`: 0 methods

**Functions**:
- `load_env_file_web()`
- `save_env_file_web(env_vars)`
- `is_sensitive_web(key)`
- `mask_sensitive_value_web(value)`
- `get_show_secrets_setting_web(env_vars)`
- `set_show_secrets_setting_web(show)`
- `markdown_filter(text)`
- `generate_theme_css_web(theme_name)`
- `get_base_context_web(request, error, success, ui_mode)`
- `_prepare_env_vars_for_template_web()`

**Content**:
```py
# src/flock/webapp/app/main.py
import json
import os  # Added import
import shutil
import urllib.parse

# Added for share link creation
import uuid
from contextlib import asynccontextmanager
from pathlib import Path
from typing import Any

import markdown2  # Import markdown2
from fastapi import (
    Depends,
    FastAPI,
    File,
    Form,
    HTTPException,
    Query,
    Request,
    UploadFile,
)
from fastapi.responses import HTMLResponse, RedirectResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from pydantic import BaseModel

from flock.core.api.endpoints import create_api_router
from flock.core.api.run_store import RunStore

# Import core Flock components and API related modules
from flock.core.flock import Flock  # For type hinting
from flock.core.logging.logging import get_logger  # For logging
from flock.core.util.spliter import parse_schema

# Import UI-specific routers
from flock.webapp.app.api import (
    agent_management,
    execution,
    flock_management,
    registry_viewer,
)
from flock.webapp.app.config import (
    DEFAULT_THEME_NAME,
    FLOCK_FILES_DIR,
    THEMES_DIR,
    get_current_theme_name,
)

# Import dependency management and config
from flock.webapp.app.dependencies import (
    get_pending_custom_endpoints_and_clear,
    get_shared_link_store,
    set_global_flock_services,
    set_global_shared_link_store,
)

# Import service functions (which now expect app_state)
from flock.webapp.app.services.flock_service import (
    clear_current_flock_service,
    create_new_flock_service,
    get_available_flock_files,
    get_flock_preview_service,
    load_flock_from_file_service,
    # Note: get_current_flock_instance/filename are removed from service,
    # as main.py will use request.app.state for this.
)

# Added for share link creation
from flock.webapp.app.services.sharing_models import SharedLinkConfig
from flock.webapp.app.services.sharing_store import (
    SharedLinkStoreInterface,
    create_shared_link_store,
)
from flock.webapp.app.theme_mapper import alacritty_to_pico

logger = get_logger("webapp.main")


try:
    from flock.core.logging.formatters.themed_formatter import (
        load_theme_from_file,
    )
    THEME_LOADER_AVAILABLE = True
except ImportError:
    logger.warning("Could not import flock.core theme loading utilities.")
    THEME_LOADER_AVAILABLE = False

# --- .env helpers (copied from original main.py for self-containment) ---
ENV_FILE_PATH = Path(".env") #Path(os.getenv("FLOCK_WEB_ENV_FILE", Path.home() / ".flock" / ".env"))
#ENV_FILE_PATH.parent.mkdir(parents=True, exist_ok=True)
SHOW_SECRETS_KEY = "SHOW_SECRETS"

def load_env_file_web() -> dict[str, str]:
    env_vars: dict[str, str] = {}
    if not ENV_FILE_PATH.exists(): return env_vars
    with open(ENV_FILE_PATH) as f: lines = f.readlines()
    for line in lines:
        line = line.strip()
        if not line: env_vars[""] = ""; continue
        if line.startswith("#"): env_vars[line] = ""; continue
        if "=" in line: k, v = line.split("=", 1); env_vars[k] = v
        else: env_vars[line] = ""
    return env_vars

def save_env_file_web(env_vars: dict[str, str]):
    try:
        with open(ENV_FILE_PATH, "w") as f:
            for k, v in env_vars.items():
                if k.startswith("#"): f.write(f"{k}\n")
                elif not k: f.write("\n")
                else: f.write(f"{k}={v}\n")
    except Exception as e: logger.error(f"[Settings] Failed to save .env: {e}")

def is_sensitive_web(key: str) -> bool:
    patterns = ["key", "token", "secret", "password", "api", "pat"]; low = key.lower()
    return any(p in low for p in patterns)

def mask_sensitive_value_web(value: str) -> str:
    if not value: return value
    if len(value) <= 4: return "••••"
    return value[:2] + "•" * (len(value) - 4) + value[-2:]

def get_show_secrets_setting_web(env_vars: dict[str, str]) -> bool:
    return env_vars.get(SHOW_SECRETS_KEY, "false").lower() == "true"

def set_show_secrets_setting_web(show: bool):
    env_vars = load_env_file_web()
    env_vars[SHOW_SECRETS_KEY] = str(show)
    save_env_file_web(env_vars)
# --- End .env helpers ---


@asynccontextmanager
async def lifespan(app: FastAPI):
    logger.info("FastAPI application starting up...")
    # Flock instance and RunStore are expected to be set on app.state
    # by `start_unified_server` in `webapp/run.py` *before* uvicorn starts the app.
    # The call to `set_global_flock_services` also happens there.    # Initialize and set the SharedLinkStore
    try:
        logger.info("Initializing SharedLinkStore using factory...")
        shared_link_store = create_shared_link_store()
        await shared_link_store.initialize() # Create tables if they don't exist
        set_global_shared_link_store(shared_link_store)
        logger.info("SharedLinkStore initialized and set globally.")
    except Exception as e:
        logger.error(f"Failed to initialize SharedLinkStore: {e}", exc_info=True)# Configure chat features with clear precedence:
    # 1. Value set by start_unified_server (programmatic)
    # 2. Environment variables (standalone mode)
    programmatic_chat_enabled = getattr(app.state, "chat_enabled", None)
    env_start_mode = os.environ.get("FLOCK_START_MODE")
    env_chat_enabled = os.environ.get("FLOCK_CHAT_ENABLED", "false").lower() == "true"

    if programmatic_chat_enabled is not None:
        # Programmatic setting takes precedence (from start_unified_server)
        should_enable_chat_routes = programmatic_chat_enabled
        logger.info(f"Using programmatic chat_enabled setting: {should_enable_chat_routes}")
    elif env_start_mode == "chat":
        should_enable_chat_routes = True
        app.state.initial_redirect_to_chat = True
        app.state.chat_enabled = True
        logger.info("FLOCK_START_MODE='chat'. Enabling chat routes and setting redirect.")
    elif env_chat_enabled:
        should_enable_chat_routes = True
        app.state.chat_enabled = True
        logger.info("FLOCK_CHAT_ENABLED='true'. Enabling chat routes.")
    else:
        should_enable_chat_routes = False
        app.state.chat_enabled = False
        logger.info("Chat routes disabled (no programmatic or environment setting).")

    if should_enable_chat_routes:
        try:
            from flock.webapp.app.chat import router as chat_router
            app.include_router(chat_router, tags=["Chat"])
            logger.info("Chat routes included in the application.")
        except Exception as e:
            logger.error(f"Failed to include chat routes during lifespan startup: {e}", exc_info=True)    # If in standalone chat mode, strip non-essential UI routes
    if env_start_mode == "chat":
        from fastapi.routing import APIRoute
        logger.info("FLOCK_START_MODE='chat'. Stripping non-chat UI routes.")

        # Define tags for routes to KEEP.
        # "Chat" for primary chat functionality.
        # "Chat Sharing" for shared chat links & pages.
        # API tags might be needed if chat agents make internal API calls or for general health/docs.
        # Public static files (/static/...) are typically handled by app.mount and not in app.router.routes directly this way.
        allowed_tags_for_chat_mode = {
            "Chat",
            "Chat Sharing",
            "Flock API Core", # Keep core API for potential underlying needs
            "Flock API Custom Endpoints" # Keep custom API endpoints
        }

        def _route_is_allowed_in_chat_mode(route: APIRoute) -> bool:
            # Keep documentation (e.g. /docs, /openapi.json - usually no tags or specific tags)
            # and non-API utility routes (often no tags).
            if not hasattr(route, "tags") or not route.tags:
                # Check common doc paths explicitly as they might not have tags or might have default tags
                if route.path in ["/docs", "/openapi.json", "/redoc"]:
                    return True
                # Allow other untagged routes for now, assuming they are essential (e.g. static mounts if they appeared here)
                # This might need refinement if untagged UI routes exist.
                return True
            return any(tag in allowed_tags_for_chat_mode for tag in route.tags)

        original_route_count = len(app.router.routes)
        app.router.routes = [r for r in app.router.routes if _route_is_allowed_in_chat_mode(r)]
        num_removed = original_route_count - len(app.router.routes)
        logger.info(f"Stripped {num_removed} routes for chat-only mode. {len(app.router.routes)} routes remaining.")

        if num_removed > 0 and hasattr(app, "openapi_schema"):
            app.openapi_schema = None # Clear cached OpenAPI schema to regenerate
            logger.info("Cleared OpenAPI schema cache due to route removal.")

    # Add custom routes if any were passed during server startup
    # These are retrieved from the dependency module where `start_unified_server` stored them.
    pending_endpoints = get_pending_custom_endpoints_and_clear()
    if pending_endpoints:
        flock_instance_from_state: Flock | None = getattr(app.state, "flock_instance", None)
        if flock_instance_from_state:
            from flock.core.api.main import (
                FlockAPI,  # Local import for this specific task
            )
            # Create a temporary FlockAPI service object just for adding routes
            temp_flock_api_service = FlockAPI(
                flock_instance_from_state,
                custom_endpoints=pending_endpoints
            )
            temp_flock_api_service.add_custom_routes_to_app(app)
            logger.info(f"Lifespan: Added {len(pending_endpoints)} custom API routes to main app.")
        else:
            logger.warning("Lifespan: Pending custom endpoints found, but no Flock instance in app.state. Cannot add custom routes.")
    yield
    logger.info("FastAPI application shutting down...")

app = FastAPI(title="Flock Web UI & API", lifespan=lifespan, docs_url="/docs",
    openapi_url="/openapi.json", root_path=os.getenv("FLOCK_ROOT_PATH", ""))
logger.info("FastAPI booting complete.")
BASE_DIR = Path(__file__).resolve().parent.parent
app.mount("/static", StaticFiles(directory=str(BASE_DIR / "static")), name="static")
templates = Jinja2Templates(directory=str(BASE_DIR / "templates"))

# Add markdown2 filter to Jinja2 environment
def markdown_filter(text):
    return markdown2.markdown(text, extras=["tables", "fenced-code-blocks"])

templates.env.filters['markdown'] = markdown_filter

core_api_router = create_api_router()
app.include_router(core_api_router, prefix="/api", tags=["Flock API Core"])
app.include_router(flock_management.router, prefix="/ui/api/flock", tags=["UI Flock Management"])
app.include_router(agent_management.router, prefix="/ui/api/flock", tags=["UI Agent Management"])
app.include_router(execution.router, prefix="/ui/api/flock", tags=["UI Execution"])
app.include_router(registry_viewer.router, prefix="/ui/api/registry", tags=["UI Registry"])

# --- Share Link API Models and Endpoint ---
class CreateShareLinkRequest(BaseModel):
    agent_name: str

class CreateShareLinkResponse(BaseModel):
    share_url: str

@app.post("/api/v1/share/link", response_model=CreateShareLinkResponse, tags=["UI Sharing"])
async def create_share_link(
    request: Request,
    request_data: CreateShareLinkRequest,
    store: SharedLinkStoreInterface = Depends(get_shared_link_store)
):
    """Creates a new shareable link for an agent."""
    share_id = uuid.uuid4().hex
    agent_name = request_data.agent_name

    if not agent_name: # Basic validation
        raise HTTPException(status_code=400, detail="Agent name cannot be empty.")

    current_flock_instance: Flock | None = getattr(request.app.state, "flock_instance", None)
    current_flock_filename: str | None = getattr(request.app.state, "flock_filename", None)

    if not current_flock_instance or not current_flock_filename:
        logger.error("Cannot create share link: No Flock is currently loaded in the application state.")
        raise HTTPException(status_code=400, detail="No Flock loaded. Cannot create share link.")

    if agent_name not in current_flock_instance.agents:
        logger.error(f"Agent '{agent_name}' not found in currently loaded Flock '{current_flock_instance.name}'.")
        raise HTTPException(status_code=404, detail=f"Agent '{agent_name}' not found in current Flock.")

    try:
        flock_file_path = FLOCK_FILES_DIR / current_flock_filename
        if not flock_file_path.is_file():
            logger.warning(f"Flock file {current_flock_filename} not found at {flock_file_path} for sharing. Using in-memory definition.")
            flock_definition_str = current_flock_instance.to_yaml()
        else:
            flock_definition_str = flock_file_path.read_text()
    except Exception as e:
        logger.error(f"Failed to get flock definition for sharing: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Could not retrieve Flock definition for sharing.")

    config = SharedLinkConfig(
        share_id=share_id,
        agent_name=agent_name,
        flock_definition=flock_definition_str
    )
    try:
        await store.save_config(config)
        share_url = f"/ui/shared-run/{share_id}" # Relative URL for client-side navigation
        logger.info(f"Created share link for agent '{agent_name}' in Flock '{current_flock_instance.name}' with ID '{share_id}'. URL: {share_url}")
        return CreateShareLinkResponse(share_url=share_url)
    except Exception as e:
        logger.error(f"Failed to create share link for agent '{agent_name}': {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Failed to create share link: {e!s}")

# --- End Share Link API ---

# --- HTMX Endpoint for Generating Share Link Snippet ---
@app.post("/ui/htmx/share/generate-link", response_class=HTMLResponse, tags=["UI Sharing HTMX"])
async def htmx_generate_share_link(
    request: Request,
    start_agent_name: str | None = Form(None),
    store: SharedLinkStoreInterface = Depends(get_shared_link_store)
):
    if not start_agent_name:
        logger.warning("HTMX generate share link: Agent name not provided.")
        return templates.TemplateResponse(
            "partials/_share_link_snippet.html",
            {"request": request, "error_message": "No agent selected to share."}
        )

    current_flock_instance: Flock | None = getattr(request.app.state, "flock_instance", None)
    current_flock_filename: str | None = getattr(request.app.state, "flock_filename", None)

    if not current_flock_instance or not current_flock_filename:
        logger.error("HTMX: Cannot create share link: No Flock is currently loaded.")
        return templates.TemplateResponse(
            "partials/_share_link_snippet.html",
            {"request": request, "error_message": "No Flock loaded. Cannot create share link."}
        )

    if start_agent_name not in current_flock_instance.agents:
        logger.error(f"HTMX: Agent '{start_agent_name}' not found in Flock '{current_flock_instance.name}'.")
        return templates.TemplateResponse(
            "partials/_share_link_snippet.html",
            {"request": request, "error_message": f"Agent '{start_agent_name}' not found in current Flock."}
        )

    try:
        flock_file_path = FLOCK_FILES_DIR / current_flock_filename
        if not flock_file_path.is_file():
            logger.warning(f"HTMX: Flock file {current_flock_filename} not found at {flock_file_path} for sharing. Using in-memory definition.")
            flock_definition_str = current_flock_instance.to_yaml()
        else:
            flock_definition_str = flock_file_path.read_text()
    except Exception as e:
        logger.error(f"HTMX: Failed to get flock definition for sharing: {e}", exc_info=True)
        return templates.TemplateResponse(
            "partials/_share_link_snippet.html",
            {"request": request, "error_message": "Could not retrieve Flock definition for sharing."}
        )

    share_id = uuid.uuid4().hex
    config = SharedLinkConfig(
        share_id=share_id,
        agent_name=start_agent_name,
        flock_definition=flock_definition_str
    )

    try:
        await store.save_config(config)
        base_url = str(request.base_url)
        full_share_url = f"{base_url.rstrip('/')}/ui/shared-run/{share_id}"

        logger.info(f"HTMX: Generated share link for agent '{start_agent_name}' in Flock '{current_flock_instance.name}' with ID '{share_id}'. URL: {full_share_url}")
        return templates.TemplateResponse(
            "partials/_share_link_snippet.html",
            {"request": request, "share_url": full_share_url, "flock_name": current_flock_instance.name, "agent_name": start_agent_name}
        )
    except Exception as e:
        logger.error(f"HTMX: Failed to create share link for agent '{start_agent_name}': {e}", exc_info=True)
        return templates.TemplateResponse(
            "partials/_share_link_snippet.html",
            {"request": request, "error_message": f"Could not generate link: {e!s}"}
        )
# --- End HTMX Endpoint ---

# --- HTMX Endpoint for Generating SHARED CHAT Link Snippet ---
@app.post("/ui/htmx/share/chat/generate-link", response_class=HTMLResponse, tags=["UI Sharing HTMX"])
async def htmx_generate_share_chat_link(
    request: Request,
    agent_name: str | None = Form(None), # This is the chat agent
    message_key: str | None = Form(None), # Changed default to None
    history_key: str | None = Form(None), # Changed default to None
    response_key: str | None = Form(None), # Changed default to None
    store: SharedLinkStoreInterface = Depends(get_shared_link_store)
):
    if not agent_name:
        logger.warning("HTMX generate share chat link: Agent name not provided.")
        return templates.TemplateResponse(
            "partials/_share_chat_link_snippet.html", # Will create this template
            {"request": request, "error_message": "No agent selected for chat sharing."}
        )

    current_flock_instance: Flock | None = getattr(request.app.state, "flock_instance", None)
    current_flock_filename: str | None = getattr(request.app.state, "flock_filename", None)

    if not current_flock_instance or not current_flock_filename:
        logger.error("HTMX Chat Share: Cannot create share link: No Flock is currently loaded.")
        return templates.TemplateResponse(
            "partials/_share_chat_link_snippet.html",
            {"request": request, "error_message": "No Flock loaded. Cannot create share link."}
        )

    if agent_name not in current_flock_instance.agents:
        logger.error(f"HTMX Chat Share: Agent '{agent_name}' not found in Flock '{current_flock_instance.name}'.")
        return templates.TemplateResponse(
            "partials/_share_chat_link_snippet.html",
            {"request": request, "error_message": f"Agent '{agent_name}' not found in current Flock."}
        )

    try:
        flock_file_path = FLOCK_FILES_DIR / current_flock_filename
        if not flock_file_path.is_file():
            logger.warning(f"HTMX Chat Share: Flock file {current_flock_filename} not found at {flock_file_path} for sharing. Using in-memory definition.")
            flock_definition_str = current_flock_instance.to_yaml()
        else:
            flock_definition_str = flock_file_path.read_text()
    except Exception as e:
        logger.error(f"HTMX Chat Share: Failed to get flock definition for sharing: {e}", exc_info=True)
        return templates.TemplateResponse(
            "partials/_share_chat_link_snippet.html",
            {"request": request, "error_message": "Could not retrieve Flock definition for sharing."}
        )

    share_id = uuid.uuid4().hex

    # Explicitly convert empty strings from form to None for optional keys
    actual_message_key = message_key if message_key else None
    actual_history_key = history_key if history_key else None
    actual_response_key = response_key if response_key else None

    config = SharedLinkConfig(
        share_id=share_id,
        agent_name=agent_name, # agent_name from form is the chat agent
        flock_definition=flock_definition_str,
        share_type="chat",
        chat_message_key=actual_message_key,
        chat_history_key=actual_history_key,
        chat_response_key=actual_response_key
    )

    try:
        await store.save_config(config)
        base_url = str(request.base_url)
        # Link to the new /chat/shared/{share_id} endpoint
        full_share_url = f"{base_url.rstrip('/')}/chat/shared/{share_id}"

        logger.info(f"HTMX: Generated share CHAT link for agent '{agent_name}' in Flock '{current_flock_instance.name}' with ID '{share_id}'. URL: {full_share_url}")
        return templates.TemplateResponse(
            "partials/_share_chat_link_snippet.html", # Will create this template
            {"request": request, "share_url": full_share_url, "flock_name": current_flock_instance.name, "agent_name": agent_name}
        )
    except Exception as e:
        logger.error(f"HTMX Chat Share: Failed to create share link for agent '{agent_name}': {e}", exc_info=True)
        return templates.TemplateResponse(
            "partials/_share_chat_link_snippet.html",
            {"request": request, "error_message": f"Could not generate chat link: {e!s}"}
        )

# --- Route for Shared Run Page ---
@app.get("/ui/shared-run/{share_id}", response_class=HTMLResponse, tags=["UI Sharing"])
async def page_shared_run(
    request: Request,
    share_id: str,
    store: SharedLinkStoreInterface = Depends(get_shared_link_store),
):
    logger.info(f"Accessed shared run page with share_id: {share_id}")
    shared_config = await store.get_config(share_id)

    if not shared_config:
        logger.warning(f"Share ID {share_id} not found.")
        return templates.TemplateResponse(
            "error_page.html",
            {"request": request, "error_title": "Link Not Found", "error_message": "The shared link does not exist or may have expired."},
            status_code=404
        )

    agent_name_from_link = shared_config.agent_name
    flock_definition_str = shared_config.flock_definition
    context: dict[str, Any] = {"request": request, "is_shared_run_page": True, "share_id": share_id}

    try:
        from flock.core.flock import Flock as ConcreteFlock
        loaded_flock = ConcreteFlock.from_yaml(flock_definition_str)

        # Store the loaded_flock instance in app.state for later retrieval
        if not hasattr(request.app.state, 'shared_flocks'):
            request.app.state.shared_flocks = {}
        request.app.state.shared_flocks[share_id] = loaded_flock
        logger.info(f"Shared Run Page: Stored Flock instance for share_id {share_id} in app.state.")

        context["flock"] = loaded_flock
        context["selected_agent_name"] = agent_name_from_link # For pre-selection & hidden field
        # flock_definition_str is no longer needed in the template for a hidden field if we reuse the instance
        # context["flock_definition_str"] = flock_definition_str
        logger.info(f"Shared Run Page: Loaded Flock '{loaded_flock.name}' for agent '{agent_name_from_link}'.")

        if agent_name_from_link not in loaded_flock.agents:
            context["error_message"] = f"Agent '{agent_name_from_link}' not found in the shared Flock definition."
            logger.warning(context["error_message"])
        else:
            agent = loaded_flock.agents[agent_name_from_link]
            input_fields = []
            if agent.input and isinstance(agent.input, str):
                try:
                    parsed_spec = parse_schema(agent.input) # parse_schema is imported at top of main.py
                    for name, type_str, description in parsed_spec:
                        field_info = {"name": name, "type": type_str.lower(), "description": description or ""}
                        if "bool" in field_info["type"]: field_info["html_type"] = "checkbox"
                        elif "int" in field_info["type"] or "float" in field_info["type"]: field_info["html_type"] = "number"
                        elif "list" in field_info["type"] or "dict" in field_info["type"]:
                            field_info["html_type"] = "textarea"; field_info["placeholder"] = f"Enter JSON for {field_info['type']}"
                        else: field_info["html_type"] = "text"
                        input_fields.append(field_info)
                    context["input_fields"] = input_fields
                except Exception as e_parse:
                    logger.error(f"Shared Run Page: Error parsing input for '{agent_name_from_link}': {e_parse}", exc_info=True)
                    context["error_message"] = f"Could not parse inputs for agent '{agent_name_from_link}'."
            else:
                context["input_fields"] = [] # Agent has no inputs defined

    except Exception as e_load:
        logger.error(f"Shared Run Page: Failed to load Flock from definition for share_id {share_id}: {e_load}", exc_info=True)
        context["error_message"] = f"Fatal: Could not load the shared Flock configuration: {e_load!s}"
        context["flock"] = None
        context["selected_agent_name"] = agent_name_from_link # Still pass for potential error display
        context["input_fields"] = []
        # context["flock_definition_str"] = flock_definition_str # Not needed if not sent to template

    try:
        current_theme_name = get_current_theme_name()
        context["theme_css"] = generate_theme_css_web(current_theme_name)
        context["active_theme_name"] = current_theme_name or DEFAULT_THEME_NAME
    except Exception as e_theme:
        logger.error(f"Shared Run Page: Error generating theme: {e_theme}", exc_info=True)
        context["theme_css"] = ""
        context["active_theme_name"] = DEFAULT_THEME_NAME

    # The shared_run_page.html will now be a simple wrapper that includes _execution_form.html
    return templates.TemplateResponse("shared_run_page.html", context)

# --- End Route for Shared Run Page ---

def generate_theme_css_web(theme_name: str | None) -> str:
    if not THEME_LOADER_AVAILABLE or THEMES_DIR is None: return ""

    chosen_theme_name_input = theme_name or get_current_theme_name() or DEFAULT_THEME_NAME

    # Sanitize the input to get only the filename component
    sanitized_name_part = Path(chosen_theme_name_input).name
    # Ensure we have a stem
    theme_stem_candidate = sanitized_name_part
    if theme_stem_candidate.endswith(".toml"):
        theme_stem_candidate = theme_stem_candidate[:-5]

    effective_theme_filename = f"{theme_stem_candidate}.toml"
    _theme_to_load_stem = theme_stem_candidate # This will be the name of the theme we attempt to load

    try:
        resolved_themes_dir = THEMES_DIR.resolve(strict=True) # Ensure THEMES_DIR itself is valid
        prospective_theme_path = resolved_themes_dir / effective_theme_filename

        # Resolve the prospective path
        resolved_theme_path = prospective_theme_path.resolve()

        # Validate:
        # 1. Path is still within the resolved THEMES_DIR
        # 2. The final filename component of the resolved path matches the intended filename
        #    (guards against symlinks or normalization changing the name unexpectedly)
        # 3. The file exists
        if (
            str(resolved_theme_path).startswith(str(resolved_themes_dir)) and
            resolved_theme_path.name == effective_theme_filename and
            resolved_theme_path.is_file() # is_file checks existence too
        ):
            theme_path = resolved_theme_path
        else:
            logger.warning(
                f"Validation failed or theme '{effective_theme_filename}' not found in '{resolved_themes_dir}'. "
                f"Attempted path: '{prospective_theme_path}'. Resolved to: '{resolved_theme_path}'. "
                f"Falling back to default theme: {DEFAULT_THEME_NAME}.toml"
            )
            _theme_to_load_stem = DEFAULT_THEME_NAME
            theme_path = resolved_themes_dir / f"{DEFAULT_THEME_NAME}.toml"
            if not theme_path.is_file():
                logger.error(f"Default theme file '{theme_path}' not found. No theme CSS will be generated.")
                return ""
    except FileNotFoundError: # THEMES_DIR does not exist
        logger.error(f"Themes directory '{THEMES_DIR}' not found. Falling back to default theme.")
        _theme_to_load_stem = DEFAULT_THEME_NAME
        # Attempt to use a conceptual default path if THEMES_DIR was bogus, though it's unlikely to succeed
        theme_path = Path(f"{DEFAULT_THEME_NAME}.toml") # This won't be in THEMES_DIR if THEMES_DIR is bad
        if not theme_path.exists(): # Check existence without assuming a base directory
             logger.error(f"Default theme file '{DEFAULT_THEME_NAME}.toml' not found at root or THEMES_DIR is inaccessible. No theme CSS.")
             return ""
    except Exception as e:
        logger.error(f"Error during theme path resolution for '{effective_theme_filename}': {e}. Falling back to default.")
        _theme_to_load_stem = DEFAULT_THEME_NAME
        theme_path = THEMES_DIR / f"{DEFAULT_THEME_NAME}.toml" if THEMES_DIR else Path(f"{DEFAULT_THEME_NAME}.toml")
        if not theme_path.exists():
            logger.error(f"Default theme file '{theme_path}' not found after error. No theme CSS.")
            return ""

    try:
        theme_dict = load_theme_from_file(str(theme_path))
        logger.debug(f"Successfully loaded theme '{_theme_to_load_stem}' from '{theme_path}'")
    except Exception as e:
        logger.error(f"Error loading theme file '{theme_path}' (intended: '{_theme_to_load_stem}.toml'): {e}")
        return ""

    pico_vars = alacritty_to_pico(theme_dict)
    if not pico_vars: return ""
    css_rules = [f"    {name}: {value};" for name, value in pico_vars.items()]
    css_string = ":root {\n" + "\n".join(css_rules) + "\n}"
    return css_string

def get_base_context_web(
    request: Request, error: str = None, success: str = None, ui_mode: str = "standalone"
) -> dict:
    flock_instance_from_state: Flock | None = getattr(request.app.state, "flock_instance", None)
    current_flock_filename_from_state: str | None = getattr(request.app.state, "flock_filename", None)
    theme_name = get_current_theme_name()
    theme_css = generate_theme_css_web(theme_name)

    return {
        "request": request,
        "current_flock": flock_instance_from_state,
        "current_filename": current_flock_filename_from_state,
        "error_message": error,
        "success_message": success,
        "ui_mode": ui_mode,
        "theme_css": theme_css,
        "active_theme_name": theme_name,
        "chat_enabled": getattr(request.app.state, "chat_enabled", False), # Reverted to app.state
    }

@app.get("/", response_class=HTMLResponse, tags=["UI Pages"])
async def page_dashboard(
    request: Request, error: str = None, success: str = None, ui_mode: str = Query(None)
):
    # Handle initial redirect if flagged during app startup
    if getattr(request.app.state, "initial_redirect_to_chat", False):
        logger.info("Initial redirect to CHAT page triggered from dashboard (FLOCK_START_MODE='chat').")
        return RedirectResponse(url="/chat", status_code=307)

    effective_ui_mode = ui_mode
    flock_is_preloaded = hasattr(request.app.state, "flock_instance") and request.app.state.flock_instance is not None

    if effective_ui_mode is None:
        effective_ui_mode = "scoped" if flock_is_preloaded else "standalone"
        if effective_ui_mode == "scoped":
             return RedirectResponse(url=f"/?ui_mode=scoped&initial_load=true", status_code=307)

    if effective_ui_mode == "standalone" and flock_is_preloaded:
        clear_current_flock_service(request.app.state) # Pass app.state
        logger.info("Switched to standalone mode, cleared preloaded Flock instance from app.state.")

    context = get_base_context_web(request, error, success, effective_ui_mode)
    flock_in_state = hasattr(request.app.state, "flock_instance") and request.app.state.flock_instance is not None

    if effective_ui_mode == "scoped":
        context["initial_content_url"] = "/ui/htmx/execution-view-container" if flock_in_state else "/ui/htmx/scoped-no-flock-view"
    else:
        context["initial_content_url"] = "/ui/htmx/load-flock-view"
    return templates.TemplateResponse("base.html", context)

@app.get("/ui/editor/{section:path}", response_class=HTMLResponse, tags=["UI Pages"])
async def page_editor_section(
    request: Request, section: str, success: str = None, error: str = None, ui_mode: str = Query("standalone")
):
    flock_instance_from_state: Flock | None = getattr(request.app.state, "flock_instance", None)
    if not flock_instance_from_state:
        err_msg = "No flock loaded. Please load or create a flock first."
        redirect_url = f"/?error={urllib.parse.quote(err_msg)}"
        if ui_mode == "scoped": redirect_url += "&ui_mode=scoped"
        return RedirectResponse(url=redirect_url, status_code=303)

    context = get_base_context_web(request, error, success, ui_mode)
    content_map = {
        "properties": "/ui/api/flock/htmx/flock-properties-form",
        "agents": "/ui/htmx/agent-manager-view",
        "execute": "/ui/htmx/execution-view-container"
    }
    context["initial_content_url"] = content_map.get(section, "/ui/htmx/load-flock-view")
    if section not in content_map: context["error_message"] = "Invalid editor section."
    return templates.TemplateResponse("base.html", context)

@app.get("/ui/registry", response_class=HTMLResponse, tags=["UI Pages"])
async def page_registry(request: Request, error: str = None, success: str = None, ui_mode: str = Query("standalone")):
    context = get_base_context_web(request, error, success, ui_mode)
    context["initial_content_url"] = "/ui/htmx/registry-viewer"
    return templates.TemplateResponse("base.html", context)

@app.get("/ui/create", response_class=HTMLResponse, tags=["UI Pages"])
async def page_create(request: Request, error: str = None, success: str = None, ui_mode: str = Query("standalone")):
    clear_current_flock_service(request.app.state) # Pass app.state
    context = get_base_context_web(request, error, success, "standalone")
    context["initial_content_url"] = "/ui/htmx/create-flock-form"
    return templates.TemplateResponse("base.html", context)

@app.get("/ui/htmx/sidebar", response_class=HTMLResponse, tags=["UI HTMX Partials"])
async def htmx_get_sidebar(request: Request, ui_mode: str = Query("standalone")):
    return templates.TemplateResponse("partials/_sidebar.html", get_base_context_web(request, ui_mode=ui_mode))

@app.get("/ui/htmx/header-flock-status", response_class=HTMLResponse, tags=["UI HTMX Partials"])
async def htmx_get_header_flock_status(request: Request, ui_mode: str = Query("standalone")):
    return templates.TemplateResponse("partials/_header_flock_status.html", get_base_context_web(request, ui_mode=ui_mode))

@app.get("/ui/htmx/load-flock-view", response_class=HTMLResponse, tags=["UI HTMX Partials"])
async def htmx_get_load_flock_view(request: Request, error: str = None, success: str = None, ui_mode: str = Query("standalone")):
    return templates.TemplateResponse("partials/_load_manager_view.html", get_base_context_web(request, error, success, ui_mode))

@app.get("/ui/htmx/dashboard-flock-file-list", response_class=HTMLResponse, tags=["UI HTMX Partials"])
async def htmx_get_dashboard_flock_file_list_partial(request: Request):
    return templates.TemplateResponse("partials/_dashboard_flock_file_list.html", {"request": request, "flock_files": get_available_flock_files()})

@app.get("/ui/htmx/dashboard-default-action-pane", response_class=HTMLResponse, tags=["UI HTMX Partials"])
async def htmx_get_dashboard_default_action_pane(request: Request):
    return HTMLResponse("""<article style="text-align:center; margin-top: 2rem; border: none; background: transparent;"><p>Select a Flock from the list to view its details and load it into the editor.</p><hr><p>Or, create a new Flock or upload an existing one using the "Create New Flock" option in the sidebar.</p></article>""")

@app.get("/ui/htmx/dashboard-flock-properties-preview/{filename}", response_class=HTMLResponse, tags=["UI HTMX Partials"])
async def htmx_get_dashboard_flock_properties_preview(request: Request, filename: str):
    preview_flock_data = get_flock_preview_service(filename)
    return templates.TemplateResponse("partials/_dashboard_flock_properties_preview.html", {"request": request, "selected_filename": filename, "preview_flock": preview_flock_data})

@app.get("/ui/htmx/create-flock-form", response_class=HTMLResponse, tags=["UI HTMX Partials"])
async def htmx_get_create_flock_form(request: Request, error: str = None, success: str = None, ui_mode: str = Query("standalone")):
    return templates.TemplateResponse("partials/_create_flock_form.html", get_base_context_web(request, error, success, ui_mode))

@app.get("/ui/htmx/agent-manager-view", response_class=HTMLResponse, tags=["UI HTMX Partials"])
async def htmx_get_agent_manager_view(request: Request):
    context = get_base_context_web(request) # This gets flock from app.state
    if not context.get("current_flock"): # Check if flock exists in the context
        return HTMLResponse("<article class='error'><p>No flock loaded. Cannot manage agents.</p></article>")
    # Pass the 'current_flock' from the context to the template as 'flock'
    return templates.TemplateResponse(
        "partials/_agent_manager_view.html",
        {"request": request, "flock": context.get("current_flock")}
    )

@app.get("/ui/htmx/registry-viewer", response_class=HTMLResponse, tags=["UI HTMX Partials"])
async def htmx_get_registry_viewer(request: Request):
    return templates.TemplateResponse("partials/_registry_viewer_content.html", get_base_context_web(request))

@app.get("/ui/htmx/execution-view-container", response_class=HTMLResponse, tags=["UI HTMX Partials"])
async def htmx_get_execution_view_container(request: Request):
    context = get_base_context_web(request)
    if not context.get("current_flock"): return HTMLResponse("<article class='error'><p>No Flock loaded. Cannot execute.</p></article>")
    return templates.TemplateResponse("partials/_execution_view_container.html", context)

@app.get("/ui/htmx/scoped-no-flock-view", response_class=HTMLResponse, tags=["UI HTMX Partials"])
async def htmx_scoped_no_flock_view(request: Request):
    return HTMLResponse("""<article style="text-align:center; margin-top: 2rem; border: none; background: transparent;"><hgroup><h2>Scoped Flock Mode</h2><h3>No Flock Loaded</h3></hgroup><p>This UI is in a scoped mode, expecting a Flock to be pre-loaded.</p><p>Please ensure the calling application provides a Flock instance.</p></article>""")

# --- Action Routes (POST requests for UI interactions) ---
@app.post("/ui/load-flock-action/by-name", response_class=HTMLResponse, tags=["UI Actions"])
async def ui_load_flock_by_name_action(request: Request, selected_flock_filename: str = Form(...)):
    loaded_flock = load_flock_from_file_service(selected_flock_filename, request.app.state)
    response_headers = {}
    ui_mode_query = request.query_params.get("ui_mode", "standalone")
    if loaded_flock:
        success_message_text = f"Flock '{loaded_flock.name}' loaded from '{selected_flock_filename}'."
        response_headers["HX-Push-Url"] = "/ui/editor/execute?ui_mode=" + ui_mode_query
        response_headers["HX-Trigger"] = json.dumps({"flockLoaded": None, "notify": {"type": "success", "message": success_message_text}})
        context = get_base_context_web(request, success=success_message_text, ui_mode=ui_mode_query)
        return templates.TemplateResponse("partials/_execution_view_container.html", context, headers=response_headers)
    else:
        error_message_text = f"Failed to load flock file '{selected_flock_filename}'."
        response_headers["HX-Trigger"] = json.dumps({"notify": {"type": "error", "message": error_message_text}})
        context = get_base_context_web(request, error=error_message_text, ui_mode=ui_mode_query)
        context["error_message_inline"] = error_message_text # For direct display in partial
        return templates.TemplateResponse("partials/_load_manager_view.html", context, headers=response_headers)

@app.post("/ui/load-flock-action/by-upload", response_class=HTMLResponse, tags=["UI Actions"])
async def ui_load_flock_by_upload_action(request: Request, flock_file_upload: UploadFile = File(...)):
    error_message_text, filename_to_load, response_headers = None, None, {}
    ui_mode_query = request.query_params.get("ui_mode", "standalone")

    if flock_file_upload and flock_file_upload.filename:
        if not flock_file_upload.filename.endswith((".yaml", ".yml", ".flock")): error_message_text = "Invalid file type."
        else:
            upload_path = FLOCK_FILES_DIR / flock_file_upload.filename
            try:
                with upload_path.open("wb") as buffer: shutil.copyfileobj(flock_file_upload.file, buffer)
                filename_to_load = flock_file_upload.filename
            except Exception as e: error_message_text = f"Upload failed: {e}"
            finally: await flock_file_upload.close()
    else: error_message_text = "No file uploaded."

    if filename_to_load and not error_message_text:
        loaded_flock = load_flock_from_file_service(filename_to_load, request.app.state)
        if loaded_flock:
            success_message_text = f"Flock '{loaded_flock.name}' loaded from '{filename_to_load}'."
            response_headers["HX-Push-Url"] = f"/ui/editor/execute?ui_mode={ui_mode_query}"
            response_headers["HX-Trigger"] = json.dumps({"flockLoaded": None, "flockFileListChanged": None, "notify": {"type": "success", "message": success_message_text}})
            context = get_base_context_web(request, success=success_message_text, ui_mode=ui_mode_query)
            return templates.TemplateResponse("partials/_execution_view_container.html", context, headers=response_headers)
        else: error_message_text = f"Failed to process uploaded '{filename_to_load}'."

    final_error_msg = error_message_text or "Upload failed."
    response_headers["HX-Trigger"] = json.dumps({"notify": {"type": "error", "message": final_error_msg}})
    context = get_base_context_web(request, error=final_error_msg, ui_mode=ui_mode_query)
    return templates.TemplateResponse("partials/_create_flock_form.html", context, headers=response_headers)

@app.post("/ui/create-flock", response_class=HTMLResponse, tags=["UI Actions"])
async def ui_create_flock_action(request: Request, flock_name: str = Form(...), default_model: str = Form(None), description: str = Form(None)):
    ui_mode_query = request.query_params.get("ui_mode", "standalone")
    if not flock_name.strip():
        context = get_base_context_web(request, error="Flock name cannot be empty.", ui_mode=ui_mode_query)
        return templates.TemplateResponse("partials/_create_flock_form.html", context)

    new_flock = create_new_flock_service(flock_name, default_model, description, request.app.state)
    success_msg_text = f"New flock '{new_flock.name}' created. Navigating to Execute page. Configure properties and agents as needed."
    response_headers = {"HX-Push-Url": f"/ui/editor/execute?ui_mode={ui_mode_query}", "HX-Trigger": json.dumps({"flockLoaded": None, "notify": {"type": "success", "message": success_msg_text}})}
    context = get_base_context_web(request, success=success_msg_text, ui_mode=ui_mode_query)
    return templates.TemplateResponse("partials/_execution_view_container.html", context, headers=response_headers)


# --- Settings Page & Endpoints ---
@app.get("/ui/settings", response_class=HTMLResponse, tags=["UI Pages"])
async def page_settings(request: Request, error: str = None, success: str = None, ui_mode: str = Query("standalone")):
    context = get_base_context_web(request, error, success, ui_mode)
    context["initial_content_url"] = "/ui/htmx/settings-view"
    return templates.TemplateResponse("base.html", context)

def _prepare_env_vars_for_template_web():
    env_vars_raw = load_env_file_web(); show_secrets = get_show_secrets_setting_web(env_vars_raw)
    env_vars_list = []
    for name, value in env_vars_raw.items():
        if name.startswith("#") or name == "": continue
        display_value = value if (not is_sensitive_web(name) or show_secrets) else mask_sensitive_value_web(value)
        env_vars_list.append({"name": name, "value": display_value})
    return env_vars_list, show_secrets

@app.get("/ui/htmx/settings-view", response_class=HTMLResponse, tags=["UI HTMX Partials"])
async def htmx_get_settings_view(request: Request):
    env_vars_list, show_secrets = _prepare_env_vars_for_template_web()
    theme_name = get_current_theme_name()
    themes_available = [p.stem for p in THEMES_DIR.glob("*.toml")] if THEMES_DIR and THEMES_DIR.exists() else []
    return templates.TemplateResponse("partials/_settings_view.html", {"request": request, "env_vars": env_vars_list, "show_secrets": show_secrets, "themes": themes_available, "current_theme": theme_name})

@app.post("/ui/htmx/toggle-show-secrets", response_class=HTMLResponse, tags=["UI Actions"])
async def htmx_toggle_show_secrets(request: Request):
    env_vars_raw = load_env_file_web(); current = get_show_secrets_setting_web(env_vars_raw)
    set_show_secrets_setting_web(not current)
    env_vars_list, show_secrets = _prepare_env_vars_for_template_web()
    return templates.TemplateResponse("partials/_env_vars_table.html", {"request": request, "env_vars": env_vars_list, "show_secrets": show_secrets})

@app.post("/ui/htmx/env-delete", response_class=HTMLResponse, tags=["UI Actions"])
async def htmx_env_delete(request: Request, var_name: str = Form(...)):
    env_vars_raw = load_env_file_web()
    if var_name in env_vars_raw: del env_vars_raw[var_name]; save_env_file_web(env_vars_raw)
    env_vars_list, show_secrets = _prepare_env_vars_for_template_web()
    return templates.TemplateResponse("partials/_env_vars_table.html", {"request": request, "env_vars": env_vars_list, "show_secrets": show_secrets})

@app.post("/ui/htmx/env-edit", response_class=HTMLResponse, tags=["UI Actions"])
async def htmx_env_edit(request: Request, var_name: str = Form(...)):
    new_value = request.headers.get("HX-Prompt")
    env_vars_list, show_secrets = _prepare_env_vars_for_template_web()
    if new_value is not None:
        env_vars_raw = load_env_file_web()
        env_vars_raw[var_name] = new_value
        save_env_file_web(env_vars_raw)
        env_vars_list, show_secrets = _prepare_env_vars_for_template_web()
    return templates.TemplateResponse("partials/_env_vars_table.html", {"request": request, "env_vars": env_vars_list, "show_secrets": show_secrets})

@app.get("/ui/htmx/env-add-form", response_class=HTMLResponse, tags=["UI HTMX Partials"])
async def htmx_env_add_form(request: Request):
    return HTMLResponse("""<form hx-post='/ui/htmx/env-add' hx-target='#env-vars-container' hx-swap='outerHTML' style='display:flex; gap:0.5rem; margin-bottom:0.5rem;'><input name='var_name' placeholder='NAME' required style='flex:2;'><input name='var_value' placeholder='VALUE' style='flex:3;'><button type='submit'>Add</button></form>""")

@app.post("/ui/htmx/env-add", response_class=HTMLResponse, tags=["UI Actions"])
async def htmx_env_add(request: Request, var_name: str = Form(...), var_value: str = Form("")):
    env_vars_raw = load_env_file_web()
    env_vars_raw[var_name] = var_value; save_env_file_web(env_vars_raw)
    env_vars_list, show_secrets = _prepare_env_vars_for_template_web()
    return templates.TemplateResponse("partials/_env_vars_table.html", {"request": request, "env_vars": env_vars_list, "show_secrets": show_secrets})

@app.get("/ui/htmx/theme-preview", response_class=HTMLResponse, tags=["UI HTMX Partials"])
async def htmx_theme_preview(request: Request, theme: str = Query(None)):
    if not THEME_LOADER_AVAILABLE:
        return HTMLResponse("<p>Theme loading functionality is not available.</p>", status_code=500)
    if THEMES_DIR is None or not THEMES_DIR.exists():
        return HTMLResponse("<p>Themes directory is not configured or does not exist.</p>", status_code=500)

    chosen_theme_name_input = theme or get_current_theme_name() or DEFAULT_THEME_NAME

    # Sanitize the input to get only the filename component
    sanitized_name_part = Path(chosen_theme_name_input).name
    # Ensure we have a stem
    theme_stem_from_input = sanitized_name_part
    if theme_stem_from_input.endswith(".toml"):
        theme_stem_from_input = theme_stem_from_input[:-5]

    theme_filename_to_load = f"{theme_stem_from_input}.toml"
    theme_name_for_display = theme_stem_from_input # Use the sanitized stem for display/logging

    try:
        resolved_themes_dir = THEMES_DIR.resolve(strict=True)
        theme_path_candidate = resolved_themes_dir / theme_filename_to_load
        resolved_theme_path = theme_path_candidate.resolve()

        if not str(resolved_theme_path).startswith(str(resolved_themes_dir)) or \
           resolved_theme_path.name != theme_filename_to_load:
            logger.warning(f"Invalid theme path access attempt for '{theme_name_for_display}'. "
                           f"Original input: '{chosen_theme_name_input}', Sanitized filename: '{theme_filename_to_load}', "
                           f"Attempted path: '{theme_path_candidate}', Resolved to: '{resolved_theme_path}'")
            return HTMLResponse(f"<p>Invalid theme name or path for '{theme_name_for_display}'.</p>", status_code=400)

        if not resolved_theme_path.is_file():
            logger.info(f"Theme preview: Theme file '{theme_filename_to_load}' not found at '{resolved_theme_path}'.")
            return HTMLResponse(f"<p>Theme '{theme_name_for_display}' not found.</p>", status_code=404)

        theme_path = resolved_theme_path
        theme_data = load_theme_from_file(str(theme_path))
        logger.debug(f"Successfully loaded theme '{theme_name_for_display}' for preview from '{theme_path}'")

    except FileNotFoundError: # For THEMES_DIR.resolve(strict=True)
        logger.error(f"Themes directory '{THEMES_DIR}' not found during preview for '{theme_name_for_display}'.")
        return HTMLResponse("<p>Themes directory not found.</p>", status_code=500)
    except Exception as e:
        logger.error(f"Error loading theme '{theme_name_for_display}' for preview (path: '{theme_path_candidate if 'theme_path_candidate' in locals() else 'unknown'}'): {e}")
        return HTMLResponse(f"<p>Error loading theme '{theme_name_for_display}': {e}</p>", status_code=500)

    css_vars = alacritty_to_pico(theme_data)
    if not css_vars:
        return HTMLResponse(f"<p>Could not convert theme '{theme_name_for_display}' to CSS variables.</p>")

    css_vars_str = ":root {\n" + "\\n".join([f"  {k}: {v};" for k, v in css_vars.items()]) + "\\n}"
    main_colors = [("Background", css_vars.get("--pico-background-color")), ("Text", css_vars.get("--pico-color")), ("Primary", css_vars.get("--pico-primary")), ("Secondary", css_vars.get("--pico-secondary")), ("Muted", css_vars.get("--pico-muted-color"))]
    return templates.TemplateResponse("partials/_theme_preview.html", {"request": request, "theme_name": theme_name_for_display, "css_vars_str": css_vars_str, "main_colors": main_colors})

@app.post("/ui/apply-theme", tags=["UI Actions"])
async def apply_theme(request: Request, theme: str = Form(...)):
    try:
        from flock.webapp.app.config import set_current_theme_name
        set_current_theme_name(theme)
        headers = {"HX-Refresh": "true"}
        return HTMLResponse("", headers=headers)
    except Exception as e: return HTMLResponse(f"Failed to apply theme: {e}", status_code=500)

@app.get("/ui/htmx/settings/env-vars", response_class=HTMLResponse, tags=["UI HTMX Partials"])
async def htmx_settings_env_vars(request: Request):
    env_vars_list, show_secrets = _prepare_env_vars_for_template_web()
    return templates.TemplateResponse("partials/_settings_env_content.html", {"request": request, "env_vars": env_vars_list, "show_secrets": show_secrets})

@app.get("/ui/htmx/settings/theme", response_class=HTMLResponse, tags=["UI HTMX Partials"])
async def htmx_settings_theme(request: Request):
    theme_name = get_current_theme_name()
    themes_available = [p.stem for p in THEMES_DIR.glob("*.toml")] if THEMES_DIR and THEMES_DIR.exists() else []
    return templates.TemplateResponse("partials/_settings_theme_content.html", {"request": request, "themes": themes_available, "current_theme": theme_name})

@app.get("/ui/chat", response_class=HTMLResponse, tags=["UI Pages"])
async def page_chat(request: Request, ui_mode: str = Query("standalone")):
    context = get_base_context_web(request, ui_mode=ui_mode)
    context["initial_content_url"] = "/ui/htmx/chat-view"
    return templates.TemplateResponse("base.html", context)

@app.get("/ui/htmx/chat-view", response_class=HTMLResponse, tags=["UI HTMX Partials"])
async def htmx_get_chat_view(request: Request):
    # Render container partial; session handled in chat router
    return templates.TemplateResponse("partials/_chat_container.html", get_base_context_web(request))

if __name__ == "__main__":
    import uvicorn
    # Ensure the dependency injection system is initialized for standalone run
    temp_run_store = RunStore()
    # Create a default/dummy Flock instance for standalone UI testing
    # This allows the UI to function without being started by `Flock.start_api()`
    dev_flock_instance = Flock(name="DevStandaloneFlock", model="test/dummy", show_flock_banner=False)

    set_global_flock_services(dev_flock_instance, temp_run_store)
    app.state.flock_instance = dev_flock_instance
    app.state.run_store = temp_run_store
    app.state.flock_filename = "development_standalone.flock.yaml"

    logger.info("Running webapp.app.main directly for development with a dummy Flock instance.")
    uvicorn.run(app, host="127.0.0.1", port=8344, reload=True)
```

### src\flock\webapp\app\theme_mapper.py

- **Lines**: 812
- **Last modified**: 2025-05-21 19:51:15

**Functions**:
- `_hex_to_rgb(hex_str)`
- `_rgb_to_hex(rgb)`
- `_rel_lum(rgb)`
- `_contrast(a, b)`
- `_mix(rgb, other, pct)`
- `_rgba(rgb, alpha)`
- `_theme_color(theme, group, name)`
- `_order_colors(theme, color_to_compare_to)`
- `_best_contrast(candidates, bg, min_ratio)`
- `alacritty_to_pico(theme)`

**Content**:
```py
from __future__ import annotations

import re

###########################
#  Low–level colour utils #
###########################

_HEX_RE = re.compile(r"^#?([0-9a-f]{6})$", re.I)

def _hex_to_rgb(hex_str: str) -> tuple[int, int, int] | None:
    if hex_str is None:
        return None
    # Clean the string - remove any leading/trailing whitespace
    hex_str = hex_str.strip()
    m = _HEX_RE.match(hex_str)
    if not m:
        return None
    h = m.group(1)
    return tuple(int(h[i : i + 2], 16) for i in (0, 2, 4))  # type: ignore[misc]

def _rgb_to_hex(rgb: tuple[int, int, int]) -> str:
    return f"#{rgb[0]:02x}{rgb[1]:02x}{rgb[2]:02x}"

def _rel_lum(rgb: tuple[int, int, int]) -> float:
    def channel(c: int) -> float:
        c = c / 255.0
        return c / 12.92 if c <= 0.03928 else ((c + 0.055) / 1.055) ** 2.4
    r, g, b = map(channel, rgb)
    return 0.2126 * r + 0.7152 * g + 0.0722 * b

def _contrast(a: tuple[int, int, int], b: tuple[int, int, int]) -> float:
    if a is None or b is None:
        return 1.0  # Default value if invalid colors provided
    try:
        la, lb = _rel_lum(a), _rel_lum(b)
        darker, lighter = sorted((la, lb))
        return (lighter + 0.05) / (darker + 0.05)
    except (TypeError, ValueError, ZeroDivisionError) as e:
        print(f"Error calculating contrast: {e}")
        return 1.0  # Default value on error

def _mix(rgb: tuple[int, int, int], other: tuple[int, int, int], pct: float) -> tuple[int, int, int]:
    if rgb is None or other is None:
        # Return a fallback color if either input is None
        return (128, 128, 128)  # Default to gray
    try:
        return tuple(int(rgb[i] * (1 - pct) + other[i] * pct) for i in range(3))
    except (TypeError, IndexError) as e:
        print(f"Error mixing colors: {e}")
        return (128, 128, 128)  # Default to gray

def _rgba(rgb: tuple[int, int, int], alpha: float) -> str:
    if rgb is None:
        rgb = (128, 128, 128)  # Default gray if rgb is None
    return f"rgba({rgb[0]}, {rgb[1]}, {rgb[2]}, {alpha})"

########################################
#  Theme → Pico-CSS conversion engine  #
########################################

AccentOrder = ["blue", "cyan", "magenta", "green", "yellow", "red"]

def _theme_color(theme: dict, group: str, name: str) -> tuple[int, int, int] | None:
    try:
        raw = (
            theme.get("colors", {})
                .get(group, {},)
                .get(name)
        )
        return _hex_to_rgb(raw) if raw else None
    except (TypeError, AttributeError) as e:
        print(f"Error extracting color {group}.{name}: {e}")
        return None

def _order_colors(theme: dict, color_to_compare_to: tuple[int, int, int]) -> list[tuple[int, int, int]]:
    colors = []

    # Extract all colors from the theme recursively
    def extract_colors(data):
        if isinstance(data, dict):
            for key, value in data.items():
                if isinstance(value, str) and value.startswith('#'):
                    rgb = _hex_to_rgb(value)
                    if rgb:
                        colors.append(rgb)
                else:
                    extract_colors(value)
        elif isinstance(data, list):
            for item in data:
                extract_colors(item)

    # Start extraction from the root of the theme
    extract_colors(theme)

    # Sort colors by contrast with color_to_compare_to, highest contrast first
    colors.sort(key=lambda c: _contrast(c, color_to_compare_to), reverse=True)

    return colors

def _best_contrast(
    candidates: list[tuple[int, int, int]],
    bg: tuple[int, int, int],
    min_ratio: float = 4.5,
) -> tuple[int, int, int]:
    if not candidates:
        # Instead of raising an exception, return a fallback color
        # Use white or black depending on the background
        return (255, 255, 255) if sum(bg) < 384 else (0, 0, 0)
    best = max(candidates, key=lambda c: _contrast(c, bg))
    if _contrast(best, bg) >= min_ratio:
        return best
    return best  # return highest available even if below threshold

def alacritty_to_pico(theme: dict) -> dict[str, str]:
    css: dict[str, str] = {}

    # 1  Main surface colours
    fixed_background = _theme_color(theme, "primary", "background") or (0, 0, 0)
    fixed_foreground = _theme_color(theme, "primary", "foreground") or (255, 255, 255)

    fixed_background2 = _theme_color(theme, "selection", "background") or (0, 0, 0)
    # Try theme's own foreground first, then fall back to palette extremes
    # fg_candidates = [
    #     _theme_color(theme, "primary", "foreground"),
    #     _theme_color(theme, "normal", "white"),
    #     _theme_color(theme, "bright", "white"),
    #     _theme_color(theme, "normal", "black"),
    #     _theme_color(theme, "bright", "black"),
    # ]
    # fg = _best_contrast([c for c in fg_candidates if c], bg)

    css["--pico-background-color"] = _rgb_to_hex(fixed_background)
    css["--pico-color"] = _rgb_to_hex(fixed_foreground)

    # 2  Pick an accent colour that stands out
    accent = None
    for shade in ("normal", "bright"):
        for name in AccentOrder:
            c = _theme_color(theme, shade, name)
            if c and _contrast(c, fixed_background) >= 3:
                accent = c
                break
        if accent:
            break
    accent = accent or fixed_foreground  # worst case

    accent_hover = _mix(accent, (255, 255, 255), 0.15)              # lighten 15 %
    accent_active = _mix(accent, (0, 0, 0), 0.15)                   # darken 15 %
    accent_focus = _rgba(accent, 0.25)                              # 25 % overlay
    accent_inv = _best_contrast([fixed_foreground, fixed_background, (255, 255, 255), (0, 0, 0)], accent, 4.5)

    css["--pico-primary"] = _rgb_to_hex(accent)
    css["--pico-primary-hover"] = _rgb_to_hex(accent_hover)
    css["--pico-primary-active"] = _rgb_to_hex(accent_active)
    css["--pico-primary-focus"] = accent_focus
    css["--pico-primary-inverse"] = _rgb_to_hex(accent_inv)

    # 3  Secondary accent = next colour in queue with sufficient contrast
    sec = None
    for name in AccentOrder:
        if sec is None and _hex_to_rgb(css["--pico-primary"]) != _theme_color(theme, "normal", name):
            for shade in ("normal", "bright"):
                c = _theme_color(theme, shade, name)
                if c and _contrast(c, fixed_background) >= 3:
                    sec = c
                    break
    sec = sec or _mix(accent, (255, 255, 255), 0.25)

    css["--pico-secondary"] = _rgb_to_hex(sec)
    css["--pico-secondary-hover"] = _rgb_to_hex(_mix(sec, (255, 255, 255), 0.15))
    css["--pico-secondary-focus"] = _rgba(sec, 0.25)
    css["--pico-secondary-active"] = _rgb_to_hex(sec)
    css["--pico-secondary-inverse"] = _rgb_to_hex(_best_contrast([fixed_foreground, fixed_background], sec, 4.5))

    # 4  Headings inherit the accent spectrum
    css["--pico-h1-color"] = css["--pico-primary"]
    css["--pico-h2-color"] = css["--pico-secondary"]
    css["--pico-h3-color"] = css["--pico-color"]  # body colour

    # 5  Muted text & borders use the least-contrasty greys we can still read
    grey_candidates = [
        _theme_color(theme, "bright", "black"),
        _theme_color(theme, "normal", "black"),
        _theme_color(theme, "bright", "white"),
        _theme_color(theme, "normal", "white"),
    ]
    muted = _best_contrast([c for c in grey_candidates if c], fixed_background, 3)
    css["--pico-muted-color"] = _rgb_to_hex(muted)
    css["--pico-border-color"] = _rgb_to_hex(_mix(muted, fixed_background, 0.5))
    css["--pico-muted-border-color"] = css["--pico-border-color"]

    # 6  Cards, selections, cursor, code — pick safe defaults
    css["--pico-card-background-color"] = css["--pico-background-color"]
    css["--pico-card-sectioning-background-color"] = _rgb_to_hex(fixed_background2)
    css["--pico-card-border-color"] = css["--pico-border-color"]

    sel_bg = _theme_color(theme, "selection", "background") or _mix(fixed_background, fixed_foreground, 0.20)
    sel_fg = _best_contrast([fixed_foreground, muted, accent, sec], sel_bg, 4.5)
    css["--pico-selection-background-color"] = _rgb_to_hex(sel_bg)
    css["--pico-selection-color"] = _rgb_to_hex(sel_fg)

    cur_bg = _theme_color(theme, "cursor", "cursor") or sel_bg

    # Try theme's own foreground first, then fall back to palette extremes
    fg_cur_candidates = [
        _theme_color(theme, "primary", "foreground"),
        _theme_color(theme, "normal", "white"),
        _theme_color(theme, "bright", "white"),
        _theme_color(theme, "normal", "blue"),
        _theme_color(theme, "bright", "blue"),
        _theme_color(theme, "normal", "cyan"),
        _theme_color(theme, "bright", "cyan"),
        _theme_color(theme, "normal", "magenta"),
        _theme_color(theme, "bright", "magenta"),
        _theme_color(theme, "normal", "green"),
        _theme_color(theme, "bright", "green"),
        _theme_color(theme, "normal", "yellow"),
        _theme_color(theme, "bright", "yellow"),
        _theme_color(theme, "normal", "red"),
        _theme_color(theme, "bright", "red"),
        _theme_color(theme, "normal", "black"),
        _theme_color(theme, "bright", "black"),
    ]
    cur_fg = _best_contrast([c for c in fg_cur_candidates if c], cur_bg)

    css["--pico-code-background-color"] = _rgb_to_hex(cur_bg)
    css["--pico-code-color"] = _rgb_to_hex(cur_fg)

    # 7  Form elements and buttons reuse the existing tokens
    css["--pico-form-element-background-color"] = css["--pico-background-color"]
    css["--pico-form-element-border-color"] = css["--pico-border-color"]
    css["--pico-form-element-color"] = css["--pico-color"]
    css["--pico-form-element-focus-color"] = css["--pico-primary-hover"]
    css["--pico-form-element-placeholder-color"] = css["--pico-muted-color"]
    css["--pico-form-element-active-border-color"] = css["--pico-primary"]
    css["--pico-form-element-active-background-color"] = css["--pico-selection-background-color"]
    css["--pico-form-element-disabled-background-color"] = _rgb_to_hex(_mix(fixed_background, fixed_foreground, 0.1))
    css["--pico-form-element-disabled-border-color"] = css["--pico-border-color"]
    css["--pico-form-element-invalid-border-color"] = _rgb_to_hex(_theme_color(theme, "normal", "red") or accent_active)
    css["--pico-form-element-invalid-focus-color"] = _rgb_to_hex(_theme_color(theme, "bright", "red") or accent_hover)

    # 8  Buttons follow primary palette by default
    css["--pico-button-base-background-color"] = css["--pico-primary"]
    css["--pico-button-base-color"] = css["--pico-primary-inverse"]
    css["--pico-button-hover-background-color"] = css["--pico-primary-hover"]
    css["--pico-button-hover-color"] = css["--pico-primary-inverse"]

    # 9  Semantic markup helpers
    yellow = _theme_color(theme, "normal", "yellow") or _mix(accent, (255, 255, 0), 0.5)
    css["--pico-mark-background-color"] = _rgba(yellow, 0.2)
    css["--pico-mark-color"] = css["--pico-color"]
    css["--pico-ins-color"] = _rgb_to_hex(_theme_color(theme, "normal", "green") or accent)
    css["--pico-del-color"] = _rgb_to_hex(_theme_color(theme, "normal", "red") or accent_active)

    # 10  Contrast helpers
    css["--pico-contrast"] = css["--pico-color"]
    css["--pico-contrast-inverse"] = css["--pico-primary-inverse"]

    return css

if __name__ == "__main__":
    import pathlib
    import sys

    try:
        import toml
        import uvicorn
        from fastapi import FastAPI, Request
        from fastapi.responses import HTMLResponse
        from fastapi.templating import Jinja2Templates
    except ImportError as e:
        print(f"Error: Required module not found: {e}")
        print("Please install required packages: pip install toml fastapi uvicorn jinja2")
        sys.exit(1)

    # Find themes directory
    current_dir = pathlib.Path(__file__).parent
    themes_dir = current_dir.parent.parent / 'themes'

    # Check if themes directory exists
    if not themes_dir.exists():
        print(f"Error: Themes directory not found at {themes_dir}")
        print(f"Current directory is {current_dir}")
        sys.exit(1)

    # Get list of all theme files
    theme_files = list(themes_dir.glob('*.toml'))

    if not theme_files:
        print(f"Error: No theme files found in {themes_dir}")
        sys.exit(1)

    # Make sure alabaster is included
    alabaster_path = themes_dir / 'alabaster.toml'
    if alabaster_path not in theme_files and alabaster_path.exists():
        theme_files.append(alabaster_path)

    # Load a few random themes plus alabaster
    selected_themes = theme_files
    if alabaster_path.exists() and alabaster_path not in selected_themes:
        selected_themes.append(alabaster_path)

    # Dictionary to store theme data
    themes = {}

    # Load each theme
    for theme_path in selected_themes:
        try:
            # Read the file as text to handle whitespace issues
            with open(theme_path) as f:
                content = f.read()

            # Remove leading/trailing whitespace from each line
            cleaned_content = '\n'.join(line.rstrip() for line in content.splitlines())

            # Parse using StringIO
            from io import StringIO
            theme_data = toml.load(StringIO(cleaned_content))
            themes[theme_path.stem] = theme_data
        except Exception as e:
            print(f"Error loading {theme_path.name}: {e}")

    # Check if any themes were successfully loaded
    if not themes:
        print("Error: No themes could be loaded successfully.")
        sys.exit(1)

    # Create a FastAPI application
    app = FastAPI(title="Theme Mapper")

    # Create template directory
    template_dir = current_dir / "templates"
    try:
        template_dir.mkdir(exist_ok=True)
    except Exception as e:
        print(f"Error creating template directory: {e}")
        sys.exit(1)

    # Create and write template file
    template_path = template_dir / "theme_mapper.html"
    template_content = '''
    <!DOCTYPE html>
    <html lang="en" data-theme="light">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Theme Mapper</title>
        <!-- Use Pico CSS only -->
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@picocss/pico@1/css/pico.min.css">
        <style>
            {{ css_vars | safe }}

            /* Only use Pico CSS classes and variables */
           
            .color-sample {
                height: 20px;
                width: 100%;
                border-radius: 4px;
                margin-bottom: 5px;
            }
            .theme-selector {
                position: fixed;
                top: 10px;
                right: 10px;
                z-index: 100;
                background-color: var(--pico-card-background-color);
                padding: 10px;
                border-radius: 8px;
                border: 1px solid var(--pico-border-color);
            }
            article {
                margin-bottom: 1rem;
            }
            .contrast-table td {
                padding: 5px;
                text-align: center;
            }
            .good-contrast {
                background-color: var(--pico-ins-color);
                color: var(--pico-background-color);
            }
            .bad-contrast {
                background-color: var(--pico-del-color);
                color: var(--pico-background-color);
            }
            
            /* New: give demo content cards a themed background so text always contrasts */
            article {
                background-color: var(--pico-card-sectioning-background-color);
                border: 1px solid var(--pico-card-border-color);
                padding: 1rem;
                border-radius: 8px;
            }
            /* New: background for the two grid columns */
            .grid > div {
                background-color: var(--pico-card-background-color);
                padding: 1rem;
                border-radius: 8px;
            }
            
            /* Override any non-pico CSS variables */
            body {
                background-color: var(--pico-background-color);
                color: var(--pico-color);
            }
            a {
                color: var(--pico-primary);
            }
            a:hover {
                color: var(--pico-primary-hover);
            }
            h1 {
                color: var(--pico-h1-color);
            }
            h2 {
                color: var(--pico-h2-color);
            }
            h3 {
                color: var(--pico-h3-color);
            }
            mark {
                background-color: var(--pico-mark-background-color);
                color: var(--pico-mark-color);
            }
            ins {
                color: var(--pico-ins-color);
            }
            del {
                color: var(--pico-del-color);
            }
            code {
                background-color: var(--pico-code-background-color);
                color: var(--pico-code-color);
            }
            button, input[type="submit"], input[type="button"] {
                background-color: var(--pico-button-base-background-color);
                color: var(--pico-button-base-color);
                border-color: var(--pico-button-base-background-color);
            }
            button:hover, input[type="submit"]:hover, input[type="button"]:hover {
                background-color: var(--pico-button-hover-background-color);
                color: var(--pico-button-hover-color);
                border-color: var(--pico-button-hover-background-color);
            }
            button.secondary, input[type="submit"].secondary, input[type="button"].secondary {
                background-color: var(--pico-secondary);
                color: var(--pico-secondary-inverse);
                border-color: var(--pico-secondary);
            }
            button.secondary:hover, input[type="submit"].secondary:hover, input[type="button"].secondary:hover {
                background-color: var(--pico-secondary-hover);
                color: var(--pico-secondary-inverse);
                border-color: var(--pico-secondary-hover);
            }
            button.contrast, input[type="submit"].contrast, input[type="button"].contrast {
                background-color: var(--pico-contrast);
                color: var(--pico-contrast-inverse);
                border-color: var(--pico-contrast);
            }
            /* Improve grid columns on wider screens */
            @media (min-width: 768px) {
                .grid {
                    display: grid;
                    grid-template-columns: repeat(auto-fit, minmax(380px, 1fr));
                    gap: 2rem;
                }
            }
            /* Ensure container can grow a little wider than Pico default */
            .container {
                max-width: 90rem; /* ~1440px */
            }
            /* Ensure tables use full-strength text colour */
            table th,
            table td {
                color: var(--pico-color);
                opacity: 1; /* override Pico's default fade */
            }
        </style>
    </head>
    <body>
        <div class="container">
            <div class="theme-selector">
                <label for="theme-select">Select Theme:</label>
                <select id="theme-select" onchange="window.location.href='/?theme=' + this.value">
                    {% for theme_name in themes %}
                    <option value="{{ theme_name }}" {% if theme_name == current_theme %}selected{% endif %}>{{ theme_name }}</option>
                    {% endfor %}
                </select>
            </div>
            
            <h1>Theme Mapper: {{ current_theme }}</h1>
            
            <div class="grid">
                <div>
                    <h2>UI Elements</h2>
                    <article>
                        <h3>Headings and Text</h3>
                        <h1>Heading 1</h1>
                        <h2>Heading 2</h2>
                        <h3>Heading 3</h3>
                        <p>Normal paragraph text. <a href="#">This is a link</a>. <mark>This is marked text</mark>.</p>
                        <p><small>This is small text</small></p>
                        <p><ins>This is inserted text</ins> and <del>this is deleted text</del>.</p>
                        <blockquote>
                            This is a blockquote with <cite>a citation</cite>.
                        </blockquote>
                        <code>This is inline code</code>
                        <pre><code>// This is a code block
function example() {
  return "Hello World";
}</code></pre>
                    </article>
                    
                    <article>
                        <h3>Buttons</h3>
                        <button>Default Button</button>
                        <button class="secondary">Secondary Button</button>
                        <button class="contrast">Contrast Button</button>
                    </article>
                    
                    <article>
                        <h3>Form Elements</h3>
                        <form>
                            <label for="text">Text Input</label>
                            <input type="text" id="text" placeholder="Text input">
                            
                            <label for="select">Select</label>
                            <select id="select">
                                <option>Option 1</option>
                                <option>Option 2</option>
                            </select>
                            
                            <label for="textarea">Textarea</label>
                            <textarea id="textarea" placeholder="Textarea"></textarea>
                            
                            <label for="invalid" aria-invalid="true">Invalid Input</label>
                            <input type="text" id="invalid" aria-invalid="true" placeholder="Invalid input">
                            
                            <fieldset>
                                <legend>Checkboxes</legend>
                                <label>
                                    <input type="checkbox" checked>
                                    Checkbox 1
                                </label>
                                <label>
                                    <input type="checkbox">
                                    Checkbox 2
                                </label>
                            </fieldset>
                            
                            <fieldset>
                                <legend>Radio Buttons</legend>
                                <label>
                                    <input type="radio" name="radio" checked>
                                    Radio 1
                                </label>
                                <label>
                                    <input type="radio" name="radio">
                                    Radio 2
                                </label>
                            </fieldset>
                        </form>
                    </article>
                </div>
                
                <div>
                    <h2>Theme Color Mapping</h2>
                    <article>
                        <h3>Main Colors</h3>
                        <div class="grid">
                            {% for color_name, color_value in main_colors %}
                            <div>
                                <div class="color-sample" style="background-color: {{ color_value }};"></div>
                                <small>{{ color_name }}<br>{{ color_value }}</small>
                            </div>
                            {% endfor %}
                        </div>
                    </article>
                    
                    <article>
                        <h3>All Pico CSS Variables</h3>
                        <div style="max-height: 300px; overflow-y: auto;">
                            <table>
                                <thead>
                                    <tr>
                                        <th>Variable</th>
                                        <th>Value</th>
                                        <th>Sample</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    {% for var_name, var_value in all_vars %}
                                    <tr>
                                        <td>{{ var_name }}</td>
                                        <td>{{ var_value }}</td>
                                        <td>
                                            {% if var_value.startswith('#') or var_value.startswith('rgb') %}
                                            <div class="color-sample" style="background-color: {{ var_value }};"></div>
                                            {% endif %}
                                        </td>
                                    </tr>
                                    {% endfor %}
                                </tbody>
                            </table>
                        </div>
                    </article>
                    
                    <article>
                        <h3>Color Contrast Checks</h3>
                        <table class="contrast-table">
                            <thead>
                                <tr>
                                    <th>Foreground</th>
                                    <th>Background</th>
                                    <th>Contrast</th>
                                    <th>WCAG AA</th>
                                </tr>
                            </thead>
                            <tbody>
                                {% for check in contrast_checks %}
                                <tr>
                                    <td>{{ check.fg_name }}</td>
                                    <td>{{ check.bg_name }}</td>
                                    <td>{{ check.contrast }}</td>
                                    <td class="{% if check.passes %}good-contrast{% else %}bad-contrast{% endif %}">
                                        {{ "Pass" if check.passes else "Fail" }}
                                    </td>
                                </tr>
                                {% endfor %}
                            </tbody>
                        </table>
                    </article>

                    <article>
                        <h3>Original Theme Colors</h3>
                        <div style="max-height: 300px; overflow-y: auto;">
                            <table>
                                <thead>
                                    <tr>
                                        <th>Group</th>
                                        <th>Name</th>
                                        <th>Value</th>
                                        <th>Sample</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    {% for group, names in original_colors.items() %}
                                        {% for name, value in names.items() %}
                                        <tr>
                                            <td>{{ group }}</td>
                                            <td>{{ name }}</td>
                                            <td>{{ value }}</td>
                                            <td><div class="color-sample" style="background-color: {{ value }};"></div></td>
                                        </tr>
                                        {% endfor %}
                                    {% endfor %}
                                </tbody>
                            </table>
                        </div>
                    </article>
                </div>
            </div>
        </div>
    </body>
    </html>
    '''

    try:
        with open(template_path, 'w') as f:
            f.write(template_content)
    except Exception as e:
        print(f"Error writing template file: {e}")
        sys.exit(1)

    # Setup Jinja2 templates
    try:
        templates = Jinja2Templates(directory=str(template_dir))
    except Exception as e:
        print(f"Error setting up Jinja2 templates: {e}")
        sys.exit(1)

    @app.get("/", response_class=HTMLResponse)
    async def index(request: Request, theme: str | None = None):
        try:
            # Get requested theme name from query parameter
            if theme is None or theme not in themes:
                # If no theme is provided or theme is invalid, default to alabaster or first theme
                theme_name = 'alabaster' if 'alabaster' in themes else next(iter(themes))
            else:
                # Use the requested theme directly
                theme_name = theme

            # Get the theme data
            theme_data = themes[theme_name]

            # Convert theme to pico css variables
            css_vars = alacritty_to_pico(theme_data)

            # Format css variables for the style tag - this is important for proper application of the theme
            css_vars_str = ":root {\n" + "\n".join([f"  {k}: {v};" for k, v in css_vars.items()]) + "\n}"

            # Prepare main colors for display
            main_colors = [
                ("Background", css_vars["--pico-background-color"]),
                ("Text", css_vars["--pico-color"]),
                ("Primary", css_vars["--pico-primary"]),
                ("Secondary", css_vars["--pico-secondary"]),
                ("Muted", css_vars["--pico-muted-color"]),
            ]

            # Extract original colors from theme
            original_colors = {}
            if "colors" in theme_data:
                for group, colors in theme_data["colors"].items():
                    if isinstance(colors, dict):
                        group_colors = {}
                        for name, value in colors.items():
                            if isinstance(value, str):
                                group_colors[name] = value.strip()
                        if group_colors:
                            original_colors[group] = group_colors

            # Check contrasts
            contrast_checks = []

            # Define relevant foreground/background variable pairs to evaluate
            contrast_pairs = [
                ("Text", "--pico-color", "Background", "--pico-background-color"),
                ("Primary", "--pico-primary", "Background", "--pico-background-color"),
                ("Secondary", "--pico-secondary", "Background", "--pico-background-color"),
                ("Muted", "--pico-muted-color", "Background", "--pico-background-color"),
                (
                    "Button Text",
                    "--pico-button-base-color",
                    "Button Background",
                    "--pico-button-base-background-color",
                ),
                (
                    "Secondary Btn Text",
                    "--pico-secondary-inverse",
                    "Secondary",
                    "--pico-secondary",
                ),
                (
                    "Contrast Text",
                    "--pico-contrast-inverse",
                    "Contrast Background",
                    "--pico-contrast",
                ),
                (
                    "Code",
                    "--pico-code-color",
                    "Code Background",
                    "--pico-code-background-color",
                ),
            ]

            for fg_name, fg_var, bg_name, bg_var in contrast_pairs:
                fg_hex = css_vars.get(fg_var)
                bg_hex = css_vars.get(bg_var)
                if not fg_hex or not bg_hex:
                    continue
                fg_rgb = _hex_to_rgb(fg_hex)
                bg_rgb = _hex_to_rgb(bg_hex)
                if fg_rgb is None or bg_rgb is None:
                    continue
                ratio = _contrast(fg_rgb, bg_rgb)
                contrast_checks.append({
                    "fg_name": fg_name,
                    "bg_name": bg_name,
                    "contrast": f"{ratio:.2f}",
                    "passes": ratio >= 4.5,
                })

            return templates.TemplateResponse(
                "theme_mapper.html",
                {
                    "request": request,
                    "css_vars": css_vars_str,
                    "themes": themes.keys(),
                    "current_theme": theme_name,
                    "main_colors": main_colors,
                    "all_vars": list(css_vars.items()),
                    "original_colors": original_colors,
                    "contrast_checks": contrast_checks
                }
            )
        except Exception as e:
            print(f"Error handling request: {e}")
            from fastapi.responses import PlainTextResponse
            return PlainTextResponse(f"Error processing theme: {e!s}", status_code=500)

    # Add a more robust error handler for the application
    @app.exception_handler(Exception)
    async def global_exception_handler(request: Request, exc: Exception):
        print(f"Global exception handler caught: {exc}")
        from fastapi.responses import PlainTextResponse
        return PlainTextResponse(f"Internal server error: {exc!s}", status_code=500)

    # Run the app
    print("Starting Theme Mapper web app...")
    print(f"Current directory: {current_dir}")
    print(f"Template directory: {template_dir} (exists: {template_dir.exists()})")
    print(f"Themes directory: {themes_dir} (exists: {themes_dir.exists()})")
    print(f"Loaded {len(themes)} themes: {', '.join(themes.keys())}")

    try:
        uvicorn.run(app, host="127.0.0.1", port=5050)
    except Exception as e:
        print(f"Error starting the web server: {e}")
        sys.exit(1)
```

### examples\05-full-projects\project-textual-project-planner\src\flock_flightplan\collector\__init__.py

- **Lines**: 168
- **Last modified**: 2025-05-22 18:59:04

**Description**: Code Collection and Analysis Sub-package.
Provides functionality to scan repositories, analyze code (primarily Python),
and generate Repository objects.

**Functions**:
- `set_app_log(log_component)`
- `run_collection(include_args, exclude_args, output_arg, config_arg, repo_name, calculate_metrics)`

**Content**:
```py
# src/pilot_rules/collector/__init__.py
"""
Code Collection and Analysis Sub-package.
Provides functionality to scan repositories, analyze code (primarily Python),
and generate Repository objects.
"""

from typing import List, Optional

# Import necessary functions from sibling modules using relative imports
from flock_flightplan.cli import utils
from flock_flightplan.collector import config
from flock_flightplan.collector import discovery
from flock_flightplan.collector import analysis
from flock_flightplan.collector import reporting
from flock_flightplan.collector.config import process_config_and_args
from flock_flightplan.collector.discovery import collect_files
from flock_flightplan.collector.analysis import analyze_code_dependencies, get_common_patterns, find_key_files
from flock_flightplan.collector.reporting import generate_repository
from flock_flightplan.cli.utils import (
    log,
    print_header,
    print_subheader,
    print_success,
    print_warning,
    print_error,
    print_file_stats,
)
from flock_flightplan.model import Repository
from flock_flightplan.model import set_app_log as model_set_app_log
from textual.widgets import RichLog

log : RichLog = None

def set_app_log(log_component):
    """Set the app's RichLog component for output"""
    global log
    log = log_component
    config.set_app_log(log)
    discovery.set_app_log(log)
    analysis.set_app_log(log)
    reporting.set_app_log(log)
    utils.set_app_log(log)
    model_set_app_log(log)

def run_collection(
    include_args: Optional[List[str]],
    exclude_args: Optional[List[str]],
    output_arg: Optional[str] = None,  # Kept for backward compatibility but used to save Repository as JSON
    config_arg: Optional[str] = None,
    repo_name: Optional[str] = None,
    calculate_metrics: bool = False,  # New parameter to control metrics calculation
) -> Repository:
    """
    Main entry point for the code collection process.

    Orchestrates configuration loading, file discovery, analysis, and Repository generation.
    
    Args:
        include_args: List of include patterns in format 'ext1,ext2:./folder'
        exclude_args: List of exclude patterns in format 'py:temp'
        output_arg: Path to output JSON file to save the repository (optional)
        config_arg: Path to optional TOML config file
        repo_name: Name for the repository (default is "Repository Analysis")
        calculate_metrics: Whether to calculate code quality metrics (default is False)
        
    Returns:
        Repository object with analyzed code data
    """
    try:
        # 1. Process Configuration and Arguments
        print_header("Code Collection Process", "magenta")
        final_sources, final_excludes, _ = process_config_and_args(
            include_args=include_args,
            exclude_args=exclude_args,
            output_arg=output_arg,
            config_arg=config_arg,
        )

        # Use provided repo_name or default
        repository_name = repo_name if repo_name else "Repository Analysis"

        # 2. Collect Files based on finalized sources and excludes
        collected_files, actual_extensions = collect_files(
            final_sources, final_excludes
        )

        if not collected_files:
            print_warning("No files found matching the specified criteria.")
            # Return minimal Repository with empty files list
            return Repository(
                name=repository_name,
                statistics="No files found matching the specified criteria.",
                project_files=[]
            )
        else:
            print_success(
                f"Found [bold green]{len(collected_files)}[/bold green] files to include in the analysis."
            )
            ext_list = ", ".join(sorted(list(actual_extensions)))
            log.write(f"File extensions found: [cyan]{ext_list}[/cyan]")

            # Display file statistics in a nice table
            print_file_stats(collected_files, "Collection Statistics")

        # 3. Perform Analysis (Conditional based on files found)
        dependencies = {}
        patterns = {}
        key_files = []

        # Only run Python-specific analysis if .py files are present
        has_python_files = ".py" in actual_extensions
        if has_python_files:
            print_subheader("Analyzing Python Dependencies", "blue")
            dependencies = analyze_code_dependencies(collected_files)
            print_subheader("Identifying Code Patterns", "blue")
            patterns = get_common_patterns(collected_files)
        else:
            print_warning("Skipping Python-specific analysis (no .py files found).")

        # Find key files (uses heuristics applicable to various file types)
        if collected_files:
            # Note: find_key_files now has its own print_subheader call
            key_files = find_key_files(collected_files, dependencies)  # Pass all files

        # 4. Generate Repository Object
        repository = generate_repository(
            files=collected_files,
            analyzed_extensions=actual_extensions,
            dependencies=dependencies,
            patterns=patterns,
            key_files=key_files,
            repo_name=repository_name,
            root_folder_display=".",  # Or derive from sources if needed
            calculate_metrics=calculate_metrics,  # Pass the metrics flag
        )
        
        # 5. Save Repository as JSON if output_arg is provided
        if output_arg:
            import json
            from pathlib import Path
            
            try:
                # Convert repository to dict and save as JSON
                repo_dict = repository.dict()
                output_path = Path(output_arg)
                output_path.parent.mkdir(parents=True, exist_ok=True)
                with open(output_path, "w", encoding="utf-8") as f:
                    json.dump(repo_dict, f, indent=2)
                print_success(f"Repository data saved to {output_path}")
            except Exception as e:
                print_error(f"Error saving repository data: {str(e)}")
        
        return repository

    except ValueError as e:
        # Configuration or argument parsing errors
        print_error(f"Configuration Error: {e}", 1)
        raise
    except Exception as e:
        # Catch-all for unexpected errors during collection/analysis/reporting
        print_error(f"An unexpected error occurred: {e}", 1)
        import traceback
        traceback.print_exc()
        raise

# Alias for backward compatibility
generate_repository_from_files = run_collection
```

### src\flock\core\api\run_store.py

- **Lines**: 224
- **Last modified**: 2025-04-16 00:11:15
- **Used by**: 1 files

**Description**: Manages the state of active and completed Flock runs.

**Classes**:
- `RunStore`: 11 methods

**Content**:
```py
# src/flock/core/api/run_store.py
"""Manages the state of active and completed Flock runs."""

import threading
from datetime import datetime
from typing import Any

from flock.core.logging.logging import get_logger

from .models import (  # Import from the models file
    FlockAPIResponse,
    FlockBatchResponse,
)

logger = get_logger("api.run_store")


class RunStore:
    """Stores and manages the state of Flock runs."""

    def __init__(self):
        self._runs: dict[str, FlockAPIResponse] = {}
        self._batches: dict[str, FlockBatchResponse] = {}
        self._lock = threading.Lock()  # Basic lock for thread safety

    def create_run(self, run_id: str) -> FlockAPIResponse:
        """Creates a new run record with 'starting' status."""
        with self._lock:
            if run_id in self._runs:
                logger.warning(f"Run ID {run_id} already exists. Overwriting.")
            response = FlockAPIResponse(
                run_id=run_id, status="starting", started_at=datetime.now()
            )
            self._runs[run_id] = response
            logger.debug(f"Created run record for run_id: {run_id}")
            return response

    def get_run(self, run_id: str) -> FlockAPIResponse | None:
        """Gets the status of a run."""
        with self._lock:
            return self._runs.get(run_id)

    def update_run_status(
        self, run_id: str, status: str, error: str | None = None
    ):
        """Updates the status and potentially error of a run."""
        with self._lock:
            if run_id in self._runs:
                self._runs[run_id].status = status
                if error:
                    self._runs[run_id].error = error
                if status in ["completed", "failed"]:
                    self._runs[run_id].completed_at = datetime.now()
                logger.debug(f"Updated status for run_id {run_id} to {status}")
            else:
                logger.warning(
                    f"Attempted to update status for non-existent run_id: {run_id}"
                )

    def update_run_result(self, run_id: str, result: dict):
        """Updates the result of a completed run."""
        with self._lock:
            if run_id in self._runs:
                # Ensure result is serializable (e.g., convert Box)
                final_result = (
                    dict(result) if hasattr(result, "to_dict") else result
                )
                self._runs[run_id].result = final_result
                self._runs[run_id].status = "completed"
                self._runs[run_id].completed_at = datetime.now()
                logger.debug(f"Updated result for completed run_id: {run_id}")
            else:
                logger.warning(
                    f"Attempted to update result for non-existent run_id: {run_id}"
                )

    def create_batch(self, batch_id: str) -> FlockBatchResponse:
        """Creates a new batch record with 'starting' status."""
        with self._lock:
            if batch_id in self._batches:
                logger.warning(
                    f"Batch ID {batch_id} already exists. Overwriting."
                )
            response = FlockBatchResponse(
                batch_id=batch_id,
                status="starting",
                results=[],
                started_at=datetime.now(),
                total_items=0,
                completed_items=0,
                progress_percentage=0.0,
            )
            self._batches[batch_id] = response
            logger.debug(f"Created batch record for batch_id: {batch_id}")
            return response

    def get_batch(self, batch_id: str) -> FlockBatchResponse | None:
        """Gets the status of a batch run."""
        with self._lock:
            return self._batches.get(batch_id)

    def update_batch_status(
        self, batch_id: str, status: str, error: str | None = None
    ):
        """Updates the status and potentially error of a batch run."""
        with self._lock:
            if batch_id in self._batches:
                self._batches[batch_id].status = status
                if error:
                    self._batches[batch_id].error = error
                if status in ["completed", "failed"]:
                    self._batches[batch_id].completed_at = datetime.now()
                    # When completed, ensure progress is 100%
                    if (
                        status == "completed"
                        and self._batches[batch_id].total_items > 0
                    ):
                        self._batches[batch_id].completed_items = self._batches[
                            batch_id
                        ].total_items
                        self._batches[batch_id].progress_percentage = 100.0
                logger.debug(
                    f"Updated status for batch_id {batch_id} to {status}"
                )
            else:
                logger.warning(
                    f"Attempted to update status for non-existent batch_id: {batch_id}"
                )

    def update_batch_result(self, batch_id: str, results: list[Any]):
        """Updates the results of a completed batch run."""
        with self._lock:
            if batch_id in self._batches:
                # Ensure results are serializable
                final_results = [
                    dict(r) if hasattr(r, "to_dict") else r for r in results
                ]
                self._batches[batch_id].results = final_results
                self._batches[batch_id].status = "completed"
                self._batches[batch_id].completed_at = datetime.now()

                # Update progress tracking
                self._batches[batch_id].completed_items = len(final_results)
                self._batches[batch_id].total_items = len(final_results)
                self._batches[batch_id].progress_percentage = 100.0

                logger.debug(
                    f"Updated results for completed batch_id: {batch_id}"
                )
            else:
                logger.warning(
                    f"Attempted to update results for non-existent batch_id: {batch_id}"
                )

    def set_batch_total_items(self, batch_id: str, total_items: int):
        """Sets the total number of items in a batch."""
        try:
            with self._lock:
                if batch_id in self._batches:
                    self._batches[batch_id].total_items = total_items
                    # Recalculate percentage
                    if total_items > 0:
                        self._batches[batch_id].progress_percentage = (
                            self._batches[batch_id].completed_items
                            / total_items
                            * 100.0
                        )
                    logger.debug(
                        f"Set total_items for batch_id {batch_id} to {total_items}"
                    )
                else:
                    logger.warning(
                        f"Attempted to set total_items for non-existent batch_id: {batch_id}"
                    )
        except Exception as e:
            logger.error(f"Error setting batch total items: {e}", exc_info=True)

    def update_batch_progress(
        self,
        batch_id: str,
        completed_items: int,
        partial_results: list[Any] = None,
    ):
        """Updates the progress of a batch run and optionally adds partial results.

        Args:
            batch_id: The ID of the batch to update
            completed_items: The number of items that have been completed
            partial_results: Optional list of results for completed items to add to the batch
        """
        try:
            with self._lock:
                if batch_id in self._batches:
                    self._batches[batch_id].completed_items = completed_items

                    # Calculate percentage if we have a total
                    if self._batches[batch_id].total_items > 0:
                        self._batches[batch_id].progress_percentage = (
                            completed_items
                            / self._batches[batch_id].total_items
                            * 100.0
                        )

                    # Add partial results if provided
                    if partial_results:
                        # Ensure results are serializable
                        final_results = [
                            dict(r) if hasattr(r, "to_dict") else r
                            for r in partial_results
                        ]
                        self._batches[batch_id].results = final_results

                    logger.debug(
                        f"Updated progress for batch_id {batch_id}: {completed_items}/{self._batches[batch_id].total_items} "
                        f"({self._batches[batch_id].progress_percentage:.1f}%)"
                    )
                else:
                    logger.warning(
                        f"Attempted to update progress for non-existent batch_id: {batch_id}"
                    )
        except Exception as e:
            logger.error(f"Error updating batch progress: {e}", exc_info=True)

    # Add methods for cleanup, persistence, etc. later
```

### src\flock\core\__init__.py

- **Lines**: 44
- **Last modified**: 2025-05-22 21:27:37

**Description**: This module contains the core classes of the flock package.

**Content**:
```py
"""This module contains the core classes of the flock package."""

from flock.core.context.context import FlockContext
from flock.core.flock import Flock
from flock.core.flock_agent import FlockAgent
from flock.core.flock_evaluator import FlockEvaluator, FlockEvaluatorConfig
from flock.core.flock_factory import FlockFactory
from flock.core.flock_module import FlockModule, FlockModuleConfig
from flock.core.flock_registry import (
    FlockRegistry,
    flock_callable,
    flock_component,
    flock_tool,
    flock_type,
    get_registry,
)
from flock.core.mcp.flock_mcp_server import (
    FlockMCPServerBase,
)
from flock.core.mcp.flock_mcp_tool_base import FlockMCPToolBase
from flock.core.mcp.mcp_client import FlockMCPClientBase
from flock.core.mcp.mcp_client_manager import FlockMCPClientManagerBase

__all__ = [
    "Flock",
    "FlockAgent",
    "FlockContext",
    "FlockEvaluator",
    "FlockEvaluatorConfig",
    "FlockFactory",
    "FlockMCPClientBase",
    "FlockMCPClientManagerBase",
    "FlockMCPServerBase",
    "FlockMCPServerConfig",
    "FlockMCPToolBase",
    "FlockModule",
    "FlockModuleConfig",
    "FlockRegistry",
    "flock_callable",
    "flock_component",
    "flock_tool",
    "flock_type",
    "get_registry",
]
```

### examples\05-full-projects\project-story-engine\main.py

- **Lines**: 114
- **Last modified**: 2025-05-22 18:59:04

**Classes**:
- `Scene`: 0 methods
- `Character`: 0 methods
- `Chapter`: 0 methods
- `Prompt`: 0 methods
- `Issue`: 0 methods
- `ComicBookSeries`: 0 methods
- `PageLayout`: 0 methods
- `IssueLayout`: 0 methods
- `Story`: 0 methods
- `StoryBible`: 0 methods

**Content**:
```py
from typing import Optional
from pydantic import BaseModel, Field
from flock.core import FlockFactory, Flock, flock_registry
from flock.core.flock_registry import flock_type

FlockRegistry = flock_registry.get_registry()

class Scene(BaseModel):
    title: str
    setting: str = Field(..., description="Setting of the scene")
    goal: str = Field(..., description="Goal of the scene")
    conflict: str = Field(..., description="Conflict of the scene")
    outcome: str = Field(..., description="Outcome of the scene")
    characters_involved: list[str] = Field(..., description="Name of characters/entities involved in the scene")
    story_beats: list[str] = Field(..., description="Story beats of the scene")


class Character(BaseModel):
    name: str = Field(..., description="Name of the character")
    role: str = Field(..., description="Role of the character")
    age: str = Field(..., description="Age of the character")
    appearance: str = Field(..., description="Appearance of the character")
    image_prompt: str = Field(..., description="Very detailed image prompt for image generation to represent the character")
    personality_traits: list[str] = Field(..., description="Personality traits of the character")
    backstory: str = Field(..., description="Backstory of the character")
    motivations: str = Field(..., description="Motivations of the character")
    weaknesses: str = Field(..., description="Weaknesses of the character")
    character_arc: str = Field(..., description="How the character evolves throughout the story")
    
class Chapter(BaseModel):
    title: str = Field(..., description="Title of the chapter")
    chapter_number: int = Field(..., description="Chapter number of the chapter")
    purpose: str = Field(..., description="Purpose of the chapter")
    summary: str = Field(..., description="Key events or chapter summary")
    scenes: list[Scene] = Field(..., description="Scenes of the chapter")
    


########################################################

class Prompt(BaseModel):
    prompt: str = Field(..., description="Detailed Prompt for image generation")
    title: str = Field(..., description="Title of the prompt")
    
# Define the whole comic book series as a whole
    
class Issue(BaseModel):
    title: str = Field(..., description="Title of the issue")
    issue_number: int = Field(..., description="Issue number of the issue")
    issue_description: str = Field(..., description="Description/Summary of the issue")
    issue_scenes: dict[int,str] = Field(..., description="Scenes of the story the issue visualizes. Key is the page number and value is the scene title as defined in the story chapters.")
    issue_cover_image_prompt: str = Field(..., description="Cover image prompt for the issue")
    number_of_pages: int = Field(..., description="Number of pages in the issue")
    number_of_panels: int = Field(..., description="Number of panels in the issue")
    linked_concept_art_prompts: list[str] = Field(..., description="Concept art prompts that are linked to the issue. The prompts are linked to the issue by the title of the prompt.")
    
class ComicBookSeries(BaseModel):
    title: str = Field(..., description="Title of the comic book series")
    issues: list[Issue] = Field(..., description="Issues of the comic book series")
    concept_art_prompts: list[Prompt] = Field(..., description="Concept art prompts for the comic book series. Includes character concept art, setting concept art, etc. Everything that needs consistency across the series.")
    
    
########################################################
    
class PageLayout(BaseModel):
    issue_number: int = Field(..., description="Issue number of the page layout")
    page_number: int = Field(..., description="Page number of the page layout")
    amount_of_panels: int = Field(..., description="Amount of panels on the page")
    layout_description: str = Field(..., description="Description of the panel layout of the page")
    page_prompt: str = Field(..., description="Prompt for the page")
    story_scene_title: str = Field(..., description="Title of the story scene that is depicted in the page")
    
class IssueLayout(Issue):
    page_layouts: list[PageLayout] = Field(..., description="Page layouts for the issue")
    
    
@flock_type 
class Story(BaseModel):
    title: str
    status: str = Field(default="Idea", description="Idea, Drafting, Revising, Completed")
    genre: list[str] = Field(..., description="Genre(s) of the story")
    tone: str = Field(..., description="Tone of the story") 
    themes: list[str] = Field(..., description="Themes of the story")
    central_conflict: str = Field(..., description="Central conflict of the story")
    brief_summary: str = Field(..., description="Brief summary of the story")
    long_summary: str = Field(..., description="Long-form summary of the story.")
    characters: list[Character] = Field(..., description="Important characters and/or entities of the story")
    chapters: list[Chapter] = Field(..., description="All chapters of the story. At least one chapter per act.")
    
@flock_type 
class StoryBible(BaseModel):
    timeline: dict[str, str]  = Field(..., description="Timeline of the story")
    worldbuilding_notes: dict[str, str]  = Field(..., description="Worldbuilding notes of the story")
    consistency_rules: list[str]  = Field(..., description="Consistency rules of the story")
    writing_reference: Optional[str] = Field(default=None, description="Writing reference and/or style guidelines")

MODEL = "gemini/gemini-2.5-pro-exp-03-25" #"groq/qwen-qwq-32b"    #"openai/gpt-4o" # 
flock = Flock(model=MODEL)


story_agent = FlockFactory.create_default_agent(name="story_agent",
                                              description="An agent that is a master storyteller",
                                              input="story_idea: str",
                                              output="story: Story, story_bible: StoryBible",
                                              max_tokens=60000,
                                              stream=True,
                                              write_to_file=True)


flock.add_agent(story_agent)
result = flock.run(start_agent=story_agent, 
                   input={'story_idea': 
                       'A short story with REACHER, DEADPOOL, BLACK WIDOW and a talking cat.'}) 

```

### examples\cookbook\github-initializer.py

- **Lines**: 103
- **Last modified**: 2025-05-22 18:59:04

**Description**: Tutorial Example: Multi-Agent Chain for Software Project Scaffolding

In this example, we build a chain of Flock agents that collaborate to scaffold a software project.
The workflow is as follows:

  1. **idea_agent:**
     Takes a simple query and returns a fun software project idea.

  2. **project_plan_agent:**
     Uses the software project idea to generate additional project details such as:
       - A catchy project name
       - A project pitch
       - A recommended tech stack
       - A project implementation plan

  3. **readme_agent:**
     Consumes the outputs of the project_plan_agent to produce a readme file.

  4. **issue_agent:**
     Uses the readme and additional project details to create GitHub issues and files.

Each agent is declared using a simple input/output signature, and the chain is established via the `hand_off` property.
Flock manages the registration and execution of agents. In this example, we run the workflow in local debug mode.

Let's see how it all comes together!

**Classes**:
- `Features`: 0 methods

**Content**:
```py
"""
Tutorial Example: Multi-Agent Chain for Software Project Scaffolding

In this example, we build a chain of Flock agents that collaborate to scaffold a software project.
The workflow is as follows:

  1. **idea_agent:**
     Takes a simple query and returns a fun software project idea.

  2. **project_plan_agent:**
     Uses the software project idea to generate additional project details such as:
       - A catchy project name
       - A project pitch
       - A recommended tech stack
       - A project implementation plan

  3. **readme_agent:**
     Consumes the outputs of the project_plan_agent to produce a readme file.

  4. **issue_agent:**
     Uses the readme and additional project details to create GitHub issues and files.

Each agent is declared using a simple input/output signature, and the chain is established via the `hand_off` property.
Flock manages the registration and execution of agents. In this example, we run the workflow in local debug mode.

Let's see how it all comes together!
"""

import asyncio
from dataclasses import dataclass

from flock.core import Flock, FlockFactory
from flock.core.tools import basic_tools
from flock.core.tools.dev_tools import github


@dataclass
class Features:
    title: str
    description: str
    acceptance_criteria: str


async def main():
    flock = Flock()

    idea_agent = FlockFactory.create_default_agent(
        name="idea_agent",
        input="query",
        output="software_project_idea",
        tools=[basic_tools.web_search_tavily],
        use_cache=True,
    )

    project_plan_agent = FlockFactory.create_default_agent(
        name="project_plan_agent",
        input="software_project_idea",
        output="catchy_project_name, project_pitch, techstack, project_implementation_plan",
        tools=[basic_tools.web_search_tavily],
        use_cache=True,
    )

    readme_agent = FlockFactory.create_default_agent(
        name="readme_agent",
        input="catchy_project_name, project_pitch, techstack, project_implementation_plan",
        output="readme",
        tools=[github.upload_readme],
        use_cache=True,
    )

    feature_agent = FlockFactory.create_default_agent(
        name="feature_agent",
        input="readme, catchy_project_name, project_pitch, techstack, project_implementation_plan",
        output="features : list[Features]",
        tools=[github.create_user_stories_as_github_issue, github.create_files],
        use_cache=True,
    )

    issue_agent = FlockFactory.create_default_agent(
        name="issue_agent",
        input="current_feature, readme, techstack, project_implementation_plan, all_feature_titles",
        output="user_stories_on_github, files_on_github",
        tools=[github.create_user_stories_as_github_issue, github.create_files],
        use_cache=True,
    )

    idea_agent.hand_off = project_plan_agent
    project_plan_agent.hand_off = readme_agent
    readme_agent.hand_off = feature_agent

    flock.add_agent(idea_agent)
    flock.add_agent(project_plan_agent)
    flock.add_agent(readme_agent)
    flock.add_agent(feature_agent)
    flock.add_agent(issue_agent)

    features: Features = await flock.run_async(
        start_agent=idea_agent,
    )


if __name__ == "__main__":
    asyncio.run(main())
```

## Design Patterns

The following design patterns appear to be used in this codebase:

### Singleton Pattern

- `scripts\code_collector.py`
- `src\flock\core\flock_registry.py`

### Factory Pattern

- `examples\02-core-concepts\tools\pet_tools.py`
- `examples\05-full-projects\project-textual-project-planner\src\flock_flightplan\cli\utils.py`
- `examples\cookbook\_old\custom-evaluator-example.py`
- `scripts\code_collector.py`
- `scripts\create_docs.py`
- `src\flock\cli\settings.py`
- `src\flock\cli\utils.py`
- `src\flock\core\api\endpoints.py`
- `src\flock\core\api\run_store.py`
- `src\flock\core\flock_factory.py`
- `src\flock\core\logging\formatters\theme_builder.py`
- `src\flock\core\logging\formatters\themed_formatter.py`
- `src\flock\core\mcp\mcp_client.py`
- `src\flock\core\mixin\dspy_integration.py`
- `src\flock\mcp\servers\sse\flock_sse_server.py`
- `src\flock\mcp\servers\stdio\flock_stdio_server.py`
- `src\flock\mcp\servers\websockets\flock_websocket_server.py`
- `src\flock\webapp\app\main.py`
- `src\flock\webapp\app\services\flock_service.py`
- `src\flock\webapp\app\services\sharing_store.py`
- `src\flock\workflow\temporal_setup.py`

### Observer Pattern

- `scripts\code_collector.py`

### Decorator Pattern

- `scripts\code_collector.py`
- `src\flock\core\logging\trace_and_logged.py`

### Mvc Components

**Models**:
- `examples\05-full-projects\project-textual-project-planner\src\flock_flightplan\model.py`
- `src\flock\core\api\models.py`
- `src\flock\webapp\app\models_ui.py`
- `src\flock\webapp\app\services\sharing_models.py`

**Views**:
- `src\flock\cli\view_results.py`
- `src\flock\webapp\app\api\registry_viewer.py`

**Controllers**:
- `src\flock\core\mcp\types\handlers.py`

## All Files

### docs\create_doc_boilerplate.py

- **Lines**: 131
- **Last modified**: 2025-04-16 00:11:15

```py
import os
from pathlib import Path

import yaml

# The mkdocs navigation structure
NAV_STRUCTURE = """
nav:
  - Home: index.md
  
  - Getting Started:
    - Quick Start: getting-started/quickstart.md
    - Installation: getting-started/installation.md
    - Basic Concepts: getting-started/concepts.md
    - Configuration: getting-started/configuration.md
    
  - Core Concepts:
    - Agents: core-concepts/agents.md
    - Type System: core-concepts/type-system.md
    - Workflows: core-concepts/workflows.md
    - Declarative Programming: core-concepts/declarative.md
    - Error Handling: core-concepts/error-handling.md
    
  - Features:
    - Agent Definition: features/agent-definition.md
    - Type Safety: features/type-safety.md
    - Pydantic Integration: features/pydantic.md
    - Agent Chaining: features/agent-chaining.md
    - Lifecycle Hooks: features/lifecycle-hooks.md
    
  - Integrations:
    - Temporal: integrations/temporal.md
    - DSPy: integrations/dspy.md
    - LiteLLM: integrations/litellm.md
    - Tavily: integrations/tavily.md
    
  - Advanced Usage:
    - Custom Agents: advanced/custom-agents.md
    - Complex Workflows: advanced/complex-workflows.md
    - Testing: advanced/testing.md
    - Performance Optimization: advanced/performance.md
    
  - Deployment:
    - Production Setup: deployment/production-setup.md
    - Monitoring: deployment/monitoring.md
    - Scalability: deployment/scalability.md
    - Security: deployment/security.md
    
  - Tutorials:
    - Basic Blog Generator: tutorials/blog-generator.md
    - Multi-Agent Systems: tutorials/multi-agent.md
    - Custom Tool Integration: tutorials/custom-tools.md
    - Error Recovery: tutorials/error-recovery.md
    
  - API Reference:
    - FlockAgent: api/flockagent.md
    - Flock Core: api/flock-core.md
    - Types: api/types.md
    - Utilities: api/utilities.md
    
  - Contributing:
    - Development Setup: contributing/development.md
    - Code Style: contributing/code-style.md
    - Testing Guide: contributing/testing.md
    - Documentation Guide: contributing/documentation.md
    
  - Architecture:
    - Overview: architecture/overview.md
    - Components: architecture/components.md
    - Design Decisions: architecture/design-decisions.md
    
  - Examples:
    - Hello Flock: examples/hello-flock.md
    - Type System Usage: examples/type-system.md
    - Pydantic Models: examples/pydantic.md
    - Chain Gang: examples/chain-gang.md
"""


def create_markdown_file(file_path: Path, title: str) -> None:
    """Create a markdown file with a title and placeholder content."""
    content = f"""# {title}

Documentation in progress...
"""
    file_path.write_text(content)


def extract_paths_from_nav(
    nav_dict: dict, paths: list, current_path: str = ""
) -> None:
    """Recursively extract all markdown file paths from the nav structure."""
    for item in nav_dict:
        if isinstance(item, dict):
            for key, value in item.items():
                if isinstance(value, str):
                    paths.append(value)
                elif isinstance(value, list):
                    extract_paths_from_nav(value, paths, current_path)


def main():
    # Parse the YAML structure
    nav_data = yaml.safe_load(NAV_STRUCTURE)

    # Extract all markdown file paths
    markdown_paths = []
    extract_paths_from_nav(nav_data["nav"], markdown_paths)

    # Create docs directory if it doesn't exist
    docs_dir = Path("docs")
    docs_dir.mkdir(exist_ok=True)

    # Create all necessary directories and markdown files
    for md_path in markdown_paths:
        # Convert path to Path object relative to docs directory
        full_path = docs_dir / md_path

        # Create parent directories if they don't exist
        full_path.parent.mkdir(parents=True, exist_ok=True)

        # Generate title from the filename
        title = os.path.splitext(full_path.name)[0].replace("-", " ").title()

        # Create the markdown file
        create_markdown_file(full_path, title)
        print(f"Created: {full_path}")


if __name__ == "__main__":
    main()
```

### examples\01-getting-started\01-hello-flock.py

- **Lines**: 44
- **Last modified**: 2025-05-25 23:15:24

```py
from flock.core import Flock, FlockFactory

# --------------------------------
# Define the model
# --------------------------------
# Flock uses litellm to talk to LLMs
# Please consult the litellm documentation for valid IDs:
# https://docs.litellm.ai/docs/providers
MODEL = "openai/gpt-4o"


# --------------------------------
# Create the flock and context
# --------------------------------
# The flock is the place where all the agents are at home
flock = Flock(name="hello_flock", description="This is your first flock!", model=MODEL)

# --------------------------------
# Create an agent
# --------------------------------
# The Flock doesn't believe in prompts (see the docs for more info)
# The Flock just declares what agents get in and what agents produce
# my_presentation_agent takes in a topic and outputs a
# funny_title, fun_slide_headers and fun_slide_summaries
presentation_agent = FlockFactory.create_default_agent(
    name="my_presentation_agent",
    input="topic",
    output="fun_title, fun_slide_headers, fun_slide_summaries",
)
flock.add_agent(presentation_agent)


# --------------------------------
# Run the flock
# --------------------------------
# Tell the flock who the starting agent is and what input to give it
flock.run(
    start_agent=presentation_agent,
    input={"topic": "A presentation about robot kittens"},
)

# YOUR TURN!
# Try changing the output definition (line 29) by replacing "fun" with "boring"
# (boring_title, boring_slide_headers, boring_slide_summaries)
```

### examples\01-getting-started\02-inputs-and-outputs.py

- **Lines**: 50
- **Last modified**: 2025-05-22 18:59:04

```py
from flock.cli.utils import print_header, print_subheader, print_success
from flock.core import Flock, FlockFactory

MODEL = "openai/gpt-4o"

flock = Flock(
    name="example_02", description="The flock input and output syntax", model=MODEL
)


# --------------------------------
# Define the agent
# --------------------------------
# We claimed that flock doesn't care about prompts.
# This was a little bit of clickbait of course.
# Flock cares very much about prompts.
# But with flock you don't prompt the agent as a whole,
# but the properties of the input and output.
#
# General syntax rules:
# "field_name: type | description"
#
# If you need to specify the agents behavior, you can do so with the description field.
presentation_agent = FlockFactory.create_default_agent(
    name="my_movie_agent",
    description="Creates a fun movie about a given topic",  # Isn't just a description, but also a control mechanism
    input="topic: str",
    output="fun_title: str | The funny title of the movie in all caps, "
    "runtime: int | The runtime of the movie in minutes, "
    "synopsis: str | A crazy over the top synopsis of the movie, "
    "characters: list[dict[str, str]] | Key is character name, Value are character description ",
)
flock.add_agent(presentation_agent)


result = flock.run(start_agent=presentation_agent, input={"topic": "AI agents"})

# --------------------------------
# The result
# --------------------------------
# The result is a boxed dictionary which enables dot-notation access to the fields.

print_header("Results")
print_subheader(result.fun_title)
print_success(result.synopsis)
print_success(result.characters)

# YOUR TURN!
# Try changing the types and descriptions of the input and output fields
# What happens if agent description is at odds with the input and output fields?
```

### examples\01-getting-started\03-using-a-tool.py

- **Lines**: 78
- **Last modified**: 2025-05-25 23:15:24

```py
from flock.core import Flock, FlockFactory
from flock.core.logging.formatters.themes import OutputTheme
from flock.tools import code_tools, web_tools

# Define the DEFAULT_MODEL in your .env file
flock = Flock(model="openai/gpt-4o")

# --------------------------------
# Create an agent
# --------------------------------
# Some additions to example 01
# - you can define the output types of the agent with standard python type hints
# - you can define the tools the agent can use
# - you can define if the agent should use the cache
#   results will get cached and if true and if the input is the same as before, the agent will return the cached result
#   this is useful for expensive operations like web scraping and for debugging
# Some people need some swag in their output
# Flock supports rendering the output as a table and you can choose a theme (out of like 300 or so)
agent = FlockFactory.create_default_agent(
    name="my_agent",
    input="url",
    output="title, headings: list[str],"
    "entities_and_metadata: list[dict[str, str]],"
    "type:Literal['news', 'blog', 'opinion piece', 'tweet']",
    tools=[web_tools.web_content_as_markdown],
    enable_rich_tables=True,  # Instead of the json output, you can use the rich library to render the output as a table
    output_theme=OutputTheme.aardvark_blue,  # flock also comes with a few themes
    use_cache=True,  # flock will cache the result of the agent and if the input is the same as before, the agent will return the cached result
    wait_for_input=True,  # flock will wait for the user to press enter before continuing after this agent's run
)
flock.add_agent(agent)


# --------------------------------
# Tools = ReAct Agent
# --------------------------------
# If tools are used, the agent will be a ReAct Agent
# ReAct Agents are agents that can use tools to solve tasks
# by planning steps, executing them and evaluating the results.
# With 'include_thought_process=True', the agent will include the thought process in the output
# This is useful for debugging and for understanding the agent's thought process
age_agent = FlockFactory.create_default_agent(
    name="my_celebrity_age_agent",
    input="a_person",
    output="persons_age_in_days",
    tools=[web_tools.web_search_duckduckgo, code_tools.code_code_eval],
    enable_rich_tables=True,
    output_theme=OutputTheme.homebrew,
    use_cache=True,  # flock will cache the result of the agent and if the input is the same as before, the agent will return the cached result
    include_thought_process=True,  # flock will include the thought process of the agent in the output if available
)
flock.add_agent(age_agent)


# --------------------------------
# Run the agent
# --------------------------------
# ATTENTION: Big table incoming
# It's worth it tho!
result = flock.run(
    start_agent=agent,
    input={
        "url": "https://lite.cnn.com/travel/alexander-the-great-macedon-persian-empire-darius/index.html"
    },
)

# To start a different agent, you can do so by calling flock.run() again with a different start_agent

result = flock.run(
    start_agent=age_agent,
    input={"a_person": "Brad Pitt"},
)

# --------------------------------
# YOUR TURN
# --------------------------------
# - Create a small research agent that can search the web for a given topic, convert the output to markdown and then use the markdown to create a beautiful report
# - Explore some of the other tools flock has to offer
```

### examples\01-getting-started\04-flock-architecture.py

- **Lines**: 67
- **Last modified**: 2025-05-22 18:59:04

```py
from flock.core import Flock, FlockAgent
from flock.evaluators.declarative.declarative_evaluator import (
    DeclarativeEvaluatorConfig,
)
from flock.modules.output.output_module import OutputModuleConfig
from flock.modules.performance.metrics_module import (
    MetricsModuleConfig,
)

MODEL = "openai/gpt-4o"
flock = Flock(name="flock_architecture", model=MODEL)

# --------------------------------
# The anatomy of a flock agent
# --------------------------------
# So far we created agents with the create_default_agent function
# This function creates an agent with a default configuration and component set up
# The default agent includes the following modules:
#         - DeclarativeEvaluator
#         - OutputModule
#         - MetricsModule
#
# Let's create an agent from scratch and look into how an agent is built
# This is the most basic agent that you can create and it'll do nothing
# An agent on its own is basically a container for the following components:
# - input/output contract, which defines what kind of data the agent can handle and what it (should) produce
# - tools, which are the tools (eg. functions) that the agent can use
# - evaluator, which defines how an agent evaluates itself to produce an output
# - modules, which can trigger during the agent's lifecycle (like on_initialize, on_post_evaluate, etc.)
#   think modules = functionality YOU as the dev want to control
#         tools = functionality THE AGENT controls
# - handoff_router, which defines what the agent should do with the output (usually give it to the next agent)

empty_agent = FlockAgent(
    name="my_naked_agent",
    input="query",
    output="answer",
    tools=None,
    evaluator=None,
    modules=None,
    handoff_router=None,
)

# Flock's core library comes with a few modules and evaluators
# You can find more about them in the docs:
# ...

# To live the declarative philosophy you just need to pass the config instance to the .add_component method
# You tell Flock what you want. Flock handles the rest
# An agent can have multiple modules but it can only have one evaluator and one handoff_router
empty_agent.add_component(
    config_instance=DeclarativeEvaluatorConfig(), component_name="declarative_evaluator"
)
empty_agent.add_component(
    config_instance=OutputModuleConfig(), component_name="output_module"
)
empty_agent.add_component(
    config_instance=MetricsModuleConfig(), component_name="metrics_module"
)

flock.add_agent(empty_agent)


flock.run(
    start_agent=empty_agent,
    input={"query": "Hello"},
)
```

### examples\02-core-concepts\01-pydantic-types.py

- **Lines**: 178
- **Last modified**: 2025-05-22 18:59:04

```py
# 02-core-concepts/01-pydantic-types.py
"""
Purpose: Demonstrate using Pydantic models to define structured output.

Use Case: Fantasy RPG Character Generator 🧙‍♂️ - Generate structured fantasy character data.

Highlights:
- Define a Pydantic model (`FantasyCharacter`) with typed fields.
- Use the `@flock_type` decorator to register the custom type.
- Define a Flock agent whose `output` signature references the Pydantic model (`list[FantasyCharacter]`).
- Flock automatically instructs the LLM to generate data matching the Pydantic schema.
- The result object contains actual instances of the Pydantic model.
"""

import os
from pprint import pprint  # Using pprint for cleaner dict/list printing
from typing import Literal

from flock.cli.utils import print_header, print_subheader, print_warning
from flock.core import Flock, FlockFactory
from flock.core.flock_registry import (
    flock_type,  # Decorator for registering custom types
)
from pydantic import BaseModel, Field  # Import Pydantic components
from rich.console import Console

# --- Configuration ---
console = Console()
MODEL = os.getenv("DEFAULT_MODEL", "openai/gpt-4o")
console.print(f"[grey50]Using model: {MODEL}[/grey50]")


# --------------------------------
# Define the Pydantic data model
# --------------------------------
# Use the @flock_type decorator so Flock knows about this custom class
# and can use it in signatures and handle serialization/deserialization.
# Check the cookbook for an example of handling images as well in 'working_with_images.py'
@flock_type
class FantasyCharacter(BaseModel):
    """Data model for fantasy RPG character information.
    Docstrings and Field descriptions can help guide the LLM.
    """

    name: str = Field(..., description="A creative fantasy character name.")
    race: Literal["human", "elf", "dwarf", "orc", "halfling"] = Field(
        ..., description="The character's race."
    )
    class_type: Literal["warrior", "mage", "rogue", "cleric", "ranger"] = Field(
        ..., description="The character's class."
    )
    level: int = Field(..., description="Character level")
    strength: int = Field(..., description="Strength stat")
    dexterity: int = Field(..., description="Dexterity stat")
    constitution: int = Field(..., description="Constitution stat")
    intelligence: int = Field(..., description="Intelligence stat")
    wisdom: int = Field(..., description="Wisdom stat")
    charisma: int = Field(..., description="Charisma stat")
    weapons: list[str] = Field(
        ..., description="A list of weapons the character carries."
    )
    backstory: str = Field(
        ..., description="A brief, engaging backstory (2-3 sentences)."
    )
    motivation: str = Field(
        ..., description="The character's motivation for their adventuring."
    )
    alignment: str = Field(
        ...,
        description="Character's moral alignment",
    )


# --------------------------------
# Create a new Flock instance
# --------------------------------
flock = Flock(name="pydantic_example", model=MODEL, show_flock_banner=False)

# --------------------------------
# Define the Agent using the Pydantic type
# --------------------------------
# The agent is responsible for generating a list of fantasy characters.
# Input: "number_of_characters" (string or int).
# Output: "character_list" which MUST be a list of FantasyCharacter objects.
character_agent = FlockFactory.create_default_agent(
    name="character_agent",
    description="Generates fantasy RPG character profiles for a specified number of characters.",
    input="number_of_characters: int | The number of fantasy character profiles to generate.",
    # Crucially, the output type refers to our registered Pydantic model:
    output="character_list: list[FantasyCharacter] | A list containing the generated character profiles.",
    temperature=0.8,  # Increase temperature slightly for more varied profiles
)
flock.add_agent(character_agent)


# --------------------------------
# Run the Flock
# --------------------------------
print_header("Generating Fantasy RPG Character Profiles")
num_to_generate = 2  # Let's generate just 2 for a quicker example
console.print(f"Requesting {num_to_generate} character profiles...")

try:
    result = flock.run(
        start_agent="character_agent",
        input={"number_of_characters": num_to_generate},
    )

    # --------------------------------
    # Display the results
    # --------------------------------
    print_subheader("Generated Character Profiles")
    if hasattr(result, "character_list") and isinstance(result.character_list, list):
        console.print(f"Received {len(result.character_list)} character profiles:")

        for i, character in enumerate(result.character_list):
            print_subheader(f"Character {i + 1}")
            # Access attributes directly - they are FantasyCharacter instances!
            if isinstance(character, FantasyCharacter):
                console.print(f"  [cyan]Name:[/cyan] {character.name}")
                console.print(f"  [cyan]Race:[/cyan] {character.race}")
                console.print(f"  [cyan]Class:[/cyan] {character.class_type}")
                console.print(f"  [cyan]Level:[/cyan] {character.level}")
                console.print(f"  [cyan]Alignment:[/cyan] {character.alignment}")
                console.print("\n  [cyan]Stats:[/cyan]")
                console.print(f"    Strength: {character.strength}")
                console.print(f"    Dexterity: {character.dexterity}")
                console.print(f"    Constitution: {character.constitution}")
                console.print(f"    Intelligence: {character.intelligence}")
                console.print(f"    Wisdom: {character.wisdom}")
                console.print(f"    Charisma: {character.charisma}")
                console.print(f"\n  [cyan]Weapons:[/cyan] {character.weapons}")
                console.print(f"  [cyan]Backstory:[/cyan] {character.backstory}\n")
                # Verify the type
                console.print(f"  [grey50](Object Type: {type(character)})[/grey50]\n")
            else:
                print_warning(
                    f"Item {i + 1} in list is not a FantasyCharacter object: {type(character)}"
                )
                pprint(character)  # Print raw if not expected type
    else:
        print_warning(
            "Agent did not return the expected 'character_list' field or it wasn't a list."
        )
        print_warning("Raw result:")
        pprint(result)  # Print the raw result if structure is wrong

except Exception as e:
    print_warning(f"Agent run failed: {e}")
    print_warning("Please ensure your API key is set and the model is accessible.")


# --- YOUR TURN! ---
# 1. Modify the `FantasyCharacter` Pydantic model:
#    - Add a new field, e.g., `homeland: str = Field(..., description="The character's place of origin")`.
#    - Change an existing field's type, e.g., make `level` a `float` to represent characters with partial levels.
#
#    The LLM will probably try to generate values fitting Dungeons and Dragons rules. Like all the stats being between 3 and 18.
#    - Add validation to a field:
#      For example, expand level range with Field(gt=20, le=100)
#      Try the following for weapons: Field(..., min_items=1, max_items=4, description="Between 1-4 weapons the character carries")
#      Try the following for alignment: Field(..., pattern=r'^(Lawful|Neutral|Chaotic) (Good|Neutral|Evil)$')
#      Remove everything good in the alignment field to make a 'bad character' generator.
#    - Try adding following properties to the model:
#      - character_id: str = Field(..., description="A unique identifier for the character.", pattern=r"(?:([+]\d{1,4})[-.\s]?)?(?:[(](\d{1,3})[)][-.\s]?)?(\d{1,4})[-.\s]?(\d{1,4})[-.\s]?(\d{1,9})")
#      What kind of format does the LLM use for the character_id?
#    Run the script again. Does the LLM adapt to generate the new field or respect the new type/validation?
#
# 2. Change the `output` signature of the `character_agent`:
#    - Instead of `List[FantasyCharacter]`, try asking for just one: `the_character: FantasyCharacter`
#      (You'll need to adjust the result processing code accordingly).
#    - Try asking for a dictionary mapping names to profiles: `characters_by_name: Dict[str, FantasyCharacter]`
#      (Again, adjust the result processing).
#
# 3. Introduce a nested Pydantic model:
#    - Define a new `@flock_type class Equipment(BaseModel): armor: str; magical_items: list[str]`.
#    - Add `equipment: Equipment` as a field to `FantasyCharacter`.
#    Run the script. Does Flock handle generating the nested structure?
```

### examples\02-core-concepts\02-declarative-vs-prompting.py

- **Lines**: 594
- **Last modified**: 2025-05-25 23:46:23

```py
# 02-core-concepts/02-declarative-vs-prompting-alt.py
"""
Purpose: Demonstrate the difference between Flock's declarative approach
         and traditional detailed prompting for structured data extraction.

Use Case: News Article Analyzer 📰 - Extract structured data from news articles.

Highlights:
- The declarative approach defines the desired output structure using Pydantic models
- The traditional approach requires detailed instructions for each extraction step
- Both approaches extract the same information, but with different levels of complexity
- Shows how easy it is to modify and extend the declarative approach
"""

import os
from datetime import datetime
from typing import List, Literal, Optional

import litellm  # For the traditional prompt example
from flock.cli.utils import print_header, print_subheader, print_success, print_warning
from flock.core import Flock, FlockFactory
from flock.core.flock_registry import flock_type
from pydantic import BaseModel, Field
from rich.console import Console
from rich.panel import Panel
from rich.table import Table

# --- Configuration ---
console = Console()
MODEL = os.getenv("DEFAULT_MODEL", "openai/gpt-4o")
console.print(f"[grey50]Using model: {MODEL}[/grey50]")

# --- Sample News Article ---
SAMPLE_ARTICLE = """
Business Tech • 3 min read
OpenAI to remain under non-profit control in change of restructuring plans
By Reuters

Updated: 3:34 PM EDT, Mon May 5, 2025

Source: CNN

OpenAI said on Monday it would remain under the control of its nonprofit parent, while pushing ahead with plans to change the structure of its for-profit arm to allow more capital-raising to keep pace in the artificial-intelligence race.

The announcement follows a storm of criticism and legal challenges, including a high-profile lawsuit filed by rival Elon Musk, who has accused the company of straying from its founding mission to develop AI for the benefit of humanity.

“We made the decision for the nonprofit to stay in control after hearing from civic leaders and having discussions with the offices of the Attorneys General of California and Delaware,” the startup said in a blog post. It added that it would work with major backer Microsoft, regulators and newly appointed nonprofit commissioners to finalize the updated plan.

The company had outlined plans in December to revamp its structure to become a public benefit corporation, saying that would help it to “raise more capital than we’d imagined,” and remove the restrictions imposed on the startup by its current nonprofit parent.

The move had raised concerns about whether OpenAI would allocate its assets to the nonprofit arm fairly, and how the company would strike a balance between making a profit and generating social and public good as it develops AI.

On Monday, OpenAI said the nonprofit parent will control the public benefit corporation and become a big shareholder in it.

Nonprofits are created to serve the public good, while public benefit corporations typically offer more flexibility to pursue profit alongside social goals.

Bret Taylor, chairman of OpenAI’s board, said Monday’s announcement means the startup will continue to have a structure “extremely close” to the current one.

CEO Sam Altman described it as a compromise “that (works) well enough for investors that they’re happy to continue to fund us to a degree we think we will need.”

As the expensive pursuit of artificial general intelligence, or AI that surpasses human intelligence, heats up, OpenAI has been looking to make changes to attract further investment.

It said in March it would raise up to $40 billion in a new funding round led by SoftBank Group at a $300 billion valuation. The round was contingent on the AI firm transitioning to for-profit status by the end of the year.

SoftBank and Microsoft did not immediately respond to requests for comment.

Some analysts said the move could hamper the startup’s ability to raise capital as aggressively as it could have without the nonprofit control.

“The nonprofit status significantly reduces OpenAI’s ability to raise capital, as investors would want to be able to generate a return on their investment, which is considerably harder if a non-profit controls a commercial entity,” said Gil Luria, analyst at D.A. Davidson.

OpenAI’s structure drew attention in November 2023 during one of the biggest boardroom dramas in Silicon Valley, where members of the nonprofit board ousted Altman over a breakdown in communication and loss of trust. He was reinstated after five days, following an outpouring of support from employees and investors.
"""


# --- Pydantic Models for Structured Data ---
@flock_type
class Person(BaseModel):
    """A person mentioned in the article."""

    name: str = Field(..., description="Full name of the person")
    role: str = Field(..., description="Role or title of the person")
    organization: Optional[str] = Field(
        None, description="Organization the person is affiliated with"
    )
    quotes: List[str] = Field(
        default_factory=list,
        description="Direct quotes from this person in the article",
    )


@flock_type
class Event(BaseModel):
    """An event described in the article."""

    name: str = Field(..., description="Name or title of the event")
    date: Optional[str] = Field(
        None, description="Date when the event occurred or will occur"
    )
    location: Optional[str] = Field(
        None, description="Location where the event took place or will take place"
    )
    attendees: Optional[int] = Field(
        None, description="Number of attendees if mentioned"
    )
    description: str = Field(..., description="Brief description of the event")


@flock_type
class Product(BaseModel):
    """A product mentioned in the article."""

    name: str = Field(..., description="Name of the product")
    company: str = Field(..., description="Company that makes the product")
    description: str = Field(..., description="Description of the product")
    release_date: Optional[str] = Field(None, description="Release date if mentioned")
    pricing: Optional[str] = Field(None, description="Pricing information if mentioned")
    features: List[str] = Field(
        default_factory=list, description="Key features of the product"
    )


@flock_type
class FinancialData(BaseModel):
    """Financial information mentioned in the article."""

    entity: str = Field(
        ..., description="Company or organization the financial data relates to"
    )
    data_type: str = Field(
        ..., description="Type of financial data (e.g., stock price, investment)"
    )
    value: str = Field(
        ..., description="The financial value with currency or percentage"
    )
    date: Optional[str] = Field(
        None, description="Date associated with the financial data"
    )
    context: str = Field(
        ..., description="Brief context about this financial information"
    )


@flock_type
class ArticleMetadata(BaseModel):
    """Metadata about the article itself."""

    title: str = Field(..., description="Title of the article")
    author: Optional[str] = Field(None, description="Author of the article")
    publication_date: Optional[str] = Field(
        None, description="Original publication date"
    )
    update_date: Optional[str] = Field(
        None, description="Last update date if different from publication"
    )
    category: str = Field(..., description="Category or topic of the article")


@flock_type
class ArticleAnalysis(BaseModel):
    """Complete structured analysis of a news article."""

    metadata: ArticleMetadata = Field(..., description="Metadata about the article")
    people: List[Person] = Field(..., description="People mentioned in the article")
    events: List[Event] = Field(..., description="Events described in the article")
    products: List[Product] = Field(
        ..., description="Products mentioned in the article"
    )
    financial_data: List[FinancialData] = Field(
        default_factory=list, description="Financial information mentioned"
    )
    key_points: List[str] = Field(
        ..., description="Key points or takeaways from the article"
    )
    sentiment: Literal["positive", "negative", "neutral", "mixed"] = Field(
        ..., description="Overall sentiment of the article"
    )


# --- Flock (Declarative) Approach ---


def run_flock_extraction(article_text: str):
    """Extracts structured data from an article using the Flock declarative approach."""
    # 1. Create Flock Instance
    flock = Flock(name="article_analyzer", model=MODEL, show_flock_banner=False)

    print_subheader("Flock (Declarative) Approach")

    # 2. Define Agent Declaratively
    # Notice how we define WHAT we want (input, output, description)
    # We don't need to explain HOW to extract each piece of data
    # Imagine the following:
    # Instead of defining our complete system by natural language we describe each property of each object
    # This granularity provides a lot of flexibility and allows us to add or change properties without changing the existing system
    article_analyzer = FlockFactory.create_default_agent(
        name="article_analyzer",
        description="Extracts structured information from news articles.",
        input="article_text: str | The full text of the news article to analyze",
        output="analysis: ArticleAnalysis | A complete structured analysis of the article content",
        temperature=0.2,  # Lower temperature for more consistent extraction
        use_cache=False,  # Disable caching for this example
    )
    flock.add_agent(article_analyzer)

    # 3. Run the Flock
    try:
        console.print("Running Flock agent for structured data extraction...")
        start_time = datetime.now()

        result = flock.run(
            start_agent=article_analyzer, input={"article_text": article_text}
        )

        end_time = datetime.now()
        execution_time = (end_time - start_time).total_seconds()

        print_success(f"Extraction completed in {execution_time:.2f} seconds")

        # 4. Display the results in a structured format
        if hasattr(result, "analysis") and isinstance(result.analysis, ArticleAnalysis):
            display_article_analysis(result.analysis, "cyan")
            return result.analysis
        else:
            print_warning("Agent did not return the expected ArticleAnalysis structure")
            return None

    except Exception as e:
        print_warning(f"Flock agent failed: {e}")
        print_warning("Ensure your API key is set and the model is accessible.")
        return None


# --- Traditional Prompting Approach ---


def run_traditional_extraction(article_text: str):
    """Extracts structured data from an article using traditional detailed prompting."""
    print_subheader("Traditional Prompting Approach")

    # 1. Craft the Detailed Prompt
    # Notice how much more explicit instruction is needed compared to the
    # Flock agent's declarative approach
    system_prompt = """You are an expert system for extracting structured information from news articles.
Your task is to analyze the provided article and extract specific data points in a JSON format."""

    user_prompt = f"""
Analyze the following news article and extract structured information according to these detailed instructions:

{article_text}

Please extract the following information in JSON format:

1. Article Metadata:
   - title: The main headline of the article
   - author: The full name of the author if mentioned
   - publication_date: The original publication date in the format mentioned
   - update_date: The last update date if different from publication date
   - category: The main topic category (e.g., Technology, Business, Politics)

2. People Mentioned:
   For each person mentioned in the article, extract:
   - name: Their full name
   - role: Their job title or role
   - organization: The company or organization they're affiliated with
   - quotes: Any direct quotes attributed to them (as a list of strings)

3. Events:
   For each significant event described, extract:
   - name: Name or title of the event
   - date: When the event occurred or will occur
   - location: Where the event took place or will take place
   - attendees: Number of attendees if mentioned
   - description: Brief description of what the event is

4. Products:
   For each product mentioned, extract:
   - name: Name of the product
   - company: Company that makes the product
   - description: Description of what the product is or does
   - release_date: When the product will be released if mentioned
   - pricing: Any pricing information mentioned
   - features: Key features of the product (as a list of strings)

5. Financial Data:
   For each piece of financial information, extract:
   - entity: The company or organization the data relates to
   - data_type: Type of financial data (stock price, investment, etc.)
   - value: The actual value with currency or percentage
   - date: Date associated with the financial data
   - context: Brief context about this financial information

6. Key Points:
   - A list of 3-5 key points or main takeaways from the article

7. Sentiment:
   - The overall sentiment of the article (positive, negative, neutral, or mixed)

Format your response as a valid JSON object with these exact keys. If information for a field is not available in the article, use null for that field or an empty list [] for list fields.
"""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt},
    ]

    # 2. Call the LLM directly
    try:
        console.print("Running direct LLM call for structured data extraction...")
        start_time = datetime.now()

        response = litellm.completion(
            model=MODEL,
            messages=messages,
            temperature=0.2,  # Match temperature for fairness
            max_tokens=2000,  # Extraction needs more tokens
        )

        end_time = datetime.now()
        execution_time = (end_time - start_time).total_seconds()

        print_success(f"Extraction completed in {execution_time:.2f} seconds")

        # 3. Process and display the results
        import json

        try:
            extraction_result = json.loads(response.choices[0].message.content.strip())
            # Convert to our Pydantic model for consistent display
            analysis = convert_json_to_article_analysis(extraction_result)
            display_article_analysis(analysis, "magenta")
            return analysis
        except json.JSONDecodeError:
            print_warning("Failed to parse JSON response from traditional approach")
            console.print(response.choices[0].message.content.strip())
            return None

    except Exception as e:
        print_warning(f"Traditional prompt failed: {e}")
        print_warning("Ensure your API key is set and the model is accessible.")
        return None


def convert_json_to_article_analysis(json_data):
    """Converts JSON data from traditional approach to ArticleAnalysis model."""
    # This function helps ensure consistent display between both approaches

    # Convert metadata
    metadata = ArticleMetadata(
        title=json_data.get("title", ""),
        author=json_data.get("author"),
        publication_date=json_data.get("publication_date"),
        update_date=json_data.get("update_date"),
        category=json_data.get("category", "Unknown"),
    )

    # Convert people
    people = []
    for person_data in json_data.get("people", []):
        people.append(
            Person(
                name=person_data.get("name", ""),
                role=person_data.get("role", ""),
                organization=person_data.get("organization"),
                quotes=person_data.get("quotes", []),
            )
        )

    # Convert events
    events = []
    for event_data in json_data.get("events", []):
        events.append(
            Event(
                name=event_data.get("name", ""),
                date=event_data.get("date"),
                location=event_data.get("location"),
                attendees=event_data.get("attendees"),
                description=event_data.get("description", ""),
            )
        )

    # Convert products
    products = []
    for product_data in json_data.get("products", []):
        products.append(
            Product(
                name=product_data.get("name", ""),
                company=product_data.get("company", ""),
                description=product_data.get("description", ""),
                release_date=product_data.get("release_date"),
                pricing=product_data.get("pricing"),
                features=product_data.get("features", []),
            )
        )

    # Convert financial data
    financial_data = []
    for finance_data in json_data.get("financial_data", []):
        financial_data.append(
            FinancialData(
                entity=finance_data.get("entity", ""),
                data_type=finance_data.get("data_type", ""),
                value=finance_data.get("value", ""),
                date=finance_data.get("date"),
                context=finance_data.get("context", ""),
            )
        )

    # Create the full analysis
    analysis = ArticleAnalysis(
        metadata=metadata,
        people=people,
        events=events,
        products=products,
        financial_data=financial_data,
        key_points=json_data.get("key_points", []),
        sentiment=json_data.get("sentiment", "neutral"),
    )

    return analysis


def display_article_analysis(analysis, color):
    """Displays the extracted article analysis in a structured format."""
    # Display metadata
    metadata_panel = Panel(
        f"[bold]Title:[/bold] {analysis.metadata.title}\n"
        f"[bold]Author:[/bold] {analysis.metadata.author or 'Not specified'}\n"
        f"[bold]Published:[/bold] {analysis.metadata.publication_date or 'Not specified'}\n"
        f"[bold]Updated:[/bold] {analysis.metadata.update_date or 'Not specified'}\n"
        f"[bold]Category:[/bold] {analysis.metadata.category}",
        title="Article Metadata",
        border_style=color,
    )
    console.print(metadata_panel)

    # Display people
    if analysis.people:
        people_table = Table(title="People Mentioned", border_style=color)
        people_table.add_column("Name", style="bold")
        people_table.add_column("Role")
        people_table.add_column("Organization")
        people_table.add_column("Quotes")

        for person in analysis.people:
            quotes = (
                "\n".join([f"• {quote}" for quote in person.quotes])
                if person.quotes
                else "None"
            )
            people_table.add_row(
                person.name, person.role, person.organization or "Not specified", quotes
            )

        console.print(people_table)

    # Display events
    if analysis.events:
        events_table = Table(title="Events", border_style=color)
        events_table.add_column("Name", style="bold")
        events_table.add_column("Date")
        events_table.add_column("Location")
        events_table.add_column("Attendees")
        events_table.add_column("Description")

        for event in analysis.events:
            events_table.add_row(
                event.name,
                event.date or "Not specified",
                event.location or "Not specified",
                str(event.attendees) if event.attendees else "Not specified",
                event.description,
            )

        console.print(events_table)

    # Display products
    if analysis.products:
        products_table = Table(title="Products", border_style=color)
        products_table.add_column("Name", style="bold")
        products_table.add_column("Company")
        products_table.add_column("Description")
        products_table.add_column("Release Date")
        products_table.add_column("Pricing")
        products_table.add_column("Features")

        for product in analysis.products:
            features = (
                "\n".join([f"• {feature}" for feature in product.features])
                if product.features
                else "None"
            )
            products_table.add_row(
                product.name,
                product.company,
                product.description,
                product.release_date or "Not specified",
                product.pricing or "Not specified",
                features,
            )

        console.print(products_table)

    # Display financial data
    if analysis.financial_data:
        finance_table = Table(title="Financial Data", border_style=color)
        finance_table.add_column("Entity", style="bold")
        finance_table.add_column("Type")
        finance_table.add_column("Value")
        finance_table.add_column("Date")
        finance_table.add_column("Context")

        for finance in analysis.financial_data:
            finance_table.add_row(
                finance.entity,
                finance.data_type,
                finance.value,
                finance.date or "Not specified",
                finance.context,
            )

        console.print(finance_table)

    # Display key points and sentiment
    key_points = "\n".join([f"• {point}" for point in analysis.key_points])
    summary_panel = Panel(
        f"[bold]Key Points:[/bold]\n{key_points}\n\n"
        f"[bold]Overall Sentiment:[/bold] {analysis.sentiment.capitalize()}",
        title="Summary",
        border_style=color,
    )
    console.print(summary_panel)


# --- Main Execution ---


def run_comparison(article_text):
    """Run both approaches and compare them."""
    print_header("News Article Structured Data Extraction Comparison")

    # Run Flock approach
    flock_result = run_flock_extraction(article_text)

    console.print("\n" + "=" * 80 + "\n")  # Separator

    # Run traditional approach
    traditional_result = run_traditional_extraction(article_text)

    # Show comparison summary
    print_header("Comparison Summary")
    console.print("\n[bold]Declarative (Flock) Approach:[/bold]")
    console.print("✓ Defines the desired output structure using Pydantic models")
    console.print("✓ No need to explain HOW to extract each piece of data")
    console.print("✓ Easy to modify by simply updating the Pydantic models")
    console.print("✓ Validation happens automatically through the type system")

    console.print("\n[bold]Traditional Prompting Approach:[/bold]")
    console.print("✗ Requires detailed instructions for each extraction step")
    console.print("✗ Needs to explain both WHAT to extract and HOW to extract it")
    console.print("✗ Modifications require rewriting the entire prompt")
    console.print("✗ No automatic validation; results may vary in structure")

    console.print("\nThe declarative approach is particularly advantageous when:")
    console.print("1. The output structure is complex with many nested fields")
    console.print("2. Requirements change frequently, requiring schema updates")
    console.print("3. Consistency across multiple runs is important")
    console.print("4. You want to focus on WHAT you need rather than HOW to get it")


if __name__ == "__main__":
    run_comparison(SAMPLE_ARTICLE)

# --- YOUR TURN! ---
# 1. Try with a different article:
#    - Find a news article online and replace SAMPLE_ARTICLE with its content
#    - Run the script again and see how both approaches handle different content
#    - Try to use flock's tools to extract data from https://lite.cnn.com/
#    - What does it mean for the complexity of the 'classic' approach?
#
# 2. Modify the extraction requirements:
#    - For the declarative approach, add a new field to one of the Pydantic models
#      (e.g., add "tone: Literal['formal', 'casual']" to ArticleMetadata)
#    - For the traditional approach, add the same requirement to the detailed prompt
#    - Compare how much easier it is to modify the declarative approach
#
# 3. Add a new entity type:
#    - Create a new Pydantic model for something else to extract (e.g., Organizations)
#    - Add it to the ArticleAnalysis model
#    - Update the traditional prompt to extract the same information
#    - Compare the effort required for both approaches
#
# 4. Try extracting from multiple articles:
#    - Create a list of articles and run both approaches on each
#    - See how consistent the results are across different inputs
```

### examples\02-core-concepts\03-simple-chaining.py

- **Lines**: 232
- **Last modified**: 2025-05-22 18:59:04

```py
# 02-core-concepts/03-simple-chaining-alt.py
"""
Purpose: Demonstrate how to chain multiple agents together to solve complex tasks.

Use Case: Fantasy Character Adventure Generator 🧙‍♂️🐉 - Create a character,
         generate an adventure, and narrate an epic conclusion.

Highlights:
- Chain multiple specialized agents together to build a complete workflow
- Each agent focuses on a specific task and passes its output to the next agent
- Show different ways to connect agents (default router, LLM router)
- Demonstrate how complex creative tasks can be broken down into manageable steps
"""

import os

from flock.cli.utils import print_header, print_subheader, print_success, print_warning
from flock.core import Flock, FlockFactory
from flock.core.logging.formatters.themes import OutputTheme
from flock.routers.default.default_router import DefaultRouterConfig
from rich.console import Console
from rich.panel import Panel

# --- Configuration ---
console = Console()
MODEL = os.getenv("DEFAULT_MODEL", "openai/gpt-4o")
console.print(f"[grey50]Using model: {MODEL}[/grey50]")

# --- Create Flock Instance ---
flock = Flock(
    name="adventure_generator",
    model=MODEL,
    show_flock_banner=False,
)

# --------------------------------
# Step 1: Create Character Generator Agent
# --------------------------------
# This agent creates a fantasy character based on user preferences
character_agent = FlockFactory.create_default_agent(
    name="character_generator",
    description="Creates detailed fantasy characters based on user preferences.",
    input="character_type: str | The general type of character the user wants to create",
    output="character_name: str | The character's full name, "
    "character_race: str | The character's fantasy race, "
    "character_class: str | The character's profession or class, "
    "character_background: str | A brief backstory for the character, "
    "character_traits: list[str] | Three to five personality traits, "
    "character_abilities: list[str] | Special abilities or skills",
    temperature=0.7,  # Higher temperature for more creative characters
    enable_rich_tables=True,
    output_theme=OutputTheme.batman,
    use_cache=False,  # Disable caching for this agent
    wait_for_input=True,  # Pause after generating the character
)

# --------------------------------
# Step 2: Create Adventure Generator Agent
# --------------------------------
# This agent creates an adventure scenario based on the character
adventure_agent = FlockFactory.create_default_agent(
    name="adventure_generator",
    description="Creates an adventure scenario tailored to a specific character.",
    input="character_name: str | The character's name,"
    "character_race: str | The character's race,"
    "character_class: str | The character's class,"
    "character_background: str | The character's backstory,"
    "character_abilities: list[str] | The character's special abilities",
    output="adventure_title: str | An epic title for the adventure,"
    "adventure_setting: str | Where the adventure takes place,"
    "adventure_quest: str | The main objective or quest,"
    "adventure_challenges: list[str] | Three to five challenges the character will face,"
    "adventure_allies: list[str] | One or two potential allies,"
    "adventure_antagonist: str | The main antagonist or obstacle",
    temperature=0.7,
    enable_rich_tables=True,
    output_theme=OutputTheme.dracula,
    wait_for_input=True,  # Pause after generating the adventure
)

# --------------------------------
# Step 3: Create Story Conclusion Agent
# --------------------------------
# This agent creates an epic conclusion to the adventure
conclusion_agent = FlockFactory.create_default_agent(
    name="conclusion_generator",
    description="Creates an epic conclusion to a character's adventure.",
    input="character_name: str | The character's name,"
    "character_race: str | The character's race,"
    "character_class: str | The character's class,"
    "character_abilities: list[str] | The character's special abilities,"
    "adventure_title: str | The adventure's title,"
    "adventure_setting: str | The adventure's setting,"
    "adventure_quest: str | The main quest,"
    "adventure_challenges: list[str] | The challenges faced,"
    "adventure_allies: list[str] | The character's allies,"
    "adventure_antagonist: str | The main antagonist",
    output="epic_conclusion: str | A dramatic and satisfying conclusion to the adventure,"
    "character_growth: str | How the character has changed or grown,"
    "sequel_hook: str | A teaser for a potential future adventure",
    temperature=0.8,  # Even higher temperature for a creative conclusion
    enable_rich_tables=True,
    output_theme=OutputTheme.synthwave,
)

# --------------------------------
# Chain the Agents Together
# --------------------------------
print_subheader("Agent Chaining Setup")
console.print("[bold]Setting up agent chain:[/bold]")
console.print("Character Generator → Adventure Generator → Conclusion Generator")

# Method 1: Using Default Router (simple, deterministic chaining)
console.print("\n[bold]Chaining Method:[/bold] Default Router (deterministic)")
character_agent.add_component(
    config_instance=DefaultRouterConfig(hand_off=adventure_agent.name),
    component_name="adventure_router",
)

adventure_agent.add_component(
    config_instance=DefaultRouterConfig(hand_off=conclusion_agent.name),
    component_name="conclusion_router",
)

# Alternative Method (commented out): Using LLM Router
# This would let the LLM decide which agent to call next based on context
# character_agent.handoff_router = LLMRouter(config=LLMRouterConfig(with_output=True))
# adventure_agent.handoff_router = LLMRouter(config=LLMRouterConfig(with_output=True))

# --------------------------------
# Add Agents to Flock
# --------------------------------
flock.add_agent(character_agent)
flock.add_agent(adventure_agent)
flock.add_agent(conclusion_agent)


# --------------------------------
# Run the Agent Chain
# --------------------------------
def run_adventure_generator(character_type):
    """Run the full agent chain to generate a character adventure."""
    print_header(f"Generating Adventure for {character_type.title()} Character")

    try:
        # Start with the character generator and the chain will continue automatically
        result = flock.run(
            start_agent=character_agent,
            input={"character_type": character_type},
        )

        # Display the final conclusion
        if hasattr(result, "epic_conclusion"):
            print_success("Adventure Complete!")

            conclusion_panel = Panel(
                result.epic_conclusion,
                title=f"The Conclusion of {result.character_name}'s Adventure",
                border_style="green",
            )
            console.print(conclusion_panel)

            growth_panel = Panel(
                result.character_growth, title="Character Growth", border_style="blue"
            )
            console.print(growth_panel)

            sequel_panel = Panel(
                result.sequel_hook, title="Next Adventure Hook", border_style="yellow"
            )
            console.print(sequel_panel)

            return result
        else:
            print_warning("The adventure chain did not complete as expected")
            return result

    except Exception as e:
        print_warning(f"Adventure generation failed: {e}")
        print_warning("Ensure your API key is set and the model is accessible.")
        return None


# --------------------------------
# Main Execution
# --------------------------------
if __name__ == "__main__":
    # Example character type
    example_character = "wizard"
    run_adventure_generator(example_character)

    print_header("How Agent Chaining Works")
    console.print(
        "1. The [bold]Character Generator[/bold] creates a detailed character"
    )
    console.print(
        "2. Its output is automatically passed to the [bold]Adventure Generator[/bold]"
    )
    console.print(
        "3. The adventure details are then passed to the [bold]Conclusion Generator[/bold]"
    )
    console.print("4. Each agent specializes in one part of the creative process")
    console.print(
        "\nThis demonstrates how complex tasks can be broken down into specialized agents,"
    )
    console.print(
        "each handling a specific subtask while passing context through the chain."
    )

# --- YOUR TURN! ---
# 1. Try with different character types:
#    - Change the example_character variable to "rogue", "paladin", "druid", etc.
#    - See how the entire adventure changes based on the initial input
#
# 2. Modify the agent chain:
#    - Add a new agent between Adventure and Conclusion (e.g., "battle_scene_generator")
#    - Update the chaining to include your new agent
#    - Adjust the inputs/outputs to pass the necessary information
#
# 3. Try different routing methods:
#    - Uncomment the LLMRouter lines and comment out the DefaultRouter lines
#    - This lets the LLM decide which agent to call next based on context
#    - Add a new agent that isn't in the main chain and see if the LLM router ever chooses it
#
# 4. Create a branching chain:
#    - Add two different conclusion agents (e.g., "happy_ending" and "tragic_ending")
#    - Use the LLM router to decide which ending to generate based on the adventure
#    - Hint: You'll need to define criteria for the router to consider
#
# 5. Add memory to the chain:
#    - Modify the agents to accept and pass along a "previous_adventures" parameter
#    - Create a character with multiple adventures that reference past events
```

### examples\02-core-concepts\04-tools-in-action.py

- **Lines**: 249
- **Last modified**: 2025-05-22 18:59:04

```py
# 02-core-concepts/04-tools-in-action-alt.py
"""
Purpose: Demonstrate how to create and use custom tools with Flock agents.

Use Case: Virtual Pet Tamagotchi 🐱 - Create a virtual pet that agents can interact with
         using custom tools to feed, play with, and check on the pet's status.

Highlights:
- Define custom tools with the @flock_tool decorator
- Show how agents can use tools to maintain state across interactions
- Demonstrate tool composition (using multiple tools together to solve a problem)
- Showcase how agents can reason about when and how to use tools
- Compare with what would be impossible in a stateless, single-prompt approach
"""

import os

from flock.cli.utils import print_header, print_subheader
from flock.core import Flock, FlockFactory
from rich.console import Console
from rich.table import Table
from tools.pet_tools import (
    buy_item,
    display_pet_status,
    feed_pet,
    get_mood_history,
    get_pet_status,
    perform_trick,
    play_with_pet,
    teach_trick,
)

# --- Configuration ---
console = Console()
MODEL = os.getenv("DEFAULT_MODEL", "openai/gpt-4o")
console.print(f"[grey50]Using model: {MODEL}[/grey50]")


# --------------------------------
# Create the Pet Caretaker Agent
# --------------------------------
# Create a Flock instance
flock = Flock(name="virtual_pet", model=MODEL, show_flock_banner=False)

# Create the pet caretaker agent with access to all our custom tools
pet_agent = FlockFactory.create_default_agent(
    name="pet_caretaker",
    description="You are a helpful virtual pet caretaker assistant. You help users take care of their virtual pet by using the available tools. "
    "For the pet's attributes: hunger, happiness, and energy, the scale is 0-100 where 0 is the worst possible state "
    "(starving, very sad, completely exhausted) and 100 is the best possible state (completely full, very happy, fully energetic).",
    input="user_request: str | The user's request about their virtual pet",
    output="response: str | Your helpful response to the user's request",
    tools=[
        get_pet_status,
        feed_pet,
        play_with_pet,
        teach_trick,
        buy_item,
        get_mood_history,
        perform_trick,
    ],
    temperature=0.7,
    include_thought_process=True,  # Show the agent's reasoning
)

# Add the agent to the flock
flock.add_agent(pet_agent)


# --------------------------------
# Main Execution
# --------------------------------
def run_pet_example():
    """Run the virtual pet example."""
    print_header("🐱 Virtual Pet Tamagotchi Example 🐱")
    console.print(
        "This example demonstrates how agents can use custom tools to maintain state and perform complex interactions."
    )

    # Display initial pet status
    print_subheader("Initial Pet Status")
    display_pet_status(get_pet_status())

    # Example 1: Simple tool usage
    print_subheader("Example 1: Simple Tool Usage")
    console.print("[bold]User Request:[/bold] How is my pet doing?")

    result = flock.run(
        start_agent=pet_agent, input={"user_request": "How is my pet doing?"}
    )
    console.print(f"[bold]Agent Response:[/bold] {result.response}")

    # Example 2: Tool composition (using multiple tools together)
    print_subheader("Example 2: Tool Composition")
    console.print("[bold]User Request:[/bold] My pet seems unhappy, what should I do?")

    result = flock.run(
        start_agent=pet_agent,
        input={"user_request": "My pet seems unhappy, what should I do?"},
    )
    console.print(f"[bold]Agent Response:[/bold] {result.response}")

    # Example 3: Complex reasoning and tool chaining
    print_subheader("Example 3: Complex Reasoning and Tool Chaining")
    console.print(
        "[bold]User Request:[/bold] I want to teach my pet a new trick, but I'm not sure if it's ready."
    )

    result = flock.run(
        start_agent=pet_agent,
        input={
            "user_request": "I want to teach my pet a new trick, but I'm not sure if it's ready."
        },
    )
    console.print(f"[bold]Agent Response:[/bold] {result.response}")

    # Example 4: Handling sequential actions
    print_subheader("Example 4: Sequential Actions")
    console.print(
        "[bold]User Request:[/bold] Feed my pet, play with it using the toy mouse, and then check if it can learn to roll over."
    )

    result = flock.run(
        start_agent=pet_agent,
        input={
            "user_request": "Feed my pet, play with it using the toy mouse, and then check if it can learn to roll over."
        },
    )
    console.print(f"[bold]Agent Response:[/bold] {result.response}")

    # Display final pet status
    print_subheader("Final Pet Status")
    display_pet_status(get_pet_status())

    # Show comparison with traditional approaches
    print_header("Why This Requires an Agent Framework")
    console.print(
        "This example demonstrates several capabilities that would be difficult or impossible with traditional, stateless LLM prompting:"
    )

    comparison_table = Table(title="Agent Framework vs. Traditional Prompting")
    comparison_table.add_column("Capability", style="cyan")
    comparison_table.add_column("Agent Framework", style="green")
    comparison_table.add_column("Traditional Prompting", style="red")

    comparison_table.add_row(
        "State Management",
        "✓ Maintains pet state across interactions",
        "✗ Cannot maintain state between prompts",
    )
    comparison_table.add_row(
        "Tool Composition",
        "✓ Can use multiple tools in sequence based on reasoning",
        "✗ Limited to pre-defined tool sequences",
    )
    comparison_table.add_row(
        "Conditional Logic",
        "✓ Can make decisions based on tool outputs",
        "✗ Cannot adapt based on dynamic data",
    )
    comparison_table.add_row(
        "Iterative Processing",
        "✓ Can process lists and repeat tool calls",
        "✗ Cannot iterate through data effectively",
    )
    comparison_table.add_row(
        "Error Handling",
        "✓ Can retry or try alternative approaches",
        "✗ No built-in error recovery",
    )

    console.print(comparison_table)


if __name__ == "__main__":
    run_pet_example()

    # Interactive mode
    print_header("🐱 Interactive Pet Caretaker Mode 🐱")
    console.print("You can now interact directly with your virtual pet!")
    console.print("Type 'exit' or 'quit' to end the session.")
    console.print("Type 'status' to see your pet's current status.\n")

    # Display current pet status
    display_pet_status(get_pet_status())

    while True:
        try:
            # Get user input
            user_request = input(
                "\n[bold cyan]What would you like to do with your pet?[/bold cyan] "
            )

            # Check for exit command
            if user_request.lower() in ["exit", "quit"]:
                console.print(
                    "[bold]Thank you for taking care of your virtual pet! Goodbye![/bold]"
                )
                break

            # Check for status command
            if user_request.lower() == "status":
                display_pet_status(get_pet_status())
                continue

            # Process the user request with the agent
            console.print("[bold]Processing your request...[/bold]")
            result = flock.run(
                start_agent=pet_agent, input={"user_request": user_request}
            )

            # Display the agent's response
            console.print(f"[bold green]Agent Response:[/bold green] {result.response}")

            # Show updated pet status after each interaction
            console.print("\n[bold]Updated Pet Status:[/bold]")
            display_pet_status(get_pet_status())

        except KeyboardInterrupt:
            console.print("\n[bold]Session interrupted. Goodbye![/bold]")
            break
        except Exception as e:
            console.print(f"[bold red]Error:[/bold red] {str(e)}")
            console.print("Please try again.")

# --- YOUR TURN! ---
# 1. Add a new tool:
#    - Create a new @flock_tool function like "give_bath" or "take_for_walk"
#    - Make it affect the pet's stats in interesting ways
#    - Add it to the pet_agent's tools list
#
# 2. Create a multi-step interaction:
#    - Try asking the agent to optimize your pet's happiness in as few steps as possible
#    - See if it can reason through the most efficient sequence of tool calls
#
# 3. Add a new pet attribute:
#    - Add a "health" or "cleanliness" attribute to the PET dictionary
#    - Update the relevant tools to affect this attribute
#    - See how the agent adapts to the new attribute
#
# 4. Create a challenging scenario:
#    - Set some of the pet's stats to extreme values (very hungry, very unhappy)
#    - Ask the agent to help you restore the pet to a happy state
#    - See if it prioritizes the most critical needs first
#
# 5. Implement a daily routine:
#    - Ask the agent to create an optimal daily care routine for your pet
#    - Have it explain which tools to use and in what order
#    - See if it considers the time-based changes to hunger and happiness
```

### examples\02-core-concepts\05-intro-to-modules.py

- **Lines**: 575
- **Last modified**: 2025-05-22 18:59:04

```py
"""Purpose: Demonstrate how to create and use modules in Flock agents.

Use Case: Virtual Pet Diary 📔 - Create a module that records significant events
         in your virtual pet's life, creating a personalized diary over time.

Highlights:
- Create a custom module with the @flock_component decorator
- Use module hooks like on_post_evaluate to capture agent interactions
- Store and retrieve persistent data across sessions
- Enhance agent capabilities without modifying core agent logic
"""

import asyncio
import json
import os
from datetime import datetime
from pathlib import Path
from typing import Any

from flock.core import Flock, FlockAgent, FlockContext, FlockFactory
from flock.core.flock_module import FlockModule, FlockModuleConfig
from flock.core.flock_registry import flock_component
from flock.core.logging.logging import get_logger
from pydantic import Field
from rich.console import Console
from rich.panel import Panel
from rich.table import Table
from tools.pet_diary import add_diary_note, get_pet_diary
from tools.pet_tools import (
    buy_item,
    feed_pet,
    get_mood_history,
    get_pet_status,
    perform_trick,
    play_with_pet,
    teach_trick,
)

# --- Configuration ---
console = Console()
MODEL = os.getenv("DEFAULT_MODEL", "openai/gpt-4o")
console.print(f"[grey50]Using model: {MODEL}[/grey50]")

logger = get_logger("pet_diary")


# --------------------------------
# Pet Diary Module Configuration
# --------------------------------
class PetDiaryModuleConfig(FlockModuleConfig):
    """Configuration for the Pet Diary Module that records significant events
    in a virtual pet's life.
    """

    diary_file: str = Field(
        default="pet_diary.json",
        description="The file to store the pet's diary entries.",
    )

    pet_name: str = Field(
        default="Pixel", description="The name of the pet for diary entries."
    )

    max_entries: int = Field(
        default=100, description="Maximum number of diary entries to keep."
    )


# --------------------------------
# Pet Diary Module Implementation
# --------------------------------
@flock_component(config_class=PetDiaryModuleConfig)
class PetDiaryModule(FlockModule):
    """A module that records significant events in a virtual pet's life,
    creating a personalized diary over time.
    """

    name: str = "pet_diary"
    config: PetDiaryModuleConfig = PetDiaryModuleConfig()
    diary_entries: list[dict] = []

    def __init__(self, name: str, config: PetDiaryModuleConfig):
        super().__init__(name=name, config=config)
        self.diary_entries = self._load_diary()

    def _load_diary(self) -> list[dict]:
        """Load diary entries from file or create empty diary."""
        diary_path = Path(self.config.diary_file)
        if diary_path.exists():
            try:
                with open(diary_path) as f:
                    return json.load(f)
            except (json.JSONDecodeError, KeyError) as e:
                logger.warning(f"Error loading diary: {e}. Creating new diary.")
                return []
        return []

    def _save_diary(self) -> None:
        """Save diary entries to file."""
        with open(self.config.diary_file, "w") as f:
            json.dump(self.diary_entries, f, indent=2)

    def add_entry(
        self, event_type: str, description: str, details: dict = None
    ) -> None:
        """Add a new diary entry.

        Args:
            event_type: Type of event (e.g., "feeding", "playing", "milestone")
            description: Description of the event
            details: Additional details about the event
        """
        entry = {
            "timestamp": datetime.now().isoformat(),
            "event_type": event_type,
            "description": description,
            "details": details or {},
        }

        self.diary_entries.append(entry)

        # Trim diary if it exceeds max entries
        if len(self.diary_entries) > self.config.max_entries:
            self.diary_entries = self.diary_entries[-self.config.max_entries :]

        self._save_diary()
        logger.info(f"Added diary entry: {description}")

    def get_recent_entries(self, count: int = 5) -> list[dict]:
        """Get the most recent diary entries."""
        return self.diary_entries[-count:] if self.diary_entries else []

    def get_entries_by_type(self, event_type: str) -> list[dict]:
        """Get diary entries of a specific type."""
        return [
            entry for entry in self.diary_entries if entry["event_type"] == event_type
        ]

    # --------------------------------
    # Flock Module Integration Hooks
    # --------------------------------
    async def on_initialize(
        self,
        agent: FlockAgent,
        inputs: dict[str, Any],
        context: FlockContext | None = None,
    ):
        """Called when the agent is initialized."""
        # Add recent diary entries to the agent's context
        recent_entries = self.get_recent_entries(3)
        if recent_entries:
            formatted_entries = "\n".join(
                [
                    f"- {datetime.fromisoformat(entry['timestamp']).strftime('%Y-%m-%d %H:%M')}: {entry['description']}"
                    for entry in recent_entries
                ]
            )

            # Enhance the agent's description with recent diary entries
            agent.description = (
                f"{agent.description}\n\n"
                f"Recent events in {self.config.pet_name}'s diary:\n{formatted_entries}"
            )

            logger.info("Added recent diary entries to agent context")

    async def on_post_evaluate(
        self,
        agent: Any,
        inputs: dict[str, Any],
        context: FlockContext | None = None,
        result: dict[str, Any] | None = None,
    ):
        """Called after the agent is evaluated."""
        if not result:
            return

        user_request = inputs.get("user_request", "")
        response = result.get("response", "")

        # Analyze the interaction to detect significant events
        self._process_interaction(user_request, response, result)

    def _process_interaction(
        self, user_request: str, response: str, result: dict[str, Any]
    ) -> None:
        """Process agent interaction to detect significant events.

        This looks at the tools called and response to determine if something
        diary-worthy happened.
        """
        # Access the agent's thought process if available
        thought_process = result.get("thought_process", "")

        # Check for feeding events
        if "feed_pet" in thought_process or "fed" in response.lower():
            # Extract what the pet was fed
            food_items = [
                "basic kibble",
                "premium kibble",
                "gourmet fish",
                "treat",
            ]
            fed_item = next(
                (item for item in food_items if item in response.lower()),
                "food",
            )

            self.add_entry(
                event_type="feeding",
                description=f"{self.config.pet_name} was fed {fed_item}.",
                details={"food": fed_item},
            )

        # Check for playing events
        if "play_with_pet" in thought_process or "played" in response.lower():
            play_activities = [
                "toy mouse",
                "ball of yarn",
                "laser pointer",
                "feather wand",
                "petting",
                "chase",
            ]
            activity = next(
                (act for act in play_activities if act in response.lower()),
                "toys",
            )

            self.add_entry(
                event_type="playing",
                description=f"{self.config.pet_name} played with {activity}.",
                details={"activity": activity},
            )

        # Check for learning new tricks
        if "teach_trick" in thought_process and "learned" in response.lower():
            trick_words = ["sit", "paw", "roll", "speak", "jump", "trick"]
            trick = next(
                (word for word in trick_words if word in response.lower()),
                "a new trick",
            )

            self.add_entry(
                event_type="milestone",
                description=f"{self.config.pet_name} learned how to {trick}!",
                details={"trick": trick},
            )

        # Check for mood changes
        mood_words = ["happy", "sad", "hungry", "tired", "ecstatic", "content"]
        if any(mood in response.lower() for mood in mood_words):
            current_mood = next(
                (mood for mood in mood_words if mood in response.lower()),
                "different",
            )

            # Only record significant mood changes
            if current_mood in ["ecstatic", "sad", "hungry"]:
                self.add_entry(
                    event_type="mood",
                    description=f"{self.config.pet_name} is feeling {current_mood} today.",
                    details={"mood": current_mood},
                )

        # Check for new items
        if "buy_item" in thought_process or "purchased" in response.lower():
            item_words = [
                "toy",
                "food",
                "treat",
                "kibble",
                "mouse",
                "yarn",
                "pointer",
                "wand",
            ]
            item = next(
                (word for word in item_words if word in response.lower()),
                "something new",
            )

            self.add_entry(
                event_type="shopping",
                description=f"You bought {item} for {self.config.pet_name}.",
                details={"item": item},
            )


# --------------------------------
# Create the Pet Agent with Diary
# --------------------------------
async def run_pet_diary_example():
    """Run the pet diary module example."""
    console.print(
        Panel.fit(
            "🐱 Virtual Pet Diary Module Example 🐱\n\n"
            "This example demonstrates how to create and use modules in Flock agents.\n"
            "The Pet Diary Module automatically records significant events in your pet's life.",
            title="Pet Diary Module",
            border_style="cyan",
        )
    )

    # Create a Flock instance
    flock = Flock(
        name="virtual_pet_with_diary",
        model=MODEL,
        show_flock_banner=False,
    )

    # Create the pet caretaker agent
    pet_agent = FlockFactory.create_default_agent(
        name="pet_caretaker",
        description=(
            "You are a helpful virtual pet caretaker assistant. "
            "You help users take care of their virtual pet by using the available tools. "
            "For the pet's attributes: hunger, happiness, and energy, the scale is 0-100 "
            "where 0 is the worst possible state (starving, very sad, completely exhausted) "
            "and 100 is the best possible state (completely full, very happy, fully energetic)."
        ),
        input="user_request: str | The user's request about their virtual pet",
        output="response: str | Your helpful response to the user's request",
        tools=[
            get_pet_diary,
            add_diary_note,
            get_pet_status,
            feed_pet,
            play_with_pet,
            teach_trick,
            buy_item,
            get_mood_history,
            perform_trick,
        ],
        temperature=0.7,
        include_thought_process=True,
    )

    # Add the Pet Diary Module to the agent
    pet_diary_config = PetDiaryModuleConfig(
        pet_name="Pixel", diary_file="pet_diary.json"
    )
    pet_agent.add_component(
        config_instance=pet_diary_config, component_name="pet_diary"
    )

    # Add the agent to the flock
    flock.add_agent(pet_agent)

    # Example 1: Show diary entries
    console.print("\n[bold cyan]Example 1: Showing pet diary entries[/bold cyan]")
    result = await flock.run_async(
        start_agent=pet_agent,
        input={"user_request": "What has my pet been up to recently?"},
    )
    console.print(f"[bold]Agent Response:[/bold] {result.response}")

    # Example 2: Add a custom diary note
    console.print("\n[bold cyan]Example 2: Adding a custom diary note[/bold cyan]")
    result = await flock.run_async(
        start_agent=pet_agent,
        input={"user_request": "Add a note that Pixel caught a virtual mouse today!"},
    )
    console.print(f"[bold]Agent Response:[/bold] {result.response}")

    # Example 3: Show updated diary
    console.print("\n[bold cyan]Example 3: Showing updated diary[/bold cyan]")
    result = await flock.run_async(
        start_agent=pet_agent,
        input={"user_request": "Show me Pixel's diary entries"},
    )
    console.print(f"[bold]Agent Response:[/bold] {result.response}")

    # Display the diary entries in a table
    diary_module = None
    for name, module in pet_agent.modules.items():
        if isinstance(module, PetDiaryModule):
            diary_module = module
            break

    if diary_module:
        entries = diary_module.get_recent_entries(10)

        if entries:
            table = Table(title=f"{diary_module.config.pet_name}'s Diary")
            table.add_column("Date", style="cyan")
            table.add_column("Event Type", style="magenta")
            table.add_column("Description", style="green")

            for entry in entries:
                date = datetime.fromisoformat(entry["timestamp"]).strftime(
                    "%Y-%m-%d %H:%M"
                )
                table.add_row(
                    date, entry["event_type"].capitalize(), entry["description"]
                )

            console.print("\n")
            console.print(table)
        else:
            console.print("\n[yellow]No diary entries found.[/yellow]")

    # Show how the module enhances the agent
    console.print(
        Panel.fit(
            "The Pet Diary Module demonstrates several key features of Flock modules:\n\n"
            "1. [bold]Persistent Storage[/bold]: Diary entries are saved to a file and loaded when needed\n"
            "2. [bold]Automatic Event Detection[/bold]: The module analyzes agent interactions to detect events\n"
            "3. [bold]Context Enhancement[/bold]: Recent diary entries are added to the agent's context\n"
            "4. [bold]Tool Integration[/bold]: The module provides tools for agents to access the diary\n"
            "5. [bold]Separation of Concerns[/bold]: Core agent logic is separate from the diary functionality",
            title="Module Benefits",
            border_style="green",
        )
    )


# --------------------------------
# Interactive Mode
# --------------------------------
async def interactive_mode():
    """Run an interactive session with the pet agent and diary module."""
    console.print(
        Panel.fit(
            "🐱 Interactive Pet Caretaker Mode 🐱\n\n"
            "You can now interact directly with your virtual pet!\n"
            "The Pet Diary Module will record significant events automatically.\n\n"
            "Type 'exit' or 'quit' to end the session.\n"
            "Type 'diary' to view recent diary entries.",
            border_style="cyan",
        )
    )

    # Create a Flock instance
    flock = Flock(name="virtual_pet_with_diary", model=MODEL, show_flock_banner=False)

    # Create the pet caretaker agent
    pet_agent = FlockFactory.create_default_agent(
        name="pet_caretaker",
        description=(
            "You are a helpful virtual pet caretaker assistant. "
            "You help users take care of their virtual pet by using the available tools. "
            "For the pet's attributes: hunger, happiness, and energy, the scale is 0-100 "
            "where 0 is the worst possible state (starving, very sad, completely exhausted) "
            "and 100 is the best possible state (completely full, very happy, fully energetic)."
        ),
        input="user_request: str | The user's request about their virtual pet",
        output="response: str | Your helpful response to the user's request",
        tools=[
            get_pet_diary,
            add_diary_note,
            get_pet_status,
            feed_pet,
            play_with_pet,
            teach_trick,
            buy_item,
            get_mood_history,
            perform_trick,
        ],
        temperature=0.7,
        include_thought_process=True,
    )

    # Add the Pet Diary Module to the agent
    pet_diary_config = PetDiaryModuleConfig(
        pet_name="Pixel", diary_file="pet_diary.json"
    )
    pet_agent.add_component(
        config_instance=pet_diary_config, component_name="pet_diary"
    )

    # Add the agent to the flock
    flock.add_agent(pet_agent)

    # Get the diary module for direct access
    diary_module = None
    for name, module in pet_agent.modules.items():
        if isinstance(module, PetDiaryModule):
            diary_module = module
            break

    # Interactive loop
    while True:
        try:
            # Get user input
            user_request = input(
                "\n[bold cyan]What would you like to do with your pet?[/bold cyan] "
            )

            # Check for exit command
            if user_request.lower() in ["exit", "quit"]:
                console.print(
                    "[bold]Thank you for taking care of your virtual pet! Goodbye![/bold]"
                )
                break

            # Check for diary command
            if user_request.lower() == "diary":
                if diary_module:
                    entries = diary_module.get_recent_entries(10)

                    if entries:
                        table = Table(title=f"{diary_module.config.pet_name}'s Diary")
                        table.add_column("Date", style="cyan")
                        table.add_column("Event Type", style="magenta")
                        table.add_column("Description", style="green")

                        for entry in entries:
                            date = datetime.fromisoformat(entry["timestamp"]).strftime(
                                "%Y-%m-%d %H:%M"
                            )
                            table.add_row(
                                date,
                                entry["event_type"].capitalize(),
                                entry["description"],
                            )

                        console.print("\n")
                        console.print(table)
                    else:
                        console.print("\n[yellow]No diary entries found.[/yellow]")
                continue

            # Process the user request with the agent
            console.print("[bold]Processing your request...[/bold]")
            result = await flock.run_async(
                start_agent=pet_agent, input={"user_request": user_request}
            )

            # Display the agent's response
            console.print(f"[bold green]Agent Response:[/bold green] {result.response}")

        except KeyboardInterrupt:
            console.print("\n[bold]Session interrupted. Goodbye![/bold]")
            break
        except Exception as e:
            console.print(f"[bold red]Error:[/bold red] {e!s}")
            console.print("Please try again.")


# --------------------------------
# Main Execution
# --------------------------------
if __name__ == "__main__":
    # Run the example
    asyncio.run(run_pet_diary_example())

    # Run interactive mode
    asyncio.run(interactive_mode())

# --- YOUR TURN! ---
# 1. Extend the diary module:
#    - Add a method to search for entries by keyword
#    - Create a tool that lets the agent search the diary
#    - Try asking questions about past events
#
# 2. Create milestone detection:
#    - Add logic to detect when the pet reaches milestones (e.g., learning 5 tricks)
#    - Have the module automatically add special entries for these events
#    - Create a tool to show all milestones achieved
#
# 3. Implement diary analytics:
#    - Add methods to analyze patterns in the diary (e.g., most common activities)
#    - Create visualizations of the pet's activities over time
#    - Have the agent suggest activities based on what the pet hasn't done recently
#
# 4. Connect with the pet state:
#    - Integrate the diary module with the pet state from the tools-in-action example
#    - Record changes in the pet's stats over time
#    - Create a "pet history" visualization showing how stats have changed
#
# 5. Create a personalized story:
#    - Add a method that generates a story based on recent diary entries
#    - Have the agent tell a bedtime story to the pet using its recent experiences
#    - Save these stories in a separate "storybook" file
```

### examples\02-core-concepts\06-context-basics.py

- **Lines**: 390
- **Last modified**: 2025-05-22 18:59:04

```py
"""
Purpose: Demonstrate how to use context lookups to access information across agents.

Use Case: Interactive Story Collaboration 📚 - Multiple agents collaborate to build
         a story together, each accessing different parts of the context.

Highlights:
- Access information from the context using special lookup syntax
- Reference outputs from other agents in the chain
- Use different lookup patterns to access specific pieces of information
- Show how context enables more complex agent interactions than simple chaining
"""

import os

from flock.cli.utils import print_header, print_subheader
from flock.core import Flock, FlockFactory
from flock.core.logging.formatters.themes import OutputTheme
from rich.console import Console
from rich.panel import Panel
from rich.table import Table

# --- Configuration ---
console = Console()
MODEL = os.getenv("DEFAULT_MODEL", "openai/gpt-4o")
console.print(f"[grey50]Using model: {MODEL}[/grey50]")

# --- Create Flock Instance ---
flock = Flock(name="story_collaboration", model=MODEL, show_flock_banner=False)

# --------------------------------
# Context Lookup Rules Explained
# --------------------------------
print_header("Context Lookup Rules in Flock")
console.print("Flock provides special syntax for accessing information across agents:")
lookup_table = Table(title="Context Lookup Patterns")
lookup_table.add_column("Pattern", style="cyan")
lookup_table.add_column("Description", style="green")
lookup_table.add_column("Example", style="yellow")

lookup_table.add_row(
    "context",
    "Returns the entire context object",
    'input: "context | The full context"',
)
lookup_table.add_row(
    "context.property",
    "Returns a specific property from the context",
    'input: "context.story_theme | The theme of our story"',
)
lookup_table.add_row(
    "def.agent_name",
    "Returns the agent definition for the given agent",
    'input: "def.character_creator | The character creator agent definition"',
)
lookup_table.add_row(
    "agent_name",
    "Returns the most recent record from the agent's history",
    'input: "character_creator | The character creator\'s output"',
)
lookup_table.add_row(
    "agent_name.property",
    "Returns a specific property from an agent's state",
    'input: "character_creator.character_name | The name of our character"',
)
lookup_table.add_row(
    "property",
    "Searches history for the most recent value of a property",
    'input: "story_setting | The setting of our story"',
)

console.print(lookup_table)
console.print(
    "\nLet's see these in action with a collaborative story creation example!\n"
)

# --------------------------------
# Step 1: Story Theme Generator Agent
# --------------------------------
# This agent generates the initial theme and setting for our story
theme_agent = FlockFactory.create_default_agent(
    name="theme_generator",
    description="Generates creative story themes and settings.",
    input="genre: str | The genre of story to create",
    output="theme_generator_theme: str | The central theme of the story\n"
    "theme_generator_setting: str | The setting where the story takes place\n"
    "theme_generator_time_period: str | The time period of the story\n"
    "theme_generator_mood: str | The overall mood or tone",
    temperature=0.7,
    enable_rich_tables=True,
    output_theme=OutputTheme.monokai,
    wait_for_input=True,
)

# --------------------------------
# Step 2: Character Creator Agent
# --------------------------------
# This agent creates characters based on the theme and setting
# Note the use of context lookups in the input to access theme_agent's outputs
character_agent = FlockFactory.create_default_agent(
    name="character_creator",
    description="Creates compelling characters that fit the story theme and setting.",
    input="theme_generator_theme: str | The central theme of the story\n"
    "theme_generator_setting: str | The setting where the story takes place\n"
    "theme_generator_time_period: str | The time period of the story\n"
    "theme_generator_mood: str | The overall mood or tone",
    output="cc_protagonist: dict | The main character of the story\n"
    "cc_antagonist: dict | The opposing character or force\n"
    "cc_supporting_character: dict | An important supporting character",
    temperature=0.7,
    enable_rich_tables=True,
    output_theme=OutputTheme.dracula,
    wait_for_input=True,
)

# --------------------------------
# Step 3: Plot Outline Agent
# --------------------------------
# This agent creates a plot outline using both theme and character information
# Note how it accesses both theme_agent and character_agent outputs
plot_agent = FlockFactory.create_default_agent(
    name="plot_outliner",
    description="Creates a compelling plot outline based on the theme and characters.",
    input="theme_generator_theme: str | The theme from the theme generator\n"
    "theme_generator_setting: str | The setting from the theme generator\n"
    "cc_protagonist: dict | The protagonist character\n"
    "cc_antagonist: dict | The antagonist character",
    output="po_plot_points: list[str] | The main plot points in sequence\n"
    "po_conflict: str | The central conflict of the story\n"
    "po_resolution: str | How the story might resolve",
    temperature=0.7,
    enable_rich_tables=True,
    output_theme=OutputTheme.synthwave,
    wait_for_input=True,
)

# --------------------------------
# Step 4: Story Opener Agent
# --------------------------------
# This agent writes the opening paragraph of the story
# Note the different ways it accesses context from previous agents
opener_agent = FlockFactory.create_default_agent(
    name="story_opener",
    description="Writes an engaging opening paragraph for the story.",
    input="theme_generator.theme_generator_mood | The mood of the story\n"
    "theme_generator.theme_generator_setting | The setting of the story\n"
    "character_creator.cc_protagonist | The protagonist\n"
    "plot_outliner.po_plot_points | The plot points",
    output="so_opening_paragraph: str | The opening paragraph of the story\n"
    "so_narrative_voice: str | The narrative voice/perspective used",
    temperature=0.8,
    enable_rich_tables=True,
    output_theme=OutputTheme.aardvark_blue,
    wait_for_input=True,
)

# --------------------------------
# Step 5: Story Critic Agent
# --------------------------------
# This agent reviews the entire story development so far
# Note how it uses "context" to access everything at once
critic_agent = FlockFactory.create_default_agent(
    name="story_critic",
    description="Reviews the story elements and provides constructive feedback.",
    input="context | The entire context with all story elements",
    output="sc_strengths: list[str] | The strongest elements of the story so far\n"
    "sc_weaknesses: list[str] | Areas that could be improved\n"
    "sc_suggestions: list[str] | Specific suggestions for improvement",
    temperature=0.7,
    enable_rich_tables=True,
    output_theme=OutputTheme.fruity,
)

# --------------------------------
# Add agents to Flock
# --------------------------------
flock.add_agent(theme_agent)
flock.add_agent(character_agent)
flock.add_agent(plot_agent)
flock.add_agent(opener_agent)
flock.add_agent(critic_agent)


# --------------------------------
# Run the collaborative story creation
# --------------------------------
def run_story_collaboration(genre: str):
    """Run the collaborative story creation process with the given genre."""
    print_header(f"Collaborative Story Creation: {genre}")

    # Step 1: Generate theme and setting
    print_subheader("Step 1: Generating Theme and Setting")
    theme_result = flock.run(start_agent=theme_agent, input={"genre": genre})

    # Step 2: Create characters based on theme
    print_subheader("Step 2: Creating Characters")
    # We could manually pass the theme agent's outputs, but instead
    # we'll let the character agent use its input definition to pull them from context
    character_result = flock.run(start_agent=character_agent)

    # Step 3: Create plot outline based on theme and characters
    print_subheader("Step 3: Creating Plot Outline")
    # Again, the plot agent will pull what it needs from the context
    plot_result = flock.run(start_agent=plot_agent)

    # Step 4: Write the opening paragraph
    print_subheader("Step 4: Writing Opening Paragraph")
    opener_result = flock.run(start_agent=opener_agent)

    # Step 5: Get critique of the whole story development
    print_subheader("Step 5: Story Critique")
    critic_result = flock.run(start_agent=critic_agent)

    # Display the final story elements
    print_header("Final Story Elements")

    # Theme and Setting
    theme_panel = Panel(
        f"[bold]Theme:[/bold] {theme_result.theme_generator_theme}\n"
        f"[bold]Setting:[/bold] {theme_result.theme_generator_setting}\n"
        f"[bold]Time Period:[/bold] {theme_result.theme_generator_time_period}\n"
        f"[bold]Mood:[/bold] {theme_result.theme_generator_mood}",
        title="Theme and Setting",
        border_style="cyan",
    )
    console.print(theme_panel)

    # Characters
    character_table = Table(title="Characters")
    character_table.add_column("Role", style="cyan")
    character_table.add_column("Details", style="green")

    character_table.add_row("Protagonist", str(character_result.cc_protagonist))
    character_table.add_row("Antagonist", str(character_result.cc_antagonist))
    character_table.add_row(
        "Supporting Character", str(character_result.cc_supporting_character)
    )

    console.print(character_table)

    # Plot
    plot_panel = Panel(
        f"[bold]Conflict:[/bold] {plot_result.po_conflict}\n\n"
        f"[bold]Plot Points:[/bold]\n"
        + "\n".join(f"- {point}" for point in plot_result.po_plot_points)
        + "\n\n"
        f"[bold]Resolution:[/bold] {plot_result.po_resolution}",
        title="Plot Outline",
        border_style="magenta",
    )
    console.print(plot_panel)

    # Opening Paragraph
    opening_panel = Panel(
        opener_result.so_opening_paragraph + "\n\n"
        f"[italic]Narrative Voice: {opener_result.so_narrative_voice}[/italic]",
        title="Opening Paragraph",
        border_style="yellow",
    )
    console.print(opening_panel)

    # Critique
    critique_panel = Panel(
        "[bold]Strengths:[/bold]\n"
        + "\n".join(f"- {strength}" for strength in critic_result.sc_strengths)
        + "\n\n"
        "[bold]Weaknesses:[/bold]\n"
        + "\n".join(f"- {weakness}" for weakness in critic_result.sc_weaknesses)
        + "\n\n"
        "[bold]Suggestions:[/bold]\n"
        + "\n".join(f"- {suggestion}" for suggestion in critic_result.sc_suggestions),
        title="Story Critique",
        border_style="red",
    )
    console.print(critique_panel)

    return {
        "theme_result": theme_result,
        "character_result": character_result,
        "plot_result": plot_result,
        "opener_result": opener_result,
        "critic_result": critic_result,
    }


# --------------------------------
# Demonstrate Context Lookup Patterns
# --------------------------------
def show_context_lookup_examples(results):
    """Show examples of how the different context lookup patterns were used."""
    print_header("Context Lookup Examples")

    examples_table = Table(title="How Context Lookups Were Used")
    examples_table.add_column("Agent", style="cyan")
    examples_table.add_column("Lookup Pattern", style="green")
    examples_table.add_column("What It Accessed", style="yellow")

    examples_table.add_row(
        "character_creator",
        "theme_generator_theme",
        f"Theme: {results['theme_result'].theme_generator_theme}",
    )

    examples_table.add_row(
        "plot_outliner",
        "cc_protagonist",
        f"Protagonist: {str(results['character_result'].cc_protagonist)[:50]}...",
    )

    examples_table.add_row(
        "story_opener",
        "theme_generator.theme_generator_setting",
        f"Setting: {results['theme_result'].theme_generator_setting}",
    )

    examples_table.add_row(
        "story_critic", "context", "The entire context with all story elements"
    )

    console.print(examples_table)

    # Explanation of benefits
    print_subheader("Benefits of Context Lookups")
    console.print(
        "1. [bold]Flexibility:[/bold] Agents can access any information from any previous agent"
    )
    console.print(
        "2. [bold]Selective Access:[/bold] Agents can choose only the specific pieces of information they need"
    )
    console.print(
        "3. [bold]Reduced Redundancy:[/bold] No need to manually pass all outputs between agents"
    )
    console.print(
        "4. [bold]Easier Maintenance:[/bold] Adding new agents or changing outputs doesn't break the chain"
    )
    console.print(
        "5. [bold]Global Context:[/bold] Agents can access the entire context when needed for holistic analysis"
    )


# --------------------------------
# Main Execution
# --------------------------------
if __name__ == "__main__":
    # Run the collaborative story creation with a fantasy genre
    results = run_story_collaboration("science fiction")

    # Show examples of the context lookup patterns
    show_context_lookup_examples(results)

    print_header("Context Lookup Rules Summary")
    console.print("The lookup rules we demonstrated:")
    console.print(
        '- [bold]"property"[/bold]: Direct property access (e.g., theme_generator_theme)'
    )
    console.print(
        '- [bold]"agent_name.property"[/bold]: Access specific agent property (e.g., theme_generator.theme_generator_setting)'
    )
    console.print(
        '- [bold]"context"[/bold]: Access the entire context (used by the critic agent)'
    )

    console.print(
        "\nThese patterns enable complex agent interactions beyond simple chaining!"
    )

# --- YOUR TURN! ---
# 1. Try different lookup patterns:
#    - Modify the input of the opener_agent to use different lookup patterns
#    - For example, try using "po_conflict" instead of "plot_outliner.po_conflict"
#    - See if the agent can still find the information in the context
#
# 2. Add a new agent that needs information from multiple previous agents:
#    - Create a "story_illustrator" agent that needs character and setting details
#    - Use different lookup patterns to access this information
#    - See how context lookups make this easier than passing all the data manually
#
# 3. Try using "def.agent_name":
#    - Add an agent that uses "def.theme_generator" in its input
#    - This gives access to the theme_generator's definition (not its outputs)
#    - Use this to create a "meta" agent that analyzes how the story was created
#
# 4. Create a context-aware agent:
#    - Add a "story_reviser" agent that takes "context" as input
#    - Have it suggest revisions based on everything that's been generated
#    - See how it can synthesize information across all previous agents
#
# 5. Experiment with fallback behavior:
#    - Try referencing a property that doesn't exist in the context
#    - See how the system handles this and what fallback values it uses
```

### examples\02-core-concepts\tools\pet_diary.py

- **Lines**: 101
- **Last modified**: 2025-05-22 18:59:04

```py
# --------------------------------
# Pet Diary Tool for Agent Access
# --------------------------------
import json
from datetime import datetime
from pathlib import Path
from typing import Dict, List

from flock.core.flock_registry import flock_tool
from flock.core.logging.logging import get_logger

logger = get_logger("pet_diary")


@flock_tool
def get_pet_diary(entries: int = 5) -> List[Dict]:
    """
    Get recent entries from your pet's diary.

    Args:
        entries: Number of recent entries to retrieve (default: 5)

    Returns:
        List of diary entries with timestamp, event type, and description
    """
    # Read diary entries directly from file
    diary_path = Path("pet_diary.json")
    if not diary_path.exists():
        return [{"error": "Pet diary not found"}]

    try:
        with open(diary_path, "r") as f:
            all_entries = json.load(f)

        # Get the most recent entries
        recent_entries = all_entries[-entries:] if all_entries else []

        # Format the entries for better readability
        formatted_entries = []
        for entry in recent_entries:
            formatted_entries.append(
                {
                    "date": datetime.fromisoformat(entry["timestamp"]).strftime(
                        "%Y-%m-%d %H:%M"
                    ),
                    "event_type": entry["event_type"],
                    "description": entry["description"],
                }
            )

        return formatted_entries
    except (json.JSONDecodeError, KeyError, ValueError) as e:
        logger.warning(f"Error reading diary: {e}")
        return [{"error": f"Error reading diary: {str(e)}"}]


@flock_tool
def add_diary_note(note: str, pet_name: str = "Pixel") -> str:
    """
    Add a custom note to your pet's diary.

    Args:
        note: The note to add to the diary
        pet_name: The name of your pet (default: Pixel)

    Returns:
        Confirmation message
    """
    diary_path = Path("pet_diary.json")

    # Load existing entries or create new diary
    if diary_path.exists():
        try:
            with open(diary_path, "r") as f:
                diary_entries = json.load(f)
        except (json.JSONDecodeError, KeyError) as e:
            logger.warning(f"Error loading diary: {e}. Creating new diary.")
            diary_entries = []
    else:
        diary_entries = []

    # Create new entry
    entry = {
        "timestamp": datetime.now().isoformat(),
        "event_type": "note",
        "description": note,
        "details": {"custom": True},
    }

    # Add entry to diary
    diary_entries.append(entry)

    # Limit the number of entries (keep most recent 100)
    if len(diary_entries) > 100:
        diary_entries = diary_entries[-100:]

    # Save updated diary
    with open(diary_path, "w") as f:
        json.dump(diary_entries, f, indent=2)

    return f"Added note to {pet_name}'s diary: {note}"
```

### examples\02-core-concepts\tools\pet_tools.py

- **Lines**: 403
- **Last modified**: 2025-05-22 18:59:04

```py
# --------------------------------
# Pet State File Management
# --------------------------------
import json
import random
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List

from flock.core.flock_registry import flock_tool
from rich.console import Console
from rich.table import Table

PET_STATE_FILE = Path("pet_state.json")
console = Console()


def load_pet_state() -> Dict:
    """Load pet state from JSON file or create default if it doesn't exist."""
    if PET_STATE_FILE.exists():
        try:
            with open(PET_STATE_FILE, "r") as f:
                pet_data = json.load(f)

                # Convert datetime strings back to datetime objects
                pet_data["last_fed"] = datetime.fromisoformat(pet_data["last_fed"])
                pet_data["last_played"] = datetime.fromisoformat(
                    pet_data["last_played"]
                )

                return pet_data
        except (json.JSONDecodeError, KeyError, ValueError) as e:
            console.print(
                f"[red]Error loading pet state: {e}. Using default state.[/red]"
            )
            return create_default_pet_state()
    else:
        return create_default_pet_state()


def save_pet_state(pet_data: Dict) -> None:
    """Save pet state to JSON file."""
    # Convert datetime objects to ISO format strings for JSON serialization
    serializable_pet = pet_data.copy()
    serializable_pet["last_fed"] = pet_data["last_fed"].isoformat()
    serializable_pet["last_played"] = pet_data["last_played"].isoformat()

    with open(PET_STATE_FILE, "w") as f:
        json.dump(serializable_pet, f, indent=2)


def create_default_pet_state() -> Dict:
    """Create default pet state."""
    default_pet = {
        "name": "Pixel",
        "type": "digital cat",
        "hunger": 50,  # 0-100 (0: starving/worst, 100: full/best)
        "happiness": 50,  # 0-100 (0: sad/worst, 100: very happy/best)
        "energy": 50,  # 0-100 (0: exhausted/worst, 100: energetic/best)
        "last_fed": datetime.now() - timedelta(hours=5),
        "last_played": datetime.now() - timedelta(hours=3),
        "inventory": ["basic kibble", "toy mouse", "ball of yarn"],
        "tricks_known": ["sit", "paw"],
        "mood_history": [],  # Track mood changes over time
    }
    save_pet_state(default_pet)
    return default_pet


# Initialize pet state
PET = load_pet_state()

# --------------------------------
# Custom Tools
# --------------------------------


@flock_tool
def get_pet_status() -> Dict:
    """
    Get the current status of your virtual pet.
    Returns all pet attributes including hunger, happiness, and energy levels.
    """
    global PET
    # Load the latest state
    PET = load_pet_state()

    # Calculate time-based changes (pet gets hungry and less happy over time)
    hours_since_fed = (datetime.now() - PET["last_fed"]).total_seconds() / 3600
    hours_since_played = (datetime.now() - PET["last_played"]).total_seconds() / 3600

    # Decrease hunger by 5 points per hour since last fed (min 0)
    PET["hunger"] = max(0, PET["hunger"] - int(hours_since_fed * 5))

    # Decrease happiness by 3 points per hour since last played (min 0)
    PET["happiness"] = max(0, PET["happiness"] - int(hours_since_played * 3))

    # Energy recovers slowly over time (max 100)
    PET["energy"] = min(100, PET["energy"] + 2)

    # Update the mood history
    current_mood = calculate_mood(PET["hunger"], PET["happiness"], PET["energy"])
    if not PET["mood_history"] or PET["mood_history"][-1] != current_mood:
        PET["mood_history"].append(current_mood)
        if len(PET["mood_history"]) > 5:
            PET["mood_history"] = PET["mood_history"][-5:]

    # Save the updated state
    save_pet_state(PET)
    return PET


@flock_tool
def feed_pet(food_item: str) -> str:
    """
    Feed your pet with a specific food item.

    Args:
        food_item: The name of the food to feed your pet

    Returns:
        A description of how your pet responded to the food
    """
    global PET
    # Load the latest state
    PET = load_pet_state()

    # Check if the food is in inventory
    if food_item not in PET["inventory"]:
        return f"{PET['name']} looks confused. You don't have {food_item} in your inventory."

    # Different foods have different effects
    food_effects = {
        "basic kibble": {"hunger": 15, "happiness": 5, "energy": 3},
        "premium kibble": {"hunger": 25, "happiness": 10, "energy": 5},
        "gourmet fish": {"hunger": 40, "happiness": 20, "energy": 10},
        "treat": {"hunger": 5, "happiness": 15, "energy": 2},
    }

    # Default effect for unknown foods
    effect = food_effects.get(food_item, {"hunger": 10, "happiness": 5, "energy": 2})

    # Apply the effects
    PET["hunger"] = min(100, PET["hunger"] + effect["hunger"])
    PET["happiness"] = min(100, PET["happiness"] + effect["happiness"])
    PET["energy"] = min(100, PET["energy"] + effect["energy"])
    PET["last_fed"] = datetime.now()

    # If it's a consumable item, remove from inventory
    if food_item != "basic kibble":  # basic kibble is unlimited
        PET["inventory"].remove(food_item)

    responses = [
        f"{PET['name']} eats the {food_item} eagerly!",
        f"{PET['name']} nibbles on the {food_item} and purrs contentedly.",
        f"{PET['name']} devours the {food_item} in seconds!",
    ]

    # Save the updated state
    save_pet_state(PET)
    return random.choice(responses)


@flock_tool
def play_with_pet(activity: str) -> str:
    """
    Play with your pet using a specific activity or toy.

    Args:
        activity: The name of the activity or toy to use

    Returns:
        A description of how your pet responded to the play session
    """
    global PET
    # Load the latest state
    PET = load_pet_state()

    # Check if the toy is in inventory
    toy_activities = ["toy mouse", "ball of yarn", "laser pointer", "feather wand"]
    if activity in toy_activities and activity not in PET["inventory"]:
        return f"{PET['name']} looks excited, but you don't have a {activity} in your inventory."

    # Different activities have different effects
    activity_effects = {
        "toy mouse": {"happiness": 20, "energy": -10},
        "ball of yarn": {"happiness": 15, "energy": -5},
        "laser pointer": {"happiness": 25, "energy": -15},
        "feather wand": {"happiness": 20, "energy": -10},
        "petting": {"happiness": 10, "energy": 0},
        "chase": {"happiness": 30, "energy": -20},
    }

    # Default effect for unknown activities
    effect = activity_effects.get(activity, {"happiness": 10, "energy": -5})

    # Check if pet has enough energy
    if PET["energy"] + effect["energy"] < 0:
        return f"{PET['name']} seems too tired to play with the {activity} right now."

    # Apply the effects
    PET["happiness"] = min(100, PET["happiness"] + effect["happiness"])
    PET["energy"] = max(0, PET["energy"] + effect["energy"])
    PET["last_played"] = datetime.now()

    responses = [
        f"{PET['name']} has a great time playing with the {activity}!",
        f"{PET['name']} jumps and pounces on the {activity} excitedly!",
        f"{PET['name']} enjoys the {activity} and looks happier now.",
    ]

    # Save the updated state
    save_pet_state(PET)
    return random.choice(responses)


@flock_tool
def teach_trick(trick_name: str) -> str:
    """
    Try to teach your pet a new trick.

    Args:
        trick_name: The name of the trick to teach

    Returns:
        A description of your pet's learning progress
    """
    global PET
    # Load the latest state
    PET = load_pet_state()

    # Check if pet already knows this trick
    if trick_name in PET["tricks_known"]:
        return f"{PET['name']} already knows how to {trick_name}!"

    # Check if pet is in the right mood to learn
    if PET["hunger"] < 30:
        return f"{PET['name']} is too hungry to focus on learning right now."

    if PET["happiness"] < 40:
        return f"{PET['name']} doesn't seem interested in learning when unhappy."

    if PET["energy"] < 20:
        return f"{PET['name']} is too tired to learn new tricks right now."

    # Teaching consumes energy and increases happiness
    PET["energy"] = max(0, PET["energy"] - 15)
    PET["happiness"] = min(100, PET["happiness"] + 10)

    # 70% chance of success if all conditions are met
    if random.random() < 0.7:
        PET["tricks_known"].append(trick_name)
        save_pet_state(PET)
        return f"Success! {PET['name']} has learned how to {trick_name}!"
    else:
        save_pet_state(PET)
        return f"{PET['name']} tried to learn {trick_name}, but needs more practice."


@flock_tool
def buy_item(item_name: str, cost: int = 10) -> str:
    """
    Buy a new item for your pet.

    Args:
        item_name: The name of the item to buy
        cost: The cost of the item (default: 10)

    Returns:
        A confirmation message about the purchase
    """
    global PET
    # Load the latest state
    PET = load_pet_state()

    # In a real app, you'd check the user's balance
    # For this example, we'll just add the item to inventory

    if item_name in PET["inventory"]:
        return f"You already have {item_name} in your inventory."

    PET["inventory"].append(item_name)
    save_pet_state(PET)
    return f"You purchased {item_name} for {cost} coins! It's been added to your inventory."


@flock_tool
def get_mood_history() -> List[str]:
    """
    Get the history of your pet's mood changes.

    Returns:
        A list of mood states from oldest to newest
    """
    global PET
    # Load the latest state
    PET = load_pet_state()
    return PET["mood_history"]


@flock_tool
def perform_trick(trick_name: str) -> str:
    """
    Ask your pet to perform a trick it knows.

    Args:
        trick_name: The name of the trick to perform

    Returns:
        A description of your pet's performance
    """
    global PET
    # Load the latest state
    PET = load_pet_state()

    if trick_name not in PET["tricks_known"]:
        return f"{PET['name']} doesn't know how to {trick_name} yet."

    if PET["energy"] < 10:
        return f"{PET['name']} is too tired to perform tricks right now."

    # Performing tricks uses a small amount of energy
    PET["energy"] = max(0, PET["energy"] - 5)
    PET["happiness"] = min(100, PET["happiness"] + 5)

    trick_responses = {
        "sit": [
            f"{PET['name']} sits down perfectly!",
            f"{PET['name']} sits and looks up at you expectantly.",
        ],
        "paw": [
            f"{PET['name']} extends a paw gracefully.",
            f"{PET['name']} reaches out with a paw and taps your hand.",
        ],
        "roll": [
            f"{PET['name']} rolls over in a complete circle!",
            f"{PET['name']} flops onto their back and rolls around playfully.",
        ],
        "speak": [
            f"{PET['name']} makes an adorable sound!",
            f"{PET['name']} meows loudly on command.",
        ],
        "jump": [
            f"{PET['name']} leaps into the air with surprising height!",
            f"{PET['name']} jumps up and does a little spin.",
        ],
    }

    responses = trick_responses.get(
        trick_name, [f"{PET['name']} performs the {trick_name} trick perfectly!"]
    )

    save_pet_state(PET)
    return random.choice(responses)


# --------------------------------
# Helper Functions
# --------------------------------


def calculate_mood(hunger: int, happiness: int, energy: int) -> str:
    """Calculate the pet's current mood based on its stats."""
    if hunger < 20:
        return "hungry"
    elif happiness < 30:
        return "sad"
    elif energy < 20:
        return "tired"
    elif happiness > 80 and hunger > 70:
        return "ecstatic"
    elif happiness > 60:
        return "happy"
    else:
        return "content"


def display_pet_status(pet_data: Dict):
    """Display the pet's status in a nice formatted way."""
    status_table = Table(title=f"{pet_data['name']} the {pet_data['type']}")

    # Add columns
    status_table.add_column("Attribute", style="cyan")
    status_table.add_column("Value", style="green")

    # Add rows for basic stats
    status_table.add_row("Hunger", f"{pet_data['hunger']}/100")
    status_table.add_row("Happiness", f"{pet_data['happiness']}/100")
    status_table.add_row("Energy", f"{pet_data['energy']}/100")

    # Calculate and add current mood
    mood = calculate_mood(pet_data["hunger"], pet_data["happiness"], pet_data["energy"])
    status_table.add_row("Current Mood", mood.capitalize())

    # Add time info
    status_table.add_row("Last Fed", pet_data["last_fed"].strftime("%H:%M:%S"))
    status_table.add_row("Last Played", pet_data["last_played"].strftime("%H:%M:%S"))

    # Add inventory and tricks
    status_table.add_row("Inventory", ", ".join(pet_data["inventory"]))
    status_table.add_row("Tricks Known", ", ".join(pet_data["tricks_known"]))

    console.print(status_table)
```

### examples\03-intermediate-guides\01-saving-loading-flocks.py

- **Lines**: 130
- **Last modified**: 2025-05-22 18:59:08

```py
"""
Demo of File Path Support in Flock Serialization

This example demonstrates:
1. Creating a custom component class
2. Creating a Flock with that component
3. Serializing it to YAML with file paths
4. Loading it back using file path fallback

Usage:
    python file_path_demo.py
"""

from typing import Dict

from flock.core import (
    Flock,
    FlockAgent,
    FlockContext,
    FlockFactory,
    FlockModule,
    FlockModuleConfig,
    flock_component,
    flock_tool,
    flock_type,
)
from pydantic import BaseModel, Field


# Define a simple module
# The config defines a language
class GreetingModuleConfig(FlockModuleConfig):
    language: str = Field(default="en")


# The module has a greeting dictionary
# and will replace the result["greeting"] with the appropriate greeting
# decoraters add the decorated entity to the registry
@flock_component
class GreetingModule(FlockModule):
    """A simple module that generates greetings."""

    config: GreetingModuleConfig = Field(default_factory=GreetingModuleConfig)
    greetings: dict[str, str] = Field(default_factory=dict)

    # The initialize method is called when the agent is initialized
    async def initialize(
        self, agent: FlockAgent, inputs: Dict, context: FlockContext
    ) -> None:
        """Initialize the module."""
        self.greetings = {
            "en": "Hello",
            "es": "Hola",
            "fr": "Bonjour",
            "de": "Guten Tag",
        }

    # The post_evaluate method is called after the agent has evaluated
    # we modify the result before it is returned
    async def post_evaluate(
        self, agent: FlockAgent, inputs: Dict, result: Dict, context: FlockContext
    ) -> Dict:
        """Post-evaluate the module."""
        name = inputs.get("name", "World")
        greeting = self.greetings.get(self.config.language, self.greetings["en"])
        result["greeting"] = f"{greeting}, {name}!"
        return {"greeting": f"{greeting}, {name}!"}


# Define a custom type
# The type is added to the registry
@flock_type
class Person(BaseModel):
    """A simple person model."""

    name: str = Field(description="The name of the person IN ALL CAPS")
    age: int
    languages: list[str] = Field(default_factory=list)


# Define a tool
# The tool is added to the registry
@flock_tool
def get_mobile_number(name: str) -> str:
    """A tool that returns a mobile number to a name."""
    return "1234567890"


def serialization():
    """Run the serialization demo."""

    # Create a Flock instance
    flock = Flock(name="file_path_demo")

    greeting_module = GreetingModule(
        name="greeting_module", config=GreetingModuleConfig(language="es")
    )

    # Create an agent using our GreetingModule
    agent = FlockFactory.create_default_agent(
        name="greeter",
        input="name: str",
        output="greeting: str, mobile_number: str",
        tools=[get_mobile_number],
    )

    agent.add_module(greeting_module)
    # Add the agent to the Flock
    flock.add_agent(agent)

    # Create another agent with a custom type
    person_agent = FlockFactory.create_default_agent(
        name="person_creator",
        input="name: str, age: int, languages: list[str]",
        output="person: Person",
    )
    flock.add_agent(person_agent)

    print("\nSerializing Flock to: file_path_demo.flock.yaml")
    flock.to_yaml_file(".flock/file_path_demo.flock.yaml", path_type="relative")

    # Display the YAML content
    print("\nYAML Content:")
    with open(".flock/file_path_demo.flock.yaml", "r") as f:
        yaml_content = f.read()
        print(yaml_content)


if __name__ == "__main__":
    serialization()
```

### examples\03-intermediate-guides\02-providing-rest-api-ui-chat.py

- **Lines**: 43
- **Last modified**: 2025-05-22 18:59:04

```py
# 02-core-concepts/02-pydantic-types.py
"""
Purpose: Demonstrate using images in Flock
"""

import os

from flock.core import Flock, FlockFactory

MODEL = os.getenv("DEFAULT_MODEL", "openai/gpt-4o")


# --------------------------------
# Create a new Flock instance
# --------------------------------
flock = Flock(name="image_example", model=MODEL)


# A whimsical agent that turns any sentence into Yoda-speak.
yoda_agent = FlockFactory.create_default_agent(
    name="yoda_translator",
    input="text",
    output="yoda_text",
)
flock.add_agent(yoda_agent)


# --------------------------------
# 2. Run Flock as a REST API
# --------------------------------


# This will start the Flock as a REST API and a UI
# It will also create a OpenAPI spec
# http://localhost:8344/docs

flock.serve()


# --- YOUR TURN! ---
# set chat=True to enable the chat interface
# http://localhost:8344/chat
# try executing "flock --web" or "flock --chat" or both in your terminal
```

### examples\03-intermediate-guides\03-custom-endpoints.py

- **Lines**: 149
- **Last modified**: 2025-05-26 17:58:28

```py
# 02-core-concepts/02-pydantic-types.py
"""Purpose: Demonstrate using images in Flock"""

import os

import dspy
from flock.core import Flock, FlockFactory
from flock.core.api.custom_endpoint import FlockEndpoint
from flock.core.flock_registry import (
    flock_type,  # Decorator for registering custom types
)
from pydantic import BaseModel, Field  # Import Pydantic components

MODEL = os.getenv("DEFAULT_MODEL", "openai/gpt-4o")

# --------------------------------
# Create a new Flock instance
# --------------------------------
flock = Flock(name="endpoint_example", model=MODEL)


# --------------------------------
# Pet agent
# --------------------------------


@flock_type
class MyPetsInputModel(BaseModel):
    image: dspy.Image = Field(..., description="A image of the pet.")


@flock_type
class MyPetsOutputModel(BaseModel):
    name: str = Field(..., description="A cute name fitting for the pet.")
    cuteness_factor: float = Field(..., description="A number between 0 and 100.")
    fur_color: str = Field(..., description="The color of the fur.")
    animal_type: str = Field(..., description="The type of animal.")
    cuteness_reasoning: str = Field(
        ..., description="A reasoning for the cuteness factor."
    )
    image_description: str = Field(..., description="A description of the image.")


pet_agent = FlockFactory.create_default_agent(
    name="pet_agent",
    input="pet_query: MyPetsInputModel",
    output="answer: MyPetsOutputModel",
)
flock.add_agent(pet_agent)


# --------------------------------
# Yoda agent
# --------------------------------


class YodaRequest(BaseModel):
    text: str


class YodaResponse(BaseModel):
    yoda_text: str


yoda_agent = FlockFactory.create_default_agent(
    name="yoda_translator",
    input="text",
    output="yoda_text",
)
flock.add_agent(yoda_agent)


# --------------------------------
# Endpoints
# --------------------------------


# Endpoint with body
async def yoda_endpoint(body: YodaRequest, flock: Flock | None = None) -> YodaResponse:  # type: ignore[valid-type]
    """Translate :pyattr:`YodaRequest.text` into the wisdom of Master Yoda."""
    result = await flock.run_async(start_agent=yoda_agent, input={"text": body.text})
    return YodaResponse(yoda_text=result["yoda_text"])


yoda_route = FlockEndpoint(
    path="/api/yoda",
    methods=["POST"],
    callback=yoda_endpoint,
    request_model=YodaRequest,
    response_model=YodaResponse,
    summary="Translate English into Yoda-speak",
    description="Fun demo endpoint powered by a single Flock agent.",
)


# Endpoint with query parameters


class ImageUrlParams(BaseModel):
    img_url: str


async def image_endpoint(query: ImageUrlParams, flock: Flock) -> MyPetsOutputModel:
    my_input = MyPetsInputModel(image=dspy.Image.from_url(query.img_url))
    result = await flock.run_async(
        start_agent="pet_agent",
        input={"pet_query": my_input},
    )
    return result.answer


img_url_route = FlockEndpoint(
    path="/api/cute-pet",
    methods=["GET"],
    callback=image_endpoint,
    query_model=ImageUrlParams,
    response_model=MyPetsOutputModel,
    summary="Calculates the cuteness of a pet",
    description="Takes an image url and returns a description of the pet.",
)


# Endpoint with query parameters without agent


class WordCountResponse(BaseModel):
    count: int


class WordCountParams(BaseModel):
    text: str


async def word_count(query: WordCountParams):
    return WordCountResponse(count=len(query.text.split()))


word_count_route = FlockEndpoint(
    path="/api/word_count",
    methods=["GET"],
    callback=word_count,
    query_model=WordCountParams,
    response_model=WordCountResponse,
    summary="Counts words in a text",
    description="Takes a text and returns the number of words in it.",
)


flock.serve(custom_endpoints=[img_url_route, word_count_route, yoda_route], chat=True)
```

### examples\03-intermediate-guides\to_do\01-building-custom-tools.py

- **Lines**: 51
- **Last modified**: 2025-05-22 18:59:04

```py
import random

from flock.core.flock import Flock
from flock.core.flock_factory import FlockFactory
from flock.core.flock_registry import flock_tool


@flock_tool
def get_specials():
    "Provides a list of specials from the menu."
    return """
        Special Soup: Clam Chowder
        Special Salad: Cobb Salad
        Special Drink: Chai Tea
        """


@flock_tool
def get_price(item: str):
    """Provides the price of the requested menu item.

    Args:
      item: The name of the menu item.
    """
    # random price between 5 and 15
    return f"${random.randint(5, 15)}"


#################################


# create a flock
flock = Flock(name="Own Tools Demo")

# create an agent
agent = FlockFactory.create_default_agent(
    name="Menu Assistant",
    description="You are a helpful assistant",
    input="query",
    output="answer",
    tools=[get_specials, get_price],
    # include_thought_process=True, # flock will include the thought process of the agent in the output if available
)

# add the agent to the flock
flock.add_agent(agent)

# run the agent
flock.run(agent, input={"query": "What is the price of the soup special?"})
# Try
# flock.run(agent, input={"query": "What is the sum of the prices of all specials?"})
```

### examples\03-intermediate-guides\to_do\01-memory.py

- **Lines**: 36
- **Last modified**: 2025-05-22 18:59:08

```py
from flock.core import Flock, FlockFactory
from flock.modules.mem0_async.async_mem0_module import AsyncMem0ModuleConfig
from flock.tools import web_tools

MODEL = "openai/gpt-4o"


flock = Flock(name="memory_flock", model=MODEL)


chatty_agent = FlockFactory.create_default_agent(
    name="my_chatty_agent",
    description="A chatty agent with a memory",
    input="query, memory",
    output="answer",
    tools=[web_tools.web_content_as_markdown],
)
chatty_agent.add_component(
    config_instance=AsyncMem0ModuleConfig(
        user_id="fred", memory_input_key="memory", agent_id="my_chatty_agent"
    )
)
flock.add_agent(chatty_agent)

flock.run(
    start_agent=chatty_agent,
    input={
        "query": "Please remember the main news of today as seen on https://lite.cnn.com/"
    },
)


# flock.run(
#     start_agent=chatty_agent,
#     input={"query": "What Trump news do you remember without using the web?"},
# )
```

### examples\03-intermediate-guides\to_do\01-memory_internal.py

- **Lines**: 30
- **Last modified**: 2025-05-22 18:59:08

```py
from flock.core import Flock, FlockFactory
from flock.modules.memory.memory_module import MemoryModuleConfig
from flock.tools import web_tools

MODEL = "openai/gpt-4o"


flock = Flock(name="memory_flock", model=MODEL)


chatty_agent = FlockFactory.create_default_agent(
    name="my_chatty_agent",
    description="A chatty agent with a memory",
    input="query, memory",
    output="answer",
    tools=[web_tools.web_content_as_markdown],
)
chatty_agent.add_component(config_instance=MemoryModuleConfig())
flock.add_agent(chatty_agent)

# flock.run(
#     start_agent=chatty_agent,
#     input={"query": "Please remember the main news of today as seen on https://lite.cnn.com/"},
# )


flock.run(
    start_agent=chatty_agent,
    input={"query": "What Trump news do you remember without using the web?"},
)
```

### examples\03-intermediate-guides\to_do\02-dynamic-routing.py

- **Lines**: 5
- **Last modified**: 2025-05-22 18:59:04

```py
"""
Placeholder for 02-dynamic-routing.py
"""

print("Hello from 02-dynamic-routing.py!")
```

### examples\03-intermediate-guides\to_do\02-memory-graph.py

- **Lines**: 71
- **Last modified**: 2025-05-22 18:59:08

```py
from flock.core import Flock, FlockFactory
from flock.modules.memory.memory_module import MemoryModuleConfig
from flock.tools import web_tools

MODEL = "openai/gpt-4o"

# docker run -p 7687:7687 memgraph/memgraph-mage:latest --schema-info-enabled=True
# Please start memgraph before running this script


# config = {
#     "embedder": {
#         "provider": "openai",
#         "config": {"model": "text-embedding-3-large", "embedding_dims": 1536},
#     },
#     "graph_store": {
#         "provider": "memgraph",
#         "config": {
#             "url": "bolt://localhost:7687",
#             "username": "memgraph",
#             "password": "xxx", # TODO: change to your memgraph password
#         },
#     },
#     "vector_store": {
#         "provider": "chroma",
#         "config": {
#             "collection_name": "flock_memory",
#             "path": ".flock/memory",
#         }
#     }
# }

flock = Flock(name="memory_graph_flock", model=MODEL)


simple_web_scraper = FlockFactory.create_default_agent(
    name="simple_web_scraper",
    description="A simple web scraper that collects the content of a given url",
    input="url",
    output="content",
    tools=[web_tools.web_content_as_markdown],
)
simple_web_scraper.add_component(
    config_instance=MemoryModuleConfig(enable_write_only_mode=True)
)
flock.add_agent(simple_web_scraper)


memory_agent = FlockFactory.create_default_agent(
    name="memory_agent",
    description="A simple agent that uses its memory to answer questions",
    input="query, memory",
    output="answer",
)
memory_agent.add_component(
    config_instance=MemoryModuleConfig(enable_read_only_mode=True)
)
flock.add_agent(memory_agent)

flock.run(
    start_agent=simple_web_scraper,
    input={
        "url": "https://lite.cnn.com/travel/alexander-the-great-macedon-persian-empire-darius/index.html"
    },
)


# flock.run(
#     start_agent=memory_agent,
#     input={"query": "What can you tell me about Alexander the Great?"},
# )
```

### examples\03-intermediate-guides\to_do\03-basic-rag-agent.py

- **Lines**: 26
- **Last modified**: 2025-05-22 18:59:04

```py
from flock.core.flock import Flock
from flock.core.flock_factory import FlockFactory
from flock.core.tools.azure_tools import azure_search_query

# create a flock
flock = Flock(name="Rag Demo")
# create an agent
agent = FlockFactory.create_default_agent(
    name="Menu Assistant",
    description="An agent that queries Azure Search N-time (as defined by the number_of_searches parameter). Every try will change the query so more relevant results are found.",
    input="query, number_of_searches",
    output="answer",
    tools=[azure_search_query],
    include_thought_process=True,
)

# add the agent to the flock
flock.add_agent(agent)

flock.to_yaml_file("flock.yaml")

# run the agent
flock.run(
    agent,
    input={"query": "Wo ist die Turnhalle von Schattdecor?", "number_of_searches": 2},
)
```

### examples\03-intermediate-guides\to_do\04-knowledge-graph-memory.py

- **Lines**: 5
- **Last modified**: 2025-05-22 18:59:04

```py
"""
Placeholder for 04-knowledge-graph-memory.py
"""

print("Hello from 04-knowledge-graph-memory.py!")
```

### examples\04-advanced-features\01-batch-processing-creativity.py

- **Lines**: 61
- **Last modified**: 2025-05-25 23:46:23

```py
from flock.core import Flock, FlockFactory

# Let's revisit the presentation agent but this time with batch processing!
MODEL = "openai/gpt-4o"


flock = Flock(
    name="example_09", description="This is a batch processing example", model=MODEL
)

# we add some more input fields to the agent
presentation_agent = FlockFactory.create_default_agent(
    name="my_presentation_agent",
    input="topic, audience, number_of_slides",
    output="fun_title, fun_slide_headers, fun_slide_summaries",
    use_cache=False,
    no_output=True,
)
flock.add_agent(presentation_agent)

# define the batch data with a list of inputs for the fields you want to change
batch_data = [
    {"topic": "Robot Kittens", "audience": "Tech Enthusiasts"},
    {"topic": "AI in Gardening", "audience": "Homeowners"},
    {"topic": "The Future of Coffee", "audience": "Foodies"},
    {"topic": "Quantum Physics for Pets", "audience": "Animal Lovers"},
    {"topic": "Underwater Basket Weaving", "audience": "Extreme Sports Enthusiasts"},
    {"topic": "Space Tourism on a Budget", "audience": "Adventurous Retirees"},
    {"topic": "Blockchain Baking", "audience": "Culinary Students"},
    {"topic": "Time Travel Tourism", "audience": "History Buffs"},
    {"topic": "Telepathic Interior Design", "audience": "Minimalists"},
    {
        "topic": "Dancing with Dinosaurs",
        "audience": "Children's Entertainment Professionals",
    },
    {"topic": "Martian Fashion Trends", "audience": "Fashion Designers"},
    {"topic": "Edible Architecture", "audience": "Urban Planners"},
    {"topic": "Antigravity Yoga", "audience": "Fitness Instructors"},
    {"topic": "Digital Smell Technology", "audience": "Perfume Connoisseurs"},
    {"topic": "Musical Vegetables", "audience": "Orchestra Conductors"},
]

# define the static data for the batch run
static_data = {"number_of_slides": 6}

# flock.to_yaml_file(".flock/batch_processing.flock.yaml")
# instead of flock.run() we use flock.run_batch()
silent_results = flock.run_batch(
    start_agent=presentation_agent,
    batch_inputs=batch_data,
    static_inputs=static_data,
    parallel=True,
    max_workers=5,
    silent_mode=True,
    return_errors=True,
    write_to_csv=".flock/batch_results.csv",
)

print("\nBatch finished. Results (or errors):")
for res in silent_results:
    print(res)
```

### examples\04-advanced-features\02-agent-evaluation-benchmark.py

- **Lines**: 144
- **Last modified**: 2025-05-25 23:46:23

```py
from pathlib import Path

from flock.core import Flock, FlockFactory
from flock.core.logging.logging import get_logger
from flock.core.tools.basic_tools import (
    code_eval,
    evaluate_math,
    get_web_content_as_markdown,
    web_search_tavily,
)

logger = get_logger("evaluate_example")

# --- 1. Define the Agent to Evaluate ---
# Let's create a simple Q&A agent that takes a 'query' and outputs an 'answer'.
# We'll map the dataset's 'question' column to our agent's 'query' input.
qa_agent = FlockFactory.create_default_agent(
    name="qa_agent",
    description="Answers questions based on general knowledge.",
    input="query: str | The question to be answered",
    output="answer: str | The concise answer to the question",
    tools=[
        web_search_tavily,
        get_web_content_as_markdown,
        code_eval,
        evaluate_math,
    ],
    model="openai/gpt-4o",  # Or your preferred model
    max_tokens=15000,
)

# --- 2. (Optional) Define an LLM Judge Agent ---
# This agent will compare the predicted vs. actual answer based on instructions.
judge_agent = FlockFactory.create_default_agent(
    name="answer_judge",
    description="Evaluates the correctness of a predicted answer compared to a reference answer.",
    input="""
        prediction: str | The answer generated by the agent being evaluated,
        reference: str | The ground truth answer from the dataset,
        question: str | The question that was asked,
        instruction: str | Optional instructions for evaluation (e.g., 'Focus on factual accuracy')
    """,
    output="""
        score: float | A score between 0.0 (incorrect) and 1.0 (perfectly correct),
        reasoning: str | Explanation for the score given
    """,
    model="openai/gpt-4o",  # Use a capable model for judging
    max_tokens=15000,
)

# --- 3. Create the Flock Instance ---
flock = Flock(name="evaluation_flock")
flock.add_agent(qa_agent)
flock.add_agent(judge_agent)  # Add the judge agent if using it as a metric

# --- 4. Define Evaluation Parameters ---

# Dataset Identifier (Hugging Face Hub)
# https://huggingface.co/datasets/smolagents/benchmark-v1
DATASET_ID = "smolagents/benchmark-v1"

# Input Mapping: Map dataset 'question' column to agent's 'query' input
input_mapping = {"question": "query"}

# Answer Mapping: Map agent's 'answer' output to dataset's 'true_answer' column
answer_mapping = {"answer": "true_answer"}

# Metrics to calculate: Use built-in names and the judge agent instance
# Note: For 'semantic_similarity', ensure 'sentence-transformers' is installed.
# Note: For 'fuzzy_match', ensure 'thefuzz[speedup]' is installed.
# Note: For 'rouge_*', ensure 'rouge-score' is installed.
metrics_to_run = [
    "exact_match",
    "fuzzy_match",
    "semantic_similarity",
    judge_agent,  # Pass the actual agent instance for LLM judging
]

# Optional: Configuration for specific metrics (e.g., threshold for fuzzy match)
metric_configs = {
    "fuzzy_match": {"threshold": 90},
    # 'llm_judge': {'instruction': 'Assess if the prediction correctly answers the core intent of the reference.'} # Config for the judge agent itself, or pass via static_inputs if needed by the judge's prompt
}

# Optional: Columns from the dataset to include in the results file
metadata_columns_to_keep = ["source", "true_reasoning"]

# Optional: Output file path
results_file = Path("evaluation_results.csv")

# --- 5. Run the Evaluation ---
print(f"Starting evaluation of agent '{qa_agent.name}' on dataset '{DATASET_ID}'...")

# Using the synchronous wrapper for simplicity in this example
try:
    evaluation_df = flock.evaluate(
        dataset=DATASET_ID,  # HF Dataset ID
        # dataset_split=DATASET_SPLIT,            # TODO: Add support for dataset splits
        start_agent=qa_agent.name,  # Agent to run
        input_mapping=input_mapping,  # How inputs map
        answer_mapping=answer_mapping,  # How outputs map to answers
        metrics=metrics_to_run,  # What metrics to compute
        metric_configs=metric_configs,  # Metric-specific settings
        metadata_columns=metadata_columns_to_keep,  # Keep 'source' column
        output_file=results_file,  # Save detailed results
        return_dataframe=True,  # Get results as DataFrame
        silent_mode=True,  # Show progress bar
        error_handling="log",  # Log errors but continue
    )

    print(f"\nEvaluation complete. Results saved to {results_file}")

    # --- 6. Display Summary ---
    print("\n--- Evaluation Summary ---")
    print(f"Total items evaluated: {len(evaluation_df)}")

    # Display average scores for numeric metrics
    numeric_metrics = evaluation_df.select_dtypes(include="number").columns
    # Filter to include only the calculated metric columns (based on metrics_to_run)
    metric_cols = [m if isinstance(m, str) else m.name for m in metrics_to_run]
    # Expand potential dict metrics (like rouge)
    flat_metric_names = []
    for m_name in metric_cols:
        matching_cols = [c for c in numeric_metrics if c.startswith(m_name)]
        if matching_cols:
            flat_metric_names.extend(matching_cols)
        elif m_name in numeric_metrics:  # Handle simple metric names directly
            flat_metric_names.append(m_name)

    if flat_metric_names:
        avg_scores = evaluation_df[flat_metric_names].mean()
        print("\nAverage Scores:")
        print(avg_scores.to_string())
    else:
        print("No numeric metrics found to average.")

    # Display head of the detailed results DataFrame
    print("\n--- Detailed Results (First 5 Rows) ---")
    print(evaluation_df.head().to_markdown(index=False))

except ValueError as ve:
    print(f"\n[ERROR] Evaluation setup failed: {ve}")
except Exception as e:
    print(f"\n[ERROR] An unexpected error occurred during evaluation: {e}")
```

### examples\04-advanced-features\03-streaming-generation.py

- **Lines**: 5
- **Last modified**: 2025-05-22 18:59:04

```py
"""
Placeholder for 03-streaming-generation.py
"""

print("Hello from 03-streaming-generation.py!")
```

### examples\04-advanced-features\04-hierarchical-memory.py

- **Lines**: 5
- **Last modified**: 2025-05-22 18:59:04

```py
"""
Placeholder for 04-hierarchical-memory.py
"""

print("Hello from 04-hierarchical-memory.py!")
```

### examples\04-advanced-features\05-api-interaction.py

- **Lines**: 5
- **Last modified**: 2025-05-22 18:59:04

```py
"""
Placeholder for 05-api-interaction.py
"""

print("Hello from 05-api-interaction.py!")
```

### examples\04-advanced-features\06-cli-interaction.py

- **Lines**: 5
- **Last modified**: 2025-05-22 18:59:04

```py
"""
Placeholder for 06-cli-interaction.py
"""

print("Hello from 06-cli-interaction.py!")
```

### examples\04-advanced-features\07-hydrator.py

- **Lines**: 76
- **Last modified**: 2025-05-22 18:59:04

```py
# example_script.py
import asyncio

from pydantic import BaseModel, Field

from flock.core.flock_registry import flock_type
from flock.core.util.hydrator import flockclass


@flock_type
class Movie(BaseModel):
    title: str
    year: int
    rating: float
    cast: list[str]
    director: str


# --- Define your Pydantic Models ---
@flockclass(model="openai/gpt-4o") 
class RandomPerson(BaseModel):
    name: str | None = None
    age: int | None = None
    bio: str | None = Field(default=None, description="A short biography")
    job: str | None = None
    fav_animal: str | None = None
    lucky_numbers: list[int] | None = None
    favorite_movies: list[Movie] | None = Field(
        default=None, description="Favorite three movies"
    )


@flockclass(model="openai/gpt-4o")
class BlogPostIdea(BaseModel):
    topic: str | None = None
    target_audience: str | None = None
    key_points: list[str] | None = None
    catchy_title: str | None = None


async def main():
    print("--- Hydrating Person ---")
    # Create an instance with some initial data
    person = RandomPerson(
        job="Software Engineer",
    )  # Age is provided, others are None
    print(f"Before hydration: {person}")

    hydrated_person = person.hydrate()

    print(f"After hydration:  {hydrated_person}")
    print(f"Jeff's Job: {hydrated_person.job}")
    print("-" * 20)

    print("\n--- Hydrating Blog Post Idea ---")
    idea = BlogPostIdea(topic="Sustainable Urban Gardening")
    print(f"Before hydration: {idea}")

    hydrated_idea = idea.hydrate()
    print(f"After hydration:  {hydrated_idea}")
    print(f"Catchy Title: {hydrated_idea.catchy_title}")
    print("-" * 20)

    # --- Example: Hydrating from scratch ---
    print("\n--- Hydrating Person (from scratch) ---")
    person_scratch = RandomPerson()  # No fields provided
    print(f"Before hydration: {person_scratch}")
    hydrated_person_scratch = person_scratch.hydrate()
    print(f"After hydration:  {hydrated_person_scratch}")
    print("-" * 20)


if __name__ == "__main__":
    # Make sure necessary API keys are set in environment variables!
    # e.g., OPENAI_API_KEY
    asyncio.run(main())
```

### examples\04-advanced-features\08-temporal.py

- **Lines**: 97
- **Last modified**: 2025-05-22 18:59:04

```py
import asyncio
from datetime import timedelta

from flock.core import Flock, FlockFactory
from flock.routers.default.default_router import DefaultRouterConfig
from flock.workflow.temporal_config import (
    TemporalActivityConfig,
    TemporalRetryPolicyConfig,
    TemporalWorkflowConfig,
)


async def main():
 
    # --------------------------------
    # Create the flock with Temporal config
    # --------------------------------
    # Set global Temporal parameters
    # In this case, we're setting the task queue to "flock-test-queue"
    # and the workflow execution timeout to 10 minutes
    # and the default activity retry policy to 2 attempts
    flock = Flock(
        enable_temporal=True,
        temporal_config=TemporalWorkflowConfig(
            task_queue="flock-test-queue",
            workflow_execution_timeout=timedelta(minutes=10),
            default_activity_retry_policy=TemporalRetryPolicyConfig(
                maximum_attempts=2
            ),
        ),
    )

    # --------------------------------
    # Create a normal agent
    # --------------------------------
    agent = FlockFactory.create_default_agent(
        name="my_presentation_agent",
        input="topic",
        output="funny_title, funny_slide_headers",
    )

    flock.add_agent(agent)

   
    # --------------------------------
    # Create a Temporal ready agent
    # --------------------------------
    # This agent is ready to be used with Temporal
    # It has a Temporal activity config that sets the start to close timeout to 1 minute
    # and the retry policy to 4 attempts
    # and the non retryable error types to "ValueError"
    content_agent = FlockFactory.create_default_agent(
        name="content_agent",
        input="funny_title, funny_slide_headers",
        output="funny_slide_content",
        temporal_activity_config=TemporalActivityConfig(
            start_to_close_timeout=timedelta(minutes=1),
            retry_policy=TemporalRetryPolicyConfig(
                maximum_attempts=4,
                initial_interval=timedelta(seconds=2),
                non_retryable_error_types=["ValueError"],
            ),
        ),
    )

    flock.add_agent(content_agent)

    # --------------------------------
    # Add the agent to the router
    # --------------------------------
    # This is the router that will be used to route the my_presentation_agent to the content agent
    agent.add_component(DefaultRouterConfig(hand_off="content_agent"))

    print(
        f"Starting Flock run on Temporal task queue: {flock.temporal_config.task_queue}"
    )

    # --------------------------------
    # Run the flock
    # --------------------------------
    # This is the main function that will be used to run the flock
    result = await flock.run_async(
        start_agent="my_presentation_agent",
        input={
            "topic": "A presentation about how good of an idea it is to combine ai agents with temporal.io"
        },
    )

    print("\n--- Result ---")
    print(result)


if __name__ == "__main__":
    print(
        "Ensure a Temporal worker is running and listening on the specified task queue(s)."
    )
    asyncio.run(main())
```

### examples\05-full-projects\project-codebase-documenter\agents.py

- **Lines**: 5
- **Last modified**: 2025-05-22 18:59:04

```py
"""
Placeholder for agents.py
"""

print("Hello from agents.py!")
```

### examples\05-full-projects\project-codebase-documenter\main.py

- **Lines**: 5
- **Last modified**: 2025-05-22 18:59:04

```py
"""
Placeholder for main.py
"""

print("Hello from main.py!")
```

### examples\05-full-projects\project-dynamic-website-gen\agents.py

- **Lines**: 5
- **Last modified**: 2025-05-22 18:59:04

```py
"""
Placeholder for agents.py
"""

print("Hello from agents.py!")
```

### examples\05-full-projects\project-dynamic-website-gen\main.py

- **Lines**: 52
- **Last modified**: 2025-05-22 18:59:04

```py
from pydantic import BaseModel, Field
from flock.core import FlockFactory, Flock, flock_type

@flock_type
class DynamicHTMLApp(BaseModel):
    name: str = Field(..., description="Name of the app")
    requirements: list[str] = Field(..., description="User specified requirements for the app")
    description: str = Field(..., description="High level description of the data and functionality of the app, as well as design decisions")
    html_content: str = Field(..., description="HTML content of the app")
    css_content: str = Field(..., description="CSS content of the app")
    js_content: str = Field(..., description="JS content of the app")
    html_file: str = Field(..., description="HTML file name")
    css_file: str = Field(..., description="CSS file name")
    js_file: str = Field(..., description="JS file name")
    
MODEL = "gemini/gemini-2.5-pro-exp-03-25" #"groq/qwen-qwq-32b"    #"openai/gpt-4o" # 
flock = Flock(model=MODEL)

app_agent = FlockFactory.create_default_agent(name="app_agent",
                                              description="An agent that generates a static html app based on the requirements and the input_data. "
                                              "The input_data is the content of a json file that contains the data to be displayed in the app."
                                              "The final app should load the input_data from json files in a folder called 'data' in the same directory as the app."
                                              "The app should present the content of the input file as if designed by a professional UX designer and dedicated to the data in the input file."
                                              "For example, if the input data is a story, the app should present the story as if it is a dedicated story app."
                                              "If for example the input data is a list of products, the app should present the products as if it is a dedicated product app.",
                                              input="requirements: str, input_data: str",
                                              output="app: DynamicHTMLApp",
                                              stream=True,
                                              max_tokens=60000)

flock.add_agent(app_agent)

requirements = "elegant, professional, color-coded, modern, dark mode"
input_file = "output/story_agent_output_20250422_225751.json"
output_dir = "output/apps/"

# Load the input data as string 
with open(input_file, 'r') as f:
    input_data = f.read()

result = flock.run(start_agent=app_agent, input={'requirements': requirements, 'input_data': input_data}) 
app = result.app

#save html, css, js to files
with open(output_dir + app.html_file, 'w') as f:
    f.write(app.html_content)
with open(output_dir + app.css_file, 'w') as f:
    f.write(app.css_content)
with open(output_dir + app.js_file, 'w') as f:
    f.write(app.js_content)

print(f"App saved to {output_dir + app.html_file}")
```

### examples\05-full-projects\project-roguelike-ai\game_logic.py

- **Lines**: 5
- **Last modified**: 2025-05-22 18:59:04

```py
"""
Placeholder for game_logic.py
"""

print("Hello from game_logic.py!")
```

### examples\05-full-projects\project-roguelike-ai\npc_agents.py

- **Lines**: 5
- **Last modified**: 2025-05-22 18:59:04

```py
"""
Placeholder for npc_agents.py
"""

print("Hello from npc_agents.py!")
```

### examples\05-full-projects\project-story-engine\agents.py

- **Lines**: 5
- **Last modified**: 2025-05-22 18:59:04

```py
"""
Placeholder for agents.py
"""

print("Hello from agents.py!")
```

### examples\05-full-projects\project-story-engine\types.py

- **Lines**: 5
- **Last modified**: 2025-05-22 18:59:04

```py
"""
Placeholder for types.py
"""

print("Hello from types.py!")
```

### examples\05-full-projects\project-textual-project-planner\src\flock_flightplan\__init__.py

- **Lines**: 13
- **Last modified**: 2025-05-22 18:59:04

```py

#import debugpy

from flock_flightplan.app import FlightPlanApp


def main() -> None:
    """Run the app."""
    app = FlightPlanApp()
    # debugpy.listen(("0.0.0.0", 5679))
    # print("⏳ Waiting for debugger attach...")
    # debugpy.wait_for_client()
    app.run()
```

### examples\05-full-projects\project-textual-project-planner\src\flock_flightplan\agents\flocky.py

- **Lines**: 47
- **Last modified**: 2025-05-22 18:59:04

```py

from datetime import datetime
import uuid
from flock.core import Flock, FlockFactory, flock_type
from pydantic import BaseModel, Field

@flock_type
class Message(BaseModel):
    content: str = Field(description="The content of the message")
    role: str = Field(description="The role of the message")
    timestamp: str = Field(default= datetime.now().strftime("%Y-%m-%d %H:%M:%S"), description="The timestamp of the message")
    id: str = Field(default=uuid.uuid4, description="The id of the message")
    context: str = Field(default="", description="The context of the message")


message_history : list[Message] = []
message_history.append(Message(content="Hello, I'm Flocky! How can I help you today? Please enter your command or message below.", role="flocky"))

async def generate_flocky_response(user_input: str, context: str)->str:
    flock = Flock(model="gpt-4.1-2025-04-14")

    flocky = FlockFactory.create_default_agent(
        name="flocky",
        description="A helpful assistant that formulates answers based on the user's input and the project plan." 
        "Project plan contains the project plan for the user. Your job is to explain the project plan to the user.",
        input="user_input: str, project_plan: str, previous_messages: list[Message]",
        output="answer: str | answer to the user's input with information from the project plan",
        write_to_file=True,
        no_output=True,
    )
    
    flock.add_agent(flocky)

    result = await flock.run_async(
        start_agent="flocky",
        input={
            "user_input": user_input,
            "project_plan": context,
            "previous_messages": message_history,
        },
    )
    answer = result.answer

    message_history.append(Message(content=user_input, role="user"))
    message_history.append(Message(content=answer, role="flocky"))

    return result
```

### examples\05-full-projects\project-textual-project-planner\src\flock_flightplan\agents\planning.py

- **Lines**: 148
- **Last modified**: 2025-05-22 18:59:04

```py
from pydantic import BaseModel, Field

from flock.core import Flock, FlockFactory, flock_type
from flock.routers.default.default_router import (
    DefaultRouter,
    DefaultRouterConfig,
)


@flock_type
class SchemaRoot(BaseModel):
    """Model representing the root element of the schema which defines the top-level components of the project."""

    allowed_children: dict[str, str] = Field(
        default={},
        description="Mapping of child types to cardinality ('single' or 'multiple')",
    )


@flock_type
class SchemaElement(BaseModel):
    """Model representing a structure schema element in the planning system.

    This model encapsulates the properties of schema elements defined in structure.yaml,
    providing a cleaner interface for working with node types.

    Attributes:
        node_type (str): The internal type identifier for this element
        display_name (str): Human-readable name for display purposes
        icon (str): Icon text representation used in the tree view
        allowed_children (Dict[str, str]): Mapping of child types to cardinality ('single' or 'multiple')
    """

    node_type: str = Field(
        description="The internal type identifier for this element"
    )
    display_name: str = Field(
        description="Human-readable name for display purposes"
    )
    emoji: str = Field(
        default="",
        description="Emoji text representation used in the tree view",
    )
    allowed_children: dict[str, str] = Field(
        default={},
        description="Mapping of child types to cardinality ('single' or 'multiple')",
    )


DESCRIPTION = """
Creates a planning structure for a project.

Example planning structure:

```yaml
root:
  allowed_children:
    ProjectPlan: multiple


ProjectPlan:
  display_name: "Project Plan" # Optional nicer name for display
  icon: " P " # Optional icon
  allowed_children:
    UserStory: multiple


UserStory:
  display_name: "User Story"
  icon: " U "
  allowed_children:
    Task: multiple

Task:
  display_name: "Task"
  icon: " T "
  allowed_children: {} # Tasks are leaf nodes here

```

"""


async def generate_planning_structure(user_input: str):
    flock = Flock(model="gpt-4.1-2025-04-14")


    project_type_agent = FlockFactory.create_default_agent(
        name="project_type_agent",
        description="A helpful assistant that determines the type of project based on the user's input.",
        input="user_input",
        output="project_type: str | high levelproject type like 'web development' - 'book writing' - 'data analysis'... and so on",
        write_to_file=True,
        no_output=True,
    )


    planning_schema_agent = FlockFactory.create_default_agent(
        name="planning_schema_agent",
        description=DESCRIPTION,
        input="project_type",
        output="root_element: SchemaRoot, implementation_plan: list[SchemaElement] | The schema structure of a plan to implement the project type. Think of the seperate steps needed to implement the user request. It should be a general plan for the type of project.",
        write_to_file=True,
        no_output=True,
    )


    template_agent = FlockFactory.create_default_agent(
        name="template_agent",
        description="An agent that generates content templates",
        input="root_element: SchemaRoot, implementation_plan: list[SchemaElement]",
        output="markdown_templates: list[tuple[str, str]] | Templates for the implementation of a project element. One template per project element. The template defines which information is needed to implement the project element. For example a template for a user story could contain the fields title - description - acceptance criteria - story points etc. The tuple contains the project element name and the template.",
        write_to_file=True,
        no_output=True,
    )

    flock.add_agent(project_type_agent)
    flock.add_agent(planning_schema_agent)
    flock.add_agent(template_agent)

    project_type_agent.handoff_router = DefaultRouter(
        name="project_type_router",
        config=DefaultRouterConfig(hand_off="planning_schema_agent"),
    )

    planning_schema_agent.handoff_router = DefaultRouter(
        name="planning_schema_router",
        config=DefaultRouterConfig(hand_off="template_agent"),
    )

    # template_agent.handoff_router = IterativeListGeneratorRouter(
    #     name="template_router",
    #     config=IterativeListGeneratorRouterConfig(
    #         target_list_field="markdown_templates",
    #         item_output_field="markdown_template : str | Template for the design of a document of the type of the project element",
    #         context_input_field="already_generated_templates",
    #     ),
    # )


    result = await flock.run_async(
        start_agent="project_type_agent",
        input={
            "user_input": user_input
        },
    )

    return result
```

### examples\05-full-projects\project-textual-project-planner\src\flock_flightplan\cli\utils.py

- **Lines**: 141
- **Last modified**: 2025-05-22 18:59:04

```py
# src/pilot_rules/collector/utils.py
import datetime
from pathlib import Path
from typing import Dict, Any, List, Optional


from rich.panel import Panel
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn
from rich.table import Table
from rich import box

from textual.widgets import RichLog



log : RichLog = None



def set_app_log(log_component):
    """Set the app's RichLog component for output"""
    global log
    log = log_component


def get_file_metadata(file_path: str) -> Dict[str, Any]:
    """Extract metadata from a file."""
    metadata = {
        "path": file_path,
        "size_bytes": 0,
        "line_count": 0,
        "last_modified": "Unknown",
        "created": "Unknown",
    }

    try:
        p = Path(file_path)
        stats = p.stat()
        metadata["size_bytes"] = stats.st_size
        metadata["last_modified"] = datetime.datetime.fromtimestamp(
            stats.st_mtime
        ).strftime("%Y-%m-%d %H:%M:%S")
        # ctime is platform dependent (creation on Windows, metadata change on Unix)
        # Use mtime as a reliable fallback for "created" if ctime is older than mtime
        ctime = stats.st_ctime
        mtime = stats.st_mtime
        best_ctime = ctime if ctime <= mtime else mtime  # Heuristic
        metadata["created"] = datetime.datetime.fromtimestamp(best_ctime).strftime(
            "%Y-%m-%d %H:%M:%S"
        )

        try:
            # Attempt to read as text, fallback for binary or encoding issues
            with p.open("r", encoding="utf-8", errors="ignore") as f:
                content = f.read()
                metadata["line_count"] = len(content.splitlines())
        except (OSError, UnicodeDecodeError) as read_err:
            # Handle cases where reading might fail (binary file, permissions etc.)
            log.write(
                f"[yellow]⚠ Warning:[/yellow] Could not read content/count lines for [cyan]{file_path}[/cyan]: [red]{read_err}[/red]"
            )
            metadata["line_count"] = 0  # Indicate unreadable or binary

    except Exception as e:
        log.write(
            f"[yellow]⚠ Warning:[/yellow] Could not get complete metadata for [cyan]{file_path}[/cyan]: [red]{e}[/red]"
        )

    return metadata


# --- Rich Formatting Utilities ---


def print_header(title: str, style: str = "blue") -> None:
    """Print a styled header with a panel."""
    log.write(
        Panel.fit(f"[bold {style}]{title}[/bold {style}]", border_style=style)
    )


def print_subheader(title: str, style: str = "cyan") -> None:
    """Print a styled subheader."""
    log.write(f"[bold {style}]== {title} ==[/bold {style}]")


def print_success(message: str) -> None:
    """Print a success message."""
    log.write(f"[bold green]✓[/bold green] {message}")


def print_error(message: str, exit_code: Optional[int] = None) -> None:
    """Print an error message and optionally exit."""
    log.write(f"[bold red]✗ ERROR:[/bold red] {message}")
    if exit_code is not None:
        exit(exit_code)


def print_warning(message: str) -> None:
    """Print a warning message."""
    log.write(f"[yellow]⚠ Warning:[/yellow] {message}")


def create_progress() -> Progress:
    """Create a standardized progress bar."""
    return Progress(
        SpinnerColumn(),
        TextColumn("[bold blue]{task.description}"),
        BarColumn(complete_style="green", finished_style="green"),
        TextColumn("[bold]{task.completed}/{task.total}"),
    )


def create_task_table(title: str) -> Table:
    """Create a standardized table for displaying task information."""
    table = Table(
        title=title, show_header=True, header_style="bold cyan", box=box.ROUNDED
    )
    return table


def print_file_stats(files: List[str], title: str = "File Statistics") -> None:
    """Print statistics about a list of files."""
    if not files:
        log.write("[yellow]No files found to display statistics.[/yellow]")
        return

    table = Table(title=title, show_header=True, header_style="bold magenta")
    table.add_column("Statistic", style="cyan")
    table.add_column("Value", style="green")

    extensions = {Path(f).suffix.lower() for f in files if Path(f).suffix}
    total_size = sum(get_file_metadata(f).get("size_bytes", 0) for f in files)
    total_lines = sum(get_file_metadata(f).get("line_count", 0) for f in files)

    table.add_row("Total Files", str(len(files)))
    table.add_row("Total Size", f"{total_size / 1024:.2f} KB")
    table.add_row("Total Lines", str(total_lines))
    table.add_row("Extensions", ", ".join(sorted(extensions)) if extensions else "None")

    log.write(table)
```

### examples\05-full-projects\project-textual-project-planner\src\flock_flightplan\collector\analysis.py

- **Lines**: 421
- **Last modified**: 2025-05-22 18:59:04

```py
# src/pilot_rules/collector/analysis.py
import ast
from pathlib import Path
from typing import List, Dict, Any, Optional, Set

# Import utility function - use relative import within the package
from flock_flightplan.cli.utils import (
    print_warning,
    print_success,
    print_subheader,
)
from textual.widgets import RichLog
log : RichLog = None

def set_app_log(log_component):
    """Set the app's RichLog component for output"""
    global log
    log = log_component


# --- Python Component Extraction ---
def extract_python_components(file_path: str) -> Dict[str, Any]:
    """Extract classes, functions, and imports from Python files."""
    components = {"classes": [], "functions": [], "imports": [], "docstring": None}

    # Ensure it's a python file before trying to parse
    if not file_path.lower().endswith(".py"):
        return components  # Return empty structure for non-python files

    try:
        # Read with error handling for encoding issues
        with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
            content = f.read()

        tree = ast.parse(content)

        # Extract module docstring
        components["docstring"] = ast.get_docstring(
            tree
        )  # Returns None if no docstring

        # Extract top-level classes and functions
        for node in ast.iter_child_nodes(tree):
            if isinstance(node, ast.ClassDef):
                class_info = {
                    "name": node.name,
                    "docstring": ast.get_docstring(node),
                    "methods": [
                        m.name
                        for m in node.body
                        if isinstance(m, (ast.FunctionDef, ast.AsyncFunctionDef))
                    ],
                }
                components["classes"].append(class_info)
            elif isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                # We consider all functions directly under the module body as "top-level" here
                func_info = {
                    "name": node.name,
                    "docstring": ast.get_docstring(node),
                    # Simplified arg extraction (just names)
                    "args": [arg.arg for arg in node.args.args],
                }
                components["functions"].append(func_info)

        # Extract all imports (simplified representation)
        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    # Store 'import x' or 'import x as y'
                    components["imports"].append(
                        f"import {alias.name}"
                        + (f" as {alias.asname}" if alias.asname else "")
                    )
            elif isinstance(node, ast.ImportFrom):
                module_part = node.module or ""
                level_dots = "." * node.level
                # Store 'from .mod import x' or 'from mod import x as y'
                imported_names = []
                for alias in node.names:
                    name_part = alias.name
                    if alias.asname:
                        name_part += f" as {alias.asname}"
                    imported_names.append(name_part)

                components["imports"].append(
                    f"from {level_dots}{module_part} import {', '.join(imported_names)}"
                )

    except SyntaxError as e:
        print_warning(
            f"Could not parse Python components in [cyan]{file_path}[/cyan] due to SyntaxError: [red]{e}[/red]"
        )
    except Exception as e:
        print_warning(
            f"Could not parse Python components in [cyan]{file_path}[/cyan]: [red]{e}[/red]"
        )

    return components


# --- Dependency Analysis ---


def get_module_prefixes(module_name: str) -> List[str]:
    """
    Generate all possible module prefixes for a given module name.
    For example, 'a.b.c' would return ['a.b.c', 'a.b', 'a']
    """
    parts = module_name.split(".")
    return [".".join(parts[:i]) for i in range(len(parts), 0, -1)]


def analyze_code_dependencies(files: List[str]) -> Dict[str, Set[str]]:
    """Analyze dependencies between Python files based on imports."""
    # Filter to only analyze python files within the provided list
    python_files = {f for f in files if f.lower().endswith(".py")}
    if not python_files:
        return {}  # No Python files to analyze

    dependencies: Dict[str, Set[str]] = {file: set() for file in python_files}
    module_map: Dict[str, str] = {}  # Map potential module names to absolute file paths
    project_root = (
        Path.cwd().resolve()
    )  # Assume CWD is project root for relative imports

    # --- Build Module Map (heuristic) ---
    # Map files within the project to their potential Python module paths
    for file_path_str in python_files:
        file_path = Path(file_path_str).resolve()
        try:
            # Attempt to create a module path relative to the project root
            relative_path = file_path.relative_to(project_root)
            parts = list(relative_path.parts)
            module_name = None
            if parts[-1] == "__init__.py":
                module_parts = parts[:-1]
                if module_parts:  # Avoid mapping root __init__.py as empty string
                    module_name = ".".join(module_parts)
            elif parts[-1].endswith(".py"):
                module_parts = parts[:-1] + [parts[-1][:-3]]  # Remove .py
                module_name = ".".join(module_parts)

            if module_name:
                # console.print(f"[dim]Mapping module '{module_name}' to '{file_path_str}'[/dim]") # Debug
                module_map[module_name] = file_path_str

        except ValueError:
            # File is outside the assumed project root, less reliable mapping
            # Map only by filename stem if not already mapped? Risky.
            # console.print(f"[dim]Debug: File {file_path_str} is outside project root {project_root}[/dim]")
            pass

    # --- Analyze Imports in Each File ---
    for file_path_str in python_files:
        file_path = Path(file_path_str).resolve()
        try:
            with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
                code = f.read()
            tree = ast.parse(code)

            for node in ast.walk(tree):
                imported_module_str = None
                target_file: Optional[str] = None

                # Handle 'import x' or 'import x.y'
                if isinstance(node, ast.Import):
                    for alias in node.names:
                        imported_module_str = alias.name
                        # Check full name and prefixes against our map
                        for prefix in get_module_prefixes(imported_module_str):
                            if prefix in module_map:
                                target_file = module_map[prefix]
                                # Ensure the target is actually one of the collected python files
                                if (
                                    target_file in python_files
                                    and target_file != file_path_str
                                ):
                                    dependencies[file_path_str].add(target_file)
                                break  # Found the longest matching prefix

                # Handle 'from x import y' or 'from .x import y'
                elif isinstance(node, ast.ImportFrom):
                    level = node.level
                    module_base = node.module or ""

                    if level == 0:  # Absolute import: 'from package import module'
                        imported_module_str = module_base
                        for prefix in get_module_prefixes(imported_module_str):
                            if prefix in module_map:
                                target_file = module_map[prefix]
                                if (
                                    target_file in python_files
                                    and target_file != file_path_str
                                ):
                                    dependencies[file_path_str].add(target_file)
                                break
                    else:  # Relative import: 'from . import x', 'from ..y import z'
                        current_dir = file_path.parent
                        base_path = current_dir
                        # Navigate up for '..' (level 2 means one level up, etc.)
                        for _ in range(level - 1):
                            base_path = base_path.parent

                        # Try to resolve the relative path
                        relative_module_parts = (
                            module_base.split(".") if module_base else []
                        )
                        target_path_base = base_path
                        for part in relative_module_parts:
                            target_path_base = target_path_base / part

                        # Check if the resolved path corresponds to a known file/module
                        # Check 1: Is it a directory with __init__.py?
                        init_py_path = (target_path_base / "__init__.py").resolve()
                        init_py_str = str(init_py_path)
                        if init_py_str in python_files and init_py_str != file_path_str:
                            dependencies[file_path_str].add(init_py_str)
                            target_file = init_py_str  # Mark as found

                        # Check 2: Is it a .py file directly?
                        module_py_path = target_path_base.with_suffix(".py").resolve()
                        module_py_str = str(module_py_path)
                        if (
                            not target_file
                            and module_py_str in python_files
                            and module_py_str != file_path_str
                        ):
                            dependencies[file_path_str].add(module_py_str)
                            target_file = module_py_str

                        # Note: This relative import resolution is basic and might miss complex cases.
                        # We are primarily checking if the base module path (e.g., `.`, `..utils`) exists.

        except SyntaxError as e:
            print_warning(
                f"Skipping import analysis in [cyan]{file_path_str}[/cyan] due to SyntaxError: [red]{e}[/red]"
            )
        except Exception as e:
            print_warning(
                f"Could not analyze imports in [cyan]{file_path_str}[/cyan]: [red]{e}[/red]"
            )

    return dependencies


# --- Pattern Detection ---


def get_common_patterns(files: List[str]) -> Dict[str, Any]:
    """Identify common code patterns across the repository."""
    patterns = {"python_patterns": {}}

    # Get all Python files
    python_files = [f for f in files if f.lower().endswith(".py")]
    if not python_files:
        return patterns  # No Python files to analyze

    # --- Python Import Patterns ---
    all_imports: Dict[str, int] = {}
    file_imports: Dict[str, List[str]] = {}

    # Basic frameworks imports to check for
    frameworks = {
        "Django": ["django", "django.db", "django.http", "django.urls", "django.views"],
        "Flask": ["flask", "flask_restful", "flask_sqlalchemy"],
        "FastAPI": ["fastapi"],
        "SQLAlchemy": ["sqlalchemy"],
        "PyTorch": ["torch"],
        "TensorFlow": ["tensorflow", "tf"],
        "Pandas": ["pandas"],
        "Numpy": ["numpy", "np"],
        "Pytest": ["pytest"],
        "Unittest": ["unittest"],
    }

    # Track framework detections
    framework_evidence: Dict[str, str] = {}

    for file_path in python_files:
        # Extract components including imports
        components = extract_python_components(file_path)
        imports = components.get("imports", [])

        # Store all the raw imports
        file_imports[file_path] = imports

        # Process each import line
        for imp in imports:
            # Normalize import line by removing "as X" aliases
            # This helps count semantically identical imports
            base_import = imp.split(" as ")[0].strip()
            all_imports[base_import] = all_imports.get(base_import, 0) + 1

            # Check for framework indicators
            for framework, indicators in frameworks.items():
                for indicator in indicators:
                    if indicator in imp.split()[1]:  # Check the module part
                        if framework not in framework_evidence:
                            framework_evidence[framework] = (
                                f"Found import '{imp}' in {Path(file_path).name}"
                            )
                        break

    # Sort by frequency
    common_imports = sorted(all_imports.items(), key=lambda x: x[1], reverse=True)
    patterns["python_patterns"]["common_imports"] = common_imports

    # Add framework detections if any
    if framework_evidence:
        patterns["python_patterns"]["framework_patterns"] = framework_evidence

    # Important: try/except to avoid failures during pattern detection
    # This is analysis code, shouldn't crash the report generation
    try:
        # Common file patterns based on naming conventions
        repository_patterns = {}
        # ... extend with more pattern detection as needed...

        # Add to patterns dict
        patterns.update(repository_patterns)
    except Exception:
        # Less prominent warning since this is enhancement, not core functionality
        # print(f"Warning: Could not analyze patterns in {file_path}: {e}") # Can be noisy
        pass

    return patterns


# --- Key File Identification ---


def find_key_files(files: List[str], dependencies: Dict[str, Set[str]]) -> List[str]:
    """
    Identify key files in the repository based on several heuristic factors.
    - Dependency count (how many files depend on this one)
    - Naming convention (e.g., main.py, __init__.py)
    - File size and location
    """
    print_subheader("Scoring files to identify key ones", "cyan")

    if not files:
        return []

    # 1. Prepare scoring dict
    scores: Dict[str, float] = {file: 0.0 for file in files}

    # 2. Score based on file naming and key locations
    key_names = ["main", "app", "core", "index", "server", "engine", "controller"]
    for file in files:
        file_path = Path(file)
        filename_stem = file_path.stem.lower()

        # Key names in filename get points
        for key in key_names:
            if key == filename_stem:
                scores[file] += 5.0  # Exact match
            elif key in filename_stem:
                scores[file] += 2.0  # Partial match

        # Special files
        if filename_stem == "__init__":
            scores[file] += 1.0
        if filename_stem == "__main__":
            scores[file] += 3.0

        # Files in root directories are often important
        try:
            rel_path = file_path.relative_to(Path.cwd())
            depth = len(rel_path.parts)
            if depth <= 2:  # In root or direct subdirectory
                scores[file] += 3.0 / depth  # More points for less depth
        except ValueError:
            # File outside cwd, skip this bonus
            pass

        # Size can indicate importance (within reason)
        try:
            size = file_path.stat().st_size
            # Log scale to avoid over-prioritizing large files
            if size > 0:
                import math

                size_score = min(3.0, math.log(size) / 3)
                scores[file] += size_score
        except OSError:
            pass

    # 3. Dependency analysis (Python)
    # Calculate how many files depend on each file (reversed dependency graph)
    dependents: Dict[str, Set[str]] = {file: set() for file in files}
    for source, targets in dependencies.items():
        for target in targets:
            if target in dependents:
                dependents[target].add(source)

    # Score based on dependent count (files that import this file)
    for file, deps in dependents.items():
        count = len(deps)
        if count > 0:
            # More weight for dependencies
            scores[file] += count * 2.0
            # console.print(f"  Score bump (deps): {Path(file).name} +{count * 2.0} (depended by {count})")

    # 4. Select top files based on scores
    # Calculate a reasonable number based on repository size
    num_key_files = min(
        10, max(3, int(len(files) * 0.1))
    )  # 10% but at least 3, at most 10

    # Sort files by score (descending) and select top N
    top_files = sorted(files, key=lambda f: scores.get(f, 0), reverse=True)[
        :num_key_files
    ]

    print_success(f"Selected top {num_key_files} files as key files.")

    # Debug info (commented out in production)
    # for i, f in enumerate(top_files):
    #      console.print(f"  {i+1}. {Path(f).name}: {scores.get(f, 0.0):.2f}")

    return top_files
```

### examples\05-full-projects\project-textual-project-planner\src\flock_flightplan\collector\config.py

- **Lines**: 251
- **Last modified**: 2025-05-22 18:59:04

```py
# src/pilot_rules/collector/config.py
import tomli
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple

from flock_flightplan.cli.utils import (
    print_subheader,
)
from textual.widgets import RichLog

DEFAULT_OUTPUT_FILENAME = "repository_analysis.md"
DEFAULT_INCLUDE_SPEC = "py:."  # Default to python files in current dir

log : RichLog = None



def set_app_log(log_component):
    """Set the app's RichLog component for output"""
    global log
    log = log_component

def parse_include_exclude_args(args: Optional[List[str]]) -> List[Dict[str, Any]]:
    """Parses include/exclude arguments like 'py,js:src' or '*:temp'."""
    parsed = []
    if not args:
        return parsed

    for arg in args:
        if ":" not in arg:
            raise ValueError(
                f"Invalid include/exclude format: '{arg}'. Expected 'EXTS:PATH' or '*:PATTERN'."
            )

        exts_str, path_pattern = arg.split(":", 1)
        extensions = [
            ext.strip().lower().lstrip(".")
            for ext in exts_str.split(",")
            if ext.strip()
        ]
        if not extensions:
            raise ValueError(f"No extensions specified in '{arg}'. Use '*' for all.")

        # Use '*' as a special marker for all extensions
        if "*" in extensions:
            extensions = ["*"]

        # Normalize path pattern to use forward slashes for consistency
        # Keep it relative for now, resolve later if needed
        path_pattern = Path(path_pattern).as_posix()

        parsed.append(
            {
                "extensions": extensions,  # List of extensions (lowercase, no dot), or ['*']
                "pattern": path_pattern,  # Path or pattern string (relative or absolute)
            }
        )
    return parsed


def load_config_from_toml(
    config_path: Path,
) -> Tuple[List[Dict], List[Dict], Optional[str]]:
    """Loads sources, excludes, and output path from a TOML file."""
    config_sources = []
    config_excludes = []
    config_output = None

    print_subheader(f"Loading configuration from: [cyan]{config_path}[/cyan]")
    try:
        with open(config_path, "rb") as f:
            config_data = tomli.load(f)

        # --- Parse sources ---
        raw_sources = config_data.get("source", [])
        if not isinstance(raw_sources, list):
            raise ValueError("Invalid config: 'source' must be an array of tables.")

        for i, src_table in enumerate(raw_sources):
            if not isinstance(src_table, dict):
                raise ValueError(
                    f"Invalid config: Item {i} in 'source' array is not a table."
                )

            exts = src_table.get("exts", ["*"])  # Default to all if not specified
            root = src_table.get("root", ".")
            exclude_patterns = src_table.get(
                "exclude", []
            )  # Excludes within a source block

            if not isinstance(exts, list) or not all(isinstance(e, str) for e in exts):
                raise ValueError(
                    f"Invalid config: 'exts' must be a list of strings in source #{i + 1}"
                )
            if not isinstance(root, str):
                raise ValueError(
                    f"Invalid config: 'root' must be a string in source #{i + 1}."
                )
            if not isinstance(exclude_patterns, list) or not all(
                isinstance(p, str) for p in exclude_patterns
            ):
                raise ValueError(
                    f"Invalid config: 'exclude' must be a list of strings in source #{i + 1}"
                )

            # Normalize extensions: lowercase, no leading dot
            normalized_exts = [e.lower().lstrip(".") for e in exts]
            if "*" in normalized_exts:
                normalized_exts = ["*"]  # Treat ['*'] as the 'all' marker

            # Store source config
            config_sources.append(
                {
                    "root": root,  # Keep relative for now, resolve later
                    "extensions": normalized_exts,
                }
            )

            # Add source-specific excludes to the global excludes list
            # Assume format '*:<pattern>' for excludes defined within a source block
            for pattern in exclude_patterns:
                config_excludes.append(
                    {"extensions": ["*"], "pattern": Path(pattern).as_posix()}
                )

        # --- Parse global output ---
        config_output = config_data.get("output")
        if config_output and not isinstance(config_output, str):
            raise ValueError("Invalid config: 'output' must be a string.")

        # --- Parse global excludes (optional top-level section) ---
        raw_global_excludes = config_data.get("exclude", [])
        if not isinstance(raw_global_excludes, list):
            raise ValueError("Invalid config: Top-level 'exclude' must be an array.")
        for i, ex_table in enumerate(raw_global_excludes):
            if not isinstance(ex_table, dict):
                raise ValueError(
                    f"Invalid config: Item {i} in top-level 'exclude' array is not a table."
                )
            exts = ex_table.get("exts", ["*"])
            pattern = ex_table.get("pattern")
            if pattern is None:
                raise ValueError(
                    f"Invalid config: 'pattern' missing in top-level exclude #{i + 1}"
                )
            if not isinstance(pattern, str):
                raise ValueError(
                    f"Invalid config: 'pattern' must be a string in top-level exclude #{i + 1}"
                )
            if not isinstance(exts, list) or not all(isinstance(e, str) for e in exts):
                raise ValueError(
                    f"Invalid config: 'exts' must be a list of strings in top-level exclude #{i + 1}"
                )

            normalized_exts = [e.lower().lstrip(".") for e in exts]
            if "*" in normalized_exts:
                normalized_exts = ["*"]

            config_excludes.append(
                {"extensions": normalized_exts, "pattern": Path(pattern).as_posix()}
            )

    except tomli.TOMLDecodeError as e:
        raise ValueError(f"Error parsing TOML config file '{config_path}': {e}")
    except FileNotFoundError:
        raise ValueError(f"Config file not found: '{config_path}'")

    return config_sources, config_excludes, config_output


def process_config_and_args(
    include_args: Optional[List[str]],
    exclude_args: Optional[List[str]],
    output_arg: Optional[str],  # Output from CLI args might be None if default used
    config_arg: Optional[str],
) -> Tuple[List[Dict], List[Dict], Path]:
    """
    Loads config, parses CLI args, merges them, and resolves paths.

    Returns:
        Tuple: (final_sources, final_excludes, final_output_path)
               Sources/Excludes contain resolved root paths and normalized patterns/extensions.
    """
    config_sources = []
    config_excludes = []
    config_output = None

    # 1. Load Config File (if provided)
    if config_arg:
        config_path = Path(config_arg)
        if config_path.is_file():
            config_sources, config_excludes, config_output = load_config_from_toml(
                config_path
            )
        else:
            # Argparse should handle file existence, but double-check
            raise ValueError(
                f"Config file path specified but not found or not a file: '{config_arg}'"
            )

    # 2. Parse CLI arguments
    cli_includes = parse_include_exclude_args(include_args)
    cli_excludes = parse_include_exclude_args(exclude_args)
    # Use output_arg directly (it incorporates the argparse default if not provided)
    cli_output = output_arg if output_arg else DEFAULT_OUTPUT_FILENAME

    # 3. Combine sources: CLI overrides config sources entirely if provided.
    final_sources_specs = []
    if cli_includes:
        log.write("[cyan]Using include sources from command line arguments.[/cyan]")
        final_sources_specs = cli_includes  # Use CLI specs directly
    elif config_sources:
        log.write("[cyan]Using include sources from configuration file.[/cyan]")
        final_sources_specs = config_sources  # Use config specs
    else:
        log.write(
            f"[yellow]No includes specified via CLI or config, defaulting to '[bold]{DEFAULT_INCLUDE_SPEC}[/bold]'.[/yellow]"
        )
        final_sources_specs = parse_include_exclude_args([DEFAULT_INCLUDE_SPEC])

    # 4. Combine excludes: CLI appends to config excludes
    # Exclude patterns remain relative path strings for fnmatch
    final_excludes = config_excludes + cli_excludes
    if final_excludes:
        log.write(
            f"Applying [bold yellow]{len(final_excludes)}[/bold yellow] exclusion rule(s)."
        )

    # 5. Determine final output path: CLI > Config > Default
    # cli_output already incorporates the default if needed
    final_output_str = cli_output if cli_output else config_output
    if not final_output_str:  # Should not happen if argparse default is set
        final_output_str = DEFAULT_OUTPUT_FILENAME

    # Resolve output path relative to CWD
    final_output_path = Path(final_output_str).resolve()
    log.write(f"Final output path: [bold green]{final_output_path}[/bold green]")

    # 6. Resolve source roots relative to CWD *after* deciding which specs to use
    resolved_final_sources = []
    for spec in final_sources_specs:
        # spec['pattern'] here is the root directory from include args or config
        resolved_root = Path(spec["pattern"]).resolve()
        resolved_final_sources.append(
            {
                "root": resolved_root,
                "extensions": spec["extensions"],  # Keep normalized extensions
            }
        )

    return resolved_final_sources, final_excludes, final_output_path
```

### examples\05-full-projects\project-textual-project-planner\src\flock_flightplan\collector\discovery.py

- **Lines**: 200
- **Last modified**: 2025-05-22 18:59:04

```py
# src/pilot_rules/collector/discovery.py
import glob
import fnmatch
from pathlib import Path
from typing import List, Dict, Any, Tuple, Set
from rich.panel import Panel
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn
from textual.widgets import RichLog

log : RichLog = None


def set_app_log(log_component):
    """Set the app's RichLog component for output"""
    global log
    log = log_component


def collect_files(
    sources: List[Dict[str, Any]], excludes: List[Dict[str, Any]]
) -> Tuple[List[str], Set[str]]:
    """
    Finds files based on source definitions (using glob.glob) and applies exclusion rules.

    Args:
        sources: List of dicts, each with 'root' (resolved Path) and 'extensions' (list or ['*']).
        excludes: List of dicts, each with 'extensions' (list or ['*']) and 'pattern' (str).

    Returns:
        Tuple: (list of absolute file paths found, set of unique extensions found (lowercase, with dot))
    """
    log.write(
        Panel.fit("[bold blue]Collecting Files[/bold blue]", border_style="blue")
    )
    all_found_files: Set[str] = set()  # Store absolute paths as strings
    project_root = Path.cwd().resolve()  # Use CWD as the reference point for excludes

    with Progress(
        SpinnerColumn(),
        TextColumn("[bold blue]{task.description}"),
        BarColumn(),
        TextColumn("[bold]{task.completed}/{task.total}"),
    ) as progress:
        source_task = progress.add_task(
            "[yellow]Processing sources...", total=len(sources)
        )

        for source in sources:
            root_path: Path = source["root"]
            extensions: List[str] = source[
                "extensions"
            ]  # Already normalized (lowercase, no dot, or ['*'])

            ext_display = extensions if extensions != ["*"] else "all"
            progress.update(
                source_task,
                description=f"[yellow]Scanning: [cyan]{root_path}[/cyan] for [green]{ext_display}[/green]",
            )

            if not root_path.is_dir():
                log.write(
                    f"[yellow]⚠ Warning:[/yellow] Source root '[cyan]{root_path}[/cyan]' is not a directory. Skipping."
                )
                progress.update(source_task, advance=1)
                continue

            found_in_source: Set[str] = set()
            if extensions == ["*"]:
                # Use glob.glob for all files recursively
                glob_pattern_str = str(root_path / "**" / "*")
                try:
                    # Use glob.glob with recursive=True
                    for filepath_str in glob.glob(glob_pattern_str, recursive=True):
                        item = Path(filepath_str)
                        # Check if it's a file (glob might return directories matching pattern too)
                        if item.is_file():
                            # Add resolved absolute path as string
                            found_in_source.add(str(item.resolve()))
                except Exception as e:
                    log.write(
                        f"[yellow]⚠ Warning:[/yellow] Error during globbing for '[cyan]{glob_pattern_str}[/cyan]': [red]{e}[/red]"
                    )
            else:
                # Specific extensions provided
                for ext in extensions:
                    # Construct pattern like '*.py'
                    pattern = f"*.{ext}"
                    glob_pattern_str = str(root_path / "**" / pattern)
                    try:
                        # Use glob.glob with recursive=True
                        for filepath_str in glob.glob(glob_pattern_str, recursive=True):
                            item = Path(filepath_str)
                            # Check if it's a file
                            if item.is_file():
                                # Add resolved absolute path as string
                                found_in_source.add(str(item.resolve()))
                    except Exception as e:
                        log.write(
                            f"[yellow]⚠ Warning:[/yellow] Error during globbing for '[cyan]{glob_pattern_str}[/cyan]': [red]{e}[/red]"
                        )

            log.write(
                f"  Found [green]{len(found_in_source)}[/green] potential files in this source."
            )
            all_found_files.update(found_in_source)
            progress.update(source_task, advance=1)

    log.write(
        f"Total unique files found before exclusion: [bold green]{len(all_found_files)}[/bold green]"
    )

    # Apply exclusion rules
    excluded_files: Set[str] = set()
    if excludes:
        log.write(
            Panel.fit(
                "[bold yellow]Applying Exclusion Rules[/bold yellow]",
                border_style="yellow",
            )
        )
        # Create a map of relative paths (from project_root) to absolute paths
        # Only consider files that are within the project root for relative matching
        relative_files_map: Dict[str, str] = {}
        for abs_path_str in all_found_files:
            abs_path = Path(abs_path_str)
            try:
                # Use POSIX paths for matching consistency
                relative_path_str = abs_path.relative_to(project_root).as_posix()
                relative_files_map[relative_path_str] = abs_path_str
            except ValueError:
                # File is outside project root, cannot be excluded by relative pattern
                pass

        relative_file_paths = set(relative_files_map.keys())

        with Progress(
            SpinnerColumn(),
            TextColumn("[bold yellow]{task.description}"),
            BarColumn(),
            TextColumn("[bold]{task.completed}/{task.total}"),
            console=log,
        ) as progress:
            exclude_task = progress.add_task(
                "[yellow]Processing exclusion rules...", total=len(excludes)
            )

            for rule in excludes:
                rule_exts: List[str] = rule[
                    "extensions"
                ]  # Normalized (lowercase, no dot, or ['*'])
                rule_pattern: str = rule["pattern"]  # Relative path pattern string

                ext_display = rule_exts if rule_exts != ["*"] else "any"
                progress.update(
                    exclude_task,
                    description=f"[yellow]Excluding: [green]{ext_display}[/green] matching [cyan]*{rule_pattern}*[/cyan]",
                )

                # Use fnmatch for flexible pattern matching against relative paths
                # Wrap the pattern to check if the rule pattern exists anywhere in the path
                pattern_to_match = f"*{rule_pattern}*"

                files_to_check = relative_file_paths
                # If rule has specific extensions, filter the files to check first
                if rule_exts != ["*"]:
                    # Match against suffix (e.g., '.py')
                    dot_exts = {f".{e}" for e in rule_exts}
                    files_to_check = {
                        rel_path
                        for rel_path in relative_file_paths
                        if Path(rel_path).suffix.lower() in dot_exts
                    }

                # Apply fnmatch to the filtered relative paths
                matched_by_rule = {
                    rel_path
                    for rel_path in files_to_check
                    if fnmatch.fnmatch(rel_path, pattern_to_match)
                }

                # Add the corresponding absolute paths to the excluded set
                for rel_path in matched_by_rule:
                    if rel_path in relative_files_map:
                        excluded_files.add(relative_files_map[rel_path])
                        # Verbose logging disabled by default
                        # console.print(f"    Excluding: [dim]{relative_files_map[rel_path]}[/dim]")

                progress.update(exclude_task, advance=1)

    log.write(f"Excluded [bold yellow]{len(excluded_files)}[/bold yellow] files.")
    final_files = sorted(list(all_found_files - excluded_files))

    # Determine actual extensions present in the final list (lowercase, with dot)
    final_extensions = {Path(f).suffix.lower() for f in final_files if Path(f).suffix}

    log.write(
        f"[bold green]✓[/bold green] Collection complete! Found [bold green]{len(final_files)}[/bold green] files with [bold cyan]{len(final_extensions)}[/bold cyan] unique extensions."
    )

    return final_files, final_extensions
```

### examples\05-full-projects\project-textual-project-planner\src\flock_flightplan\collector\metrics.py

- **Lines**: 229
- **Last modified**: 2025-05-22 18:59:04

```py
"""
Code quality metrics calculations for the code collector.
Provides functions to analyze code complexity, maintainability and other quality metrics.
"""

import ast
from pathlib import Path
from typing import Dict, Any, List

import radon.metrics as radon_metrics
import radon.raw as radon_raw
from radon.visitors import ComplexityVisitor

from flock_flightplan.cli.utils import print_warning


def calculate_python_metrics(file_path: str) -> Dict[str, Any]:
    """
    Calculate code quality metrics for a Python file.
    
    Args:
        file_path: Path to the Python file
    
    Returns:
        Dictionary containing various code quality metrics
    """
    metrics = {
        "cyclomatic_complexity": None,
        "maintainability_index": None,
        "raw_metrics": None,
        "code_smells": [],
        "complexity_by_function": []
    }
    
    if not file_path.lower().endswith('.py'):
        return metrics
    
    try:
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            code = f.read()
        
        # Calculate raw metrics (lines of code, comments, etc.)
        raw_metrics = radon_raw.analyze(code)
        metrics["raw_metrics"] = {
            "loc": raw_metrics.loc,  # Lines of code (excluding comments)
            "lloc": raw_metrics.lloc,  # Logical lines of code
            "sloc": raw_metrics.sloc,  # Source lines of code 
            "comments": raw_metrics.comments,  # Number of comments
            "multi": raw_metrics.multi,  # Number of multi-line strings
            "blank": raw_metrics.blank,  # Number of blank lines
            "comment_ratio": raw_metrics.comments / raw_metrics.loc if raw_metrics.loc > 0 else 0
        }
        
        # Calculate maintainability index (0-100, higher is better)
        mi = radon_metrics.mi_visit(code, multi=True)
        metrics["maintainability_index"] = {
            "value": mi,
            "rank": _get_maintainability_rank(mi)
        }
        
        # Calculate cyclomatic complexity
        try:
            # Get complexity for the entire file
            complexity = ComplexityVisitor.from_code(code)
            if complexity.total_complexity is not None:
                # Calculate average complexity manually if needed
                total_complexity = complexity.total_complexity
                num_functions = len(complexity.functions) if complexity.functions else 1
                avg_complexity = total_complexity / num_functions if num_functions > 0 else total_complexity
                
                metrics["cyclomatic_complexity"] = {
                    "total": total_complexity,
                    "average": avg_complexity,
                    "rank": _get_complexity_rank(avg_complexity)
                }
            
            # Get complexity for each function/method
            for item in complexity.functions:
                metrics["complexity_by_function"].append({
                    "name": item.name,
                    "line_number": item.lineno,
                    "complexity": item.complexity,
                    "rank": _get_complexity_rank(item.complexity)
                })
                
                # Identify code smells based on complexity
                if item.complexity > 10:
                    metrics["code_smells"].append({
                        "type": "high_complexity",
                        "location": f"{item.name} (line {item.lineno})",
                        "description": f"Function has high cyclomatic complexity ({item.complexity})",
                        "suggestion": "Consider refactoring into smaller functions"
                    })
        except SyntaxError:
            # Fall back to simpler analysis if visitor fails
            pass
        
        # Check for additional code smells
        metrics["code_smells"].extend(_detect_code_smells(code, file_path))
        
    except Exception as e:
        print_warning(f"Could not analyze metrics in {file_path}: {str(e)}")
    
    return metrics


def _get_complexity_rank(complexity: float) -> str:
    """
    Convert a cyclomatic complexity score to a letter rank.
    
    Args:
        complexity: The cyclomatic complexity value
    
    Returns:
        Letter rank from A (best) to F (worst)
    """
    if complexity <= 5:
        return "A"  # Low - good
    elif complexity <= 10:
        return "B"  # Medium - acceptable
    elif complexity <= 20:
        return "C"  # High - concerning
    elif complexity <= 30:
        return "D"  # Very high - problematic
    else:
        return "F"  # Extremely high - needs immediate refactoring


def _get_maintainability_rank(mi_value: float) -> str:
    """
    Convert a maintainability index to a letter rank.
    
    Args:
        mi_value: The maintainability index value
    
    Returns:
        Letter rank from A (best) to F (worst)
    """
    if mi_value >= 85:
        return "A"  # Highly maintainable
    elif mi_value >= 65:
        return "B"  # Maintainable
    elif mi_value >= 40:
        return "C"  # Moderately maintainable
    elif mi_value >= 25:
        return "D"  # Difficult to maintain
    else:
        return "F"  # Very difficult to maintain


def _detect_code_smells(code: str, file_path: str) -> List[Dict[str, str]]:
    """
    Detect common code smells in Python code.
    
    Args:
        code: Python code as a string
        file_path: Path to the Python file (for reporting)
    
    Returns:
        List of detected code smells with descriptions
    """
    smells = []
    
    try:
        tree = ast.parse(code)
        
        # Check for long functions (by line count)
        for node in ast.walk(tree):
            if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                func_lines = node.end_lineno - node.lineno if hasattr(node, 'end_lineno') else 0
                if func_lines > 50:
                    smells.append({
                        "type": "long_function",
                        "location": f"{node.name} (line {node.lineno})",
                        "description": f"Function is too long ({func_lines} lines)",
                        "suggestion": "Consider breaking into smaller functions"
                    })
                
                # Check for too many parameters
                if len(node.args.args) > 7:  # Including self for methods
                    smells.append({
                        "type": "too_many_parameters",
                        "location": f"{node.name} (line {node.lineno})",
                        "description": f"Function has too many parameters ({len(node.args.args)})",
                        "suggestion": "Consider using a class or data objects to group parameters"
                    })
                    
        # Check for too many imports (module level)
        import_count = 0
        for node in ast.walk(tree):
            if isinstance(node, (ast.Import, ast.ImportFrom)):
                import_count += len(node.names)
        
        if import_count > 15:
            smells.append({
                "type": "too_many_imports",
                "location": f"{Path(file_path).name}",
                "description": f"Module has too many imports ({import_count})",
                "suggestion": "Consider refactoring to reduce dependencies"
            })
            
    except Exception:
        # Silently fail for syntax errors or other parsing issues
        pass
    
    return smells


def calculate_file_metrics(file_path: str) -> Dict[str, Any]:
    """
    Calculate appropriate metrics based on file type.
    
    Args:
        file_path: Path to the file
    
    Returns:
        Dictionary of metrics appropriate for the file type
    """
    metrics = {}
    
    # Handle Python files
    if file_path.lower().endswith('.py'):
        metrics = calculate_python_metrics(file_path)
    
    # TODO: Add support for other languages (JavaScript, etc.)
    # elif file_path.lower().endswith('.js'):
    #     metrics = calculate_javascript_metrics(file_path)
    
    return metrics 
```

### examples\05-full-projects\project-textual-project-planner\src\flock_flightplan\collector\reporting.py

- **Lines**: 570
- **Last modified**: 2025-05-22 18:59:04

```py
# src/pilot_rules/collector/reporting.py
import datetime
from pathlib import Path
from typing import List, Dict, Any, Set, Tuple

# Import functions from sibling modules
from flock_flightplan.cli.utils import (
    get_file_metadata,
    print_header,
    print_success,
    print_warning,
    print_error,
    print_subheader,
)
from flock_flightplan.collector.analysis import extract_python_components  # Import needed analysis functions
from flock_flightplan.collector.metrics import calculate_file_metrics  # Import the new metrics module
from flock_flightplan.model import Repository, ProjectFile, ProjectCodeFile
from textual.widgets import RichLog

log : RichLog = None
def set_app_log(log_component):
    """Set the app's RichLog component for output"""
    global log
    log = log_component
# --- Folder Tree Generation ---
# (generate_folder_tree function remains the same as the previous version)
def generate_folder_tree(root_folder_path: Path, included_files: List[str]) -> str:
    """Generate an ASCII folder tree representation for included files relative to a root."""
    tree_lines: List[str] = []
    included_files_set = {Path(f).resolve() for f in included_files}  # Absolute paths

    # Store relative paths from the root_folder_path for display and structure building
    # We only include paths *under* the specified root_folder_path in the tree display
    included_relative_paths: Dict[Path, bool] = {}  # Map relative path -> is_file
    all_parent_dirs: Set[Path] = set()  # Set of relative directory paths

    for abs_path in included_files_set:
        try:
            rel_path = abs_path.relative_to(root_folder_path)
            included_relative_paths[rel_path] = True  # Mark as file
            # Add all parent directories of this file
            parent = rel_path.parent
            while parent != Path("."):  # Stop before adding '.' itself
                if (
                    parent not in included_relative_paths
                ):  # Avoid marking parent as file if dir listed later
                    included_relative_paths[parent] = False  # Mark as directory
                all_parent_dirs.add(parent)
                parent = parent.parent
        except ValueError:
            # File is not under the root_folder_path, skip it in this tree view
            continue

    # Combine files and their necessary parent directories
    sorted_paths = sorted(included_relative_paths.keys(), key=lambda p: p.parts)

    # --- Tree building logic ---
    # Based on relative paths and depth
    tree_lines.append(f"{root_folder_path.name}/")  # Start with the root dir name

    entries_by_parent: Dict[
        Path, List[Tuple[Path, bool]]
    ] = {}  # parent -> list of (child, is_file)
    for rel_path, is_file in included_relative_paths.items():
        parent = rel_path.parent
        if parent not in entries_by_parent:
            entries_by_parent[parent] = []
        entries_by_parent[parent].append((rel_path, is_file))

    # Sort children within each parent directory
    for parent in entries_by_parent:
        entries_by_parent[parent].sort(
            key=lambda item: (not item[1], item[0].parts)
        )  # Dirs first, then alpha

    processed_paths = set()  # To avoid duplicates if a dir is both parent and included

    def build_tree_recursive(parent_rel_path: Path, prefix: str):
        if parent_rel_path not in entries_by_parent:
            return

        children = entries_by_parent[parent_rel_path]
        for i, (child_rel_path, is_file) in enumerate(children):
            if child_rel_path in processed_paths:
                continue

            is_last = i == len(children) - 1
            connector = "└── " if is_last else "├── "
            entry_name = child_rel_path.name
            display_name = f"{entry_name}{'' if is_file else '/'}"
            tree_lines.append(f"{prefix}{connector}{display_name}")
            processed_paths.add(child_rel_path)

            if not is_file:  # If it's a directory, recurse
                new_prefix = f"{prefix}{'    ' if is_last else '│   '}"
                build_tree_recursive(child_rel_path, new_prefix)

    # Start recursion from the root ('.') relative path
    build_tree_recursive(Path("."), "")

    # Join lines, ensuring the root is handled correctly if empty
    if (
        len(tree_lines) == 1 and not included_relative_paths
    ):  # Only root line, no files/dirs under it
        tree_lines[0] = f"└── {root_folder_path.name}/"  # Adjust prefix for empty tree

    return "\n".join(tree_lines)


# --- Markdown Generation ---
def generate_markdown(
    files: List[str],  # List of absolute paths
    analyzed_extensions: Set[
        str
    ],  # Set of actual extensions found (e.g., '.py', '.js')
    dependencies: Dict[str, Set[str]],  # Python dependencies
    patterns: Dict[str, Any],  # Detected patterns
    key_files: List[str],  # List of absolute paths for key files
    output_path: Path,
    root_folder_display: str = ".",  # How to display the root in summary/tree
) -> None:
    """Generate a comprehensive markdown document about the codebase."""
    print_header("Generating Report", "green")
    log.write(f"Output file: [cyan]{output_path}[/cyan]")
    output_dir = output_path.parent
    output_dir.mkdir(parents=True, exist_ok=True)  # Ensure output directory exists
    report_base_path = (
        Path.cwd()
    )  # Use CWD as the base for relative paths in the report

    has_python_files = ".py" in analyzed_extensions

    with open(output_path, "w", encoding="utf-8") as md_file:
        # --- Header ---
        md_file.write("# Code Repository Analysis\n\n")
        timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f")[
            :-3
        ]  # ms precision
        md_file.write(f"Generated on {timestamp}\n\n")

        # --- Repository Summary ---
        md_file.write("## Repository Summary\n\n")
        ext_list_str = (
            ", ".join(sorted(list(analyzed_extensions)))
            if analyzed_extensions
            else "N/A"
        )
        md_file.write(f"- **Extensions analyzed**: `{ext_list_str}`\n")
        md_file.write(f"- **Number of files analyzed**: {len(files)}\n")
        md_file.write(
            f"- **Analysis Root (for display)**: `{root_folder_display}`\n"
        )  # Indicate the main perspective

        total_lines = 0
        if files:
            try:
                total_lines = sum(
                    get_file_metadata(f).get("line_count", 0) for f in files
                )
            except Exception as e:
                print_warning(f"Could not calculate total lines accurately: {e}")
                total_lines = "N/A"
        else:
            total_lines = 0
        md_file.write(f"- **Total lines of code (approx)**: {total_lines}\n\n")

        # --- Project Structure ---
        md_file.write("## Project Structure (Relative View)\n\n")
        md_file.write("```\n")
        try:
            root_for_tree = Path(root_folder_display).resolve()
            if root_for_tree.is_dir():
                md_file.write(generate_folder_tree(root_for_tree, files))
            else:
                print_warning(
                    f"Display root '{root_folder_display}' not found or not a directory, using CWD for tree."
                )
                md_file.write(generate_folder_tree(report_base_path, files))
        except Exception as tree_err:
            print_error(f"Error generating folder tree: {tree_err}")
            md_file.write(f"Error generating folder tree: {tree_err}")
        md_file.write("\n```\n\n")

        # --- Key Files Section ---
        md_file.write("## Key Files\n\n")
        if key_files:
            md_file.write(
                "These files appear central based on dependencies, naming, and size:\n\n"
            )
            for file_abs_path in key_files:
                try:
                    rel_path = str(Path(file_abs_path).relative_to(report_base_path))
                except ValueError:
                    rel_path = file_abs_path  # Fallback to absolute if not relative

                md_file.write(f"### {rel_path}\n\n")
                metadata = get_file_metadata(file_abs_path)
                md_file.write(f"- **Lines**: {metadata.get('line_count', 'N/A')}\n")
                md_file.write(
                    f"- **Size**: {metadata.get('size_bytes', 0) / 1024:.2f} KB\n"
                )
                md_file.write(
                    f"- **Last modified**: {metadata.get('last_modified', 'Unknown')}\n"
                )

                # Dependency info (Python only)
                if has_python_files and file_abs_path in dependencies:
                    dependent_files_abs = {
                        f for f, deps in dependencies.items() if file_abs_path in deps
                    }
                    if dependent_files_abs:
                        md_file.write(
                            f"- **Used by**: {len(dependent_files_abs)} other analyzed Python file(s)\n"
                        )

                # Python component analysis
                if file_abs_path.lower().endswith(".py"):
                    components = extract_python_components(
                        file_abs_path
                    )  # Use imported function
                    if components.get("docstring"):
                        docstring_summary = (
                            components["docstring"].strip().split("\n", 1)[0]
                        )[:150]
                        md_file.write(
                            f"\n**Description**: {docstring_summary}{'...' if len(components['docstring']) > 150 else ''}\n"
                        )
                    if components.get("classes"):
                        md_file.write("\n**Classes**:\n")
                        for cls in components["classes"][:5]:
                            md_file.write(
                                f"- `{cls['name']}` ({len(cls['methods'])} methods)\n"
                            )
                        if len(components["classes"]) > 5:
                            md_file.write("- ... (and more)\n")
                    if components.get("functions"):
                        md_file.write("\n**Functions**:\n")
                        for func in components["functions"][:5]:
                            md_file.write(f"- `{func['name']}(...)`\n")
                        if len(components["functions"]) > 5:
                            md_file.write("- ... (and more)\n")

                # ==================================
                # --- Include FULL File Content ---
                md_file.write("\n**Content**:\n")  # Changed from "Content Snippet"
                file_ext = Path(file_abs_path).suffix.lower()
                lang_hint = file_ext.lstrip(".") if file_ext else ""
                md_file.write(f"```{lang_hint}\n")
                try:
                    with open(
                        file_abs_path, "r", encoding="utf-8", errors="ignore"
                    ) as code_file:
                        # Read the entire file content
                        full_content = code_file.read()
                        md_file.write(full_content)
                        # Ensure a newline at the end of the code block if file doesn't have one
                        if not full_content.endswith("\n"):
                            md_file.write("\n")
                except Exception as e:
                    md_file.write(f"Error reading file: {e}\n")
                md_file.write("```\n\n")

        # --- Other Markdown Files Section ---
        md_file.write("## Other Files\n\n")
        md_file.write("This section includes content of all other analyzed files that aren't in the key files list.\n\n")
        
        # Filter out key files
        other_files = [f for f in files if f not in key_files]
        
        if other_files:
            for file_abs_path in other_files:
                try:
                    rel_path = str(Path(file_abs_path).relative_to(report_base_path))
                except ValueError:
                    rel_path = file_abs_path  # Fallback to absolute if not relative

                md_file.write(f"### {rel_path}\n\n")
                metadata = get_file_metadata(file_abs_path)
                md_file.write(f"- **Lines**: {metadata.get('line_count', 'N/A')}\n")
                md_file.write(
                    f"- **Size**: {metadata.get('size_bytes', 0) / 1024:.2f} KB\n"
                )
                md_file.write(
                    f"- **Last modified**: {metadata.get('last_modified', 'Unknown')}\n"
                )

                # Include full file content
                md_file.write("\n**Content**:\n")
                file_ext = Path(file_abs_path).suffix.lower()
                lang_hint = file_ext.lstrip(".") if file_ext else ""
                md_file.write(f"```{lang_hint}\n")
                try:
                    with open(
                        file_abs_path, "r", encoding="utf-8", errors="ignore"
                    ) as code_file:
                        # Read the entire file content
                        full_content = code_file.read()
                        md_file.write(full_content)
                        # Ensure a newline at the end of the code block if file doesn't have one
                        if not full_content.endswith("\n"):
                            md_file.write("\n")
                except Exception as e:
                    md_file.write(f"Error reading file: {e}\n")
                md_file.write("```\n\n")
        else:
            md_file.write("No additional files found.\n\n")

        # --- Python Dependency Analysis (if applicable) ---
        if has_python_files and dependencies:
            md_file.write("## Python Dependencies\n\n")
            md_file.write(
                "This section shows Python modules and their dependencies within the project.\n\n"
            )

            dep_count = sum(len(deps) for deps in dependencies.values())
            if dep_count > 0:
                md_file.write("### Internal Dependencies\n\n")
                md_file.write("```mermaid\ngraph TD;\n")
                # Generate mermaid.js compatible graph nodes and edges
                node_ids = {}
                for i, file_path in enumerate(dependencies.keys()):
                    try:
                        rel_path = str(Path(file_path).relative_to(report_base_path))
                    except ValueError:
                        rel_path = str(
                            Path(file_path).name
                        )  # Just use filename if not relative
                    node_id = f"F{i}"
                    node_ids[file_path] = node_id
                    # Escape any problematic characters in label
                    label = rel_path.replace('"', '\\"')
                    md_file.write(f'    {node_id}["{label}"];\n')

                # Add edges for dependencies
                for file_path, deps in dependencies.items():
                    if not deps:
                        continue
                    source_id = node_ids[file_path]
                    for dep in deps:
                        if dep in node_ids:  # Ensure dep is in our analyzed files
                            target_id = node_ids[dep]
                            md_file.write(f"    {source_id} --> {target_id};\n")
                md_file.write("```\n\n")

                # Add plain text dependency list as fallback
                md_file.write("### Dependency List (Plain Text)\n\n")
                for file_path, deps in dependencies.items():
                    if not deps:
                        continue  # Skip files with no dependencies
                    try:
                        rel_path = str(Path(file_path).relative_to(report_base_path))
                    except ValueError:
                        rel_path = file_path
                    md_file.write(f"- **{rel_path}** depends on:\n")
                    for dep in sorted(deps):
                        try:
                            dep_rel = str(Path(dep).relative_to(report_base_path))
                        except ValueError:
                            dep_rel = dep
                        md_file.write(f"  - {dep_rel}\n")

        # --- Common Code Patterns ---
        if patterns and patterns.get("python_patterns"):
            py_patterns = patterns["python_patterns"]
            if py_patterns:
                md_file.write("## Common Code Patterns\n\n")
                md_file.write("### Python Patterns\n\n")

                if py_patterns.get("common_imports"):
                    md_file.write("#### Common Imports\n\n")
                    for imp, count in py_patterns["common_imports"][:10]:
                        md_file.write(f"- `{imp}` ({count} files)\n")
                    if len(py_patterns["common_imports"]) > 10:
                        md_file.write("- *(and more...)*\n")
                    md_file.write("\n")

                if py_patterns.get("framework_patterns"):
                    md_file.write("#### Framework Detection\n\n")
                    for framework, evidence in py_patterns[
                        "framework_patterns"
                    ].items():
                        md_file.write(f"- **{framework}**: {evidence}\n")
                    md_file.write("\n")

    # Final success message
    print_success(f"Markdown report generated successfully at '{output_path}'")


# --- Repository Object Generation ---
def generate_repository(
    files: List[str],  # List of absolute paths
    analyzed_extensions: Set[str],  # Set of actual extensions found (e.g., '.py', '.js')
    dependencies: Dict[str, Set[str]],  # Python dependencies
    patterns: Dict[str, Any],  # Detected patterns
    key_files: List[str],  # List of absolute paths for key files
    repo_name: str = "Repository Analysis",
    root_folder_display: str = ".",  # How to display the root in summary/tree
    calculate_metrics: bool = False,  # New parameter to control metrics calculation
) -> Repository:
    """Generate a Repository object with analyzed code structure and content."""
    print_header("Generating Repository Object", "green")
    report_base_path = Path.cwd()  # Use CWD as the base for relative paths in the report

    has_python_files = ".py" in analyzed_extensions

    # Generate statistics
    ext_list_str = ", ".join(sorted(list(analyzed_extensions))) if analyzed_extensions else "N/A"
    total_files = len(files)
    
    total_lines = 0
    if files:
        try:
            total_lines = sum(get_file_metadata(f).get("line_count", 0) for f in files)
        except Exception as e:
            print_warning(f"Could not calculate total lines accurately: {e}")
            total_lines = 0
    
    statistics = f"""
- Extensions analyzed: {ext_list_str}
- Number of files analyzed: {total_files}
- Total lines of code (approx): {total_lines}
"""

    # Process files to create ProjectFile objects
    project_files = []
    
    # First create a mapping of absolute paths to file_ids
    file_id_mapping = {}
    for i, file_abs_path in enumerate(files):
        try:
            rel_path = str(Path(file_abs_path).relative_to(report_base_path))
        except ValueError:
            rel_path = file_abs_path  # Fallback to absolute if not relative
        
        file_id = f"file_{i}"
        file_id_mapping[file_abs_path] = file_id
    
    # Pre-calculate metrics for Python files to include in statistics, but only if metrics are enabled
    python_files = [f for f in files if f.lower().endswith('.py')]
    metrics_by_file = {}
    
    if calculate_metrics and python_files:
        print_subheader("Calculating Code Quality Metrics", "blue")
        
        # Track overall metrics
        total_complexity = 0
        files_with_complexity = 0
        total_maintainability = 0
        files_with_maintainability = 0
        complexity_ranks = {"A": 0, "B": 0, "C": 0, "D": 0, "F": 0}
        maintainability_ranks = {"A": 0, "B": 0, "C": 0, "D": 0, "F": 0}
        total_code_smells = 0
        
        # Calculate metrics for each Python file
        for file_path in python_files:
            metrics = calculate_file_metrics(file_path)
            metrics_by_file[file_path] = metrics
            
            # Aggregate statistics
            cc = metrics.get("cyclomatic_complexity", {})
            if cc and "total" in cc:
                total_complexity += cc["total"]
                files_with_complexity += 1
                if "rank" in cc:
                    complexity_ranks[cc["rank"]] = complexity_ranks.get(cc["rank"], 0) + 1
            
            mi = metrics.get("maintainability_index", {})
            if mi and "value" in mi:
                total_maintainability += mi["value"]
                files_with_maintainability += 1
                if "rank" in mi:
                    maintainability_ranks[mi["rank"]] = maintainability_ranks.get(mi["rank"], 0) + 1
            
            smells = metrics.get("code_smells", [])
            total_code_smells += len(smells)
        
        # Add code quality metrics to statistics
        avg_complexity = total_complexity / files_with_complexity if files_with_complexity > 0 else 0
        avg_maintainability = total_maintainability / files_with_maintainability if files_with_maintainability > 0 else 0
        
        complexity_distribution = ", ".join([f"{rank}: {count}" for rank, count in complexity_ranks.items() if count > 0])
        maintainability_distribution = ", ".join([f"{rank}: {count}" for rank, count in maintainability_ranks.items() if count > 0])
        
        quality_stats = f"""
- Average cyclomatic complexity: {avg_complexity:.2f}
- Complexity distribution: {complexity_distribution}
- Average maintainability index: {avg_maintainability:.2f}
- Maintainability distribution: {maintainability_distribution}
- Total code smells detected: {total_code_smells}
"""
        
        # Add code quality metrics to main statistics if metrics were calculated
        if files_with_complexity > 0 or files_with_maintainability > 0:
            statistics += quality_stats

    # Now create ProjectFile objects with proper dependencies
    for file_abs_path in files:
        try:
            rel_path = str(Path(file_abs_path).relative_to(report_base_path))
        except ValueError:
            rel_path = file_abs_path  # Fallback to absolute if not relative
            
        metadata = get_file_metadata(file_abs_path)
        file_id = file_id_mapping[file_abs_path]
        
        try:
            with open(file_abs_path, "r", encoding="utf-8", errors="ignore") as code_file:
                content = code_file.read()
        except Exception as e:
            print_warning(f"Could not read file content for {rel_path}: {e}")
            content = f"Error reading file: {str(e)}"
        
        # Generate description based on file type
        description = f"File at {rel_path}"
        if file_abs_path.lower().endswith(".py"):
            components = extract_python_components(file_abs_path)
            if components.get("docstring"):
                docstring_summary = components["docstring"].strip().split("\n", 1)[0][:150]
                description = docstring_summary + ('...' if len(components["docstring"]) > 150 else '')
            
            # For Python files, create ProjectCodeFile with dependencies
            file_deps = []
            file_used_by = []
            
            # Find dependencies
            if has_python_files and file_abs_path in dependencies:
                file_deps = [file_id_mapping[dep] for dep in dependencies[file_abs_path] if dep in file_id_mapping]
                
                # Find files that depend on this file
                dependent_files_abs = {f for f, deps in dependencies.items() if file_abs_path in deps}
                file_used_by = [file_id_mapping[dep] for dep in dependent_files_abs if dep in file_id_mapping]
            
            # Use pre-calculated metrics if available, otherwise calculate them now
            complexity_metrics = {}
            if calculate_metrics:
                complexity_metrics = metrics_by_file.get(file_abs_path, {})
                if not complexity_metrics:
                    complexity_metrics = calculate_file_metrics(file_abs_path)
            
            project_file = ProjectCodeFile(
                file_id=file_id,
                description=description,
                file_path=rel_path,
                content=content,
                line_count=metadata.get('line_count', 0),
                dependencies=file_deps,
                used_by=file_used_by,
                complexity_metrics=complexity_metrics
            )
        else:
            # Regular ProjectFile for non-Python files
            project_file = ProjectFile(
                file_id=file_id,
                description=description,
                file_path=rel_path,
                content=content,
                line_count=metadata.get('line_count', 0)
            )
        
        project_files.append(project_file)
    
    # Create and return the Repository object
    repository = Repository(
        name=repo_name,
        statistics=statistics,
        project_files=project_files
    )
    
    print_success(f"Successfully generated Repository object with {len(project_files)} files")
    return repository
```

### examples\05-full-projects\project-textual-project-planner\src\flock_flightplan\model.py

- **Lines**: 586
- **Last modified**: 2025-05-22 18:59:04

```py
from typing import Any, Literal, Optional
from flock.core.flock_registry import flock_type
from pydantic import BaseModel, Field
from rich.table import Table
from rich import box
from pathlib import Path
from textual.widgets import RichLog
from rich.markdown import Markdown
log : RichLog = None

def set_app_log(log_component):
    """Set the app's RichLog component for output"""
    global log
    log = log_component

@flock_type
class OutputData(BaseModel):
    name: str = Field(..., description="Name of the output")
    description: str = Field(
        ...,
        description="High level description of the data and functionality of the app, as well as design decisions. In beautiful markdown.",
    )
    output_dictionary_definition: str = Field(
        ..., description="Explanation of the output dictionary and the data it contains"
    )
    output: dict[str, Any] = Field(
        ...,
        description="The output dictionary. Usually a dictionary with keys equals paths to files, and values equal the content of the files.",
    )

    # beautiful rendering of the output
    def render_summary(self):
        log.write("\n")
        log.write(f"[bold blue]{self.name}")
        log.write("\n")
        log.write(self.description)
        log.write("\n")
        log.write(self.output_dictionary_definition)

    def render_output_files(self, output_prefix: str = ".project/"):
        """
        Renders the output files in a beautiful table format.

        Args:
            console: The Rich console instance to use for output
            output_prefix: Optional prefix to prepend to file paths (defaults to '.project/')
        """
        log.write("\n")
        log.write("[bold cyan]Output Files")
        log.write("\n")

        # Create a nice table to display the files
        files_table = Table(title="Generated Files", box=box.ROUNDED)
        files_table.add_column("File Path", style="cyan")
        files_table.add_column("Status", style="green")
        files_table.add_column("Size", style="magenta")

        file_count = 0

        # Process each output file
        for file_path, content in self.output.items():
            file_path_with_prefix = f"{output_prefix}{file_path}"
            # if is directory create and skip
            if Path(file_path_with_prefix).is_dir():
                Path(file_path_with_prefix).mkdir(parents=True, exist_ok=True)
                continue

            file_count += 1

            # Calculate the content size
            content_size = len(content) if content else 0
            size_display = (
                f"{content_size / 1024:.2f} KB"
                if content_size > 1024
                else f"{content_size} bytes"
            )

            # Create entry in the table
            try:
                # Create the directory if it doesn't exist
                Path(file_path_with_prefix).parent.mkdir(parents=True, exist_ok=True)

                # Write the file content
                with open(file_path_with_prefix, "w", encoding="utf-8") as f:
                    f.write(content)

                files_table.add_row(
                    file_path_with_prefix, "[green]✓ Created[/green]", size_display
                )
            except Exception as e:
                files_table.add_row(
                    file_path_with_prefix, f"[red]✗ Error: {str(e)}[/red]", size_display
                )

        if file_count > 0:
            log.write(files_table)
            log.write(
                f"\n[green]Successfully generated {file_count} files.[/green]\n"
            )
        else:
            log.write("[yellow]No output files were generated.[/yellow]\n")

        log.write("[bold cyan]End of Output")

class ProjectFile(BaseModel):
    file_id: str = Field(..., description="Unique identifier for the file")
    description: str = Field(..., description="Description of the file")
    file_path: str = Field(..., description="Path to the file")
    content: str = Field(..., description="Content of the file")
    line_count: int = Field(..., description="Number of lines in the file")

class ProjectCodeFile(ProjectFile):
    dependencies: list[str] = Field(..., description="List of file ids that must be created before this one")
    used_by: list[str] = Field(..., description="List of file ids that depend on this one")
    complexity_metrics: dict[str, Any] = Field(default_factory=dict, description="Code quality and complexity metrics")


@flock_type
class Repository(BaseModel):
    name: str = Field(..., description="Name of the repository")
    statistics: str = Field(..., description="Statistics of the repository")
    project_files: list[ProjectFile | ProjectCodeFile] = Field(..., description="Output data of the repository")

    def render_summary(self) -> None:
        """
        Render a summary of the repository in a beautiful format.
        
        Args:
            console: The Rich console instance to use for output
        """
        log.write("\n")
        log.write(f"[bold blue]{self.name}")
        log.write("\n")
        
        # Create a table for statistics
        stats_table = Table(title="Repository Statistics", box=box.ROUNDED)
        stats_table.add_column("Statistic", style="cyan")
        stats_table.add_column("Value", style="green")
        
        # Parse statistics string into individual items
        for stat_line in self.statistics.strip().split('\n'):
            if stat_line and '-' in stat_line:
                key, value = stat_line.split(':', 1) if ':' in stat_line else stat_line.split('-', 1)
                stats_table.add_row(key.strip('- '), value.strip())
        
        log.write(stats_table)
        log.write("\n")
    
    def render_files(self, max_files: int = 20) -> None:
        """
        Render the repository files in a beautiful table format.
        
        Args:
            console: The Rich console instance to use for output
            max_files: Maximum number of files to display (default: 20)
        """
        log.write("\n")
        log.write("[bold cyan]Repository Files")
        log.write("\n")
        
        # Create a table for files
        files_table = Table(title="Project Files", box=box.ROUNDED)
        files_table.add_column("File ID", style="cyan")
        files_table.add_column("Path", style="magenta")
        files_table.add_column("Lines", style="green")
        files_table.add_column("Type", style="yellow")
        files_table.add_column("Complexity", style="red")
        files_table.add_column("Maintainability", style="blue")
        
        # Add files to the table (limited to max_files)
        for file in self.project_files[:max_files]:
            file_type = "Code File" if isinstance(file, ProjectCodeFile) else "File"
            
            complexity = "-"
            maintainability = "-"
            if isinstance(file, ProjectCodeFile) and file.complexity_metrics:
                cc = file.complexity_metrics.get("cyclomatic_complexity", {})
                mi = file.complexity_metrics.get("maintainability_index", {})
                
                if cc and "rank" in cc:
                    complexity = f"{cc.get('total', '?')} ({cc.get('rank', '?')})"
                
                if mi and "rank" in mi:
                    maintainability = f"{int(mi.get('value', 0))} ({mi.get('rank', '?')})"
            
            files_table.add_row(
                file.file_id,
                file.file_path,
                str(file.line_count),
                file_type,
                complexity,
                maintainability
            )
        
        if len(self.project_files) > max_files:
            files_table.add_row(
                "...",
                f"[yellow]And {len(self.project_files) - max_files} more files...[/yellow]",
                "",
                "",
                "",
                ""
            )
        
        log.write(files_table)
        
        # Dependency information (only if there are ProjectCodeFiles)
        code_files = [f for f in self.project_files if isinstance(f, ProjectCodeFile)]
        if code_files:
            log.write("\n")
            log.write("[bold cyan]File Dependencies")
            log.write("\n")
            
            deps_table = Table(title="Dependencies Between Files", box=box.ROUNDED)
            deps_table.add_column("File", style="cyan")
            deps_table.add_column("Depends On", style="magenta")
            deps_table.add_column("Used By", style="green")
            
            for file in code_files[:max(10, max_files // 2)]:  # Show fewer files in dependency table
                # Get actual file paths instead of IDs for better readability
                depends_on_paths = []
                for dep_id in file.dependencies:
                    for f in self.project_files:
                        if f.file_id == dep_id:
                            depends_on_paths.append(f.file_path)
                            break
                
                used_by_paths = []
                for used_id in file.used_by:
                    for f in self.project_files:
                        if f.file_id == used_id:
                            used_by_paths.append(f.file_path)
                            break
                
                deps_table.add_row(
                    file.file_path,
                    "\n".join(depends_on_paths[:5]) + ("\n..." if len(depends_on_paths) > 5 else ""),
                    "\n".join(used_by_paths[:5]) + ("\n..." if len(used_by_paths) > 5 else "")
                )
            
            if len(code_files) > max(10, max_files // 2):
                deps_table.add_row(
                    "[yellow]And more files...[/yellow]",
                    "",
                    ""
                )
                
            log.write(deps_table)
            
            # New Code Metrics Table
            files_with_metrics = [f for f in code_files if isinstance(f, ProjectCodeFile) and f.complexity_metrics]
            if files_with_metrics:
                log.write("\n")
                log.write("[bold cyan]Code Quality Metrics")
                log.write("\n")
                
                metrics_table = Table(title="Code Complexity and Quality", box=box.ROUNDED)
                metrics_table.add_column("File", style="cyan")
                metrics_table.add_column("Cyclomatic Complexity", style="red")
                metrics_table.add_column("Maintainability", style="blue")
                metrics_table.add_column("Code Smells", style="yellow")
                
                for file in files_with_metrics[:max(10, max_files // 2)]:
                    metrics = file.complexity_metrics
                    cc = metrics.get("cyclomatic_complexity", {})
                    mi = metrics.get("maintainability_index", {})
                    smells = metrics.get("code_smells", [])
                    
                    cc_display = "[grey]-[/grey]"
                    if cc:
                        rank = cc.get("rank", "?")
                        rank_color = {
                            "A": "green", "B": "green",
                            "C": "yellow", "D": "red",
                            "F": "red bold"
                        }.get(rank, "white")
                        cc_display = f"Total: {cc.get('total', '?')} | Avg: {cc.get('average', '?')} | Rank: [{rank_color}]{rank}[/{rank_color}]"
                    
                    mi_display = "[grey]-[/grey]"
                    if mi:
                        rank = mi.get("rank", "?")
                        rank_color = {
                            "A": "green", "B": "green",
                            "C": "yellow", "D": "red",
                            "F": "red bold"
                        }.get(rank, "white")
                        mi_display = f"Index: {int(mi.get('value', 0))} | Rank: [{rank_color}]{rank}[/{rank_color}]"
                    
                    smells_display = "[grey]None detected[/grey]"
                    if smells:
                        smells_list = []
                        for i, smell in enumerate(smells[:3]):  # Show up to 3 smells
                            smells_list.append(f"{smell['type']} in {smell['location']}")
                        if len(smells) > 3:
                            smells_list.append(f"...and {len(smells) - 3} more")
                        smells_display = "\n".join(smells_list)
                    
                    metrics_table.add_row(
                        file.file_path,
                        cc_display,
                        mi_display,
                        smells_display
                    )
                
                if len(files_with_metrics) > max(10, max_files // 2):
                    metrics_table.add_row(
                        "[yellow]And more files...[/yellow]",
                        "",
                        "",
                        ""
                    )
                    
                log.write(metrics_table)
        
        log.write("[bold cyan]End of Repository")
    
    def save_to_json(self, output_path: str) -> bool:
        """
        Save the repository data to a JSON file.
        
        Args:
            output_path: The path where to save the JSON file
            console: Optional Rich console for output messages
            
        Returns:
            True if successful, False otherwise
        """
        import json
        from pathlib import Path
        
        try:
            # Convert repository to dict and save as JSON
            repo_dict = self.dict()
            file_path = Path(output_path)
            file_path.parent.mkdir(parents=True, exist_ok=True)
            
            with open(file_path, "w", encoding="utf-8") as f:
                json.dump(repo_dict, f, indent=2)
            
            log.write(f"\n[green]Repository data saved to {output_path}[/green]")
            
            return True
        except Exception as e:
            log.write(f"\n[red]Error saving repository data: {str(e)}[/red]")
            return False
            
    def save_to_markdown(self, output_path: str) -> bool:
        """
        Save the repository data to a Markdown file.
        This exports the same data as save_to_json but in Markdown format.
        
        Args:
            output_path: The path where to save the Markdown file
            console: Optional Rich console for output messages
            
        Returns:
            True if successful, False otherwise
        """
        from pathlib import Path
        
        try:
            file_path = Path(output_path)
            file_path.parent.mkdir(parents=True, exist_ok=True)
            
            with open(file_path, "w", encoding="utf-8") as md_file:
                # Write repository header and basic info
                md_file.write(f"# {self.name}\n\n")
                
                # Write statistics section
                md_file.write("## Repository Statistics\n\n")
                for stat_line in self.statistics.strip().split('\n'):
                    if stat_line and '-' in stat_line:
                        if ':' in stat_line:
                            key, value = stat_line.split(':', 1)
                        else:
                            key, value = stat_line.split('-', 1)
                        md_file.write(f"- **{key.strip('- ')}**: {value.strip()}\n")
                md_file.write("\n")
                
                # Write files section with complete data for each file
                md_file.write("## Project Files\n\n")
                
                for i, file in enumerate(self.project_files):
                    # Make a header for each file
                    md_file.write(f"### {i+1}. {file.file_path}\n\n")
                    md_file.write(f"- **File ID**: {file.file_id}\n")
                    md_file.write(f"- **Type**: {'Code File' if isinstance(file, ProjectCodeFile) else 'File'}\n")
                    md_file.write(f"- **Line Count**: {file.line_count}\n")
                    md_file.write(f"- **Description**: {file.description}\n")
                    
                    # If it's a code file, include additional information
                    if isinstance(file, ProjectCodeFile):
                        # Dependencies
                        if file.dependencies:
                            md_file.write("- **Dependencies**:\n")
                            for dep_id in file.dependencies:
                                md_file.write(f"  - {dep_id}\n")
                        else:
                            md_file.write("- **Dependencies**: None\n")
                        
                        # Used by
                        if file.used_by:
                            md_file.write("- **Used By**:\n")
                            for used_id in file.used_by:
                                md_file.write(f"  - {used_id}\n")
                        else:
                            md_file.write("- **Used By**: None\n")
                        
                        # Complexity metrics
                        if file.complexity_metrics:
                            md_file.write("- **Complexity Metrics**:\n")
                            
                            # Cyclomatic complexity
                            if "cyclomatic_complexity" in file.complexity_metrics:
                                cc = file.complexity_metrics["cyclomatic_complexity"]
                                md_file.write("  - **Cyclomatic Complexity**:\n")
                                for cc_key, cc_value in cc.items():
                                    md_file.write(f"    - {cc_key}: {cc_value}\n")
                            
                            # Maintainability index
                            if "maintainability_index" in file.complexity_metrics:
                                mi = file.complexity_metrics["maintainability_index"]
                                md_file.write("  - **Maintainability Index**:\n")
                                for mi_key, mi_value in mi.items():
                                    md_file.write(f"    - {mi_key}: {mi_value}\n")
                            
                            # Code smells
                            if "code_smells" in file.complexity_metrics and file.complexity_metrics["code_smells"]:
                                smells = file.complexity_metrics["code_smells"]
                                md_file.write("  - **Code Smells**:\n")
                                for smell in smells:
                                    md_file.write(f"    - Type: {smell.get('type', 'Unknown')}, Location: {smell.get('location', 'Unknown')}\n")
                                    if "details" in smell:
                                        md_file.write(f"      Details: {smell['details']}\n")
                    
                    # Include the complete file content
                    md_file.write("\n**Content**:\n")
                    md_file.write("```\n")
                    md_file.write(file.content)
                    md_file.write("\n```\n\n")
                    
                    # Add a separator between files
                    md_file.write("---\n\n")
                
                # Add overall statistics as a summary at the end
                md_file.write("## Summary\n\n")
                md_file.write(f"- **Total Files**: {len(self.project_files)}\n")
                md_file.write(f"- **Code Files**: {len([f for f in self.project_files if isinstance(f, ProjectCodeFile)])}\n")
                md_file.write(f"- **Regular Files**: {len([f for f in self.project_files if not isinstance(f, ProjectCodeFile)])}\n")
                
                # Calculate total lines of code
                total_lines = sum(f.line_count for f in self.project_files)
                md_file.write(f"- **Total Lines of Code**: {total_lines}\n")
            
            log.write(f"\n[green]Repository data saved to {output_path}[/green]")
            
            return True
        except Exception as e:
            log.write(f"\n[red]Error saving repository data to Markdown: {str(e)}[/red]")
            import traceback
            log.write(f"\n[red]{traceback.format_exc()}[/red]")
            return False

@flock_type
class UserStory(BaseModel):
    user_story_id: str = Field(..., description="Unique identifier for the user story")
    status: Literal["active", "created", "done"] = Field(..., description="Status of the user story")
    description: str = Field(..., description="Description of the user story")
    definition_of_done: list[str] = Field(..., description="List of criteria for the user story to be considered done")
    tasks: list[str] = Field(..., description="List of task ids that are part of this user story")
    story_points: int = Field(..., description="Number of story points for the user story")
    dependencies: list[str] = Field(..., description="List of user story ids that must be completed before this one")
    used_by: list[str] = Field(..., description="List of user story ids that depend on this one")

@flock_type
class Task(BaseModel):
    task_id: str = Field(..., description="Unique identifier for the task")
    status: Literal["active", "created", "done"] = Field(..., description="Status of the task")
    acceptance_criteria: list[str] = Field(..., description="List of acceptance criteria for the task")
    description: str = Field(..., description="Description of the task")
    estimated_lines_of_code: int = Field(..., description="Estimated number of lines of code for the task")
    dependencies: list[str] = Field(..., description="List of task ids that must be completed before this one")
    used_by: list[str] = Field(..., description="List of task ids that depend on this one")

@flock_type
class ToDoItem(BaseModel):
    todo_id: str = Field(..., description="Unique identifier for the todo item")
    user_story_id: str = Field(..., description="Unique identifier for the user story")
    task_id: str = Field(..., description="Unique identifier for the task")
    cli_command_linux: str | None = Field(..., description="valid CLI command to be executed on linux")
    cli_command_windows: str | None = Field(..., description="valid CLI command to be executed on windows")
    cli_command_macos: str | None = Field(..., description="valid CLI command to be executed on macos")
    file_content: str | None = Field(..., description="Complete content of the file if action is create_file or update_file")
    description: str = Field(..., description="Description and/or reasoning of the todo item")

@flock_type
class TaskAndToDoItemList(BaseModel):
    tasks: list[Task] = Field(..., description="List of tasks")
    todo_items: list[ToDoItem] = Field(..., description="List of todo items")
    



@flock_type
class Project(BaseModel):
    name: str = Field(..., description="Name of the project")
    description: str = Field(..., description="Description of the project")
    implementation_plan: str = Field(..., description="High Level Implementation plan for the project in beautiful markdown")
    readme: str = Field(..., description="README.md file for the project in beautiful markdown")
    requirements: list[str] = Field(..., description="List of feature requirements for the project")
    tech_stack: list[str] = Field(..., description="List of technologies used in the project")
    user_stories: list[UserStory] | None = Field(..., description="List of user stories for the project")
    # tasks: list[Task]| None = Field(..., description="List of tasks for the project")
    # project_files: list[ProjectFile | ProjectCodeFile] = Field(..., description="Output data of the project")

    def render_summary(self) -> None:
        """
        Render a summary of the project in a beautiful format.
        
        Args:
            console: The Rich console instance to use for output
        """
        log.write("\n")
        log.write(f"[bold blue]{self.name}")
        log.write("\n")
        log.write(self.description)
        log.write("\n")

        log.write(Markdown(self.implementation_plan))
        log.write("\n")
        log.write(Markdown(self.readme))
        log.write("\n")
        
        # Create a table for requirements
        req_table = Table(title="Project Requirements", box=box.ROUNDED)
        req_table.add_column("Requirement", style="cyan")
        
        for req in self.requirements:
            req_table.add_row(req)
            
        log.write(req_table)
        log.write("\n")
        
        # Create a table for tech stack
        tech_table = Table(title="Technology Stack", box=box.ROUNDED)
        tech_table.add_column("Technology", style="green")
        
        for tech in self.tech_stack:
            tech_table.add_row(tech)
            
        log.write(tech_table)
        log.write("\n")
        
        # Summary of user stories and tasks
        if self.user_stories:
            log.write("\n")
            log.write("[bold cyan]User Stories")
            log.write(f"\n[bold]Total User Stories:[/bold] {len(self.user_stories)}")
            for user_story in self.user_stories:
                log.write(f"\n[bold]User Story:[/bold] {user_story.user_story_id}")
                log.write(f"[bold]Description:[/bold] {user_story.description}")
                log.write(f"[bold]Definition of Done:[/bold] {user_story.definition_of_done}")
                #console.print(f"[bold]Tasks:[/bold] {user_story.tasks}")
                log.write(f"[bold]Story Points:[/bold] {user_story.story_points}")
        
        # if self.tasks:
        #     console.print("\n")
        #     console.rule("[bold cyan]Tasks")
        #     console.print(f"\n[bold]Total Tasks:[/bold] {len(self.tasks)}")
        #     for task in self.tasks:
        #         console.print(f"\n[bold]Task:[/bold] {task.task_id}")
        #         console.print(f"[bold]Description:[/bold] {task.description}")
        #         console.print(f"[bold]Acceptance Criteria:[/bold] {task.acceptance_criteria}")
        #         console.print(f"[bold]Estimated Lines of Code:[/bold] {task.estimated_lines_of_code}")
        #         console.print(f"[bold]Dependencies:[/bold] {task.dependencies}")
        #         console.print(f"[bold]Used By:[/bold] {task.used_by}")
        
        # # Summary of files
        # console.print("\n")
        # console.rule("[bold cyan]Files")
        # console.print(f"\n[bold]Total Files:[/bold] {len(self.project_files)}")
        
        # console.rule("[bold cyan]End of Project Summary")


    
```

### examples\05-full-projects\project-textual-project-planner\src\flock_flightplan\project_tree.py

- **Lines**: 105
- **Last modified**: 2025-05-22 18:59:04

```py
# src/flock_flightplan/project_tree.py
from textual.widgets import Tree
from textual.widgets.tree import TreeNode
# Import the specific event type
from textual.message import Message # Base class for messages
from rich.style import Style

class ProjectTree(Tree):
    """Tree widget for displaying the project structure."""

    def __init__(self, data):
        # Use a more descriptive label or leave it empty if preferred
        super().__init__("Project Structure")
        self.data = data
        self.node_types = {}  # Maps tree nodes IDs to their node_types
        self.node_selected_handler = None  # Callback for node selection
        self.build_tree()

    def build_tree(self):
        """Build the tree structure from the data."""
        self.clear() # Clear existing nodes if rebuilding
        self.root.expand()
        root_node: TreeNode = self.root # type hint

        # Get allowed children of root
        root_allowed = self.data.root_element.allowed_children

        # Create a mapping of node types to their definitions
        node_types = {node.node_type: node for node in self.data.implementation_plan}

        # Recursively build the tree starting from the children of the conceptual root
        for child_type, cardinality in root_allowed.items():
            if child_type in node_types:
                # Add top-level nodes directly to the root
                child_node = root_node.add(f"{node_types[child_type].emoji} {node_types[child_type].display_name}")
                child_node.expand() # Optionally expand top-level nodes
                # Store the node type for this tree node using its ID
                self.node_types[child_node.id] = child_type
                self._add_children(child_node, node_types[child_type], node_types)
            else:
                # Log or handle cases where a node_type is missing in implementation_plan
                self.app.log.warning(f"Node type '{child_type}' defined in root_element but not found in implementation_plan.")
                
        self.root.expand_all()

    # Override the rich_style property to handle potential style issues
    @property
    def rich_style(self) -> Style:
        """Override to ensure we always return a valid Style object."""
        try:
            return super().rich_style
        except AttributeError:
            # Return a default style if there's an issue with the original style
            return Style()

    def _add_children(self, parent_node: TreeNode, node_data, node_types):
        """Recursively add children to a node."""
        allowed_children = node_data.allowed_children
        for child_type, cardinality in allowed_children.items():
            if child_type in node_types:
                # Check if cardinality allows multiple instances
                create_multiple = cardinality in ["many", "multiple", "0..*", "1..*", "*"]
                
                # Number of instances to create (1 by default, 3 if multiple allowed)
                num_instances = 3 if create_multiple else 1
                
                for i in range(num_instances):
                    # Add instance number suffix if multiple
                    suffix = f" #{i+1}" if create_multiple else ""
                    label = f"{node_types[child_type].emoji} {node_types[child_type].display_name}{suffix}"
                    
                    child_node = parent_node.add(label)
                    # Store the node type for this tree node using its ID
                    self.node_types[child_node.id] = child_type
                    # Recurse
                    self._add_children(child_node, node_types[child_type], node_types)
            else:
                 self.app.log.warning(f"Node type '{child_type}' defined in allowed_children for '{node_data.node_type}' but not found in implementation_plan.")


    def on_mount(self) -> None:
        """Called when the widget is mounted."""
        # Ensure the root node is selected by default if desired, or none
        #self.select_node(self.root) # Selects "Project Structure"
        self.root.expand_all()

    # Use the specific event type for clarity and better type checking
    def on_tree_node_selected(self, event: Tree.NodeSelected) -> None:
        """Handle tree node selection event."""
        try:
            event.stop() # Prevent the event bubbling further if needed
            node: TreeNode = event.node

            # Check if the selected node is the root node itself or a node we added
            if node is not self.root and node.id in self.node_types:
                node_type = self.node_types[node.id]
                if self.node_selected_handler:
                    # self.app.log(f"Node selected: ID={node.id}, Type={node_type}") # Debugging line
                    self.node_selected_handler(node_type)
        except Exception as e:
            # Catch any errors that might occur during node selection
            self.app.log.error(f"Error handling node selection: {e}")
            # Optionally notify the user
            if hasattr(self.app, 'notify'):
                self.app.notify(f"Error selecting node: {str(e)}", severity="error")
```

### examples\05-full-projects\project-textual-project-planner\src\flock_flightplan\utils.py

- **Lines**: 35
- **Last modified**: 2025-05-22 18:59:04

```py
from io import StringIO
from rich.console import Console
from rich.text import Text  

from textual.widgets import Static
buffer = StringIO()
console = Console(file=buffer)



cli_text = Text()

def cli_init(log: Static):
    banner_text = Text(
        """
                                                                                                                         
    _/_/_/_/       _/       _/                     _/          _/                         _/                             
   _/             _/                  _/_/_/      _/_/_/    _/_/_/_/        _/_/_/       _/        _/_/_/      _/_/_/    
  _/_/_/         _/       _/       _/    _/      _/    _/    _/            _/    _/     _/      _/    _/      _/    _/   
 _/             _/       _/       _/    _/      _/    _/    _/            _/    _/     _/      _/    _/      _/    _/    
_/             _/       _/         _/_/_/      _/    _/      _/_/        _/_/_/       _/        _/_/_/      _/    _/     
                                      _/                                _/                                               
                                 _/_/                                  _/                                                

""",
        style="bold orange"
    )

    banner_text.append(Text("plan anything - implement everything - ｐｏｗｅｒｅｄ ｂｙ ＦＬＯＣＫ\n", style="bold white"))

    log.update(banner_text)




```

### examples\cookbook\_old\agentic_chat.py

- **Lines**: 111
- **Last modified**: 2025-05-22 18:59:04

```py
"""
Title: Reasoning assistant with self managed memory
"""

import warnings
from datetime import datetime

from flock.core.flock_router import HandOffRequest
from flock.core.tools import basic_tools

warnings.simplefilter("error", UserWarning)
import asyncio
from dataclasses import dataclass, field

from flock.core.flock import Flock
from flock.core.flock_agent import FlockAgent, FlockAgentConfig
from rich.console import Console
from rich.panel import Panel
from rich.prompt import Prompt


@dataclass
class Chat:
    chat_history: list[str] = field(default_factory=list)
    user_query: str = ""
    answer_to_query: str = ""
    memory: str = ""

    async def before_response(self, agent, inputs):
        console = Console()

        # Use a Rich-styled prompt to get user input
        self.user_query = Prompt.ask("[bold cyan]User[/bold cyan]")
        inputs["user_query"] = self.user_query

    # Triggers after the agent responds to the user query
    async def after_response(self, agent: FlockAgent, inputs, outputs):
        # Update answer and history based on the agent's outputs
        console = Console()
        self.answer_to_query = outputs["answer_to_query"]
        self.chat_history.append(
            {"user": self.user_query, "assistant": self.answer_to_query}
        )

        agent.save_memory_graph("chat_memory_graph.json")
        agent.export_memory_graph("chat_memory_graph.png")

        # Display the assistant's reasoning (if available) in a styled panel
        reasoning = outputs.get("reasoning", "")
        if reasoning:
            reasoning_panel = Panel(
                reasoning,
                title="[bold blue]Assistant Reasoning[/bold blue]",
                border_style="blue",
            )
            console.print(reasoning_panel)

        # Display the assistant's answer in a styled panel
        answer_panel = Panel(
            self.answer_to_query,
            title="[bold green]Assistant Answer[/bold green]",
            border_style="green",
        )
        console.print(answer_panel)

    # Triggers at handoff to the next agent
    def hand_off(self, context, result):
        if self.user_query.lower() == "goodbye":
            return None
        return HandOffRequest(next_agent="chatty")


MODEL = "openai/gpt-4o"


async def main():
    chat_helper = Chat()
    flock = Flock(model=MODEL, local_debug=True)

    memory_config = FlockAgentMemoryConfig()
    memory_config.storage_type = "json"
    memory_config.file_path = "chat_memory_graph.json"

    chatty = FlockAgent(
        name="chatty",
        description=f"""You are Chatty, a friendly assistant that loves to chat. 
                    Today is {datetime.now().strftime("%A, %B %d, %Y")}.
                    """,
        input="user_query",
        output="answer_to_query",
        initialize_callback=chat_helper.before_response,
        terminate_callback=chat_helper.after_response,
        config=FlockAgentConfig(disable_output=True),
        tools=[
            basic_tools.web_search_duckduckgo,
            basic_tools.get_web_content_as_markdown,
            basic_tools.code_eval,
        ],
        memory_enabled=True,
        memory_config=memory_config,
    )

    flock.add_agent(chatty)

    chatty.hand_off = chat_helper.hand_off

    await flock.run_async(start_agent=chatty, input={"user_query": ""})


if __name__ == "__main__":
    asyncio.run(main())
```

### examples\cookbook\_old\custom-evaluator-example.py

- **Lines**: 746
- **Last modified**: 2025-05-22 18:59:04

```py
"""
Repository Analyzer

This example demonstrates how to use Flock to create a system that analyzes a repository
and generates a comprehensive knowledge database about it.
"""

import os
import sys
from typing import Any, Dict, List

from flock.core import Flock, FlockAgent
from flock.core.tools import basic_tools

# Define custom evaluators for the agents


class RepoStructureEvaluator:
    """Custom evaluator for the repository structure analyzer agent."""

    async def evaluate(self, agent, inputs, tools):
        """
        Analyze the repository structure and identify key files.

        Args:
            agent: The agent instance
            inputs: The input parameters
            tools: The available tools

        Returns:
            Dictionary with the repository analysis results
        """
        repo_path = inputs["repo_path"]

        # Get the repository name from the path
        repo_name = os.path.basename(os.path.abspath(repo_path))

        # Get the repository structure
        file_structure = get_repo_structure(repo_path)

        # Check if README.md exists
        readme_path = os.path.join(repo_path, "README.md")
        readme_content = ""
        if os.path.exists(readme_path):
            with open(readme_path, "r", encoding="utf-8", errors="ignore") as f:
                readme_content = f.read()

        # Identify key files
        key_files = identify_key_files(file_structure, readme_content)

        return {
            "repo_name": repo_name,
            "key_files": key_files,
            "file_structure": file_structure,
            "readme_content": readme_content,
        }


# Define the agents

# 1. Repository Structure Analyzer
# This agent analyzes the repository structure and identifies key files
repo_structure_analyzer = FlockAgent(
    name="repo_structure_analyzer",
    input="repo_path: str | Path to the repository to analyze",
    output="""
        repo_name: str | Name of the repository,
        key_files: list[str] | List of key files to analyze in detail,
        file_structure: dict | Dictionary representing the repository structure,
        readme_content: str | Content of the README file if it exists
    """,
    tools=[basic_tools.read_from_file],
)

# Set custom evaluator
repo_structure_analyzer.evaluate = RepoStructureEvaluator().evaluate


class FileContentEvaluator:
    """Custom evaluator for the file content analyzer agent."""

    async def evaluate(self, agent, inputs, tools):
        """
        Analyze the content of key files to understand their purpose and functionality.

        Args:
            agent: The agent instance
            inputs: The input parameters
            tools: The available tools

        Returns:
            Dictionary with the file analysis results
        """
        repo_path = inputs["repo_path"]
        key_files = inputs["key_files"]

        file_analyses = {}
        core_components = []
        key_concepts = []

        # Analyze each key file
        for file_path in key_files:
            full_path = os.path.join(repo_path, file_path)
            if not os.path.exists(full_path):
                continue

            try:
                with open(full_path, "r", encoding="utf-8", errors="ignore") as f:
                    content = f.read()

                # Basic analysis of the file
                analysis = self._analyze_file(file_path, content)
                file_analyses[file_path] = analysis

                # Identify core components
                if analysis["type"] == "class" or analysis["type"] == "module":
                    component = {
                        "name": analysis["name"],
                        "description": analysis["summary"],
                        "detailed_description": analysis["description"],
                        "file_path": file_path,
                        "features": analysis["features"],
                    }
                    core_components.append(component)

                # Identify key concepts
                for concept in analysis["concepts"]:
                    key_concept = {
                        "name": concept["name"],
                        "description": concept["description"],
                        "detailed_description": concept["detailed_description"],
                    }

                    # Check if the concept already exists
                    if not any(c["name"] == key_concept["name"] for c in key_concepts):
                        key_concepts.append(key_concept)
            except Exception as e:
                file_analyses[file_path] = {
                    "error": str(e),
                    "type": "unknown",
                    "name": os.path.basename(file_path),
                    "summary": "Error analyzing file",
                    "description": f"Error analyzing file: {str(e)}",
                    "features": [],
                    "concepts": [],
                }

        return {
            "file_analyses": file_analyses,
            "core_components": core_components,
            "key_concepts": key_concepts,
        }

    def _analyze_file(self, file_path: str, content: str) -> Dict[str, Any]:
        """
        Analyze a file to understand its purpose and functionality.

        Args:
            file_path: Path to the file
            content: Content of the file

        Returns:
            Dictionary with the file analysis results
        """
        # Determine the file type
        file_type = "unknown"
        name = os.path.basename(file_path)
        summary = ""
        description = ""
        features = []
        concepts = []

        # Extract the file extension
        _, ext = os.path.splitext(file_path)

        if ext == ".py":
            file_type = "python"

            # Check if it's a class definition
            if "class " in content:
                file_type = "class"

                # Extract class name
                import re

                class_match = re.search(r"class\s+(\w+)", content)
                if class_match:
                    name = class_match.group(1)

                # Extract docstring
                docstring_match = re.search(
                    r'class\s+\w+.*?:\s*?"""(.*?)"""', content, re.DOTALL
                )
                if docstring_match:
                    docstring = docstring_match.group(1).strip()
                    lines = docstring.split("\n")
                    if lines:
                        summary = lines[0].strip()
                        description = "\n".join(lines[1:]).strip()

                # Extract methods as features
                method_matches = re.finditer(r"def\s+(\w+)\s*\(", content)
                for match in method_matches:
                    method_name = match.group(1)
                    if not method_name.startswith("_") or method_name.startswith("__"):
                        features.append(method_name)

            # Check if it's a module
            elif "__init__.py" in file_path:
                file_type = "module"

                # Extract module name
                name = os.path.basename(os.path.dirname(file_path))

                # Extract docstring
                docstring_match = re.search(r'"""(.*?)"""', content, re.DOTALL)
                if docstring_match:
                    docstring = docstring_match.group(1).strip()
                    lines = docstring.split("\n")
                    if lines:
                        summary = lines[0].strip()
                        description = "\n".join(lines[1:]).strip()

                # Extract functions and classes as features
                function_matches = re.finditer(r"def\s+(\w+)\s*\(", content)
                for match in function_matches:
                    function_name = match.group(1)
                    if not function_name.startswith("_"):
                        features.append(function_name)

                class_matches = re.finditer(r"class\s+(\w+)", content)
                for match in class_matches:
                    class_name = match.group(1)
                    features.append(class_name)

        elif ext == ".md":
            file_type = "markdown"

            # Extract title
            import re

            title_match = re.search(r"#\s+(.*)", content)
            if title_match:
                name = title_match.group(1).strip()

            # Extract summary
            lines = content.split("\n")
            for line in lines:
                if line.strip() and not line.startswith("#"):
                    summary = line.strip()
                    break

            # Extract description
            description = content

            # Extract concepts
            concept_matches = re.finditer(r"##\s+(.*)", content)
            for match in concept_matches:
                concept_name = match.group(1).strip()
                concept_start = match.end()

                # Find the end of the concept
                concept_end = len(content)
                next_match = re.search(r"##\s+", content[concept_start:])
                if next_match:
                    concept_end = concept_start + next_match.start()

                concept_content = content[concept_start:concept_end].strip()

                # Extract the first paragraph as the description
                concept_description = ""
                concept_lines = concept_content.split("\n")
                for line in concept_lines:
                    if line.strip():
                        concept_description = line.strip()
                        break

                concepts.append(
                    {
                        "name": concept_name,
                        "description": concept_description,
                        "detailed_description": concept_content,
                    }
                )

        # If we couldn't extract a summary, use the first non-empty line
        if not summary:
            lines = content.split("\n")
            for line in lines:
                if line.strip():
                    summary = line.strip()
                    break

        # If we couldn't extract a description, use the first few lines
        if not description:
            lines = content.split("\n")
            description = "\n".join(lines[:10])

        # If we couldn't extract any features, use the file name
        if not features:
            features.append(name)

        # If we couldn't extract any concepts, create a default one
        if not concepts:
            concepts.append(
                {
                    "name": name,
                    "description": summary,
                    "detailed_description": description,
                }
            )

        return {
            "type": file_type,
            "name": name,
            "summary": summary,
            "description": description,
            "features": features,
            "concepts": concepts,
        }


# 2. File Content Analyzer
# This agent analyzes the content of key files to understand their purpose and functionality
file_content_analyzer = FlockAgent(
    name="file_content_analyzer",
    input="""
        repo_path: str | Path to the repository,
        key_files: list[str] | List of key files to analyze
    """,
    output="""
        file_analyses: dict | Dictionary mapping file paths to their analysis,
        core_components: list[dict] | List of core components identified in the codebase,
        key_concepts: list[dict] | List of key concepts identified in the codebase
    """,
    tools=[basic_tools.read_from_file],
)

# Set custom evaluator
file_content_analyzer.evaluate = FileContentEvaluator().evaluate


class DocumentationGeneratorEvaluator:
    """Custom evaluator for the documentation generator agent."""

    async def evaluate(self, agent, inputs, tools):
        """
        Generate comprehensive documentation based on the repository analysis.

        Args:
            agent: The agent instance
            inputs: The input parameters
            tools: The available tools

        Returns:
            Dictionary with the documentation files
        """
        repo_path = inputs["repo_path"]
        repo_name = inputs["repo_name"]
        file_structure = inputs["file_structure"]
        readme_content = inputs["readme_content"]
        file_analyses = inputs["file_analyses"]
        core_components = inputs["core_components"]
        key_concepts = inputs["key_concepts"]

        # Create the documentation structure
        documentation_files = create_documentation_structure(
            repo_name, core_components, key_concepts
        )

        return {"documentation_files": documentation_files}


# 3. Documentation Generator
# This agent generates comprehensive documentation based on the repository analysis
documentation_generator = FlockAgent(
    name="documentation_generator",
    input="""
        repo_path: str | Path to the repository,
        repo_name: str | Name of the repository,
        file_structure: dict | Dictionary representing the repository structure,
        readme_content: str | Content of the README file if it exists,
        file_analyses: dict | Dictionary mapping file paths to their analysis,
        core_components: list[dict] | List of core components identified in the codebase,
        key_concepts: list[dict] | List of key concepts identified in the codebase
    """,
    output="""
        documentation_files: dict | Dictionary mapping file paths to their content
    """,
    tools=[basic_tools.save_to_file],
)

# Set custom evaluator
documentation_generator.evaluate = DocumentationGeneratorEvaluator().evaluate

# Set up the agent chain
repo_structure_analyzer.hand_off = file_content_analyzer
file_content_analyzer.hand_off = documentation_generator

# Alternative way to set up the agent chain (as shown in examples/02_cook_book/long_research_no_handoff.py)
# This would be used if we wanted to do custom processing between agent runs
# For example:
"""
# Instead of using hand_off, we could do:
result = flock.run(
    start_agent=repo_structure_analyzer,
    input={"repo_path": repo_path}
)

# Then process the result and run the next agent
file_analysis_result = flock.run(
    start_agent=file_content_analyzer,
    input={
        "repo_path": repo_path,
        "key_files": result["key_files"]
    }
)

# Then process the result and run the next agent
documentation_result = flock.run(
    start_agent=documentation_generator,
    input={
        "repo_path": repo_path,
        "repo_name": result["repo_name"],
        "file_structure": result["file_structure"],
        "readme_content": result["readme_content"],
        "file_analyses": file_analysis_result["file_analyses"],
        "core_components": file_analysis_result["core_components"],
        "key_concepts": file_analysis_result["key_concepts"]
    }
)
"""

# Helper functions for the agents


def get_repo_structure(repo_path: str) -> Dict[str, Any]:
    """
    Recursively get the structure of a repository.

    Args:
        repo_path: Path to the repository

    Returns:
        Dictionary representing the repository structure
    """
    result = {}

    for root, dirs, files in os.walk(repo_path):
        # Skip hidden directories and files
        dirs[:] = [d for d in dirs if not d.startswith(".")]
        files = [f for f in files if not f.startswith(".")]

        # Skip virtual environments
        if "venv" in dirs:
            dirs.remove("venv")
        if "env" in dirs:
            dirs.remove("env")
        if "__pycache__" in dirs:
            dirs.remove("__pycache__")

        # Get the relative path
        rel_path = os.path.relpath(root, repo_path)
        if rel_path == ".":
            rel_path = ""

        # Add files to the result
        if files:
            result[rel_path] = files

    return result


def identify_key_files(
    repo_structure: Dict[str, Any], readme_content: str = ""
) -> List[str]:
    """
    Identify key files in the repository based on the structure and README content.

    Args:
        repo_structure: Dictionary representing the repository structure
        readme_content: Content of the README file if it exists

    Returns:
        List of key files to analyze in detail
    """
    key_files = []

    # Look for common important files
    for path, files in repo_structure.items():
        for file in files:
            file_path = os.path.join(path, file) if path else file

            # Main module files
            if file == "__init__.py":
                key_files.append(file_path)

            # Main implementation files
            if file.endswith(".py") and not file.startswith("test_"):
                key_files.append(file_path)

            # Configuration files
            if file in ["setup.py", "pyproject.toml", "requirements.txt"]:
                key_files.append(file_path)

            # Documentation files
            if file.endswith(".md") and file != "README.md":
                key_files.append(file_path)

    # Limit to a reasonable number of files
    return key_files[:20]  # Limit to 20 key files


def create_documentation_structure(
    repo_name: str, core_components: List[Dict], key_concepts: List[Dict]
) -> Dict[str, str]:
    """
    Create a documentation structure based on the repository analysis.

    Args:
        repo_name: Name of the repository
        core_components: List of core components identified in the codebase
        key_concepts: List of key concepts identified in the codebase

    Returns:
        Dictionary mapping file paths to their content
    """
    docs = {}

    # Create README.md
    docs["README.md"] = f"""# {repo_name} Documentation

This folder contains comprehensive documentation about the {repo_name} framework.

## Purpose

This documentation serves as a knowledge base for understanding the {repo_name} framework, its architecture, components, features, and usage patterns. It is designed to provide a complete overview that can be quickly consumed to gain a deep understanding of the framework.

This is a living document that should be continuously updated. Whenever new information about the framework is discovered that is not yet included in this documentation, it should be added to the appropriate files. This ensures that the documentation remains comprehensive and up-to-date.

The `tasks` subfolder contains a log of all activities performed related to this documentation, which helps track what has been done and what still needs to be done.

## Contents

- [index.md](index.md) - Table of contents and overview of the documentation
- [overview.md](overview.md) - High-level overview of the framework
- [core-components.md](core-components.md) - Detailed information about the core components
- [architecture.md](architecture.md) - Information about the architecture and design decisions
- [features.md](features.md) - Key features
- [examples.md](examples.md) - Example usage patterns
- [file_lookup.md](file_lookup.md) - Links between key concepts and code files
- [tasks/](tasks/) - Log of all activities performed related to this documentation

## How to Use This Documentation

Start with the [index.md](index.md) file, which provides a table of contents and overview of the documentation. From there, you can navigate to specific topics of interest.

For a quick understanding, read the [overview.md](overview.md) file, which provides a high-level overview of the framework.

For more detailed information about specific aspects, refer to the corresponding documentation files.
"""

    # Create index.md
    docs["index.md"] = f"""# {repo_name} Framework Documentation

This documentation provides a comprehensive overview of the {repo_name} framework.

## Table of Contents

1. [Overview](overview.md)
   - Key Concepts
   - Core Components
   - Architecture

2. [Core Components](core-components.md)
   {os.linesep.join([f"   - {component['name']}" for component in core_components])}

3. [Architecture](architecture.md)
   - High-Level Architecture
   - Component Relationships
   - Design Decisions

4. [Features](features.md)
   {os.linesep.join([f"   - {concept['name']}" for concept in key_concepts])}

5. [Examples](examples.md)
   - Basic Example
   - Advanced Examples

6. [File Lookup](file_lookup.md)
   - Core Components
   - Key Files
   - Examples
"""

    # Create overview.md
    docs["overview.md"] = f"""# {repo_name} Framework Overview

This document provides a high-level overview of the {repo_name} framework.

## Key Concepts

{os.linesep.join([f"- **{concept['name']}**: {concept['description']}" for concept in key_concepts])}

## Core Components

{os.linesep.join([f"- **{component['name']}**: {component['description']}" for component in core_components])}

## Architecture

The {repo_name} framework is designed with a modular architecture that separates concerns and allows for flexibility and extensibility.
"""

    # Create core-components.md
    docs["core-components.md"] = f"""# {repo_name} Core Components

This document provides detailed information about the core components of the {repo_name} framework.

{os.linesep.join([f"## {component['name']}{os.linesep}{os.linesep}{component['detailed_description']}{os.linesep}" for component in core_components])}
"""

    # Create file_lookup.md
    docs["file_lookup.md"] = f"""# {repo_name} Framework Code File Lookup

This document provides links between key concepts in the {repo_name} framework and the corresponding code files where they are implemented.

## Core Components

{os.linesep.join([f"### {component['name']}{os.linesep}{os.linesep}- **Implementation**: {component['file_path']}{os.linesep}- **Key Features**:{os.linesep}{os.linesep.join(['  - ' + feature for feature in component['features']])}{os.linesep}" for component in core_components])}
"""

    # Create tasks folder and task_log.md
    docs["tasks/task_log.md"] = f"""# Task Log

This file logs all tasks performed related to the {repo_name} framework documentation.

## Initial Documentation Creation

1. Created the documentation folder as a knowledge base for {repo_name} framework information.
2. Analyzed the {repo_name} framework by examining key source files.
3. Created comprehensive documentation files:
   - overview.md - High-level overview of the framework
   - core-components.md - Detailed information about the core components
   - architecture.md - Information about the architecture and design decisions
   - features.md - Key features
   - examples.md - Example usage patterns
   - file_lookup.md - Links between key concepts and code files
   - index.md - Table of contents and overview of the documentation
   - README.md - Introduction to the documentation
4. Created a tasks subfolder to protocol all activities.

### Future Tasks

1. Continue to update documentation as new information is discovered.
2. Add more detailed information about specific components as needed.
3. Keep the file_lookup.md updated with new files and components.
4. Add more examples and use cases as they are discovered.
"""

    # Create empty files for other documentation
    docs["architecture.md"] = f"""# {repo_name} Architecture

This document provides an overview of the {repo_name} framework's architecture and design decisions.

## High-Level Architecture

The {repo_name} framework is designed with a modular architecture that separates concerns and allows for flexibility and extensibility.

## Component Relationships

The main components of the {repo_name} framework and their relationships.

## Design Decisions

Key design decisions that shaped the {repo_name} framework.
"""

    docs["features.md"] = f"""# {repo_name} Key Features

This document outlines the key features of the {repo_name} framework.

{os.linesep.join([f"## {concept['name']}{os.linesep}{os.linesep}{concept['detailed_description']}{os.linesep}" for concept in key_concepts])}
"""

    docs["examples.md"] = f"""# {repo_name} Examples

This document provides examples of how to use the {repo_name} framework for various use cases.

## Basic Example

A simple example of using the {repo_name} framework.

## Advanced Examples

More complex examples of using the {repo_name} framework.
"""

    return docs


def main():
    """Main function to run the repository analyzer."""
    if len(sys.argv) < 2:
        print("Usage: python repo_analyzer.py <repo_path> [output_path]")
        sys.exit(1)

    repo_path = sys.argv[1]
    output_path = (
        sys.argv[2]
        if len(sys.argv) > 2
        else os.path.join(repo_path, "docs", "generated")
    )

    # Create the Flock instance
    flock = Flock(model="openai/gpt-4o")

    # Add the agents to the flock
    flock.add_agent(repo_structure_analyzer)
    flock.add_agent(file_content_analyzer)
    flock.add_agent(documentation_generator)

    # Run the flock
    result = flock.run(
        start_agent=repo_structure_analyzer, input={"repo_path": repo_path}
    )

    # Create the output directory if it doesn't exist
    os.makedirs(output_path, exist_ok=True)
    os.makedirs(os.path.join(output_path, "tasks"), exist_ok=True)

    # Save the documentation files
    for file_path, content in result["documentation_files"].items():
        full_path = os.path.join(output_path, file_path)

        # Create directories if needed
        os.makedirs(os.path.dirname(full_path), exist_ok=True)

        # Write the file
        with open(full_path, "w") as f:
            f.write(content)

    print(f"Documentation generated successfully in {output_path}")


if __name__ == "__main__":
    main()
```

### examples\cookbook\_old\working_with_images_2.py

- **Lines**: 97
- **Last modified**: 2025-05-22 18:59:04

```py
import matplotlib.pyplot as plt
import numpy as np
from dspy import Image
from flock.core import Flock, FlockAgent
from flock.core.flock_agent import FlockAgentConfig, FlockAgentOutputConfig
from flock.core.logging.formatters.themes import OutputTheme
from pydantic import BaseModel, Field


# Class for parts of the final image
class ImagePart(BaseModel):
    image_part: str = Field(description="Part of the image to draw")
    list_of_coordinates: list[tuple[float, float]] = Field(
        default_factory=list,
        description="List of coordinates to connect to create a part of the image. X<10 - Y<10 - coordinates are floats - use this accuracy for better results",
    )
    matplotlib_color: str = Field(
        default="b", description="Color of the line in the plot"
    )


# global variables
MODEL = "openai/gpt-4"
image: Image = None
image_parts: list[ImagePart] = None
counter = 0


# draws the image by iterating over the list of image parts and connecting the coordinates
async def draw_image(agent, input, output):
    global image_parts
    global image
    global counter
    counter += 1

    image_parts = output["list_of_all_image_parts"]

    plt.figure(figsize=(10, 10))

    for image_part in image_parts:
        coordinates = np.array(
            image_part.list_of_coordinates
        )  # Convert list to numpy array
        if len(coordinates) > 1:
            plt.plot(
                coordinates[:, 0],
                coordinates[:, 1],
                marker="x",
                linestyle="-",
                markersize=5,
                color=image_part.matplotlib_color,
            )

    plt.axis("equal")  # Keep aspect ratio
    plt.grid(True)

    save_path = f"plot_{counter}.png"
    plt.savefig(save_path, dpi=300)

    image = Image.from_file(save_path)

    plt.show()


# if there is a previous image, load it and give it to the agent
async def load_prev_image(agent: FlockAgent, inputs):
    global image
    global image_parts
    if image is not None:
        agent.description = "Draws an image by connecting the coordinates of the image parts. Improves the image by adding new parts to the previous image and/or changing them."
        agent.input = "subject_to_draw: str, prev_image: dspy.Image | result of rendered image parts, prev_image_parts: list[ImagePart] | previously generated image parts"
        inputs["prev_image"] = image
        inputs["prev_image_parts"] = image_parts


# Generate the plot

flock = Flock(local_debug=True)

config = FlockAgentConfig(agent_type_override="ChainOfThought")

agent = FlockAgent(
    name="the_painter",
    input="subject_to_draw: str",
    description="Draws an image by connecting the coordinates of the image parts. 0/0 is bottom left corner - 10/10 is top right corner",
    output="list_of_all_image_parts: list[ImagePart] | list of all image parts to draw by connecting the coordinates",
    config=config,
    terminate_callback=draw_image,
    initialize_callback=load_prev_image,
    output_config=FlockAgentOutputConfig(
        render_table=True, theme=OutputTheme.abernathy
    ),
)

agent.hand_off = agent

result = flock.run(start_agent=agent, agents=[agent])
```

### examples\cookbook\defining-complex-pydantic-types.py

- **Lines**: 585
- **Last modified**: 2025-05-22 18:59:04

```py
"""
Defining Complex Pydantic Types in Flock

Purpose: Demonstrate how to create and use complex nested Pydantic models with Flock.

Use Case: Space Exploration Game 🚀 - Generate detailed space missions with nested structures.

Highlights:
- Define multiple nested Pydantic models with complex relationships
- Use Field validation constraints to set up possible values
- Demonstrate enum types, lists of models, and dictionaries
- Show how Flock handles complex nested structures

This example is to evaluate the max possible complexity for a Flock model.
This currently will sometimes not work with gpt-4o! ('sometimes' like 1/10 times)

A run will take ca. 50s to complete.
"""

import enum
import os
from datetime import datetime
from typing import Any, Dict, List, Literal, Optional

from flock.core import Flock, FlockFactory
from flock.core.flock_registry import flock_type
from pydantic import BaseModel, Field, field_validator, model_validator
from rich.console import Console
from rich.panel import Panel
from rich.table import Table

# --- Configuration ---
console = Console()
MODEL = os.getenv("DEFAULT_MODEL", "openai/gpt-4o")
console.print(f"[grey50]Using model: {MODEL}[/grey50]")


# --------------------------------
# Define the Pydantic data models
# --------------------------------


# Enum for planet types
class PlanetType(str, enum.Enum):
    TERRESTRIAL = "terrestrial"
    GAS_GIANT = "gas_giant"
    ICE_GIANT = "ice_giant"
    DWARF = "dwarf"
    SUPER_EARTH = "super_earth"
    HOT_JUPITER = "hot_jupiter"


# Enum for resource rarity
class ResourceRarity(str, enum.Enum):
    COMMON = "common"
    UNCOMMON = "uncommon"
    RARE = "rare"
    EXOTIC = "exotic"
    LEGENDARY = "legendary"


@flock_type
class Coordinates(BaseModel):
    """3D coordinates in space."""

    x: float = Field(
        ...,
        description="X coordinate in light years from Sol (between -1000 and 1000)",
        ge=-1000,
        le=1000,
    )
    y: float = Field(
        ...,
        description="Y coordinate in light years from Sol (between -1000 and 1000)",
        ge=-1000,
        le=1000,
    )
    z: float = Field(
        ...,
        description="Z coordinate in light years from Sol (between -1000 and 1000)",
        ge=-1000,
        le=1000,
    )

    def distance_from_sol(self) -> float:
        """Calculate distance from Sol (0,0,0)."""
        return (self.x**2 + self.y**2 + self.z**2) ** 0.5


@flock_type
class Resource(BaseModel):
    """A resource that can be harvested from a celestial body."""

    name: str = Field(
        ...,
        description="Name of the resource (must be capitalized)",
        pattern=r"^[A-Z].*",
    )
    rarity: ResourceRarity = Field(
        ..., description="Rarity classification of the resource"
    )
    value_per_unit: int = Field(
        ...,
        description="Value per unit in credits. High-value resources (>1000) should have hazard level of at least 3",
        gt=0,
    )
    hazard_level: int = Field(
        ...,
        description="Hazard level from 0-10. High hazard resources (>8) should have value of at least 500 credits",
        ge=0,
        le=10,
    )

    @field_validator("name")
    @classmethod
    def name_must_be_scientific(cls, v: str) -> str:
        """Ensure resource names follow scientific naming convention."""
        if not v[0].isupper():
            raise ValueError("Resource names must be capitalized")
        return v

    @model_validator(mode="after")
    def check_value_hazard_correlation(self) -> "Resource":
        """Ensure high-value resources tend to be more hazardous."""
        value = self.value_per_unit
        hazard = self.hazard_level

        # Very valuable resources should have some hazard
        if value > 1000 and hazard < 3:
            raise ValueError(
                "High-value resources must have a hazard level of at least 3"
            )

        # Very hazardous resources should have some value
        if hazard > 8 and value < 500:
            raise ValueError(
                "Highly hazardous resources must have a value of at least 500 credits"
            )

        return self


@flock_type
class Atmosphere(BaseModel):
    """Atmospheric composition and conditions."""

    breathable: bool = Field(
        ..., description="Whether the atmosphere is breathable by humans"
    )
    composition: Dict[str, float] = Field(
        ...,
        description="Chemical composition as element:percentage. Percentages should sum to approximately 100%",
    )
    pressure: float = Field(
        ..., description="Atmospheric pressure in Earth atmospheres", ge=0
    )
    temperature_range: tuple[float, float] = Field(
        ...,
        description="Temperature range in Celsius. Minimum temperature must be less than maximum temperature",
    )

    @field_validator("composition")
    @classmethod
    def composition_must_sum_to_100(cls, v: Dict[str, float]) -> Dict[str, float]:
        """Ensure composition percentages sum to approximately 100%."""
        total = sum(v.values())
        if not 99.5 <= total <= 100.5:
            raise ValueError(
                f"Composition percentages must sum to approximately 100% (got {total}%)"
            )
        return v

    @field_validator("temperature_range")
    @classmethod
    def validate_temperature_range(cls, v: tuple[float, float]) -> tuple[float, float]:
        """Ensure min temperature is less than max temperature."""
        if v[0] >= v[1]:
            raise ValueError(
                f"Minimum temperature ({v[0]}) must be less than maximum temperature ({v[1]})"
            )
        return v


@flock_type
class CelestialBody(BaseModel):
    """Base class for all celestial bodies."""

    name: str = Field(
        ...,
        description="Name of the celestial body (must be capitalized)",
        pattern=r"^[A-Z].*",
    )
    mass: float = Field(..., description="Mass in Earth masses", gt=0)
    radius: float = Field(..., description="Radius in Earth radii", gt=0)
    coordinates: Coordinates = Field(..., description="Location in space")
    resources: List[Resource] = Field(
        default_factory=list, description="Available resources"
    )

    @field_validator("name")
    @classmethod
    def name_must_be_proper(cls, v: str) -> str:
        """Ensure celestial body names are properly capitalized."""
        if not v[0].isupper():
            raise ValueError("Celestial body names must be capitalized")
        return v


@flock_type
class Planet(CelestialBody):
    """A planet that can be explored."""

    type: PlanetType = Field(..., description="Type of planet")
    habitable_zone: bool = Field(
        ...,
        description="Whether the planet is in the habitable zone. Note: Gas giants and ice giants cannot be in habitable zone",
    )
    atmosphere: Optional[Atmosphere] = Field(
        None,
        description="Atmospheric conditions if present. Note: Gas giants and ice giants must have an atmosphere",
    )
    moons: int = Field(0, description="Number of moons", ge=0)
    rings: bool = Field(False, description="Whether the planet has rings")
    colonized: bool = Field(False, description="Whether the planet has been colonized")

    @model_validator(mode="after")
    def validate_planet_properties(self) -> "Planet":
        """Validate planet properties based on type."""
        planet_type = self.type
        atmosphere = self.atmosphere

        # Gas giants and ice giants must have an atmosphere
        if (
            planet_type in [PlanetType.GAS_GIANT, PlanetType.ICE_GIANT]
            and atmosphere is None
        ):
            raise ValueError(f"{planet_type} planets must have an atmosphere")

        # Gas giants and ice giants cannot be habitable
        if (
            planet_type
            in [
                PlanetType.GAS_GIANT,
                PlanetType.ICE_GIANT,
            ]
            and self.habitable_zone
        ):
            raise ValueError(f"{planet_type} planets cannot be in the habitable zone")

        return self


@flock_type
class CrewMember(BaseModel):
    """A crew member for a space mission."""

    name: str = Field(
        ...,
        description="Full name of the crew member (must include first and last name and nothing else so no titles or nicknames)",
        pattern=r"^[A-Za-z]+ [A-Za-z]+",
    )
    specialization: Literal[
        "pilot", "engineer", "scientist", "doctor", "security", "diplomat"
    ] = Field(..., description="Primary role")
    experience_years: int = Field(..., description="Years of experience", ge=0)
    special_skills: List[str] = Field(
        default_factory=list, description="Special skills and abilities"
    )

    @field_validator("name")
    @classmethod
    def name_must_have_space(cls, v: str) -> str:
        """Ensure name has at least first and last name."""
        if " " not in v:
            raise ValueError("Name must include both first and last name")
        return v


@flock_type
class Spacecraft(BaseModel):
    """A spacecraft for interplanetary missions."""

    name: str = Field(
        ...,
        description="Name of the spacecraft (must be capitalized)",
        pattern=r"^[A-Z].*",
    )
    class_type: str = Field(..., description="Class/model of the spacecraft")
    max_crew: int = Field(..., description="Maximum crew capacity", gt=0)
    range_light_years: float = Field(
        ..., description="Maximum range in light years", gt=0
    )
    fuel_capacity: float = Field(..., description="Fuel capacity in tons", gt=0)
    cargo_capacity: float = Field(..., description="Cargo capacity in tons", ge=0)
    special_equipment: List[str] = Field(
        default_factory=list, description="Special equipment installed"
    )

    @field_validator("name")
    @classmethod
    def validate_spacecraft_name(cls, v: str) -> str:
        """Ensure spacecraft name is properly formatted."""
        if not v[0].isupper():
            raise ValueError("Spacecraft names must be capitalized")
        return v


@flock_type
class MissionObjective(BaseModel):
    """An objective for a space mission."""

    title: str = Field(..., description="Title of the objective")
    description: str = Field(..., description="Detailed description")
    priority: Literal["low", "medium", "high", "critical"] = Field(
        ...,
        description="Priority level. Critical priority missions must have rewards of at least 10,000 credits",
    )
    estimated_duration_days: float = Field(
        ..., description="Estimated time to complete in days", gt=0
    )
    reward_credits: int = Field(
        ...,
        description="Reward for completion in credits. Critical missions require at least 10,000 credits",
        ge=0,
    )

    @model_validator(mode="after")
    def validate_priority_reward(self) -> "MissionObjective":
        """Ensure critical missions have appropriate rewards."""
        if self.priority == "critical" and self.reward_credits < 10000:
            raise ValueError(
                "Critical missions must have rewards of at least 10,000 credits"
            )
        return self


@flock_type
class SpaceMission(BaseModel):
    """A complete space exploration mission."""

    mission_id: str = Field(..., description="Unique mission identifier")
    title: str = Field(..., description="Mission title")
    spacecraft: Spacecraft = Field(
        ..., description="Spacecraft assigned to the mission"
    )
    crew: List[CrewMember] = Field(
        ...,
        description="Crew members assigned to the mission. Must not exceed spacecraft capacity. Must include at least one pilot and one engineer",
    )
    destination: Planet = Field(..., description="Primary destination")
    secondary_destinations: List[CelestialBody] = Field(
        default_factory=list, description="Secondary destinations"
    )
    objectives: List[MissionObjective] = Field(..., description="Mission objectives")
    launch_date: datetime = Field(..., description="Planned launch date")
    estimated_return_date: datetime = Field(
        ..., description="Estimated return date. Must be after launch date"
    )
    mission_brief: str = Field(..., description="Brief mission summary")
    risk_assessment: str = Field(..., description="Risk assessment summary")

    @field_validator("crew")
    @classmethod
    def validate_crew_size(cls, v: List[CrewMember], info: Any) -> List[CrewMember]:
        """Ensure crew size doesn't exceed spacecraft capacity."""
        if info.data.get("spacecraft") and len(v) > info.data["spacecraft"].max_crew:
            raise ValueError(
                f"Crew size ({len(v)}) exceeds spacecraft capacity ({info.data['spacecraft'].max_crew})"
            )
        return v

    @field_validator("estimated_return_date")
    @classmethod
    def return_after_launch(cls, v: datetime, info: Any) -> datetime:
        """Ensure return date is after launch date."""
        if info.data.get("launch_date") and v <= info.data["launch_date"]:
            raise ValueError("Estimated return date must be after launch date")
        return v

    @model_validator(mode="after")
    def validate_mission_requirements(self) -> "SpaceMission":
        """Ensure mission has necessary specialists."""
        specializations = [member.specialization for member in self.crew]

        # Every mission needs a pilot
        if "pilot" not in specializations:
            raise ValueError("Mission requires at least one pilot")

        # Every mission needs an engineer
        if "engineer" not in specializations:
            raise ValueError("Mission requires at least one engineer")

        return self


# --------------------------------
# Create a new Flock instance
# --------------------------------
flock = Flock(name="space_mission_generator", model=MODEL, show_flock_banner=False)

# --------------------------------
# Define the Agent using the Pydantic type
# --------------------------------
mission_agent = FlockFactory.create_default_agent(
    name="mission_agent",
    description="Generates detailed space exploration mission profiles.",
    input="mission_parameters: dict | Parameters for mission generation including mission type and risk level and crew size.",
    output="mission: SpaceMission | A complete space mission profile with all required details.",
    temperature=0.7,
    use_cache=False,
    max_tokens=16384,  # 16k tokens is the max for gpt-4o
)
flock.add_agent(mission_agent)


# --------------------------------
# Run the Flock
# --------------------------------
def run_example():
    console.print(
        Panel.fit(
            "[bold cyan]Space Exploration Mission Generator[/bold cyan]",
            subtitle="Complex Pydantic Models Example",
        )
    )

    # Example mission parameters
    mission_params = {
        "mission_type": "exploration",
        "risk_level": "medium",
        "crew_size": 4,
        "mission_duration_days": 120,
        "special_requirements": ["resource survey", "first contact protocol"],
    }

    console.print("[yellow]Generating space mission profile...[/yellow]")

    try:
        result = flock.run(
            start_agent="mission_agent",
            input={"mission_parameters": mission_params},
        )

        # Display the mission details
        if hasattr(result, "mission") and isinstance(result.mission, SpaceMission):
            mission = result.mission

            # Create a rich table for mission details
            mission_table = Table(
                title=f"Mission: {mission.title} ({mission.mission_id})"
            )
            mission_table.add_column("Property", style="cyan")
            mission_table.add_column("Value", style="green")

            mission_table.add_row(
                "Spacecraft",
                f"{mission.spacecraft.name} ({mission.spacecraft.class_type})",
            )
            mission_table.add_row(
                "Destination",
                f"{mission.destination.name} ({mission.destination.type.value})",
            )
            mission_table.add_row(
                "Launch Date", mission.launch_date.strftime("%Y-%m-%d")
            )
            mission_table.add_row(
                "Return Date", mission.estimated_return_date.strftime("%Y-%m-%d")
            )
            mission_table.add_row("Mission Brief", mission.mission_brief)
            mission_table.add_row("Risk Assessment", mission.risk_assessment)

            console.print(mission_table)

            # Display crew details
            crew_table = Table(title="Crew Manifest")
            crew_table.add_column("Name", style="cyan")
            crew_table.add_column("Role", style="green")
            crew_table.add_column("Experience", style="yellow")
            crew_table.add_column("Special Skills", style="magenta")

            for crew_member in mission.crew:
                crew_table.add_row(
                    crew_member.name,
                    crew_member.specialization,
                    f"{crew_member.experience_years} years",
                    ", ".join(crew_member.special_skills)
                    if crew_member.special_skills
                    else "None",
                )

            console.print(crew_table)

            # Display mission objectives
            objectives_table = Table(title="Mission Objectives")
            objectives_table.add_column("Title", style="cyan")
            objectives_table.add_column("Priority", style="green")
            objectives_table.add_column("Duration", style="yellow")
            objectives_table.add_column("Reward", style="magenta")

            for objective in mission.objectives:
                objectives_table.add_row(
                    objective.title,
                    objective.priority,
                    f"{objective.estimated_duration_days} days",
                    f"{objective.reward_credits:,} credits",
                )

            console.print(objectives_table)

            # Display destination details
            dest = mission.destination
            console.print(
                Panel.fit(
                    f"[bold]Primary Destination: {dest.name}[/bold]\n"
                    f"Type: {dest.type.value}\n"
                    f"Mass: {dest.mass} Earth masses\n"
                    f"Radius: {dest.radius} Earth radii\n"
                    f"Distance from Sol: {dest.coordinates.distance_from_sol():.2f} light years\n"
                    f"Habitable: {'Yes' if dest.habitable_zone else 'No'}\n"
                    f"Colonized: {'Yes' if dest.colonized else 'No'}\n"
                    f"Moons: {dest.moons}\n"
                    f"Rings: {'Yes' if dest.rings else 'No'}\n"
                    f"Resources: {', '.join(r.name for r in dest.resources) if dest.resources else 'None detected'}"
                )
            )

            # Display spacecraft details
            spacecraft = mission.spacecraft
            console.print(
                Panel.fit(
                    f"[bold]Spacecraft: {spacecraft.name}[/bold]\n"
                    f"Class: {spacecraft.class_type}\n"
                    f"Max Crew: {spacecraft.max_crew}\n"
                    f"Range: {spacecraft.range_light_years} light years\n"
                    f"Fuel Capacity: {spacecraft.fuel_capacity} tons\n"
                    f"Cargo Capacity: {spacecraft.cargo_capacity} tons\n"
                    f"Special Equipment: {', '.join(spacecraft.special_equipment) if spacecraft.special_equipment else 'None'}"
                )
            )

            console.print(f"[grey50](Object Type: {type(mission)})[/grey50]")

        else:
            console.print(
                "[bold red]Agent did not return the expected 'mission' field or it wasn't a SpaceMission object.[/bold red]"
            )
            console.print("[bold red]Raw result:[/bold red]")
            console.print(result)

    except Exception as e:
        console.print(f"[bold red]Error: {e}[/bold red]")
        console.print(
            "[bold red]Please ensure your API key is set and the model is accessible.[/bold red]"
        )


# --------------------------------
# Run the example if this script is executed directly
# --------------------------------
if __name__ == "__main__":
    run_example()


# --- YOUR TURN! ---
# 1. Extend the `SpaceMission` model:
#    - Add a new nested model, e.g., `@flock_type class MissionLog(BaseModel): entry_date: datetime; author: str; content: str`
#    - Add a field to SpaceMission: `logs: List[MissionLog] = Field(default_factory=list)`
#
# 2. Add more complex constraints through Field descriptions:
#    - Add a description to crew field to ensure each specialization appears at most twice
#    - Add a description to estimated_return_date to ensure it matches expected mission duration
#
# 3. Create a new mission type:
#    - Define a new `@flock_type class RescueMission(SpaceMission)` that inherits from SpaceMission
#    - Add fields specific to rescue missions like `rescue_target: str` and `emergency_level: int`
#    - Add a description to ensure rescue missions have a doctor in the crew
#
# 4. Experiment with Union types:
#    - Create a `@flock_type class AsteroidBelt(CelestialBody)` with specific asteroid properties
#    - Change `secondary_destinations: List[CelestialBody]` to `secondary_destinations: List[Union[Planet, AsteroidBelt]]`
#    Does Flock handle generating the correct type based on context?
#
# 5. Try to break the model:
#    - Add as much complexity as possible to the model
#    - Reduce the max_tokens to 4096 and see if it makes the model fail more often
```

### examples\cookbook\long_form_web_search.py

- **Lines**: 66
- **Last modified**: 2025-05-22 18:59:08

```py
"""
Title: Building huge documents without a hand off

In this example, we'll outline a thorough overview of a topic and then draft the content for each section.

We do this without using an explicit handoff between the outline and draft agents, but by using flock itself to manage the flow.

This way you can build create workflows that need a transformation of data from one agent to another without the need for a handoff.

This example implements https://dspy.ai/#__tabbed_2_6 to also highlight the ability to build dspy pipelines with flock.
"""

import asyncio

from flock.core import Flock, FlockFactory
from flock.core.tools import basic_tools


async def main():
    flock = Flock(local_debug=True)

    outline_agent = FlockFactory.create_default_agent(
        name="outline_agent",
        description="Outline a thorough overview of a topic.",
        input="topic",
        output="title,sections: list[str],section_subheadings: dict[str, list[str]]|mapping from section headings to subheadings",
        tools=[basic_tools.web_search_tavily, basic_tools.get_web_content_as_markdown],
    )

    draft_agent = FlockFactory.create_default_agent(
        name="draft_agent",
        input="flock.topic,flock.section_heading,flock.section_subheadings: list[str]",
        output="content|markdown-formatted section",
        tools=[basic_tools.web_search_tavily, basic_tools.get_web_content_as_markdown],
    )

    flock.add_agent(outline_agent)
    flock.add_agent(draft_agent)

    # Instead defining handoff between agents, we just use flock to run the outline agent
    result = await flock.run_async(
        start_agent=outline_agent,
    )

    sections = []
    # We then do our processing (in this case formatting the content) and run the draft agent for each section
    for heading, subheadings in result.section_subheadings.items():
        section, subheadings = (
            f"## {heading}",
            [f"### {subheading}" for subheading in subheadings],
        )
        result_content = await flock.run_async(
            input={
                "topic": result.topic,
                "section_heading": section,
                "section_subheadings": subheadings,
            },
            start_agent=draft_agent,
        )
        sections.append(result_content.content)
        with open("output.md", "w") as f:
            f.write("\n\n".join(sections))


if __name__ == "__main__":
    asyncio.run(main())
```

### examples\cookbook\overriding_agent_hooks.py

- **Lines**: 84
- **Last modified**: 2025-05-22 18:59:04

```py
"""
Tutorial Example: Creating a agent chain, without agents

also called state machine

In this example, we create a simple two-agent chain:
  1. DoublerAgent: Receives a number ("value") and outputs its double ("doubled").
  2. AdderAgent: Takes the "doubled" value from the previous agent and adds 5 to produce "result".

The special thing about this example is that we don't use any external tools or LLMs.
Instead, we create a simple chain of agents that pass data between each other.

"""

import asyncio

from flock.core.flock import Flock
from flock.core.flock_agent import FlockAgent


# Define a simple agent that doubles the input value.
class DoublerAgent(FlockAgent):
    async def evaluate(self, inputs: dict[str, any]) -> dict[str, any]:
        # Retrieve the input value (defaulting to 0 if not provided)
        value = inputs.get("value", 0)
        # Return the doubled value
        return {"doubled": value * 2}


# Define another agent that adds 5 to the doubled value.
class AdderAgent(FlockAgent):
    async def evaluate(self, inputs: dict[str, any]) -> dict[str, any]:
        # Retrieve the "doubled" value (defaulting to 0 if not provided)
        doubled = inputs.get("doubled", 0)
        # Return the final result after adding 5
        return {"result": doubled + 5}


async def main():
    # --------------------------------
    # Create the flock
    # --------------------------------
    # Create a Flock instance in local debug mode (no Temporal needed for this simple demo)
    flock = Flock()

    # --------------------------------
    # Create the agents
    # --------------------------------
    # Define the doubler agent:
    doubler = DoublerAgent(
        name="doubler_agent",
        input="value: int | The number to double",
        output="doubled: int | The doubled value",
    )

    # Define the adder agent:
    adder = AdderAgent(
        name="adder_agent",
        input="doubled: int | The doubled value from the previous agent",
        output="result: int | The final result after adding 5",
    )

    # --------------------------------
    # Set up hand-off
    # --------------------------------
    # Link the agents so that the output of doubler is passed to adder automatically.
    doubler.hand_off = adder

    # Register both agents with the flock.
    flock.add_agent(doubler)
    flock.add_agent(adder)

    # --------------------------------
    # Run the agent chain
    # --------------------------------
    # Start the workflow with the doubler agent and provide the initial input.
    result = await flock.run_async(start_agent=doubler, input={"value": 10})

    # Print the final result. Expected output: result should be (10*2)+5 = 25.
    print(result)


if __name__ == "__main__":
    asyncio.run(main())
```

### examples\cookbook\simple-error-handling.py

- **Lines**: 5
- **Last modified**: 2025-05-22 18:59:04

```py
"""
Placeholder for simple-error-handling.py
"""

print("Hello from simple-error-handling.py!")
```

### examples\cookbook\using-specific-llm-provider.py

- **Lines**: 51
- **Last modified**: 2025-05-22 18:59:04

```py
from flock.core import Flock, FlockFactory

# --------------------------------
# Let's talk about models!
# --------------------------------
# Flock uses litellm to talk to LLMs
# Please consult the litellm documentation:
# https://docs.litellm.ai/docs/providers


# In this example we want to use a model via ollama to use a local model
# For this you need to install ollama:
# https://ollama.com/download
# And download a model - We will use a model of the qwen3 family which are SOTA in their respective parameter count:
# choose one which fits your hardware
# https://ollama.com/library/qwen3
# ollama run qwen3

# All you have to do is make sure the model id matches what litellm expects
# https://docs.litellm.ai/docs/providers/ollama
# Tip: the ollama_chat/model format seems to work better than ollama/model
MODEL = "ollama_chat/qwen3"

# Let's try a more difficult to set up provider
# We will use azure open ai
# https://docs.litellm.ai/docs/providers/azure

# The documentation tells you which env variables you need to set
# AZURE_API_KEY
# AZURE_API_BASE
# AZURE_API_VERSION

# Then set the model again with the correct id
# MODEL = "azure/<your_deploymet>"


flock = Flock(name="hello_flock", description="This is your first flock!", model=MODEL)


presentation_agent = FlockFactory.create_default_agent(
    name="my_presentation_agent",
    input="topic",
    output="fun_title, fun_slide_headers, fun_slide_summaries",
)
flock.add_agent(presentation_agent)


flock.run(
    start_agent=presentation_agent,
    input={"topic": "A presentation about robot kittens"},
)
```

### examples\cookbook\working-with-images.py

- **Lines**: 72
- **Last modified**: 2025-05-22 18:59:04

```py
# 02-core-concepts/02-pydantic-types.py
"""
Purpose: Demonstrate using images in Flock
"""

import os

import dspy
from flock.cli.utils import print_warning
from flock.core import Flock, FlockFactory
from flock.core.flock_registry import (
    flock_type,  # Decorator for registering custom types
)
from pydantic import BaseModel, Field  # Import Pydantic components

MODEL = os.getenv("DEFAULT_MODEL", "openai/gpt-4o")


@flock_type
class MyPetsInputModel(BaseModel):
    name: str = Field(..., description="The name of the pet.")
    image: dspy.Image = Field(..., description="A image of the pet.")


@flock_type
class MyPetsOutputModel(BaseModel):
    cuteness_factor: float = Field(..., description="A number between 0 and 100.")
    fur_color: str = Field(..., description="The color of the fur.")
    animal_type: str = Field(..., description="The type of animal.")
    cuteness_reasoning: str = Field(
        ..., description="A reasoning for the cuteness factor."
    )
    image_description: str = Field(..., description="A description of the image.")


# --------------------------------
# Create a new Flock instance
# --------------------------------
flock = Flock(name="image_example", model=MODEL)


pet_agent = FlockFactory.create_default_agent(
    name="pet_agent",
    input="pet_query: MyPetsInputModel",
    output="answer: MyPetsOutputModel",
)
flock.add_agent(pet_agent)


# --------------------------------
# Run the Flock
# --------------------------------

try:
    my_input = MyPetsInputModel(
        name="luna", image=dspy.Image.from_file(".assets/luna.jpg")
    )

    # also try lucy!

    # my_input = MyPetsInputModel(
    #     name="lucy", image=dspy.Image.from_file(".assets/lucy.jpg")
    # )
    result = flock.run(
        start_agent="pet_agent",
        input={"pet_query": my_input},
    )


except Exception as e:
    print_warning(f"Agent run failed: {e}")
    print_warning("Please ensure your API key is set and the model is accessible.")
```

### scripts\code_collector.py

- **Lines**: 704
- **Last modified**: 2025-04-16 00:11:15

```py
#!/usr/bin/env python3
"""Code Repository Analyzer

This script generates a comprehensive Markdown document of a code repository,
optimized for LLM consumption and understanding.
"""

import ast
import datetime
import glob
import os
from typing import Any


def find_files(folder: str, extension: str) -> list[str]:
    """Find all files with the specified extension in the folder and subfolders."""
    pattern = os.path.join(folder, f"**/*{extension}")
    return sorted(glob.glob(pattern, recursive=True))


def get_file_metadata(file_path: str) -> dict[str, Any]:
    """Extract metadata from a file."""
    metadata = {
        "path": file_path,
        "size_bytes": 0,
        "line_count": 0,
        "last_modified": "Unknown",
        "created": "Unknown",
    }

    try:
        stats = os.stat(file_path)
        metadata["size_bytes"] = stats.st_size
        metadata["last_modified"] = datetime.datetime.fromtimestamp(
            stats.st_mtime
        ).strftime("%Y-%m-%d %H:%M:%S")
        metadata["created"] = datetime.datetime.fromtimestamp(
            stats.st_ctime
        ).strftime("%Y-%m-%d %H:%M:%S")

        with open(file_path, encoding="utf-8") as f:
            content = f.read()
            metadata["line_count"] = len(content.splitlines())
    except Exception as e:
        print(f"Warning: Could not get complete metadata for {file_path}: {e}")

    return metadata


def extract_python_components(file_path: str) -> dict[str, Any]:
    """Extract classes, functions, and imports from Python files."""
    components = {
        "classes": [],
        "functions": [],
        "imports": [],
        "docstring": None,
    }

    try:
        with open(file_path, encoding="utf-8") as f:
            content = f.read()

        tree = ast.parse(content)

        # Extract module docstring
        if ast.get_docstring(tree):
            components["docstring"] = ast.get_docstring(tree)

        # Helper to determine if a function is top-level or a method
        def is_top_level_function(node):
            # Check if the function is defined inside a class
            for parent_node in ast.walk(tree):
                if isinstance(parent_node, ast.ClassDef):
                    for child in parent_node.body:
                        if (
                            child is node
                        ):  # This is a direct reference comparison
                            return False
            return True

        # Extract top-level classes and functions
        for node in ast.iter_child_nodes(tree):
            if isinstance(node, ast.ClassDef):
                class_info = {
                    "name": node.name,
                    "docstring": ast.get_docstring(node),
                    "methods": [
                        m.name
                        for m in node.body
                        if isinstance(m, ast.FunctionDef)
                    ],
                }
                components["classes"].append(class_info)
            elif isinstance(node, ast.FunctionDef):
                func_info = {
                    "name": node.name,
                    "docstring": ast.get_docstring(node),
                    "args": [
                        arg.arg for arg in node.args.args if hasattr(arg, "arg")
                    ],
                }
                components["functions"].append(func_info)

        # Extract all imports
        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for name in node.names:
                    components["imports"].append(name.name)
            elif isinstance(node, ast.ImportFrom):
                module = node.module or ""
                for name in node.names:
                    components["imports"].append(f"{module}.{name.name}")

    except Exception as e:
        print(f"Warning: Could not parse Python components in {file_path}: {e}")

    return components


def analyze_code_dependencies(files: list[str]) -> dict[str, set[str]]:
    """Analyze dependencies between Python files based on imports."""
    dependencies = {file: set() for file in files}

    # Create a mapping from module names to file paths
    module_map = {}
    package_map = {}

    for file_path in files:
        if not file_path.endswith(".py"):
            continue

        # Handle both absolute and relative paths
        abs_path = os.path.abspath(file_path)

        # Map file paths to potential module names
        # First, try to extract package structure
        parts = []
        current = abs_path

        # Build the full module path
        while current:
            parent = os.path.dirname(current)

            # If we've reached the top or left the project directory, stop
            if parent == current or not os.path.exists(
                os.path.join(parent, "__init__.py")
            ):
                break

            parts.insert(0, os.path.basename(current))
            current = parent

        # Use the file name (without .py) as the last part
        base_name = os.path.basename(file_path)
        if base_name.endswith(".py"):
            if base_name != "__init__.py":
                module_name = os.path.splitext(base_name)[0]
                parts.append(module_name)

            full_module_name = ".".join(parts) if parts else None
            if full_module_name:
                module_map[full_module_name] = file_path

            # Also map short names for common imports
            if module_name := os.path.splitext(base_name)[0]:
                # Don't overwrite existing mappings with short names
                if module_name not in module_map:
                    module_map[module_name] = file_path

            # Map package names
            for i in range(len(parts)):
                package_name = ".".join(parts[: i + 1])
                package_map[package_name] = os.path.dirname(file_path)

    # Now analyze imports in each file
    for file_path in files:
        if not file_path.endswith(".py"):
            continue

        try:
            with open(file_path, encoding="utf-8") as f:
                code = f.read()

            tree = ast.parse(code)

            for node in ast.walk(tree):
                # Handle direct imports: import x, import x.y
                if isinstance(node, ast.Import):
                    for name in node.names:
                        # Check for the full module path
                        module_path = name.name
                        if module_path in module_map:
                            dependencies[file_path].add(module_map[module_path])

                        # Check for package imports
                        parts = module_path.split(".")
                        for i in range(len(parts), 0, -1):
                            prefix = ".".join(parts[:i])
                            if prefix in module_map:
                                dependencies[file_path].add(module_map[prefix])
                                break

                # Handle from imports: from x import y, from x.y import z
                elif isinstance(node, ast.ImportFrom):
                    if node.module:  # from x import y
                        # See if the module is in our map
                        if node.module in module_map:
                            dependencies[file_path].add(module_map[node.module])

                        # Check for package imports
                        for prefix in get_module_prefixes(node.module):
                            if prefix in module_map:
                                dependencies[file_path].add(module_map[prefix])

                    # Handle relative imports: from . import x, from .. import y
                    if node.level > 0:  # Relative import
                        # Get the directory of the current file
                        dir_path = os.path.dirname(file_path)

                        # Go up levels according to the number of dots
                        for _ in range(node.level - 1):
                            dir_path = os.path.dirname(dir_path)

                        # Try to find matching imports
                        for name in node.names:
                            if node.module:
                                target_module = f"{node.module}.{name.name}"
                            else:
                                target_module = name.name

                            # Check for the module within the relative directory
                            rel_path = os.path.join(
                                dir_path, target_module.replace(".", os.sep)
                            )

                            # Try with .py extension first
                            py_path = f"{rel_path}.py"
                            if os.path.exists(py_path) and py_path in files:
                                dependencies[file_path].add(py_path)

                            # Try as directory with __init__.py
                            init_path = os.path.join(rel_path, "__init__.py")
                            if os.path.exists(init_path) and init_path in files:
                                dependencies[file_path].add(init_path)

        except Exception as e:
            print(f"Warning: Could not analyze imports in {file_path}: {e}")

    return dependencies


def get_module_prefixes(module_name: str) -> list[str]:
    """Generate all possible module prefixes for a given module name.
    For example, 'a.b.c' would return ['a.b.c', 'a.b', 'a']
    """
    parts = module_name.split(".")
    return [".".join(parts[:i]) for i in range(len(parts), 0, -1)]


def generate_folder_tree(folder: str, included_files: list[str]) -> str:
    """Generate an ASCII folder tree representation, only showing directories and files that are included."""
    tree_output = []
    included_paths = set(included_files)

    # Get all directories containing included files
    included_dirs = set()
    for file_path in included_paths:
        dir_path = os.path.dirname(file_path)
        while dir_path and dir_path != folder:
            included_dirs.add(dir_path)
            dir_path = os.path.dirname(dir_path)

    def _generate_tree(dir_path: str, prefix: str = "", is_last: bool = True):
        # Get the directory name
        dir_name = os.path.basename(dir_path) or dir_path

        # Add the current directory to the output
        tree_output.append(
            f"{prefix}{'└── ' if is_last else '├── '}{dir_name}/"
        )

        # Update prefix for children
        new_prefix = f"{prefix}{'    ' if is_last else '│   '}"

        # Get relevant entries in the directory
        try:
            entries = os.listdir(dir_path)
            relevant_dirs = []
            relevant_files = []

            for entry in entries:
                entry_path = os.path.join(dir_path, entry)
                if os.path.isdir(entry_path):
                    # Include directory if it or any of its subdirectories contain included files
                    if (
                        any(
                            f.startswith(entry_path + os.sep)
                            for f in included_paths
                        )
                        or entry_path in included_dirs
                    ):
                        relevant_dirs.append(entry)
                elif entry_path in included_paths:
                    # Only include the specific files we're interested in
                    relevant_files.append(entry)

            # Sort entries for consistent output
            relevant_dirs.sort()
            relevant_files.sort()

            # Process relevant subdirectories
            for i, entry in enumerate(relevant_dirs):
                entry_path = os.path.join(dir_path, entry)
                is_last_dir = i == len(relevant_dirs) - 1
                is_last_item = is_last_dir and len(relevant_files) == 0
                _generate_tree(entry_path, new_prefix, is_last_item)

            # Process relevant files
            for i, entry in enumerate(relevant_files):
                is_last_file = i == len(relevant_files) - 1
                tree_output.append(
                    f"{new_prefix}{'└── ' if is_last_file else '├── '}{entry}"
                )

        except (PermissionError, FileNotFoundError):
            return

    # Start the recursion from the root folder
    _generate_tree(folder)

    return "\n".join(tree_output)


def get_common_patterns(files: list[str]) -> dict[str, list[str]]:
    """Identify common design patterns in the codebase."""
    patterns = {
        "singleton": [],
        "factory": [],
        "observer": [],
        "decorator": [],
        "mvc_components": {"models": [], "views": [], "controllers": []},
    }

    for file_path in files:
        if not file_path.endswith(".py"):
            continue

        try:
            with open(file_path, encoding="utf-8") as f:
                content = f.read().lower()

            # Check for singleton pattern
            if "instance = none" in content and "__new__" in content:
                patterns["singleton"].append(file_path)

            # Check for factory pattern
            if "factory" in os.path.basename(file_path).lower() or (
                "def create" in content and "return" in content
            ):
                patterns["factory"].append(file_path)

            # Check for observer pattern
            if ("observer" in content or "listener" in content) and (
                "notify" in content or "update" in content
            ):
                patterns["observer"].append(file_path)

            # Check for decorator pattern
            if "decorator" in os.path.basename(file_path).lower() or (
                "def wrapper" in content and "return wrapper" in content
            ):
                patterns["decorator"].append(file_path)

            # Check for MVC components
            if "model" in os.path.basename(file_path).lower():
                patterns["mvc_components"]["models"].append(file_path)
            elif "view" in os.path.basename(file_path).lower():
                patterns["mvc_components"]["views"].append(file_path)
            elif (
                "controller" in os.path.basename(file_path).lower()
                or "handler" in os.path.basename(file_path).lower()
            ):
                patterns["mvc_components"]["controllers"].append(file_path)

        except Exception:
            continue

    # Remove empty categories
    for key in list(patterns.keys()):
        if isinstance(patterns[key], list) and not patterns[key]:
            patterns.pop(key)
        elif isinstance(patterns[key], dict):
            empty = True
            for subkey in patterns[key]:
                if patterns[key][subkey]:
                    empty = False
                    break
            if empty:
                patterns.pop(key)

    return patterns


def find_key_files(
    files: list[str], dependencies: dict[str, set[str]]
) -> list[str]:
    """Identify key files based on dependencies and naming conventions."""
    # Initialize scores for each file
    scores = {file: 0 for file in files}

    # Track how many files depend on each file (dependents)
    dependent_count = {file: 0 for file in files}
    for file, deps in dependencies.items():
        for dep in deps:
            if dep in dependent_count:
                dependent_count[dep] += 1

    # Score by number of files that depend on this file
    for file, count in dependent_count.items():
        scores[file] += count * 2

    # Score by file naming heuristics
    for file in files:
        base_name = os.path.basename(file).lower()

        # Core files
        if any(
            core_name in base_name
            for core_name in ["main", "app", "core", "init", "cli"]
        ):
            scores[file] += 5

        # Configuration and settings
        if any(
            config_name in base_name
            for config_name in ["config", "settings", "constants"]
        ):
            scores[file] += 3

        # Base classes and abstract components
        if any(
            base_name in base_name
            for base_name in ["base", "abstract", "interface", "factory"]
        ):
            scores[file] += 2

        # Utilities and helpers
        if any(
            util_name in base_name
            for util_name in ["util", "helper", "common", "tools"]
        ):
            scores[file] += 1

        # Score directories by importance
        dir_path = os.path.dirname(file)
        if "core" in dir_path.lower():
            scores[file] += 2
        if "main" in dir_path.lower():
            scores[file] += 1

        # Score by file size and complexity
        try:
            metadata = get_file_metadata(file)
            line_count = metadata["line_count"]
            scores[file] += min(line_count / 50, 3)  # Cap at 3 points for size

            # Additional points for very significant files
            if line_count > 200:
                scores[file] += 1
        except Exception:
            pass

        # Score by extension - Python files are often more important
        if file.endswith(".py"):
            scores[file] += 1

        # Examples and documentation are important but not as much as core files
        if "example" in file.lower() or "demo" in file.lower():
            scores[file] += 0.5

    # Sort by score in descending order
    key_files = sorted(files, key=lambda f: scores[f], reverse=True)

    # Debugging info
    print(f"Top 5 key files with scores:")
    for file in key_files[:5]:
        print(f"  {file}: {scores[file]:.1f} points")

    # Return top 25% of files or at least 5 files (if available)
    num_key_files = max(min(len(files) // 4, 20), min(5, len(files)))
    return key_files[:num_key_files]


def generate_markdown_string(
    files: list[str],
    extension: str,
    folder: str,
    key_files: list[str],
    dependencies: dict[str, set[str]],
    patterns: dict[str, list[str]],
) -> str:
    """Generate a comprehensive markdown document about the codebase as a string."""
    md_content = []

    # Write header
    md_content.append(f"# Code Repository Analysis\n")
    md_content.append(f"Generated on {datetime.datetime.now()}\n\n")

    # Write repository summary
    md_content.append("## Repository Summary\n\n")
    md_content.append(f"- **Extension analyzed**: `{extension}`\n")
    md_content.append(f"- **Number of files**: {len(files)}\n")
    md_content.append(f"- **Root folder**: `{folder}`\n")

    total_lines = sum(get_file_metadata(f)["line_count"] for f in files)
    md_content.append(f"- **Total lines of code**: {total_lines}\n\n")

    # Generate and write folder tree
    md_content.append("## Project Structure\n\n")
    md_content.append("```\n")
    md_content.append(generate_folder_tree(folder, files))
    md_content.append("\n```\n\n")

    # Write key files section
    md_content.append("## Key Files\n\n")
    md_content.append(
        "These files appear to be central to the codebase based on dependencies and naming conventions:\n\n"
    )

    for file in key_files:
        rel_path = os.path.relpath(file, folder)
        md_content.append(f"### {rel_path}\n\n")

        metadata = get_file_metadata(file)
        md_content.append(f"- **Lines**: {metadata['line_count']}\n")
        md_content.append(f"- **Last modified**: {metadata['last_modified']}\n")

        # Add dependency info
        dependent_files = [
            os.path.relpath(f, folder)
            for f in dependencies
            if file in dependencies[f]
        ]
        if dependent_files:
            md_content.append(f"- **Used by**: {len(dependent_files)} files\n")

        # For Python files, add component analysis
        if file.endswith(".py"):
            components = extract_python_components(file)

            if components["docstring"]:
                md_content.append(
                    f"\n**Description**: {components['docstring'].strip()}\n"
                )

            if components["classes"]:
                md_content.append("\n**Classes**:\n")
                for cls in components["classes"]:
                    md_content.append(
                        f"- `{cls['name']}`: {len(cls['methods'])} methods\n"
                    )

            if components["functions"]:
                md_content.append("\n**Functions**:\n")
                for func in components["functions"]:
                    md_content.append(
                        f"- `{func['name']}({', '.join(func['args'])})`\n"
                    )

        md_content.append("\n**Content**:\n")
        md_content.append(f"```{extension.lstrip('.')}\n")

        # Read and write file content
        try:
            with open(file, encoding="utf-8") as code_file:
                content = code_file.read()
                md_content.append(content)
                if not content.endswith("\n"):
                    md_content.append("\n")
        except Exception as e:
            md_content.append(f"Error reading file: {e!s}\n")

        md_content.append("```\n\n")

    # Write design patterns section if any were detected
    if patterns:
        md_content.append("## Design Patterns\n\n")
        md_content.append(
            "The following design patterns appear to be used in this codebase:\n\n"
        )

        for pattern, files_list in patterns.items():
            if isinstance(files_list, list) and files_list:
                md_content.append(f"### {pattern.title()} Pattern\n\n")
                for f in files_list:
                    md_content.append(f"- `{os.path.relpath(f, folder)}`\n")
                md_content.append("\n")
            elif isinstance(files_list, dict):
                md_content.append(
                    f"### {pattern.replace('_', ' ').title()}\n\n"
                )
                for subpattern, subfiles in files_list.items():
                    if subfiles:
                        md_content.append(f"**{subpattern.title()}**:\n")
                        for f in subfiles:
                            md_content.append(
                                f"- `{os.path.relpath(f, folder)}`\n"
                            )
                        md_content.append("\n")

    # Write all other files section
    md_content.append("## All Files\n\n")

    for file in files:
        if file in key_files:
            continue  # Skip files already detailed in key files section

        rel_path = os.path.relpath(file, folder)
        md_content.append(f"### {rel_path}\n\n")

        metadata = get_file_metadata(file)
        md_content.append(f"- **Lines**: {metadata['line_count']}\n")
        md_content.append(
            f"- **Last modified**: {metadata['last_modified']}\n\n"
        )

        md_content.append("```" + extension.lstrip(".") + "\n")

        # Read and write file content
        try:
            with open(file, encoding="utf-8") as code_file:
                content = code_file.read()
                md_content.append(content)
                if not content.endswith("\n"):
                    md_content.append("\n")
        except Exception as e:
            md_content.append(f"Error reading file: {e!s}\n")

        md_content.append("```\n\n")

    return "".join(md_content)


def collect_code(extension: str = ".py", folder: str = ".") -> str:
    """Main function to analyze code repository and generate markdown string.

    Args:
        extension: File extension to analyze
        folder: Root folder to analyze

    Returns:
        A string containing the markdown analysis
    """
    print(f"Analyzing {extension} files from {folder}...")

    # Find all matching files
    files = find_files(folder, extension)
    print(f"Found {len(files)} files")

    # Get dependencies
    dependencies = analyze_code_dependencies(files)

    # Find key files
    key_files = find_key_files(files, dependencies)

    # Get design patterns
    patterns = get_common_patterns(files)

    # Generate markdown content
    markdown_content = generate_markdown_string(
        files, extension, folder, key_files, dependencies, patterns
    )
    print(f"Repository analysis complete.")

    return markdown_content


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(
        description="Analyze code repository for LLM consumption"
    )
    parser.add_argument(
        "-c", "--extension", default=".py", help="File extension to analyze"
    )
    parser.add_argument("-f", "--folder", default=".", help="Folder to analyze")
    parser.add_argument(
        "-o",
        "--output",
        default="repository_analysis.md",
        help="Output markdown file",
    )

    args = parser.parse_args()

    # Generate the markdown content
    markdown_content = collect_code(args.extension, args.folder)

    # Write the content to the output file
    with open(args.output, "w", encoding="utf-8") as output_file:
        output_file.write(markdown_content)

    print(f"Output written to '{args.output}'")
```

### scripts\create_docs.py

- **Lines**: 135
- **Last modified**: 2025-04-16 00:11:15

```py
from pathlib import Path
from typing import Any

import yaml  # <--- Keep standard import

# --- Configuration ---
MKDOCS_YAML_PATH = "mkdocs.yml"
DOCS_DIR = Path("docs")
PLACEHOLDER_CONTENT = """---
hide: # Optional: Hide table of contents on simple pages
  - toc
---

# {title}

*Documentation in progress...*
"""
# --- End Configuration ---


def create_markdown_file(file_path: Path, title: str) -> None:
    """Creates a markdown file with a title and placeholder content."""
    file_path.parent.mkdir(parents=True, exist_ok=True)
    content = PLACEHOLDER_CONTENT.format(title=title)
    if not file_path.exists():
        try:
            file_path.write_text(content, encoding="utf-8")
            print(f"Created: {file_path}")
        except OSError as e:
            print(f"Error creating file {file_path}: {e}")
    else:
        print(f"Skipped (already exists): {file_path}")


def process_nav_item(item: Any, base_dir: Path) -> None:
    """Recursively processes items from the MkDocs nav structure."""
    if isinstance(item, str):
        if ":" in item:
            title, path_str = item.split(":", 1)
            title = title.strip()
            path_str = path_str.strip()
        else:
            path_str = item.strip()
            title = (
                Path(path_str).stem.replace("-", " ").replace("_", " ").title()
            )
            if Path(path_str).name == "index.md":
                parent_name = Path(path_str).parent.name
                if parent_name and parent_name != ".":
                    title = (
                        parent_name.replace("-", " ").replace("_", " ").title()
                        + " Overview"
                    )
                else:
                    title = "Home"

        if path_str.endswith(".md"):
            file_path = base_dir / path_str
            create_markdown_file(file_path, title)

    elif isinstance(item, dict):
        for key, value in item.items():
            if isinstance(value, str) and value.endswith(".md"):
                file_path = base_dir / value
                create_markdown_file(file_path, key)
            elif isinstance(value, list):
                for sub_item in value:
                    process_nav_item(sub_item, base_dir)
            else:
                print(
                    f"Warning: Unexpected value type in nav for key '{key}': {type(value)}"
                )

    elif isinstance(item, list):
        for sub_item in item:
            process_nav_item(sub_item, base_dir)

    else:
        print(f"Warning: Unexpected item type in nav: {type(item)}")


def main():
    """Loads mkdocs.yml, parses the nav structure, and creates placeholder files."""
    DOCS_DIR.mkdir(exist_ok=True)

    try:
        with open(MKDOCS_YAML_PATH, encoding="utf-8") as f:
            # --- CHANGE HERE: Use yaml.Loader instead of yaml.safe_load ---
            config = yaml.load(f, Loader=yaml.Loader)
            # --- END CHANGE ---
    except FileNotFoundError:
        print(f"Error: {MKDOCS_YAML_PATH} not found.")
        return
    except yaml.YAMLError as e:
        print(f"Error parsing {MKDOCS_YAML_PATH}: {e}")
        return

    nav_structure = config.get("nav")
    if not nav_structure or not isinstance(nav_structure, list):
        print("Error: 'nav' structure not found or invalid in mkdocs.yml.")
        return

    print(f"Processing navigation structure from {MKDOCS_YAML_PATH}...")
    for item in nav_structure:
        process_nav_item(item, DOCS_DIR)

    # --- Create essential assets directory and placeholder files ---
    assets_dir = DOCS_DIR / "assets" / "images"
    assets_dir.mkdir(parents=True, exist_ok=True)
    placeholder_logo = assets_dir / "flock_logo_small.png"
    placeholder_favicon = assets_dir / "favicon.png"
    if not placeholder_logo.exists():
        placeholder_logo.touch()
        print(f"Created placeholder: {placeholder_logo}")
    if not placeholder_favicon.exists():
        placeholder_favicon.touch()
        print(f"Created placeholder: {placeholder_favicon}")

    css_dir = DOCS_DIR / "stylesheets"
    css_dir.mkdir(exist_ok=True)
    placeholder_css = css_dir / "extra.css"
    if not placeholder_css.exists():
        placeholder_css.touch()
        print(f"Created placeholder: {placeholder_css}")
    # --- End assets creation ---

    print("\nBoilerplate documentation structure created.")
    print(
        f"You can now start filling in the content in the '{DOCS_DIR}' directory."
    )
    print("Run 'mkdocs serve' to view the documentation locally.")


if __name__ == "__main__":
    main()
```

### scripts\ensure_uv.py

- **Lines**: 20
- **Last modified**: 2025-05-06 12:21:20

```py
"""Ensure uv is installed."""

import shutil
import subprocess
import sys


def has_uv():
    """Check if uv is installed."""
    return shutil.which("uv") is not None


def install_uv():
    """Install uv via pip."""
    print("Installing uv via pip...")
    subprocess.check_call([sys.executable, "-m", "pip", "install", "uv"])


if not has_uv():
    install_uv()
```

### src\flock\adapter\__init__.py

- **Lines**: 14
- **Last modified**: 2025-05-21 19:51:15

```py
from __future__ import annotations

"""Adapter package for pluggable vector-store back-ends.

Importing the package will NOT import heavy third-party clients by default –
individual adapters are only loaded when referenced explicitly.
"""

from .vector_base import VectorAdapter, VectorHit

__all__ = [
    "VectorAdapter",
    "VectorHit",
]
```

### src\flock\adapter\azure_adapter.py

- **Lines**: 68
- **Last modified**: 2025-05-21 19:51:15

```py
from __future__ import annotations

from typing import Any

from .vector_base import VectorAdapter, VectorHit


class AzureSearchAdapter(VectorAdapter):
    """Adapter for Azure Cognitive Search vector capabilities."""

    def __init__(
        self,
        *,
        endpoint: str,
        key: str,
        index_name: str,
        embedding_field: str = "embedding",
    ) -> None:
        super().__init__()
        try:
            from azure.core.credentials import AzureKeyCredential
            from azure.search.documents import SearchClient
        except ImportError as e:
            raise RuntimeError("azure-search-documents package is required for AzureSearchAdapter") from e

        self._client = SearchClient(
            endpoint=endpoint,
            index_name=index_name,
            credential=AzureKeyCredential(key),
        )
        self._embedding_field = embedding_field

    # -----------------------------
    def add(
        self,
        *,
        id: str,
        content: str,
        embedding: list[float],
        metadata: dict[str, Any] | None = None,
    ) -> None:
        document = {
            "id": id,
            "content": content,
            self._embedding_field: embedding,
            **(metadata or {}),
        }
        # Upload is sync but returns iterator; consume to check errors
        list(self._client.upload_documents(documents=[document]))

    def query(self, *, embedding: list[float], k: int) -> list[VectorHit]:
        results = self._client.search(
            search_text=None,
            vector=embedding,
            k=k,
            vector_fields=self._embedding_field,
        )
        hits: list[VectorHit] = []
        for doc in results:
            hits.append(
                VectorHit(
                    id=doc["id"],
                    content=doc.get("content"),
                    metadata={k: v for k, v in doc.items() if k not in ("id", "content", self._embedding_field, "@search.score")},
                    score=doc["@search.score"],
                )
            )
        return hits
```

### src\flock\adapter\chroma_adapter.py

- **Lines**: 73
- **Last modified**: 2025-05-21 19:51:15

```py
from __future__ import annotations

from pathlib import Path
from typing import Any

from .vector_base import VectorAdapter, VectorHit


class ChromaAdapter(VectorAdapter):
    """Adapter for Chroma vector DB (local or HTTP)."""

    def __init__(
        self,
        *,
        collection: str = "flock_memories",
        host: str | None = None,
        port: int = 8000,
        path: str | None = "./vector_store",
    ) -> None:
        super().__init__()
        try:
            import chromadb
            from chromadb.config import Settings
        except ImportError as e:
            raise RuntimeError("chromadb is required for ChromaAdapter") from e

        if host:
            client = chromadb.HttpClient(host=host, port=port)
        else:
            p = Path(path or "./vector_store")
            p.mkdir(parents=True, exist_ok=True)
            client = chromadb.PersistentClient(settings=Settings(path=str(p)))

        self._collection = client.get_or_create_collection(collection)

    # -------------------------------
    # VectorAdapter implementation
    # -------------------------------
    def add(
        self,
        *,
        id: str,
        content: str,
        embedding: list[float],
        metadata: dict[str, Any] | None = None,
    ) -> None:
        self._collection.add(
            ids=[id],
            documents=[content],
            embeddings=[embedding],
            metadatas=[metadata or {}],
        )

    def query(self, *, embedding: list[float], k: int) -> list[VectorHit]:
        res = self._collection.query(
            query_embeddings=[embedding],
            n_results=k,
            include=["documents", "metadatas", "distances", "ids"],
        )
        hits: list[VectorHit] = []
        if res and res["ids"]:
            for idx in range(len(res["ids"][0])):
                dist = res["distances"][0][idx]
                score = 1 - dist  # Convert L2 → similarity
                hits.append(
                    VectorHit(
                        id=res["ids"][0][idx],
                        content=res["documents"][0][idx],
                        metadata=res["metadatas"][0][idx],
                        score=score,
                    )
                )
        return hits
```

### src\flock\adapter\faiss_adapter.py

- **Lines**: 97
- **Last modified**: 2025-05-21 19:51:15

```py
from __future__ import annotations

from pathlib import Path
from typing import Any

import numpy as np

from .vector_base import VectorAdapter, VectorHit


class FAISSAdapter(VectorAdapter):
    """Simple on-disk FAISS vector store.

    Index is stored in `index_path` (flat L2).  Metadata & content are kept in a
    parallel JSONL file for quick prototyping; not optimised for massive scale.
    """

    def __init__(self, *, index_path: str = "./faiss.index") -> None:
        super().__init__()
        try:
            import faiss  # type: ignore
        except ImportError as e:
            raise RuntimeError("faiss library is required for FAISSAdapter") from e

        self._faiss = __import__("faiss")  # lazy alias
        self._index_path = Path(index_path)
        self._meta_path = self._index_path.with_suffix(".meta.jsonl")
        self._metadata: dict[int, dict[str, Any]] = {}

        if self._index_path.exists():
            self._index = self._faiss.read_index(str(self._index_path))
            # Load metadata
            if self._meta_path.exists():
                import json

                with open(self._meta_path) as f:
                    for line_no, line in enumerate(f):
                        self._metadata[line_no] = json.loads(line)
        else:
            self._index = None  # created on first add

    # -----------------------------
    def _ensure_index(self, dim: int):
        if self._index is None:
            self._index = self._faiss.IndexFlatL2(dim)

    def add(
        self,
        *,
        id: str,
        content: str,
        embedding: list[float],
        metadata: dict[str, Any] | None = None,
    ) -> None:
        import json

        vec = np.array([embedding], dtype="float32")
        self._ensure_index(vec.shape[1])
        self._index.add(vec)
        # Row id is current size - 1
        row_id = self._index.ntotal - 1
        self._metadata[row_id] = {
            "id": id,
            "content": content,
            "metadata": metadata or {},
        }
        # Append metadata to file for persistence
        self._meta_path.parent.mkdir(parents=True, exist_ok=True)
        with open(self._meta_path, "a") as f:
            f.write(json.dumps(self._metadata[row_id]) + "\n")
        # Persist index lazily every 100 inserts
        if row_id % 100 == 0:
            self._faiss.write_index(self._index, str(self._index_path))

    def query(self, *, embedding: list[float], k: int) -> list[VectorHit]:
        if self._index is None or self._index.ntotal == 0:
            return []
        vec = np.array([embedding], dtype="float32")
        distances, indices = self._index.search(vec, k)
        hits: list[VectorHit] = []
        for dist, idx in zip(distances[0], indices[0]):
            if idx == -1:
                continue
            meta = self._metadata.get(idx, {})
            hits.append(
                VectorHit(
                    id=meta.get("id", str(idx)),
                    content=meta.get("content"),
                    metadata=meta.get("metadata", {}),
                    score=1 - float(dist),  # approximate similarity
                )
            )
        return hits

    def close(self) -> None:
        if self._index is not None:
            self._faiss.write_index(self._index, str(self._index_path))
```

### src\flock\adapter\pinecone_adapter.py

- **Lines**: 51
- **Last modified**: 2025-05-21 19:51:15

```py
from __future__ import annotations

from typing import Any

from .vector_base import VectorAdapter, VectorHit


class PineconeAdapter(VectorAdapter):
    """Adapter for Pinecone vector DB."""

    def __init__(
        self,
        *,
        api_key: str,
        environment: str,
        index: str,
    ) -> None:
        super().__init__()
        try:
            import pinecone
        except ImportError as e:
            raise RuntimeError("pinecone-client is required for PineconeAdapter") from e

        pinecone.init(api_key=api_key, environment=environment)
        self._index = pinecone.Index(index)

    # -------------------------------
    def add(
        self,
        *,
        id: str,
        content: str,
        embedding: list[float],
        metadata: dict[str, Any] | None = None,
    ) -> None:
        meta = {"content": content, **(metadata or {})}
        self._index.upsert(vectors=[(id, embedding, meta)])

    def query(self, *, embedding: list[float], k: int) -> list[VectorHit]:
        res = self._index.query(vector=embedding, top_k=k, include_values=False, include_metadata=True)
        hits: list[VectorHit] = []
        for match in res.matches or []:
            hits.append(
                VectorHit(
                    id=match.id,
                    content=match.metadata.get("content") if match.metadata else None,
                    metadata={k: v for k, v in (match.metadata or {}).items() if k != "content"},
                    score=match.score,
                )
            )
        return hits
```

### src\flock\cli\config.py

- **Lines**: 8
- **Last modified**: 2025-04-16 00:11:15

```py
def init_config_file():
    """Initialize the config file."""
    pass


def load_config_file():
    """Load the config file."""
    pass
```

### src\flock\cli\constants.py

- **Lines**: 36
- **Last modified**: 2025-05-21 19:51:15

```py
"""Constants for the CLI module."""

CLI_CFG_FILE = ".env"
CLI_DEFAULT_ENV_VARS = {
    "FLICK_API_KEY": "flock-api-key",
    "FLICK_API_URL": "https://api.flock.com",
    "FLICK_API_VERSION": "v1",
    "FLICK_API_KEY": "flock-api-key",
    "FLICK_API_URL": "https://api.flock.com",
    "FLICK_API_VERSION": "v1",
}
CLI_DEFAULT_FOLDER = ".flock"

CLI_CREATE_AGENT = "Create an agent"
CLI_CREATE_FLOCK = "Create a new Flock"
CLI_LOAD_AGENT = "Load an agent"
CLI_LOAD_FLOCK = "Load a *.flock file"
CLI_THEME_BUILDER = "Theme builder"
CLI_LOAD_EXAMPLE = "Load a example"
CLI_SETTINGS = "Settings"
CLI_NOTES = "'Magpie' release notes"
CLI_START_WEB_SERVER = "Start web server"
CLI_REGISTRY_MANAGEMENT = "Registry management"
CLI_EXIT = "Exit"
CLI_CHOICES = [
    CLI_CREATE_AGENT,
    CLI_CREATE_FLOCK,
    CLI_LOAD_AGENT,
    CLI_LOAD_FLOCK,
    CLI_LOAD_EXAMPLE,
    CLI_THEME_BUILDER,
    CLI_REGISTRY_MANAGEMENT,
    CLI_SETTINGS,
    CLI_START_WEB_SERVER,
    CLI_EXIT,
]
```

### src\flock\cli\create_agent.py

- **Lines**: 1
- **Last modified**: 2025-04-16 00:11:15

```py
# TODO
```

### src\flock\cli\create_flock.py

- **Lines**: 280
- **Last modified**: 2025-05-22 21:27:37

```py
"""Create a new Flock through a guided wizard.

This module provides a wizard-like interface for creating new Flock instances,
with options for basic configuration and initial agent creation.
"""

from datetime import datetime
from pathlib import Path

import questionary
from rich.console import Console
from rich.panel import Panel

from flock.cli.constants import CLI_DEFAULT_FOLDER
from flock.cli.loaded_flock_cli import start_loaded_flock_cli
from flock.core.flock import Flock
from flock.core.flock_factory import FlockFactory
from flock.core.logging.logging import get_logger
from flock.core.util.cli_helper import init_console

# Create console instance
console = Console()
logger = get_logger("cli.create_flock")


def create_flock():
    """Create a new Flock through a guided wizard."""
    init_console()
    console.print(Panel("[bold green]Create New Flock[/]"), justify="center")
    console.line()

    # Step 1: Basic Flock Configuration
    console.print("[bold]Step 1: Basic Flock Configuration[/]")
    console.line()

    flock_name = questionary.text(
        "Enter a name for this Flock:",
        default="",
    ).ask()

    # Get description
    description = questionary.text(
        "Enter a description for this Flock (optional):",
        default="",
    ).ask()

    # Default model selection
    default_models = [
        "openai/gpt-4o",
        "openai/gpt-3.5-turbo",
        "anthropic/claude-3-opus-20240229",
        "anthropic/claude-3-sonnet-20240229",
        "gemini/gemini-1.5-pro",
        "Other (specify)",
    ]

    model_choice = questionary.select(
        "Select a default model:",
        choices=default_models,
    ).ask()

    if model_choice == "Other (specify)":
        model = questionary.text(
            "Enter the model identifier:",
            default="openai/gpt-4o",
        ).ask()
    else:
        model = model_choice

    # Execution options
    # enable_temporal = questionary.confirm(
    #     "Enable Temporal for distributed execution?",
    #     default=False,
    # ).ask()
    enable_temporal = False

    # Create the Flock instance
    flock = Flock(
        name=flock_name,
        model=model,
        description=description,
        enable_temporal=enable_temporal,
    )

    console.print("\n[green]✓[/] Flock created successfully!")
    console.line()

    # Step 2: Create Initial Agent (optional)
    create_agent = questionary.confirm(
        "Would you like to create an initial agent?",
        default=True,
    ).ask()

    if create_agent:
        _create_initial_agent(flock)

    # Step 3: Save Options
    console.print("\n[bold]Step 3: Save Options[/]")
    console.line()

    save_choice = questionary.select(
        "What would you like to do with this Flock?",
        choices=[
            "Save to YAML file",
            "Continue in CLI without saving",
            "Execute immediately",
            "Cancel and discard",
        ],
    ).ask()

    if save_choice == "Save to YAML file":
        _save_flock_to_yaml(flock)

        # Ask if user wants to continue working with this Flock
        continue_with_flock = questionary.confirm(
            "Would you like to continue working with this Flock in the CLI?",
            default=True,
        ).ask()

        if continue_with_flock:
            start_loaded_flock_cli(flock, server_name="New Flock")

    elif save_choice == "Continue in CLI without saving":
        start_loaded_flock_cli(flock, server_name="New Flock")

    elif save_choice == "Execute immediately":
        from flock.cli.execute_flock import execute_flock

        try:
            execute_flock(flock)
        except ImportError:
            console.print(
                "[yellow]Execute functionality not yet implemented.[/]"
            )
            input("\nPress Enter to continue...")
            start_loaded_flock_cli(flock, server_name="New Flock")


def _create_initial_agent(flock):
    """Create an initial agent for the Flock.

    Args:
        flock: The Flock instance to add the agent to
    """
    console.print("\n[bold]Step 2: Create Initial Agent[/]")
    console.line()

    # Get agent name
    name = questionary.text(
        "Enter a name for the agent:",
        default="my_agent",
    ).ask()

    # Get agent description
    description = questionary.text(
        "Enter a description for the agent (optional):",
        default="",
    ).ask()

    # Get input specification
    input_spec = questionary.text(
        "Enter input specification (e.g., 'query: str | The search query'):",
        default="query",
    ).ask()

    # Get output specification
    output_spec = questionary.text(
        "Enter output specification (e.g., 'result: str | The generated result'):",
        default="result",
    ).ask()

    # Additional options
    use_cache = questionary.confirm(
        "Enable caching for this agent?",
        default=True,
    ).ask()

    enable_rich_tables = questionary.confirm(
        "Enable rich table output for this agent?",
        default=True,
    ).ask()

    # Create the agent
    agent = FlockFactory.create_default_agent(
        name=name,
        description=description,
        input=input_spec,
        output=output_spec,
        use_cache=use_cache,
        enable_rich_tables=enable_rich_tables,
    )

    # Add the agent to the Flock
    flock.add_agent(agent)
    console.print(f"\n[green]✓[/] Agent '{name}' created and added to Flock!")


def _save_flock_to_yaml(flock):
    """Save the Flock to a YAML file.

    Args:
        flock: The Flock instance to save
    """
    # Get file path
    # default = flock.name + current date in 04_04_2025 format
    default_name = f"{flock.name}_{datetime.now().strftime('%m_%d_%Y')}"
    file_path = questionary.text(
        "Enter file path to save Flock:",
        default=default_name,
    ).ask()

    # Ensure the file has the correct extension
    if not file_path.endswith((".yaml", ".yml")):
        file_path += ".flock.yaml"

    file_path = CLI_DEFAULT_FOLDER + "/" + file_path

    # Create directory if it doesn't exist
    save_path = Path(file_path)
    save_path.parent.mkdir(parents=True, exist_ok=True)

    # Ask about path_type
    path_type_choice = questionary.select(
        "How should file paths be formatted?",
        choices=[
            "absolute (full paths, best for local use)",
            "relative (relative paths, better for sharing)",
        ],
        default="absolute (full paths, best for local use)",
    ).ask()

    # Extract just the first word
    path_type = path_type_choice.split()[0]

    console.print(
        f"[bold]Path type selected: [green]{path_type}[/green][/bold]"
    )

    try:
        # Check if the flock has tools to provide a helpful message
        has_tools = False
        for agent in flock.agents.values():
            if agent.tools and len(agent.tools) > 0:
                has_tools = True
                break

        # Save the Flock to YAML with proper tool serialization
        logger.info(f"Saving Flock to {file_path}")
        flock.to_yaml_file(file_path, path_type=path_type)
        console.print(
            f"\n[green]✓[/] Flock saved to {file_path} with {path_type} paths"
        )

        # Provide helpful information about tool serialization
        if has_tools:
            console.print("\n[bold blue]Tools Information:[/]")
            console.print(
                "This Flock contains tools that have been serialized as callable references."
            )
            console.print(
                "When loading this Flock on another system, ensure that:"
            )
            console.print(
                "  - The tools/functions are registered in the Flock registry"
            )
            console.print(
                "  - The containing modules are available in the Python path"
            )
    except Exception as e:
        logger.error(f"Error saving Flock: {e}", exc_info=True)
        console.print(f"\n[bold red]Error saving Flock:[/] {e!s}")

        # Provide guidance on potential issues with tool serialization
        if "callable" in str(e).lower() or "registry" in str(e).lower():
            console.print(
                "\n[yellow]This error might be related to tool serialization.[/]"
            )
            console.print(
                "[yellow]Check if all tools are properly registered in the Flock registry.[/]"
            )
```

### src\flock\cli\execute_flock.py

- **Lines**: 620
- **Last modified**: 2025-05-22 21:27:37

```py
"""Execute a Flock instance with a selected agent.

This module provides functionality to execute a Flock instance with
a selected agent and input configuration, including batch processing.
"""

import json
import os

import questionary
from rich.console import Console
from rich.panel import Panel

from flock.core.flock import Flock
from flock.core.logging.logging import configure_logging
from flock.core.util.cli_helper import init_console

# Create console instance
console = Console()

# Try importing pandas for DataFrame support
try:
    import pandas as pd

    PANDAS_AVAILABLE = True
except ImportError:
    pd = None
    PANDAS_AVAILABLE = False


def execute_flock(flock: Flock):
    """Execute a Flock instance.

    Args:
        flock: The Flock instance to execute
    """
    if not flock:
        console.print("[bold red]Error: No Flock instance provided.[/]")
        return

    agent_names = list(flock._agents.keys())

    if not agent_names:
        console.print("[yellow]No agents in this Flock to execute.[/]")
        return

    init_console()
    console.print(Panel("[bold green]Execute Flock[/]"), justify="center")

    # Step 1: Select start agent
    console.print("\n[bold]Step 1: Select Start Agent[/]")

    start_agent_name = questionary.select(
        "Select an agent to start with:",
        choices=agent_names,
    ).ask()

    if not start_agent_name:
        return

    start_agent = flock._agents[start_agent_name]

    # Step 2: Configure input
    console.print("\n[bold]Step 2: Configure Input[/]")

    # Parse input schema
    input_fields = _parse_input_schema(start_agent.input)

    # If we couldn't parse any fields, ask for generic input
    if not input_fields:
        raw_input = questionary.text(
            "Enter input (JSON format):",
            default="{}",
        ).ask()

        try:
            input_data = json.loads(raw_input)
        except json.JSONDecodeError:
            console.print("[bold red]Error: Invalid JSON input.[/]")
            return
    else:
        # Otherwise, ask for each field
        input_data = {}

        for field, info in input_fields.items():
            field_type = info.get("type", "str")
            description = info.get("description", "")
            prompt = f"Enter value for '{field}'"

            if description:
                prompt += f" ({description})"

            prompt += ":"

            value = questionary.text(prompt).ask()

            # Convert value to appropriate type
            if field_type == "int":
                try:
                    value = int(value)
                except ValueError:
                    console.print(
                        f"[yellow]Warning: Could not convert value to int, using as string.[/]"
                    )

            input_data[field] = value

    # Step 3: Run Options
    console.print("\n[bold]Step 3: Run Options[/]")

    # Logging options
    enable_logging = questionary.confirm(
        "Enable logging?",
        default=False,
    ).ask()
    if enable_logging:
        log_level = questionary.select(
            "Minimum log level:",
            choices=["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"],
            default="ERROR",
        ).ask()
        configure_logging(flock_level=log_level, external_level=log_level)

    # Preview input
    console.print("\n[bold]Input Preview:[/]")
    console.print(json.dumps(input_data, indent=2))

    # Confirm execution
    confirm = questionary.confirm(
        "Execute Flock with this configuration?",
        default=True,
    ).ask()

    if not confirm:
        return

    # Execute the Flock
    console.print("\n[bold]Executing Flock...[/]")

    try:
        # Logging was configured earlier if enabled

        # Run the Flock
        result = flock.run(
            start_agent=start_agent_name,
            input=input_data,
        )

        # Display result
        console.print("\n[bold green]Execution Complete![/]")

        if result and enable_logging:
            console.print("\n[bold]Result:[/]")
            if isinstance(result, dict):
                # Display as formatted JSON
                console.print(json.dumps(result, indent=2))
            else:
                # Display as plain text
                console.print(str(result))

    except Exception as e:
        console.print(f"\n[bold red]Error during execution:[/] {e!s}")


def _parse_input_schema(input_schema: str) -> dict[str, dict[str, str]]:
    """Parse the input schema string into a field dictionary.

    Args:
        input_schema: The input schema string (e.g., "query: str | The search query")

    Returns:
        A dictionary mapping field names to field info (type, description)
    """
    if not input_schema:
        return {}

    fields = {}

    try:
        # Split by comma for multiple fields
        for field_def in input_schema.split(","):
            field_def = field_def.strip()

            # Check for type hint with colon
            if ":" in field_def:
                field_name, rest = field_def.split(":", 1)
                field_name = field_name.strip()
                rest = rest.strip()

                # Check for description with pipe
                if "|" in rest:
                    field_type, description = rest.split("|", 1)
                    fields[field_name] = {
                        "type": field_type.strip(),
                        "description": description.strip(),
                    }
                else:
                    fields[field_name] = {"type": rest.strip()}
            else:
                # Just a field name without type hint
                if "|" in field_def:
                    field_name, description = field_def.split("|", 1)
                    fields[field_name.strip()] = {
                        "description": description.strip()
                    }
                else:
                    fields[field_def.strip()] = {}

    except Exception as e:
        console.print(
            f"[yellow]Warning: Could not parse input schema: {e!s}[/]"
        )
        return {}

    return fields


def execute_flock_batch(flock: Flock):
    """Execute a Flock instance in batch mode.

    Args:
        flock: The Flock instance to execute
    """
    if not flock:
        console.print("[bold red]Error: No Flock instance provided.[/]")
        return

    agent_names = list(flock._agents.keys())

    if not agent_names:
        console.print("[yellow]No agents in this Flock to execute.[/]")
        return

    init_console()
    console.print(
        Panel("[bold green]Execute Flock - Batch Mode[/]"), justify="center"
    )

    # Step 1: Select start agent
    console.print("\n[bold]Step 1: Select Start Agent[/]")

    start_agent_name = questionary.select(
        "Select an agent to start with:",
        choices=agent_names,
    ).ask()

    if not start_agent_name:
        return

    start_agent = flock._agents[start_agent_name]

    # Step 2: Configure batch input source
    console.print("\n[bold]Step 2: Select Batch Input Source[/]")

    if not PANDAS_AVAILABLE:
        console.print(
            "[yellow]Warning: pandas not available. CSV input/output functionality will be limited.[/]"
        )

    input_source_choices = ["Enter batch items manually"]

    if PANDAS_AVAILABLE:
        input_source_choices.insert(0, "Load from CSV file")

    input_source = questionary.select(
        "How would you like to provide batch inputs?",
        choices=input_source_choices,
    ).ask()

    if not input_source:
        return

    batch_inputs = []
    input_mapping = {}

    if input_source == "Load from CSV file" and PANDAS_AVAILABLE:
        # Ask for CSV file path
        csv_path = questionary.path(
            "Enter path to CSV file:",
        ).ask()

        if not csv_path:
            return

        try:
            # Validate path exists
            if not os.path.exists(csv_path):
                console.print(
                    f"[bold red]Error: File '{csv_path}' does not exist.[/]"
                )
                return

            # Preview CSV
            df = pd.read_csv(csv_path)
            console.print("\n[bold]CSV Preview (first 5 rows):[/]")
            console.print(df.head().to_string())

            # Configure column mapping
            console.print("\n[bold]Configure Column Mapping:[/]")

            # Parse input schema (if available)
            input_fields = _parse_input_schema(start_agent.input)

            # If we have input fields, map CSV columns to them
            if input_fields:
                columns = df.columns.tolist()

                for field in input_fields.keys():
                    field_desc = input_fields[field].get("description", "")
                    prompt = f"Select column for '{field}'"

                    if field_desc:
                        prompt += f" ({field_desc})"

                    selected_col = questionary.select(
                        prompt,
                        choices=["(Skip this field)"] + columns,
                    ).ask()

                    if selected_col and selected_col != "(Skip this field)":
                        input_mapping[selected_col] = field
            else:
                # No schema, ask user to map columns manually
                columns = df.columns.tolist()

                for col in columns:
                    mapping = questionary.text(
                        f"Map column '{col}' to input field (leave empty to ignore):",
                    ).ask()

                    if mapping:
                        input_mapping[col] = mapping

            if not input_mapping:
                console.print("[yellow]Warning: No column mapping defined.[/]")
                if not questionary.confirm(
                    "Continue without mapping?", default=False
                ).ask():
                    return

            # Use the CSV file path directly
            batch_inputs = csv_path

        except Exception as e:
            console.print(f"[bold red]Error loading CSV: {e}[/]")
            return

    elif input_source == "Enter batch items manually":
        # Parse input schema
        input_fields = _parse_input_schema(start_agent.input)

        if not input_fields:
            console.print(
                "[yellow]No input schema available. Using JSON input.[/]"
            )

            while True:
                raw_input = questionary.text(
                    "Enter batch item as JSON (empty to finish):",
                    default="{}",
                ).ask()

                if not raw_input:
                    break

                try:
                    item_data = json.loads(raw_input)
                    batch_inputs.append(item_data)
                    console.print(f"[green]Added item {len(batch_inputs)}[/]")
                except json.JSONDecodeError:
                    console.print("[bold red]Error: Invalid JSON input.[/]")

        else:
            # We have input fields, ask for each field for each item
            item_count = 1

            while True:
                console.print(f"\n[bold]Batch Item {item_count}[/]")

                item_data = {}
                for field, info in input_fields.items():
                    field_type = info.get("type", "str")
                    description = info.get("description", "")
                    prompt = f"Enter value for '{field}'"

                    if description:
                        prompt += f" ({description})"

                    prompt += " (empty to skip):"

                    value = questionary.text(prompt).ask()

                    if not value:
                        continue

                    # Convert value to appropriate type
                    if field_type == "int":
                        try:
                            value = int(value)
                        except ValueError:
                            console.print(
                                f"[yellow]Warning: Could not convert value to int, using as string.[/]"
                            )

                    item_data[field] = value

                if item_data:
                    batch_inputs.append(item_data)
                    console.print(f"[green]Added item {len(batch_inputs)}[/]")

                if not questionary.confirm(
                    "Add another batch item?",
                    default=len(batch_inputs)
                    < 2,  # Default to yes if we have less than 2 items
                ).ask():
                    break

                item_count += 1

    if isinstance(batch_inputs, list) and not batch_inputs:
        console.print("[yellow]No batch items defined. Exiting.[/]")
        return

    # Step 3: Configure static inputs (if needed)
    static_inputs = {}

    if questionary.confirm(
        "Would you like to add static inputs (common to all batch items)?",
        default=False,
    ).ask():
        console.print("\n[bold]Configure Static Inputs[/]")

        raw_static = questionary.text(
            "Enter static inputs as JSON:",
            default="{}",
        ).ask()

        try:
            static_inputs = json.loads(raw_static)
        except json.JSONDecodeError:
            console.print(
                "[bold red]Error: Invalid JSON for static inputs. Proceeding without static inputs.[/]"
            )
            static_inputs = {}

    # Step 4: Configure batch execution options
    console.print("\n[bold]Step 4: Configure Batch Execution Options[/]")

    # Determine if we should use Temporal
    use_temporal = False
    # if questionary.confirm(
    #     f"Override Temporal setting? (Current: {flock.enable_temporal})",
    #     default=False,
    # ).ask():
    #     use_temporal = questionary.confirm(
    #         "Use Temporal for batch execution?",
    #         default=flock.enable_temporal,
    #     ).ask()

    # Configure parallelism
    parallel = True
    max_workers = 5

    if not flock.enable_temporal if use_temporal is None else not use_temporal:
        parallel = questionary.confirm(
            "Run batch items in parallel?",
            default=True,
        ).ask()

        if parallel:
            max_workers_input = questionary.text(
                "Maximum number of parallel workers:",
                default="5",
            ).ask()

            try:
                max_workers = int(max_workers_input)
            except ValueError:
                console.print(
                    "[yellow]Invalid worker count. Using default (5).[/]"
                )
                max_workers = 5

    # Configure output options
    silent_mode = questionary.confirm(
        "Use silent mode with progress bar? (Recommended for large batches)",
        default=True,
    ).ask()

    write_to_csv = None
    if (
        PANDAS_AVAILABLE
        and questionary.confirm(
            "Write results to CSV file?",
            default=True,
        ).ask()
    ):
        write_to_csv = questionary.text(
            "CSV output path:",
            default="batch_results.csv",
        ).ask()

        hide_columns = questionary.text(
            "Hide columns (comma-separated - leave blank for hiding no columns):",
            default="",
        ).ask()

        hide_columns = hide_columns.split(",") if hide_columns else []

        delimiter = questionary.text(
            "Delimiter (default is comma):",
            default=",",
        ).ask()

    # Logging options
    enable_logging = questionary.confirm(
        "Enable logging?",
        default=False,
    ).ask()
    if enable_logging:
        log_level = questionary.select(
            "Minimum log level:",
            choices=["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"],
            default="ERROR",
        ).ask()
        configure_logging(log_level)

    # Preview configuration
    console.print("\n[bold]Batch Configuration Preview:[/]")
    console.print(f"Agent: {start_agent_name}")

    if isinstance(batch_inputs, str):
        console.print(f"Input Source: CSV file ({batch_inputs})")
        console.print(f"Column Mapping: {input_mapping}")
    else:
        console.print(f"Input Source: Manual entry ({len(batch_inputs)} items)")

    if static_inputs:
        console.print(f"Static Inputs: {json.dumps(static_inputs, indent=2)}")

    # temporal_status = (
    #     "Default" if use_temporal is None else ("Yes" if use_temporal else "No")
    # )
    # console.print(f"Use Temporal: {temporal_status}")

    if not (flock.enable_temporal if use_temporal is None else use_temporal):
        console.print(f"Parallel Execution: {parallel}")
        if parallel:
            console.print(f"Max Workers: {max_workers}")

    console.print(f"Silent Mode: {silent_mode}")

    if write_to_csv:
        console.print(f"Write Results to: {write_to_csv}")

    # Confirm execution
    confirm = questionary.confirm(
        "Execute batch with this configuration?",
        default=True,
    ).ask()

    if not confirm:
        return

    # Execute the batch
    console.print("\n[bold]Executing Batch...[/]")

    try:
        # Logging was already configured above if enabled

        # Run the batch
        results = flock.run_batch(
            start_agent=start_agent_name,
            batch_inputs=batch_inputs,
            input_mapping=input_mapping or None,
            static_inputs=static_inputs or None,
            parallel=parallel,
            max_workers=max_workers,
            use_temporal=use_temporal,
            box_results=True,
            return_errors=True,
            silent_mode=silent_mode,
            write_to_csv=write_to_csv,
            hide_columns=hide_columns,
            delimiter=delimiter,
        )

        # Display results summary
        console.print("\n[bold green]Batch Execution Complete![/]")

        success_count = sum(1 for r in results if not isinstance(r, Exception))
        error_count = sum(1 for r in results if isinstance(r, Exception))

        console.print(f"Total Items: {len(results)}")
        console.print(f"Successful: {success_count}")

        if error_count > 0:
            console.print(f"[bold red]Errors: {error_count}[/]")

        # Ask if user wants to see detailed results
        if questionary.confirm(
            "View detailed results?",
            default=False,
        ).ask():
            for i, result in enumerate(results):
                console.print(f"\n[bold]Item {i + 1}:[/]")
                if isinstance(result, Exception):
                    console.print(f"[bold red]Error: {result}[/]")
                else:
                    # Display as formatted JSON
                    try:
                        console.print(json.dumps(result, indent=2))
                    except:
                        console.print(str(result))

        if write_to_csv:
            console.print(f"\n[green]Results written to: {write_to_csv}[/]")

    except Exception as e:
        console.print(f"\n[bold red]Error during batch execution:[/] {e!s}")
```

### src\flock\cli\load_agent.py

- **Lines**: 1
- **Last modified**: 2025-04-16 00:11:15

```py
# TODO
```

### src\flock\cli\load_examples.py

- **Lines**: 1
- **Last modified**: 2025-04-16 00:11:15

```py
# TODO
```

### src\flock\cli\load_flock.py

- **Lines**: 192
- **Last modified**: 2025-04-16 00:11:15

```py
"""Load a Flock from a file."""

from pathlib import Path

import questionary
from rich.console import Console
from rich.markdown import Markdown

from flock.cli.loaded_flock_cli import start_loaded_flock_cli
from flock.core.flock import Flock
from flock.core.logging.logging import get_logger

logger = get_logger("cli.load_flock")


def filter(file_path) -> bool:
    """Filter function for file selection."""
    path = Path(file_path)
    if path.is_dir():
        return True
    return path.is_file() and (
        path.suffix == ".flock"
        or path.suffix == ".yaml"
        or path.suffix == ".yml"
    )


def load_flock():
    """Load a Flock from a file."""
    console = Console()

    console.print(
        "\nPlease select a *.flock, *.yaml, or *.yml file\n", style="bold green"
    )

    result = questionary.path("", file_filter=filter).ask()

    if not result:
        return

    selected_file = Path(result)
    if selected_file.is_file():
        console.print(f"Selected file: {selected_file}", style="bold green")

        try:
            # Try loading with detailed error handling
            try:
                logger.info(f"Attempting to load Flock from: {result}")
                flock = Flock.load_from_file(result)
            except ImportError as e:
                # Handle missing module path errors
                if "No module named" in str(e):
                    console.print(
                        f"[yellow]Warning: Module import failed: {e}[/]"
                    )
                    console.print(
                        "[yellow]Trying file path fallback mechanism...[/]"
                    )
                    # Re-try loading with fallback
                    flock = Flock.load_from_file(result)
                else:
                    raise  # Re-raise if it's not a missing module error
            except KeyError as e:
                # This could be caused by missing tool references
                if "__callable_ref__" in str(e):
                    console.print(
                        f"[yellow]Warning: Tool reference error: {e}[/]"
                    )
                    console.print(
                        "[yellow]This may be due to missing tool registrations. Attempting to scan for tools...[/]"
                    )
                    # Scan for tools and retry
                    from flock.cli.registry_management import (
                        auto_registration_scanner,
                    )

                    auto_registration_scanner()
                    # Try loading again
                    flock = Flock.load_from_file(result)
                else:
                    raise  # Re-raise if it's not a tool reference error

            console.line()
            console.print(
                Markdown("# Flock loaded successfully"), style="bold green"
            )
            console.line()

            # Instead of just running the Flock, start our enhanced CLI
            start_loaded_flock_cli(
                flock, server_name=f"Flock - {selected_file.name}"
            )

        except Exception as e:
            console.print(f"Error loading Flock: {e!s}", style="bold red")
            logger.error(f"Failed to load Flock: {e}", exc_info=True)

            # Add more detailed error information for specific errors
            if "No module named" in str(e):
                console.print(
                    "\n[yellow]This error might be due to missing module paths.[/]"
                )
                console.print(
                    "[yellow]Component references may need to be updated with file paths.[/]"
                )

                # Show the option to scan the directory for components
                fix_paths = questionary.confirm(
                    "Would you like to scan directories for components to fix missing imports?",
                    default=True,
                ).ask()

                if fix_paths:
                    from flock.cli.registry_management import (
                        auto_registration_scanner,
                    )

                    auto_registration_scanner()

                    # Try loading again
                    console.print(
                        "\n[yellow]Attempting to load Flock again...[/]"
                    )
                    try:
                        flock = Flock.load_from_file(result)
                        console.line()
                        console.print(
                            Markdown(
                                "# Flock loaded successfully after component scan"
                            ),
                            style="bold green",
                        )
                        console.line()

                        start_loaded_flock_cli(
                            flock, server_name=f"Flock - {selected_file.name}"
                        )
                        return
                    except Exception as e2:
                        console.print(
                            f"Error loading Flock after scan: {e2!s}",
                            style="bold red",
                        )

            # Handle tool reference issues
            elif "__callable_ref__" in str(e) or "callable" in str(e).lower():
                console.print(
                    "\n[yellow]This error might be due to missing tool registrations.[/]"
                )

                # Show the option to scan the directory for tools
                fix_tools = questionary.confirm(
                    "Would you like to scan directories for tools to fix missing references?",
                    default=True,
                ).ask()

                if fix_tools:
                    from flock.cli.registry_management import (
                        auto_registration_scanner,
                    )

                    console.print(
                        "\n[yellow]Scanning for tools and callables...[/]"
                    )
                    auto_registration_scanner()

                    # Try loading again
                    console.print(
                        "\n[yellow]Attempting to load Flock again...[/]"
                    )
                    try:
                        flock = Flock.load_from_file(result)
                        console.line()
                        console.print(
                            Markdown(
                                "# Flock loaded successfully after tool scan"
                            ),
                            style="bold green",
                        )
                        console.line()

                        start_loaded_flock_cli(
                            flock, server_name=f"Flock - {selected_file.name}"
                        )
                        return
                    except Exception as e2:
                        console.print(
                            f"Error loading Flock after tool scan: {e2!s}",
                            style="bold red",
                        )

            input("\nPress Enter to continue...")
```

### src\flock\cli\load_release_notes.py

- **Lines**: 20
- **Last modified**: 2025-04-19 02:05:04

```py
from pathlib import Path


def load_release_notes():
    """Load release notes."""
    from rich.console import Console
    from rich.markdown import Markdown

    from flock.core.util.cli_helper import display_hummingbird, init_console

    console = Console()
    file_path = Path(__file__).parent / "assets" / "release_notes.md"

    init_console()
    console.print(Markdown("# *'Hummingbird'* Release Notes"))
    display_hummingbird()
    with open(file_path) as file:
        release_notes = file.read()

    console.print(Markdown(release_notes))
```

### src\flock\cli\manage_agents.py

- **Lines**: 459
- **Last modified**: 2025-05-21 19:51:15

```py
"""Agent management functionality for the Flock CLI.

This module provides a CLI interface for managing agents within a Flock system,
including listing, adding, editing, and removing agents.
"""

import questionary
from rich.box import Box
from rich.console import Console
from rich.panel import Panel
from rich.table import Table

from flock.core.flock import Flock
from flock.core.flock_agent import FlockAgent
from flock.core.flock_factory import FlockFactory
from flock.core.util.cli_helper import init_console

# Create console instance
console = Console()


def manage_agents(flock: Flock):
    """Agent management entry point.

    Args:
        flock: The Flock instance containing agents to manage
    """
    if not flock:
        console.print("[bold red]Error: No Flock instance provided.[/]")
        return

    while True:
        init_console()
        console.print(Panel("[bold green]Agent Manager[/]"), justify="center")

        agent_names = list(flock._agents.keys())
        console.print(f"Flock contains [bold cyan]{len(agent_names)}[/] agents")

        if agent_names:
            console.print(f"Agents: {', '.join(agent_names)}")
        else:
            console.print("[yellow]No agents in this Flock yet.[/]")

        console.line()

        # Main menu
        choice = questionary.select(
            "What would you like to do?",
            choices=[
                questionary.Separator(line=" "),
                "List All Agents",
                "Add New Agent",
                "Edit Agent",
                "Remove Agent",
                "Export Agent to YAML",
                "Import Agent from YAML",
                questionary.Separator(),
                "Back to Main Menu",
            ],
        ).ask()

        if choice == "List All Agents":
            _list_agents(flock)
        elif choice == "Add New Agent":
            _add_agent(flock)
        elif choice == "Edit Agent":
            _edit_agent(flock)
        elif choice == "Remove Agent":
            _remove_agent(flock)
        elif choice == "Export Agent to YAML":
            _export_agent(flock)
        elif choice == "Import Agent from YAML":
            _import_agent(flock)
        elif choice == "Back to Main Menu":
            break

        if choice != "Back to Main Menu":
            input("\nPress Enter to continue...")


def _list_agents(flock: Flock):
    """List all agents in the Flock.

    Args:
        flock: The Flock instance
    """
    agent_names = list(flock._agents.keys())

    if not agent_names:
        console.print("[yellow]No agents in this Flock.[/]")
        return

    # Create table for agents
    table = Table(title="Agents in Flock")
    table.add_column("Name", style="cyan")
    table.add_column("Model", style="green")
    table.add_column("Description", style="yellow")
    table.add_column("Input", style="magenta")
    table.add_column("Output", style="magenta")

    for name in agent_names:
        agent = flock._agents[name]

        # Format model nicely
        model = agent.model or flock.model or "Default"

        # Format description (truncate if too long)
        description = agent.resolved_description
        if description and len(description) > 30:
            description = description[:27] + "..."
        elif not description:
            description = "N/A"

        # Format input/output (truncate if too long)
        input_str = str(agent.input)
        if len(input_str) > 30:
            input_str = input_str[:27] + "..."

        output_str = str(agent.output)
        if len(output_str) > 30:
            output_str = output_str[:27] + "..."

        table.add_row(
            name,
            model,
            description,
            input_str,
            output_str,
        )

    console.print(table)

    # Option to view detailed info for a specific agent
    if len(agent_names) > 0:
        view_details = questionary.confirm(
            "View detailed information for an agent?",
            default=False,
        ).ask()

        if view_details:
            agent_to_view = questionary.select(
                "Select an agent to view:",
                choices=agent_names,
            ).ask()

            if agent_to_view:
                _view_agent_details(flock._agents[agent_to_view])


def _view_agent_details(agent: FlockAgent):
    """Display detailed information about an agent.

    Args:
        agent: The agent to display details for
    """
    init_console()
    console.print(
        Panel(f"[bold green]Agent Details: {agent.name}[/]"), justify="center"
    )

    # Create a panel for each section
    basic_info = Table(show_header=False, box=Box.ROUNDED, padding=(0, 2))
    basic_info.add_column("Property", style="cyan")
    basic_info.add_column("Value", style="green")

    basic_info.add_row("Name", agent.name)
    basic_info.add_row("Model", str(agent.model or "Default"))
    basic_info.add_row("Description", agent.resolved_description if agent.resolved_description else "N/A")
    basic_info.add_row("Input", str(agent.input))
    basic_info.add_row("Output", str(agent.output))
    basic_info.add_row("Write to File", str(agent.write_to_file))
    basic_info.add_row("Wait for input", str(agent.wait_for_input))

    console.print(Panel(basic_info, title="Basic Information"))

    # Evaluator info
    evaluator_info = (
        f"Type: {type(agent.evaluator).__name__ if agent.evaluator else 'None'}"
    )
    console.print(Panel(evaluator_info, title="Evaluator"))

    # Router info
    router_info = f"Type: {type(agent.handoff_router).__name__ if agent.handoff_router else 'None'}"
    console.print(Panel(router_info, title="Router"))

    # Tools
    if agent.tools:
        tool_names = [t.__name__ for t in agent.tools]
        tools_info = ", ".join(tool_names)
    else:
        tools_info = "None"

    console.print(Panel(tools_info, title="Tools"))

    # Modules
    if agent.modules:
        module_table = Table(show_header=True)
        module_table.add_column("Name", style="cyan")
        module_table.add_column("Type", style="green")
        module_table.add_column("Enabled", style="yellow")

        for name, module in agent.modules.items():
            module_table.add_row(
                name,
                type(module).__name__,
                "Yes" if module.config.enabled else "No",
            )

        console.print(Panel(module_table, title="Modules"))
    else:
        console.print(Panel("None", title="Modules"))


def _add_agent(flock: Flock):
    """Add a new agent to the Flock.

    Args:
        flock: The Flock instance to add the agent to
    """
    console.print("\n[bold]Add New Agent[/]")
    console.line()

    # Get agent name
    name = questionary.text(
        "Enter a name for the agent:",
        default="my_agent",
    ).ask()

    # Check for name conflicts
    if name in flock._agents:
        console.print(
            f"[bold red]Error: An agent named '{name}' already exists.[/]"
        )
        return

    # Get agent description
    description = questionary.text(
        "Enter a description for the agent (optional):",
        default="",
    ).ask()

    # Get input specification
    input_spec = questionary.text(
        "Enter input specification (e.g., 'query: str | The search query'):",
        default="query",
    ).ask()

    # Get output specification
    output_spec = questionary.text(
        "Enter output specification (e.g., 'result: str | The generated result'):",
        default="result",
    ).ask()

    # Model selection
    use_flock_model = questionary.confirm(
        f"Use Flock's default model ({flock.model or 'None'})? Select 'n' to specify a different model.",
        default=True,
    ).ask()

    if use_flock_model:
        model = None  # Use Flock's default
    else:
        default_models = [
            "openai/gpt-4o",
            "openai/gpt-3.5-turbo",
            "anthropic/claude-3-opus-20240229",
            "anthropic/claude-3-sonnet-20240229",
            "gemini/gemini-1.5-pro",
            "Other (specify)",
        ]

        model_choice = questionary.select(
            "Select a model:",
            choices=default_models,
        ).ask()

        if model_choice == "Other (specify)":
            model = questionary.text(
                "Enter the model identifier:",
                default="openai/gpt-4o",
            ).ask()
        else:
            model = model_choice

    # Additional options
    use_cache = questionary.confirm(
        "Enable caching for this agent?",
        default=True,
    ).ask()

    enable_rich_tables = questionary.confirm(
        "Enable rich table output for this agent?",
        default=True,
    ).ask()

    # Create the agent
    agent = FlockFactory.create_default_agent(
        name=name,
        description=description,
        model=model,
        input=input_spec,
        output=output_spec,
        use_cache=use_cache,
        enable_rich_tables=enable_rich_tables,
    )

    # Add the agent to the Flock
    flock.add_agent(agent)
    console.print(f"\n[green]✓[/] Agent '{name}' created and added to Flock!")


def _edit_agent(flock: Flock):
    """Edit an existing agent in the Flock.

    Args:
        flock: The Flock instance containing the agent to edit
    """
    agent_names = list(flock._agents.keys())

    if not agent_names:
        console.print("[yellow]No agents in this Flock to edit.[/]")
        return

    # Select agent to edit
    agent_name = questionary.select(
        "Select an agent to edit:",
        choices=agent_names,
    ).ask()

    if not agent_name:
        return

    agent = flock._agents[agent_name]

    if not agent:
        console.print(f"[bold red]Agent '{agent_name}' not found.[/]")
        return

    console.print(f"\n[bold underline]Details for Agent: {agent.name}[/]")
    basic_info = Table(show_header=False, box=Box.ROUNDED, padding=(0, 2))
    basic_info.add_row("Name", agent.name)
    description = agent.resolved_description
    basic_info.add_row("Description", description if description else "N/A")
    basic_info.add_row("Model", agent.model or "Flock Default")
    basic_info.add_row("Input Signature", str(agent.input))

    # Choose edit method
    edit_choice = questionary.select(
        "How would you like to edit this agent?",
        choices=[
            "Use Abstract Editor (Field by Field)",
            "Edit YAML Directly",
            "Cancel",
        ],
    ).ask()

    if edit_choice == "Use Abstract Editor (Field by Field)":
        # Not fully implemented yet
        console.print(
            "[yellow]Abstract editor not fully implemented. Opening YAML editor instead.[/]"
        )
        from flock.cli.yaml_editor import yaml_editor

        updated_agent = yaml_editor(agent)
        if updated_agent and isinstance(updated_agent, FlockAgent):
            flock._agents[agent_name] = updated_agent

    elif edit_choice == "Edit YAML Directly":
        from flock.cli.yaml_editor import _edit_yaml_directly

        updated_agent = _edit_yaml_directly(agent)
        if updated_agent and isinstance(updated_agent, FlockAgent):
            flock._agents[agent_name] = updated_agent
            console.print(f"\n[green]✓[/] Agent '{agent_name}' updated!")


def _remove_agent(flock: Flock):
    """Remove an agent from the Flock.

    Args:
        flock: The Flock instance containing the agent to remove
    """
    agent_names = list(flock._agents.keys())

    if not agent_names:
        console.print("[yellow]No agents in this Flock to remove.[/]")
        return

    # Select agent to remove
    agent_name = questionary.select(
        "Select an agent to remove:",
        choices=agent_names,
    ).ask()

    if not agent_name:
        return

    # Confirm deletion
    confirm = questionary.confirm(
        f"Are you sure you want to remove agent '{agent_name}'?",
        default=False,
    ).ask()

    if confirm:
        del flock._agents[agent_name]
        console.print(f"\n[green]✓[/] Agent '{agent_name}' removed from Flock!")


def _export_agent(flock: Flock):
    """Export an agent to a YAML file.

    Args:
        flock: The Flock instance containing the agent to export
    """
    agent_names = list(flock._agents.keys())

    if not agent_names:
        console.print("[yellow]No agents in this Flock to export.[/]")
        return

    # Select agent to export
    agent_name = questionary.select(
        "Select an agent to export:",
        choices=agent_names,
    ).ask()

    if not agent_name:
        return

    agent = flock._agents[agent_name]

    # Get file path
    file_path = questionary.text(
        "Enter file path to save agent:",
        default=f"{agent_name}.agent.yaml",
    ).ask()

    # Ensure the file has the correct extension
    if not file_path.endswith((".yaml", ".yml")):
        file_path += ".yaml"

    try:
        # Save the agent to YAML
        agent.to_yaml_file(file_path)
        console.print(
            f"\n[green]✓[/] Agent '{agent_name}' exported to {file_path}"
        )
    except Exception as e:
        console.print(f"\n[bold red]Error exporting agent:[/] {e!s}")


def _import_agent(flock: Flock):
    """Import an agent from a YAML file.

    Args:
        flock: The Flock instance to import the agent into
    """
    console.print("[yellow]Import functionality not yet implemented.[/]")
    # TODO: Implement agent import from YAML file
```

### src\flock\cli\registry_management.py

- **Lines**: 889
- **Last modified**: 2025-04-16 00:11:15

```py
"""Registry Management Module for the Flock CLI."""

import datetime
import importlib
import inspect
import os
from dataclasses import is_dataclass
from pathlib import Path
from typing import Any

import questionary
from rich.console import Console
from rich.panel import Panel
from rich.progress import BarColumn, Progress, SpinnerColumn, TextColumn
from rich.table import Table

from flock.core.flock_registry import (
    get_registry,
)
from flock.core.logging.logging import get_logger

logger = get_logger("registry_cli")
console = Console()

# Constants for registry item types
REGISTRY_CATEGORIES = ["Agent", "Callable", "Type", "Component"]
REGISTRY_ACTIONS = [
    "View Registry Contents",
    "Add Item to Registry",
    "Remove Item from Registry",
    "Auto-Registration Scanner",
    "Export Registry",
    "Back to Main Menu",
]


def manage_registry() -> None:
    """Main function for managing the Flock Registry from the CLI."""
    while True:
        console.clear()
        console.print(
            Panel("[bold blue]Flock Registry Management[/]"), justify="center"
        )
        console.line()

        # Show registry stats
        display_registry_stats()

        action = questionary.select(
            "What would you like to do?",
            choices=REGISTRY_ACTIONS,
        ).ask()

        if action == "View Registry Contents":
            view_registry_contents()
        elif action == "Add Item to Registry":
            add_item_to_registry()
        elif action == "Remove Item from Registry":
            remove_item_from_registry()
        elif action == "Auto-Registration Scanner":
            auto_registration_scanner()
        elif action == "Export Registry":
            export_registry()
        elif action == "Back to Main Menu":
            break

        input("\nPress Enter to continue...")


def display_registry_stats() -> None:
    """Display statistics about the current registry contents."""
    registry = get_registry()

    table = Table(title="Registry Statistics")
    table.add_column("Category", style="cyan")
    table.add_column("Count", style="green")

    table.add_row("Agents", str(len(registry._agents)))
    table.add_row("Callables", str(len(registry._callables)))
    table.add_row("Types", str(len(registry._types)))
    table.add_row("Components", str(len(registry._components)))

    console.print(table)


def view_registry_contents(
    category: str | None = None, search_pattern: str | None = None
) -> None:
    """Display registry contents with filtering options."""
    registry = get_registry()

    if category is None:
        category = questionary.select(
            "Select a category to view:",
            choices=REGISTRY_CATEGORIES + ["All Categories"],
        ).ask()

    if search_pattern is None:
        search_pattern = questionary.text(
            "Enter search pattern (leave empty to show all):"
        ).ask()

    console.clear()

    if category == "All Categories" or category == "Agent":
        display_registry_section("Agents", registry._agents, search_pattern)

    if category == "All Categories" or category == "Callable":
        display_registry_section(
            "Callables", registry._callables, search_pattern
        )

    if category == "All Categories" or category == "Type":
        display_registry_section("Types", registry._types, search_pattern)

    if category == "All Categories" or category == "Component":
        display_registry_section(
            "Components", registry._components, search_pattern
        )


def display_registry_section(
    title: str, items: dict[str, Any], search_pattern: str
) -> None:
    """Display a section of registry items in a table."""
    filtered_items = {
        k: v
        for k, v in items.items()
        if not search_pattern or search_pattern.lower() in k.lower()
    }

    if not filtered_items:
        console.print(
            f"[yellow]No {title.lower()} found matching the search pattern.[/]"
        )
        return

    table = Table(title=f"Registered {title}")
    table.add_column("Name/Path", style="cyan")
    table.add_column("Type", style="green")

    # Add file path column for components
    if title == "Components":
        table.add_column("File Path", style="yellow")

    for name, item in filtered_items.items():
        item_type = type(item).__name__

        if title == "Components":
            # Try to get the file path for component classes
            file_path = (
                inspect.getfile(item) if inspect.isclass(item) else "N/A"
            )
            table.add_row(name, item_type, file_path)
        else:
            table.add_row(name, item_type)

    console.print(table)
    console.print(f"Total: {len(filtered_items)} {title.lower()}")


def add_item_to_registry() -> None:
    """Add an item to the registry manually."""
    registry = get_registry()

    item_type = questionary.select(
        "What type of item do you want to add?",
        choices=["agent", "callable", "type", "component"],
    ).ask()

    # For component types, offer file path option
    use_file_path = False
    if item_type == "component":
        path_type = questionary.select(
            "How do you want to specify the component?",
            choices=["Module Path", "File Path"],
        ).ask()
        use_file_path = path_type == "File Path"

    if use_file_path:
        file_path = questionary.path(
            "Enter the file path to the component:", only_directories=False
        ).ask()

        if not file_path or not os.path.exists(file_path):
            console.print(f"[red]Error: File {file_path} does not exist[/]")
            return False

        module_name = questionary.text(
            "Enter the component class name in the file:"
        ).ask()

        try:
            # Use dynamic import to load the module from file path
            import importlib.util

            spec = importlib.util.spec_from_file_location(
                "temp_module", file_path
            )
            module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(module)

            if not hasattr(module, module_name):
                console.print(
                    f"[red]Error: {module_name} not found in {file_path}[/]"
                )
                return False

            item = getattr(module, module_name)
        except Exception as e:
            console.print(f"[red]Error importing from file: {e!s}[/]")
            return False
    else:
        module_path = questionary.text(
            "Enter the module path (e.g., 'your_module.submodule'):"
        ).ask()

        item_name = questionary.text(
            "Enter the item name within the module:"
        ).ask()

        try:
            # Attempt to import the module
            module = importlib.import_module(module_path)

            # Get the item from the module
            if not hasattr(module, item_name):
                console.print(
                    f"[red]Error: {item_name} not found in {module_path}[/]"
                )
                return False

            item = getattr(module, item_name)
        except Exception as e:
            console.print(f"[red]Error importing module: {e!s}[/]")
            return False

    alias = questionary.text(
        "Enter an alias (optional, press Enter to skip):"
    ).ask()

    if not alias:
        alias = None

    # Register the item based on its type
    try:
        if item_type == "agent":
            registry.register_agent(item)
            console.print(
                f"[green]Successfully registered agent: {item_name}[/]"
            )
        elif item_type == "callable":
            result = registry.register_callable(item, alias)
            console.print(
                f"[green]Successfully registered callable: {result}[/]"
            )
        elif item_type == "type":
            result = registry.register_type(item, alias)
            console.print(f"[green]Successfully registered type: {result}[/]")
        elif item_type == "component":
            result = registry.register_component(item, alias)
            # Store the file path information if we loaded from a file
            if use_file_path and hasattr(registry, "_component_file_paths"):
                # Check if the registry has component file paths attribute
                # This will be added to registry in our update
                registry._component_file_paths[result] = file_path
            console.print(
                f"[green]Successfully registered component: {result}[/]"
            )
    except Exception as e:
        console.print(f"[red]Error registering item: {e!s}[/]")
        return False

    return True


def remove_item_from_registry() -> None:
    """Remove an item from the registry."""
    registry = get_registry()

    item_type = questionary.select(
        "What type of item do you want to remove?",
        choices=["agent", "callable", "type", "component"],
    ).ask()

    # Get the appropriate dictionary based on item type
    if item_type == "agent":
        items = registry._agents
    elif item_type == "callable":
        items = registry._callables
    elif item_type == "type":
        items = registry._types
    elif item_type == "component":
        items = registry._components

    if not items:
        console.print(f"[yellow]No {item_type}s registered.[/]")
        return False

    # Create a list of items for selection
    item_names = list(items.keys())
    item_name = questionary.select(
        f"Select the {item_type} to remove:",
        choices=item_names + ["Cancel"],
    ).ask()

    if item_name == "Cancel":
        return False

    # Ask for confirmation
    confirm = questionary.confirm(
        f"Are you sure you want to remove {item_name}?",
        default=False,
    ).ask()

    if not confirm:
        console.print("[yellow]Operation cancelled.[/]")
        return False

    # Remove the item
    try:
        if item_type == "agent":
            del registry._agents[item_name]
        elif item_type == "callable":
            del registry._callables[item_name]
        elif item_type == "type":
            del registry._types[item_name]
        elif item_type == "component":
            del registry._components[item_name]

        console.print(
            f"[green]Successfully removed {item_type}: {item_name}[/]"
        )
        return True

    except Exception as e:
        console.print(f"[red]Error: {e!s}[/]")
        return False


def auto_registration_scanner() -> None:
    """Launch the auto-registration scanner interface."""
    console.clear()
    console.print(
        Panel("[bold blue]Auto-Registration Scanner[/]"), justify="center"
    )
    console.line()

    console.print(
        "This utility will scan Python files for components, types, callables (tools), and agents that can be registered."
    )
    console.print(
        "[yellow]Note: Registration is required for proper serialization and deserialization of your Flock.[/]"
    )
    console.line()

    # Target directory selection
    def path_filter(path):
        """Filter paths for selection."""
        if os.path.isdir(path):
            return True
        return path.endswith(".py")

    target_path = questionary.path(
        "Select directory to scan:", file_filter=path_filter
    ).ask()

    if not target_path or not os.path.exists(target_path):
        console.print("[red]Invalid path selected. Aborting.[/]")
        return

    is_recursive = questionary.confirm(
        "Scan recursively (include subdirectories)?", default=True
    ).ask()

    auto_register = questionary.confirm(
        "Automatically register items found during scan?", default=True
    ).ask()

    # Special callout for tools/callables
    console.print(
        "[bold blue]Tool Registration:[/] This scanner will look for functions that can be used as tools."
    )
    console.print(
        "These will be registered as callables and can be properly serialized in your Flock YAML."
    )
    console.line()

    with Progress(
        SpinnerColumn(),
        TextColumn("[bold blue]{task.description}"),
        BarColumn(),
        TextColumn("[bold green]{task.completed}/{task.total}"),
        console=console,
    ) as progress:
        task_id = progress.add_task(
            "Scanning for registry items...", total=None
        )

        # Perform the scan
        results = scan_for_registry_items(
            target_path, recursive=is_recursive, auto_register=auto_register
        )

        # Mark task as complete
        progress.update(task_id, completed=1, total=1)
        console.line()

    # Display results
    console.print("[bold green]Scan Complete![/]")
    console.line()

    total_found = sum(len(items) for items in results.values())
    total_categories = sum(1 for items in results.values() if items)

    console.print(
        f"Found {total_found} items across {total_categories} categories."
    )

    # Enhanced report section
    table = Table(title="Scan Results")
    table.add_column("Category", style="cyan")
    table.add_column("Count", style="green")
    table.add_column("Example Items", style="blue")

    for category, items in results.items():
        if items:
            examples = ", ".join(items[:3])
            if len(items) > 3:
                examples += ", ..."
            table.add_row(category, str(len(items)), examples)
        else:
            table.add_row(category, "0", "")

    console.print(table)
    console.line()

    # Callout for tools and future serialization
    if results.get("callables"):
        console.print(
            "[bold green]Note:[/] Found callable functions that can be used as tools."
        )
        console.print(
            "These functions will now be properly serialized as callable references in your Flock YAML."
        )
        console.print(
            "When sharing Flocks, ensure these callables are registered on the target system."
        )
        console.line()

    # Show details options
    if total_found > 0:
        view_details = questionary.confirm(
            "Would you like to view detailed results?", default=True
        ).ask()

        if view_details:
            view_registry_contents()  # Show the registry contents after scan


def scan_for_registry_items(
    target_path: str, recursive: bool = True, auto_register: bool = False
) -> dict[str, list[str]]:
    """Scan directory for potential registry items and optionally register them."""
    results = {
        "Agents": [],
        "Callables": [],
        "Types": [],
        "Components": [],
        "Potential Items": [],
    }

    registry = get_registry()
    path = Path(target_path)

    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        BarColumn(),
        TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
    ) as progress:
        scan_task = progress.add_task(f"Scanning {target_path}...", total=100)

        # If path is a file, scan it directly
        if path.is_file() and path.suffix == ".py":
            module_path = get_module_path_from_file(path)
            if module_path:
                scan_python_file(path, module_path, results, auto_register)
            progress.update(scan_task, completed=100)

        # If path is a directory, scan all Python files
        elif path.is_dir():
            python_files = []
            if recursive:
                for root, _, files in os.walk(path):
                    python_files.extend(
                        [
                            Path(os.path.join(root, f))
                            for f in files
                            if f.endswith(".py")
                        ]
                    )
            else:
                python_files = [p for p in path.glob("*.py")]

            total_files = len(python_files)
            for i, file_path in enumerate(python_files):
                module_path = get_module_path_from_file(file_path)
                if module_path:
                    scan_python_file(
                        file_path, module_path, results, auto_register
                    )
                progress.update(
                    scan_task, completed=(i + 1) / total_files * 100
                )

    return results


def get_module_path_from_file(file_path: Path) -> str | None:
    """Convert a file path to a module path for import."""
    try:
        # Get absolute path
        abs_path = file_path.resolve()

        # Check if it's a Python file
        if abs_path.suffix != ".py":
            return None

        # Get the directory containing the file
        file_dir = abs_path.parent

        # Find the nearest parent directory with __init__.py
        # to determine the package root
        package_root = None
        current_dir = file_dir
        while current_dir != current_dir.parent:
            if (current_dir / "__init__.py").exists():
                if package_root is None:
                    package_root = current_dir
            else:
                # We've reached a directory without __init__.py
                # If we found a package root earlier, use that
                if package_root is not None:
                    break
            current_dir = current_dir.parent

        # If no package root was found, this file can't be imported as a module
        if package_root is None:
            return None

        # Calculate the module path
        rel_path = abs_path.relative_to(package_root.parent)
        module_path = str(rel_path.with_suffix("")).replace(os.sep, ".")

        return module_path

    except Exception as e:
        logger.error(f"Error determining module path: {e}")
        return None


def scan_python_file(
    file_path: Path,
    module_path: str,
    results: dict[str, list[str]],
    auto_register: bool,
) -> None:
    """Scan a Python file for registry-eligible items."""
    try:
        # Try to import the module
        module = importlib.import_module(module_path)

        # Scan for classes and functions
        for name, obj in inspect.getmembers(module):
            if name.startswith("_"):
                continue

            # Check for registry decorator presence
            is_registry_item = False

            # Check for classes
            if inspect.isclass(obj):
                # Check if it has a FlockAgent as a base class
                if is_flock_agent(obj):
                    if auto_register:
                        get_registry().register_agent(obj)
                    results["Agents"].append(f"{module_path}.{name}")
                    is_registry_item = True

                # Check for components
                elif has_component_base(obj):
                    if auto_register:
                        get_registry().register_component(obj)
                    results["Components"].append(f"{module_path}.{name}")
                    is_registry_item = True

                # Check for Pydantic models or dataclasses
                elif is_potential_type(obj):
                    if auto_register:
                        get_registry().register_type(obj)
                    results["Types"].append(f"{module_path}.{name}")
                    is_registry_item = True

                # If not already identified but seems like a potential candidate
                elif not is_registry_item and is_potential_registry_candidate(
                    obj
                ):
                    results["Potential Items"].append(
                        f"{module_path}.{name} (class)"
                    )

            # Check for functions (potential callables/tools)
            elif inspect.isfunction(obj) and obj.__module__ == module.__name__:
                if auto_register:
                    get_registry().register_callable(obj)
                results["Callables"].append(f"{module_path}.{name}")
                is_registry_item = True

    except (ImportError, AttributeError) as e:
        logger.warning(f"Could not import {module_path}: {e}")
    except Exception as e:
        logger.error(f"Error scanning {file_path}: {e}")


def is_flock_agent(cls: type) -> bool:
    """Check if a class is a FlockAgent or a subclass of FlockAgent."""
    try:
        from flock.core.flock_agent import FlockAgent

        return issubclass(cls, FlockAgent)
    except (ImportError, TypeError):
        # If FlockAgent can't be imported or cls is not a class
        return False


def has_component_base(cls: type) -> bool:
    """Check if a class has a base class that looks like a Flock component."""
    try:
        # Common Flock component base classes
        component_bases = ["FlockModule", "FlockEvaluator", "FlockRouter"]
        bases = [base.__name__ for base in cls.__mro__]
        return any(base in bases for base in component_bases)
    except (AttributeError, TypeError):
        return False


def is_potential_type(cls: type) -> bool:
    """Check if a class is a Pydantic model or dataclass."""
    try:
        from pydantic import BaseModel

        return issubclass(cls, BaseModel) or is_dataclass(cls)
    except (ImportError, TypeError):
        return False


def is_potential_registry_candidate(obj: Any) -> bool:
    """Check if an object seems like it could be registry-eligible."""
    # This is a heuristic function to identify potential registry candidates
    if inspect.isclass(obj):
        # Classes with "Flock" in their name
        if "Flock" in obj.__name__:
            return True

        # Classes with docstrings mentioning certain keywords
        if obj.__doc__ and any(
            kw in obj.__doc__.lower()
            for kw in [
                "agent",
                "flock",
                "tool",
                "module",
                "evaluator",
                "router",
            ]
        ):
            return True

    elif inspect.isfunction(obj):
        # Functions with docstrings mentioning certain keywords
        if obj.__doc__ and any(
            kw in obj.__doc__.lower() for kw in ["tool", "agent", "flock"]
        ):
            return True

    return False


def export_registry() -> None:
    """Export registry contents to a file."""
    registry = get_registry()

    # Select what to export
    export_items = questionary.checkbox(
        "Select what to export:",
        choices=[
            questionary.Choice("Agents", checked=True),
            questionary.Choice("Callables (Tools)", checked=True),
            questionary.Choice("Types", checked=True),
            questionary.Choice("Components", checked=True),
            questionary.Choice("File Paths", checked=True),
        ],
    ).ask()

    if not export_items:
        console.print("[yellow]No items selected for export.[/]")
        return

    # Select export format
    export_format = questionary.select(
        "Select export format:",
        choices=["YAML", "JSON", "Python"],
    ).ask()

    # Select path type for serialization
    path_type = questionary.select(
        "How should file paths be formatted?",
        choices=[
            "absolute (full paths, best for local use)",
            "relative (relative paths, better for sharing)",
        ],
        default="absolute (full paths, best for local use)",
    ).ask()

    # Extract just the first word
    path_type = path_type.split()[0]

    console.print(
        f"\n[bold]Path type selected: [green]{path_type}[/green][/bold]"
    )
    if path_type == "relative":
        console.print(
            "Relative paths are recommended when sharing Flocks between systems.\n"
            "They'll be converted to paths relative to the current directory."
        )
    else:
        console.print(
            "Absolute paths work best for local usage but may not work correctly\n"
            "when sharing with others or moving files."
        )
    console.line()

    # Get file path for export
    file_path = questionary.path(
        "Enter file path for export:",
        default=f"flock_registry_export.{export_format.lower()}",
    ).ask()

    if not file_path:
        return

    # Prepare export data
    export_data = {}

    if "Agents" in export_items:
        export_data["agents"] = list(registry._agents.keys())

    if "Callables (Tools)" in export_items:
        export_data["callables"] = list(registry._callables.keys())

        # Add serialization format information for tools
        callable_details = {}
        for callable_name in registry._callables.keys():
            callable_obj = registry._callables[callable_name]
            file_path_value = (
                inspect.getfile(callable_obj)
                if callable_obj and inspect.isfunction(callable_obj)
                else "Unknown"
            )

            # Convert to relative path if needed
            if path_type == "relative" and file_path_value != "Unknown":
                try:
                    file_path_value = os.path.relpath(file_path_value)
                except ValueError:
                    # Keep as absolute if can't make relative
                    pass

            callable_details[callable_name] = {
                "module": callable_obj.__module__,
                "file": file_path_value,
                "type": "function"
                if inspect.isfunction(callable_obj)
                else "other_callable",
            }
        export_data["callable_details"] = callable_details

    if "Types" in export_items:
        export_data["types"] = list(registry._types.keys())

    if "Components" in export_items:
        export_data["components"] = list(registry._components.keys())

        # Include file paths if selected
        if "File Paths" in export_items and hasattr(
            registry, "_component_file_paths"
        ):
            export_data["component_file_paths"] = {}
            for component_name in registry._components.keys():
                # Get the file path if available
                if component_name in registry._component_file_paths:
                    file_path_value = registry._component_file_paths[
                        component_name
                    ]

                    # Convert to relative path if needed
                    if path_type == "relative" and file_path_value:
                        try:
                            file_path_value = os.path.relpath(file_path_value)
                        except ValueError:
                            # Keep as absolute if can't make relative
                            pass

                    export_data["component_file_paths"][component_name] = (
                        file_path_value
                    )

    # Add metadata about serialization format
    export_data["metadata"] = {
        "export_date": datetime.datetime.now().isoformat(),
        "flock_version": "0.3.41",  # Update with actual version
        "serialization_format": {
            "tools": "Callable reference names",
            "components": "Module and class names",
            "types": "Module and class names",
        },
        "path_type": path_type,
    }

    # Add serialization settings as a top-level element
    export_data["serialization_settings"] = {"path_type": path_type}

    try:
        # Export the data
        if export_format == "YAML":
            import yaml

            with open(file_path, "w") as f:
                yaml.dump(export_data, f, default_flow_style=False)
        elif export_format == "JSON":
            import json

            with open(file_path, "w") as f:
                json.dump(export_data, f, indent=2)
        elif export_format == "Python":
            with open(file_path, "w") as f:
                f.write("# Flock Registry Export\n")
                f.write(f"# Generated on {datetime.datetime.now()}\n\n")
                f.write("registry_data = ")
                f.write(repr(export_data))
                f.write("\n")

        console.print(f"[green]Registry exported to {file_path}[/]")
        console.print(f"[green]Paths formatted as: {path_type}[/]")

        # Print information about tool serialization if tools were exported
        if "Callables (Tools)" in export_items and registry._callables:
            console.print("\n[bold blue]Tool Serialization Information:[/]")
            console.print(
                "Tools in Flock are now serialized as callable references rather than dictionaries."
            )
            console.print(
                "This makes YAML files more readable and simplifies tool management."
            )
            console.print("When loading a Flock with tools:")
            console.print("  1. Tools must be registered in the registry")
            console.print("  2. The tools' modules must be importable")
            console.print(
                "  3. Tool functions have the same signature across systems"
            )

            # Show example of how a tool would appear in YAML
            if registry._callables:
                console.print("\n[bold green]Example tool in YAML:[/]")
                example_callable = next(iter(registry._callables.keys()))
                console.print(
                    f"  - {example_callable}  # Function name reference"
                )
                console.print("instead of the old format:")
                console.print(f"  - __callable_ref__: {example_callable}")

    except Exception as e:
        console.print(f"[red]Error exporting registry: {e}[/]")
        logger.error(f"Failed to export registry: {e}", exc_info=True)


if __name__ == "__main__":
    manage_registry()
```

### src\flock\cli\runner.py

- **Lines**: 41
- **Last modified**: 2025-04-16 00:11:15

```py
# src/flock/cli/runner.py
"""Provides functionality to start the Flock CLI."""

from typing import TYPE_CHECKING

from flock.core.logging.logging import get_logger

if TYPE_CHECKING:
    from flock.core.flock import Flock

logger = get_logger("cli.runner")


def start_flock_cli(
    flock: "Flock",
    server_name: str = "Flock CLI",
    show_results: bool = False,
    edit_mode: bool = False,
) -> None:
    """Start a CLI interface for the given Flock instance."""
    try:
        # Import CLI function locally
        from flock.cli.loaded_flock_cli import start_loaded_flock_cli
    except ImportError:
        logger.error(
            "CLI components not found. Cannot start CLI. "
            "Ensure the CLI modules are properly installed/available."
        )
        return

    logger.info(
        f"Starting CLI interface for loaded Flock instance '{flock.name}' ({len(flock.agents)} agents)"
    )

    # Pass the Flock instance to the CLI entry point
    start_loaded_flock_cli(
        flock=flock,
        server_name=server_name,
        show_results=show_results,
        edit_mode=edit_mode,
    )
```

### src\flock\cli\settings.py

- **Lines**: 857
- **Last modified**: 2025-04-16 00:11:15

```py
"""Settings editor for the Flock CLI.

This module provides functionality to view, edit, add, and delete
environment variables in the .env file.
"""

import os
import shutil
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import math

import questionary
from rich.console import Console
from rich.panel import Panel
from rich.table import Table
from rich.text import Text

from flock.core.util.cli_helper import init_console

# Constants
ENV_FILE = ".env"
ENV_TEMPLATE_FILE = ".env_template"
ENV_PROFILE_PREFIX = ".env_"
DEFAULT_PROFILE_COMMENT = "# Profile: {profile_name}"
SHOW_SECRETS_KEY = "SHOW_SECRETS"
VARS_PER_PAGE_KEY = "VARS_PER_PAGE"
DEFAULT_VARS_PER_PAGE = 20

console = Console()


def settings_editor():
    """Main entry point for the settings editor."""
    while True:
        init_console()
        console.print(Panel("[bold green]Environment Settings Editor[/]"), justify="center")
        
        # Get current profile name
        current_profile = get_current_profile()
        if current_profile:
            console.print(f"Current Profile: [bold cyan]{current_profile}[/]")
        else:
            console.print("No profile detected")

        console.line()
            
        choice = questionary.select(
            "What would you like to do?",
            choices=[
                questionary.Separator(line=" "),
                "View all environment variables",
                "Edit an environment variable",
                "Add a new environment variable",
                "Delete an environment variable",
                questionary.Separator(),
                "Manage environment profiles",
                questionary.Separator(),
                "Toggle show secrets",
                "Change variables per page",
                questionary.Separator(),
                "Back to main menu",
            ],
        ).ask()
        
        if choice == "View all environment variables":
            view_env_variables()
        elif choice == "Edit an environment variable":
            edit_env_variable()
        elif choice == "Add a new environment variable":
            add_env_variable()
        elif choice == "Delete an environment variable":
            delete_env_variable()
        elif choice == "Manage environment profiles":
            manage_profiles()
        elif choice == "Toggle show secrets":
            toggle_show_secrets()
        elif choice == "Change variables per page":
            change_vars_per_page()
        elif choice == "Back to main menu":
            break
        
        if choice != "Back to main menu":
            input("\nPress Enter to continue...")


def view_env_variables(page: int = 1, page_size: Optional[int] = None):
    """View all environment variables with pagination.
    
    Args:
        page: Page number to display
        page_size: Number of variables per page (if None, use the setting in .env)
    """
    env_vars = load_env_file()
    
    # If page_size is not specified, get it from settings
    if page_size is None:
        page_size = get_vars_per_page_setting(env_vars)
    
    # Calculate pagination
    total_vars = len(env_vars)
    total_pages = math.ceil(total_vars / page_size) if total_vars > 0 else 1
    
    # Validate page number
    page = min(max(1, page), total_pages)
    
    start_idx = (page - 1) * page_size
    end_idx = min(start_idx + page_size, total_vars)
    
    # Get current page variables
    current_page_vars = list(env_vars.items())[start_idx:end_idx]
    
    # Check if secrets should be shown
    show_secrets = get_show_secrets_setting(env_vars)
    
    # Create table
    table = Table(title=f"Environment Variables (Page {page}/{total_pages}, {page_size} per page)")
    table.add_column("Name", style="cyan")
    table.add_column("Value", style="green")
    
    # Show secrets status
    secrets_status = "[green]ON[/]" if show_secrets else "[red]OFF[/]"
    init_console()
    console.print(f"Show Secrets: {secrets_status}")
    
    for key, value in current_page_vars:
        # Skip lines that are comments or empty
        if key.startswith('#') or not key:
            continue
            
        # Mask sensitive values if show_secrets is False
        if is_sensitive(key) and not show_secrets:
            masked_value = mask_sensitive_value(value)
            table.add_row(key, masked_value)
        else:
            table.add_row(key, value)
    
    console.print(table)
    
    # Pagination controls with more intuitive shortcuts
    console.print("\nNavigation: ", end="")
    if page > 1:
        console.print("[bold]Previous (p)[/] | ", end="")
    if page < total_pages:
        console.print("[bold]Next (n)[/] | ", end="")
    if show_secrets:
        console.print("[bold]Hide secrets (h)[/] | ", end="")
    else:
        console.print("[bold]Show secrets (s)[/] | ", end="")
    console.print("[bold]Change variables per page (v)[/] | ", end="")
    console.print("[bold]Back (b)[/]")
    
    # Handle navigation
    while True:
        key = input("Enter option: ").lower()
        if key == 'p' and page > 1:
            view_env_variables(page - 1, page_size)
            break
        elif key == 'n' and page < total_pages:
            view_env_variables(page + 1, page_size)
            break
        elif key == 's' and not show_secrets:
            # Confirm showing secrets
            confirm = questionary.confirm("Are you sure you want to show sensitive values?").ask()
            if confirm:
                set_show_secrets_setting(True)
                view_env_variables(page, page_size)
            break
        elif key == 'h' and show_secrets:
            set_show_secrets_setting(False)
            view_env_variables(page, page_size)
            break
        elif key == 'v':
            new_page_size = change_vars_per_page()
            if new_page_size:
                view_env_variables(1, new_page_size)  # Reset to first page with new page size
            break
        elif key == 'b':
            break


def change_vars_per_page():
    """Change the number of variables displayed per page.
    
    Returns:
        The new page size or None if cancelled
    """
    env_vars = load_env_file()
    current_setting = get_vars_per_page_setting(env_vars)
    
    console.print(f"Current variables per page: [cyan]{current_setting}[/]")
    
    # Predefined options plus custom option
    page_size_options = ["10", "20", "30", "50", "Custom", "Cancel"]
    
    choice = questionary.select(
        "Select number of variables per page:",
        choices=page_size_options,
    ).ask()
    
    if choice == "Cancel":
        return None
    
    if choice == "Custom":
        while True:
            try:
                custom_size = questionary.text(
                    "Enter custom page size (5-100):",
                    default=str(current_setting)
                ).ask()
                
                if not custom_size:
                    return None
                
                new_size = int(custom_size)
                if 5 <= new_size <= 100:
                    break
                else:
                    console.print("[yellow]Page size must be between 5 and 100.[/]")
            except ValueError:
                console.print("[yellow]Please enter a valid number.[/]")
    else:
        new_size = int(choice)
    
    # Save the setting
    set_vars_per_page_setting(new_size)
    console.print(f"[green]Variables per page set to {new_size}.[/]")
    
    return new_size


def get_vars_per_page_setting(env_vars: Dict[str, str] = None) -> int:
    """Get the current variables per page setting.
    
    Args:
        env_vars: Optional dictionary of environment variables
        
    Returns:
        Number of variables per page
    """
    if env_vars is None:
        env_vars = load_env_file()
    
    if VARS_PER_PAGE_KEY in env_vars:
        try:
            page_size = int(env_vars[VARS_PER_PAGE_KEY])
            # Ensure the value is within reasonable bounds
            if 5 <= page_size <= 100:
                return page_size
        except ValueError:
            pass
    
    return DEFAULT_VARS_PER_PAGE


def set_vars_per_page_setting(page_size: int):
    """Set the variables per page setting.
    
    Args:
        page_size: Number of variables to display per page
    """
    env_vars = load_env_file()
    env_vars[VARS_PER_PAGE_KEY] = str(page_size)
    save_env_file(env_vars)


def toggle_show_secrets():
    """Toggle the show secrets setting."""
    env_vars = load_env_file()
    current_setting = get_show_secrets_setting(env_vars)
    
    if current_setting:
        console.print("Currently showing sensitive values. Do you want to hide them?")
        confirm = questionary.confirm("Hide sensitive values?").ask()
        if confirm:
            set_show_secrets_setting(False)
            console.print("[green]Sensitive values will now be masked.[/]")
    else:
        console.print("[yellow]Warning:[/] Showing sensitive values can expose sensitive information.")
        confirm = questionary.confirm("Are you sure you want to show sensitive values?").ask()
        if confirm:
            set_show_secrets_setting(True)
            console.print("[green]Sensitive values will now be shown.[/]")


def get_show_secrets_setting(env_vars: Dict[str, str] = None) -> bool:
    """Get the current show secrets setting.
    
    Args:
        env_vars: Optional dictionary of environment variables
        
    Returns:
        True if secrets should be shown, False otherwise
    """
    if env_vars is None:
        env_vars = load_env_file()
    
    if SHOW_SECRETS_KEY in env_vars:
        return env_vars[SHOW_SECRETS_KEY].lower() == 'true'
    
    return False


def set_show_secrets_setting(show_secrets: bool):
    """Set the show secrets setting.
    
    Args:
        show_secrets: Whether to show secrets
    """
    env_vars = load_env_file()
    env_vars[SHOW_SECRETS_KEY] = str(show_secrets)
    save_env_file(env_vars)


def edit_env_variable():
    """Edit an environment variable."""
    # Get list of variables
    env_vars = load_env_file()
    
    if not env_vars:
        console.print("[yellow]No environment variables found to edit.[/]")
        return
    
    # Filter out comments
    variables = [k for k in env_vars.keys() if not k.startswith('#') and k]
    
    # Display variables with selection
    init_console()
    console.print("Select a variable to edit:")
    
    # Let user select a variable to edit
    var_name = questionary.select(
        "Select a variable to edit:",
        choices=variables + ["Cancel"],
    ).ask()
    
    if var_name == "Cancel":
        return
    
    current_value = env_vars[var_name]
    is_sensitive_var = is_sensitive(var_name)
    
    if is_sensitive_var:
        console.print(f"[yellow]Warning:[/] You are editing a sensitive value: {var_name}")
        confirm = questionary.confirm("Are you sure you want to continue?").ask()
        if not confirm:
            return
    
    # Show current value (masked if sensitive and show_secrets is False)
    show_secrets = get_show_secrets_setting(env_vars)
    if is_sensitive_var and not show_secrets:
        console.print(f"Current value: {mask_sensitive_value(current_value)}")
    else:
        console.print(f"Current value: {current_value}")
    
    # Get new value with hint
    console.print("[italic]Enter new value (or leave empty to cancel)[/]")
    new_value = questionary.text("Enter new value:", default=current_value).ask()
    
    if new_value is None:
        console.print("[yellow]Edit cancelled.[/]")
        return
    
    if new_value == "":
        # Confirm if user wants to set an empty value or cancel
        confirm = questionary.confirm("Do you want to set an empty value? Select No to cancel.", default=False).ask()
        if not confirm:
            console.print("[yellow]Edit cancelled.[/]")
            return
    
    if new_value == current_value:
        console.print("[yellow]No changes made.[/]")
        return
    
    # Update the value
    env_vars[var_name] = new_value
    save_env_file(env_vars)
    console.print(f"[green]Updated {var_name} successfully.[/]")


def add_env_variable():
    """Add a new environment variable."""
    env_vars = load_env_file()
    
    console.print("[italic]Enter variable name (or leave empty to go back)[/]")
    
    # Get variable name
    while True:
        var_name = questionary.text("Enter variable name:").ask()
        
        if not var_name:
            # Ask if user wants to go back
            go_back = questionary.confirm("Do you want to go back to the settings menu?", default=True).ask()
            if go_back:
                return
            else:
                console.print("[italic]Please enter a variable name (or leave empty to go back)[/]")
                continue
            
        if var_name in env_vars and not var_name.startswith('#'):
            console.print(f"[yellow]Variable {var_name} already exists. Please use edit instead.[/]")
            continue
            
        break
    
    # Get variable value
    var_value = questionary.text("Enter variable value:").ask()
    
    # Add to env_vars
    env_vars[var_name] = var_value
    save_env_file(env_vars)
    console.print(f"[green]Added {var_name} successfully.[/]")


def delete_env_variable():
    """Delete an environment variable."""
    # Get list of variables
    env_vars = load_env_file()
    
    if not env_vars:
        console.print("[yellow]No environment variables found to delete.[/]")
        return
    
    # Filter out comments
    variables = [k for k in env_vars.keys() if not k.startswith('#') and k]
    
    # Display variables with selection
    init_console()
    console.print("Select a variable to delete:")
    
    # Let user select a variable to delete with hint
    var_name = questionary.select(
        "Select a variable to delete:",
        choices=variables + ["Cancel"],
    ).ask()
    
    if var_name == "Cancel":
        return
    
    # Confirm deletion
    confirm = questionary.confirm(f"Are you sure you want to delete {var_name}?").ask()
    if not confirm:
        console.print("[yellow]Deletion cancelled.[/]")
        return
    
    # Delete the variable
    del env_vars[var_name]
    save_env_file(env_vars)
    console.print(f"[green]Deleted {var_name} successfully.[/]")


def manage_profiles():
    """Manage environment profiles."""
    init_console()
    console.print(Panel("[bold green]Environment Profile Management[/]"), justify="center")
    
    # Get current profile and available profiles
    current_profile = get_current_profile()
    available_profiles = get_available_profiles()
    
    if current_profile:
        console.print(f"Current Profile: [bold cyan]{current_profile}[/]")
    
    if not available_profiles:
        console.print("[yellow]No profiles found.[/]")
    else:
        console.print("Available Profiles:")
        for profile in available_profiles:
            if profile == current_profile:
                console.print(f"  [bold cyan]{profile} (active)[/]")
            else:
                console.print(f"  {profile}")

    console.line()
    
    # Profile management options
    choice = questionary.select(
        "What would you like to do?",
        choices=[
            questionary.Separator(line=" "),
            "Switch to a different profile",
            "Create a new profile",
            "Rename a profile",
            "Delete a profile",
            "Back to settings menu",
        ],
    ).ask()
    
    if choice == "Switch to a different profile":
        switch_profile()
    elif choice == "Create a new profile":
        create_profile()
    elif choice == "Rename a profile":
        rename_profile()
    elif choice == "Delete a profile":
        delete_profile()


def switch_profile():
    """Switch to a different environment profile."""
    available_profiles = get_available_profiles()
    current_profile = get_current_profile()
    
    if not available_profiles:
        console.print("[yellow]No profiles available to switch to.[/]")
        return
    
    # Remove current profile from the list to avoid switching to the same profile
    selectable_profiles = [p for p in available_profiles if p != current_profile]
    
    if not selectable_profiles:
        console.print("[yellow]No other profiles available to switch to.[/]")
        return
    
    target_profile = questionary.select(
        "Select a profile to switch to:",
        choices=selectable_profiles + ["Cancel"],
    ).ask()
    
    if target_profile == "Cancel":
        return
    
    # Confirm switch
    confirm = questionary.confirm(f"Are you sure you want to switch to the {target_profile} profile?").ask()
    if not confirm:
        return
    
    # Backup current .env file
    backup_env_file()
    
    # Copy selected profile to .env
    source_file = f"{ENV_PROFILE_PREFIX}{target_profile}"
    if os.path.exists(source_file):
        shutil.copy2(source_file, ENV_FILE)
        console.print(f"[green]Switched to {target_profile} profile successfully.[/]")
    else:
        console.print(f"[red]Error: Could not find profile file {source_file}.[/]")


def create_profile():
    """Create a new environment profile."""
    profile_name = questionary.text("Enter new profile name:").ask()
    
    if not profile_name:
        console.print("[yellow]Profile name cannot be empty.[/]")
        return
    
    # Check if profile already exists
    target_file = f"{ENV_PROFILE_PREFIX}{profile_name}"
    if os.path.exists(target_file):
        console.print(f"[yellow]Profile {profile_name} already exists.[/]")
        return
    
    # Determine source file - use current .env or template
    source_choices = ["Current environment (.env)", ".env_template"]
    if os.path.exists(ENV_TEMPLATE_FILE):
        source_choices.append(ENV_TEMPLATE_FILE)
    
    source_choice = questionary.select(
        "Create profile based on:",
        choices=source_choices + ["Cancel"],
    ).ask()
    
    if source_choice == "Cancel":
        return
    
    source_file = ENV_FILE if source_choice == "Current environment (.env)" else ENV_TEMPLATE_FILE
    
    if not os.path.exists(source_file):
        console.print(f"[red]Error: Source file {source_file} not found.[/]")
        return
    
    # Create new profile file
    try:
        # Copy source file
        shutil.copy2(source_file, target_file)
        
        # Add profile header if it doesn't exist
        with open(target_file, 'r') as file:
            content = file.read()
        
        if not content.startswith("# Profile:"):
            with open(target_file, 'w') as file:
                profile_header = DEFAULT_PROFILE_COMMENT.format(profile_name=profile_name)
                file.write(f"{profile_header}\n{content}")
        
        console.print(f"[green]Created {profile_name} profile successfully.[/]")
    except Exception as e:
        console.print(f"[red]Error creating profile: {str(e)}[/]")


def rename_profile():
    """Rename an existing profile."""
    available_profiles = get_available_profiles()
    current_profile = get_current_profile()
    
    if not available_profiles:
        console.print("[yellow]No profiles available to rename.[/]")
        return
    
    # Let user select a profile to rename
    profile_to_rename = questionary.select(
        "Select a profile to rename:",
        choices=available_profiles + ["Cancel"],
    ).ask()
    
    if profile_to_rename == "Cancel":
        return
    
    # Get new name
    new_name = questionary.text("Enter new profile name:").ask()
    
    if not new_name:
        console.print("[yellow]New profile name cannot be empty.[/]")
        return
    
    if new_name in available_profiles:
        console.print(f"[yellow]Profile {new_name} already exists.[/]")
        return
    
    # Rename profile file
    source_file = f"{ENV_PROFILE_PREFIX}{profile_to_rename}"
    target_file = f"{ENV_PROFILE_PREFIX}{new_name}"
    
    try:
        # Read content of the source file
        with open(source_file, 'r') as file:
            content = file.readlines()
        
        # Update profile header if it exists
        if content and content[0].startswith("# Profile:"):
            content[0] = DEFAULT_PROFILE_COMMENT.format(profile_name=new_name) + "\n"
        
        # Write to new file
        with open(target_file, 'w') as file:
            file.writelines(content)
        
        # Remove old file
        os.remove(source_file)
        
        # If this was the current profile, update .env as well
        if profile_to_rename == current_profile:
            with open(ENV_FILE, 'r') as file:
                content = file.readlines()
            
            if content and content[0].startswith("# Profile:"):
                content[0] = DEFAULT_PROFILE_COMMENT.format(profile_name=new_name) + "\n"
            
            with open(ENV_FILE, 'w') as file:
                file.writelines(content)
        
        console.print(f"[green]Renamed {profile_to_rename} to {new_name} successfully.[/]")
    except Exception as e:
        console.print(f"[red]Error renaming profile: {str(e)}[/]")


def delete_profile():
    """Delete an existing profile."""
    available_profiles = get_available_profiles()
    current_profile = get_current_profile()
    
    if not available_profiles:
        console.print("[yellow]No profiles available to delete.[/]")
        return
    
    # Let user select a profile to delete
    profile_to_delete = questionary.select(
        "Select a profile to delete:",
        choices=available_profiles + ["Cancel"],
    ).ask()
    
    if profile_to_delete == "Cancel":
        return
    
    # Confirm deletion
    confirm = questionary.confirm(
        f"Are you sure you want to delete the {profile_to_delete} profile? This cannot be undone."
    ).ask()
    
    if not confirm:
        return
    
    # Delete profile file
    profile_file = f"{ENV_PROFILE_PREFIX}{profile_to_delete}"
    
    try:
        os.remove(profile_file)
        
        # Warn if deleting the current profile
        if profile_to_delete == current_profile:
            console.print(
                f"[yellow]Warning: You deleted the currently active profile. "
                f"The .env file still contains those settings but is no longer marked as a profile.[/]"
            )
            
            # Remove profile header from .env
            with open(ENV_FILE, 'r') as file:
                content = file.readlines()
            
            if content and content[0].startswith("# Profile:"):
                content = content[1:]
                with open(ENV_FILE, 'w') as file:
                    file.writelines(content)
        
        console.print(f"[green]Deleted {profile_to_delete} profile successfully.[/]")
    except Exception as e:
        console.print(f"[red]Error deleting profile: {str(e)}[/]")


def is_sensitive(key: str) -> bool:
    """Check if a variable is considered sensitive.
    
    Args:
        key: The variable name
        
    Returns:
        True if sensitive, False otherwise
    """
    sensitive_patterns = ['key', 'token', 'secret', 'password', 'api', 'pat']
    key_lower = key.lower()
    return any(pattern in key_lower for pattern in sensitive_patterns)


def mask_sensitive_value(value: str) -> str:
    """Mask a sensitive value.
    
    Args:
        value: The sensitive value
        
    Returns:
        Masked value
    """
    if not value:
        return value
    
    if len(value) <= 4:
        return "••••"
    
    # Show first 2 and last 2 characters
    return value[:2] + "•" * (len(value) - 4) + value[-2:]


def get_current_profile() -> Optional[str]:
    """Get the name of the current active profile.
    
    Returns:
        Profile name or None if no profile is active
    """
    if not os.path.exists(ENV_FILE):
        return None
    
    try:
        with open(ENV_FILE, 'r') as file:
            first_line = file.readline().strip()
            
        if first_line.startswith("# Profile:"):
            return first_line.replace("# Profile:", "").strip()
    except Exception:
        pass
    
    return None


def get_available_profiles() -> List[str]:
    """Get a list of available profiles.
    
    Returns:
        List of profile names
    """
    profiles = []
    
    for file in os.listdir():
        if file.startswith(ENV_PROFILE_PREFIX):
            profile_name = file[len(ENV_PROFILE_PREFIX):]
            profiles.append(profile_name)
    
    return profiles


def backup_env_file():
    """Create a backup of the current .env file."""
    if not os.path.exists(ENV_FILE):
        return
    
    backup_file = f"{ENV_FILE}.bak"
    shutil.copy2(ENV_FILE, backup_file)


def load_env_file() -> Dict[str, str]:
    """Load the .env file into a dictionary.
    
    Returns:
        Dictionary of environment variables
    """
    env_vars = {}
    
    if not os.path.exists(ENV_FILE):
        console.print(f"[yellow]Warning: {ENV_FILE} file not found.[/]")
        return env_vars
    
    try:
        with open(ENV_FILE, 'r') as file:
            lines = file.readlines()
            
        # Process each line
        for line in lines:
            line = line.strip()
            
            # Skip empty lines
            if not line:
                env_vars[""] = ""
                continue
            
            # Handle comments
            if line.startswith('#'):
                env_vars[line] = ""
                continue
            
            # Handle regular variables
            if '=' in line:
                key, value = line.split('=', 1)
                env_vars[key] = value
            else:
                # Handle lines without equals sign
                env_vars[line] = ""
                
    except Exception as e:
        console.print(f"[red]Error loading .env file: {str(e)}[/]")
    
    return env_vars


def save_env_file(env_vars: Dict[str, str]):
    """Save environment variables back to the .env file.
    
    Args:
        env_vars: Dictionary of environment variables
    """
    # Create backup
    backup_env_file()
    
    try:
        with open(ENV_FILE, 'w') as file:
            for key, value in env_vars.items():
                if key.startswith('#'):
                    # Write comments as is
                    file.write(f"{key}\n")
                elif not key:
                    # Write empty lines
                    file.write("\n")
                else:
                    # Write regular variables
                    file.write(f"{key}={value}\n")
                    
        console.print("[green]Settings saved successfully.[/]")
    except Exception as e:
        console.print(f"[red]Error saving .env file: {str(e)}[/]")
```

### src\flock\cli\utils.py

- **Lines**: 135
- **Last modified**: 2025-04-16 00:11:15

```py
# src/pilot_rules/collector/utils.py
import datetime
from pathlib import Path
from typing import Any

from rich import box
from rich.console import Console
from rich.panel import Panel
from rich.progress import BarColumn, Progress, SpinnerColumn, TextColumn
from rich.table import Table

# Create a shared console instance for consistent styling
console = Console()


def get_file_metadata(file_path: str) -> dict[str, Any]:
    """Extract metadata from a file."""
    metadata = {
        "path": file_path,
        "size_bytes": 0,
        "line_count": 0,
        "last_modified": "Unknown",
        "created": "Unknown",
    }

    try:
        p = Path(file_path)
        stats = p.stat()
        metadata["size_bytes"] = stats.st_size
        metadata["last_modified"] = datetime.datetime.fromtimestamp(
            stats.st_mtime
        ).strftime("%Y-%m-%d %H:%M:%S")
        # ctime is platform dependent (creation on Windows, metadata change on Unix)
        # Use mtime as a reliable fallback for "created" if ctime is older than mtime
        ctime = stats.st_ctime
        mtime = stats.st_mtime
        best_ctime = ctime if ctime <= mtime else mtime  # Heuristic
        metadata["created"] = datetime.datetime.fromtimestamp(
            best_ctime
        ).strftime("%Y-%m-%d %H:%M:%S")

        try:
            # Attempt to read as text, fallback for binary or encoding issues
            with p.open("r", encoding="utf-8", errors="ignore") as f:
                content = f.read()
                metadata["line_count"] = len(content.splitlines())
        except (OSError, UnicodeDecodeError) as read_err:
            # Handle cases where reading might fail (binary file, permissions etc.)
            console.print(
                f"[yellow]⚠ Warning:[/yellow] Could not read content/count lines for [cyan]{file_path}[/cyan]: [red]{read_err}[/red]"
            )
            metadata["line_count"] = 0  # Indicate unreadable or binary

    except Exception as e:
        console.print(
            f"[yellow]⚠ Warning:[/yellow] Could not get complete metadata for [cyan]{file_path}[/cyan]: [red]{e}[/red]"
        )

    return metadata


# --- Rich Formatting Utilities ---


def print_header(title: str, style: str = "blue") -> None:
    """Print a styled header with a panel."""
    console.rule()
    console.print(
        Panel.fit(f"[bold {style}]{title}[/bold {style}]", border_style=style)
    )


def print_subheader(title: str, style: str = "cyan") -> None:
    """Print a styled subheader."""
    console.print(f"[bold {style}]== {title} ==[/bold {style}]")


def print_success(message: str) -> None:
    """Print a success message."""
    console.print(f"[bold green]✓[/bold green] {message}")


def print_error(message: str, exit_code: int | None = None) -> None:
    """Print an error message and optionally exit."""
    console.print(f"[bold red]✗ ERROR:[/bold red] {message}")
    if exit_code is not None:
        exit(exit_code)


def print_warning(message: str) -> None:
    """Print a warning message."""
    console.print(f"[yellow]⚠ Warning:[/yellow] {message}")


def create_progress() -> Progress:
    """Create a standardized progress bar."""
    return Progress(
        SpinnerColumn(),
        TextColumn("[bold blue]{task.description}"),
        BarColumn(complete_style="green", finished_style="green"),
        TextColumn("[bold]{task.completed}/{task.total}"),
        console=console,
    )


def create_task_table(title: str) -> Table:
    """Create a standardized table for displaying task information."""
    table = Table(
        title=title, show_header=True, header_style="bold cyan", box=box.ROUNDED
    )
    return table


def print_file_stats(files: list[str], title: str = "File Statistics") -> None:
    """Print statistics about a list of files."""
    if not files:
        console.print("[yellow]No files found to display statistics.[/yellow]")
        return

    table = Table(title=title, show_header=True, header_style="bold magenta")
    table.add_column("Statistic", style="cyan")
    table.add_column("Value", style="green")

    extensions = {Path(f).suffix.lower() for f in files if Path(f).suffix}
    total_size = sum(get_file_metadata(f).get("size_bytes", 0) for f in files)
    total_lines = sum(get_file_metadata(f).get("line_count", 0) for f in files)

    table.add_row("Total Files", str(len(files)))
    table.add_row("Total Size", f"{total_size / 1024:.2f} KB")
    table.add_row("Total Lines", str(total_lines))
    table.add_row(
        "Extensions", ", ".join(sorted(extensions)) if extensions else "None"
    )

    console.print(table)
```

### src\flock\cli\view_results.py

- **Lines**: 29
- **Last modified**: 2025-04-16 00:11:15

```py
"""View execution results and history.

This module provides functionality to view the results of previous Flock executions.
"""

from rich.console import Console
from rich.panel import Panel

from flock.core.flock import Flock
from flock.core.util.cli_helper import init_console

# Create console instance
console = Console()


def view_results(flock: Flock):
    """View execution results for a Flock instance.

    Args:
        flock: The Flock instance to view results for
    """
    init_console()
    console.print(Panel("[bold green]View Results[/]"), justify="center")
    console.print(
        "[yellow]Results history functionality not yet implemented.[/]"
    )
    console.print(
        "This feature will allow viewing and filtering past execution results."
    )
```

### src\flock\cli\yaml_editor.py

- **Lines**: 396
- **Last modified**: 2025-04-16 00:11:15

```py
"""YAML Editor for Flock CLI.

This module provides functionality to view, edit, and validate YAML configurations
for Flock and FlockAgent instances.
"""

import os
import subprocess
import tempfile
from pathlib import Path

import questionary
import yaml
from rich.console import Console
from rich.panel import Panel
from rich.syntax import Syntax
from rich.table import Table

from flock.core.flock import Flock
from flock.core.flock_agent import FlockAgent
from flock.core.util.cli_helper import init_console

# Create console instance
console = Console()


def yaml_editor(flock_or_agent: Flock | FlockAgent | None = None):
    """YAML Editor main entry point.

    Args:
        flock_or_agent: Optional Flock or FlockAgent instance to edit
    """
    init_console()
    console.print(Panel("[bold green]YAML Editor[/]"), justify="center")

    if flock_or_agent is None:
        # If no object provided, provide options to load from file
        _yaml_file_browser()
        return

    while True:
        init_console()
        console.print(Panel("[bold green]YAML Editor[/]"), justify="center")

        # Determine object type
        if isinstance(flock_or_agent, Flock):
            obj_type = "Flock"
            console.print(
                f"Editing [bold cyan]Flock[/] with {len(flock_or_agent._agents)} agents"
            )
        elif isinstance(flock_or_agent, FlockAgent):
            obj_type = "FlockAgent"
            console.print(
                f"Editing [bold cyan]FlockAgent[/]: {flock_or_agent.name}"
            )
        else:
            console.print("[bold red]Error: Unknown object type[/]")
            input("\nPress Enter to continue...")
            return

        console.line()

        choice = questionary.select(
            "What would you like to do?",
            choices=[
                questionary.Separator(line=" "),
                "View Current YAML",
                "Edit YAML Directly",
                "Abstract Editor (Visual)",
                "Validate YAML",
                "Save to File",
                questionary.Separator(),
                "Back to Main Menu",
            ],
        ).ask()

        if choice == "View Current YAML":
            _view_yaml(flock_or_agent)
        elif choice == "Edit YAML Directly":
            flock_or_agent = _edit_yaml_directly(flock_or_agent)
        elif choice == "Abstract Editor (Visual)":
            flock_or_agent = _abstract_editor(flock_or_agent)
        elif choice == "Validate YAML":
            _validate_yaml(flock_or_agent)
        elif choice == "Save to File":
            _save_to_file(flock_or_agent)
        elif choice == "Back to Main Menu":
            break

        if choice != "Back to Main Menu":
            input("\nPress Enter to continue...")


def _yaml_file_browser():
    """Browser for YAML files to load."""
    console.print("\n[bold]YAML File Browser[/]")
    console.line()

    current_dir = os.getcwd()
    console.print(f"Current directory: [cyan]{current_dir}[/]")

    # List .yaml/.yml files in current directory
    yaml_files = list(Path(current_dir).glob("*.yaml")) + list(
        Path(current_dir).glob("*.yml")
    )

    if not yaml_files:
        console.print("[yellow]No YAML files found in current directory.[/]")
        input("\nPress Enter to continue...")
        return

    # Display files
    table = Table(title="YAML Files")
    table.add_column("Filename", style="cyan")
    table.add_column("Size", style="green")
    table.add_column("Last Modified", style="yellow")

    for file in yaml_files:
        table.add_row(
            file.name, f"{file.stat().st_size} bytes", f"{file.stat().st_mtime}"
        )

    console.print(table)

    # TODO: Add file selection and loading


def _view_yaml(obj: Flock | FlockAgent):
    """View the YAML representation of an object.

    Args:
        obj: The object to view as YAML
    """
    yaml_str = obj.to_yaml()

    # Add file path information header if it's a Flock with component file paths
    if isinstance(obj, Flock) and hasattr(obj, "_component_file_paths"):
        has_file_paths = bool(getattr(obj, "_component_file_paths", {}))
        if has_file_paths:
            console.print(
                "[bold yellow]Note: This Flock contains components with file paths[/]"
            )

    # Display with syntax highlighting
    syntax = Syntax(
        yaml_str,
        "yaml",
        theme="monokai",
        line_numbers=True,
        code_width=100,
        word_wrap=True,
    )

    init_console()
    console.print(Panel("[bold green]YAML View[/]"), justify="center")
    console.print(syntax)

    # Show file path information if available
    if isinstance(obj, Flock):
        # Get registry for checking file paths
        try:
            from flock.core.flock_registry import get_registry

            registry = get_registry()

            if (
                hasattr(registry, "_component_file_paths")
                and registry._component_file_paths
            ):
                # Get component names in this Flock
                components = set()
                for agent in obj._agents.values():
                    if hasattr(agent, "module") and agent.module:
                        module_path = getattr(agent.module, "module_path", None)
                        if module_path:
                            components.add(module_path)

                # Show file paths for components in this Flock
                file_paths = []
                for component_name in components:
                    if component_name in registry._component_file_paths:
                        file_paths.append(
                            (
                                component_name,
                                registry._component_file_paths[component_name],
                            )
                        )

                if file_paths:
                    console.print("\n[bold cyan]Component File Paths:[/]")
                    table = Table()
                    table.add_column("Component", style="green")
                    table.add_column("File Path", style="yellow")

                    for component_name, file_path in file_paths:
                        table.add_row(component_name, file_path)

                    console.print(table)
        except ImportError:
            pass  # Skip if registry is not available


def _edit_yaml_directly(obj: Flock | FlockAgent) -> Flock | FlockAgent:
    """Edit the YAML representation directly using an external editor.

    Args:
        obj: The object to edit

    Returns:
        The updated object
    """
    # Convert to YAML
    yaml_str = obj.to_yaml()

    # Get file path information if it's a Flock
    component_file_paths = {}
    if isinstance(obj, Flock):
        try:
            from flock.core.flock_registry import get_registry

            registry = get_registry()

            if hasattr(registry, "_component_file_paths"):
                # Save the file paths to restore later
                component_file_paths = registry._component_file_paths.copy()
        except ImportError:
            pass

    # Create a temporary file
    with tempfile.NamedTemporaryFile(
        suffix=".yaml", mode="w+", delete=False
    ) as tmp:
        tmp.write(yaml_str)
        tmp_path = tmp.name

    try:
        # Determine which editor to use
        editor = os.environ.get(
            "EDITOR", "notepad" if os.name == "nt" else "nano"
        )

        # Open the editor
        console.print(
            f"\nOpening {editor} to edit YAML. Save and exit when done."
        )
        subprocess.call([editor, tmp_path])

        # Read updated YAML
        with open(tmp_path) as f:
            updated_yaml = f.read()

        # Parse back to object
        try:
            if isinstance(obj, Flock):
                updated_obj = Flock.from_yaml(updated_yaml)

                # Restore file path information
                if component_file_paths:
                    from flock.core.flock_registry import get_registry

                    registry = get_registry()

                    if not hasattr(registry, "_component_file_paths"):
                        registry._component_file_paths = {}

                    # Merge the updated registry with the saved file paths
                    for (
                        component_name,
                        file_path,
                    ) in component_file_paths.items():
                        if component_name in registry._components:
                            registry._component_file_paths[component_name] = (
                                file_path
                            )

                console.print("\n[green]✓[/] YAML parsed successfully!")
                return updated_obj
            elif isinstance(obj, FlockAgent):
                updated_obj = FlockAgent.from_yaml(updated_yaml)
                console.print("\n[green]✓[/] YAML parsed successfully!")
                return updated_obj
        except Exception as e:
            console.print(f"\n[bold red]Error parsing YAML:[/] {e!s}")
            console.print("\nKeeping original object.")
            return obj

    finally:
        # Clean up the temporary file
        try:
            os.unlink(tmp_path)
        except Exception:
            pass

    return obj


def _abstract_editor(obj: Flock | FlockAgent) -> Flock | FlockAgent:
    """Edit object using an abstract form-based editor.

    Args:
        obj: The object to edit

    Returns:
        The updated object
    """
    console.print("\n[yellow]Abstract visual editor not yet implemented.[/]")
    console.print("Will provide a form-based editor for each field.")

    # For now, just return the original object
    return obj


def _validate_yaml(obj: Flock | FlockAgent):
    """Validate the YAML representation of an object.

    Args:
        obj: The object to validate
    """
    try:
        yaml_str = obj.to_yaml()

        # Attempt to parse with PyYAML
        yaml.safe_load(yaml_str)

        # Attempt to deserialize back to object
        if isinstance(obj, Flock):
            Flock.from_yaml(yaml_str)
        elif isinstance(obj, FlockAgent):
            FlockAgent.from_yaml(yaml_str)

        console.print("\n[green]✓[/] YAML validation successful!")
    except Exception as e:
        console.print(f"\n[bold red]YAML validation failed:[/] {e!s}")


def _save_to_file(obj: Flock | FlockAgent):
    """Save object to a YAML file.

    Args:
        obj: The object to save
    """
    # Determine default filename based on object type
    if isinstance(obj, Flock):
        default_name = "my_flock.flock.yaml"
    elif isinstance(obj, FlockAgent):
        default_name = f"{obj.name}.agent.yaml"
    else:
        default_name = "unknown.yaml"

    # Get file path
    file_path = questionary.text(
        "Enter file path to save YAML:",
        default=default_name,
    ).ask()

    # Ensure the file has the correct extension
    if not file_path.endswith((".yaml", ".yml")):
        file_path += ".yaml"

    # Create directory if it doesn't exist
    save_path = Path(file_path)
    save_path.parent.mkdir(parents=True, exist_ok=True)

    # For Flock instances, ask about path_type
    path_type = "absolute"  # Default
    if isinstance(obj, Flock):
        path_type_choice = questionary.select(
            "How should file paths be formatted?",
            choices=[
                "absolute (full paths, best for local use)",
                "relative (relative paths, better for sharing)",
            ],
            default="absolute (full paths, best for local use)",
        ).ask()

        # Extract just the first word
        path_type = path_type_choice.split()[0]

        console.print(
            f"[bold]Path type selected: [green]{path_type}[/green][/bold]"
        )

    try:
        # Save to file with path_type for Flock instances
        if isinstance(obj, Flock):
            obj.to_yaml_file(file_path, path_type=path_type)
            console.print(
                f"\n[green]✓[/] Saved to {file_path} with {path_type} paths"
            )
        else:
            # For FlockAgent or other types, use the original method
            with open(file_path, "w") as f:
                f.write(obj.to_yaml())
            console.print(f"\n[green]✓[/] Saved to {file_path}")
    except Exception as e:
        console.print(f"\n[bold red]Error saving file:[/] {e!s}")
```

### src\flock\config.py

- **Lines**: 56
- **Last modified**: 2025-05-21 19:51:15

```py
# flock/config.py
import os

from decouple import config

from flock.core.logging.telemetry import TelemetryConfig

cfg_file = os.path.expanduser(f"~/.flock/flock.cfg")


# -- Connection and External Service Configurations --
TEMPORAL_SERVER_URL = config("TEMPORAL_SERVER_URL", "localhost:7233")
DEFAULT_MODEL = config("DEFAULT_MODEL", "openai/gpt-4o")


# API Keys and related settings
TAVILY_API_KEY = config("TAVILY_API_KEY", "")
GITHUB_PAT = config("GITHUB_PAT", "")
GITHUB_REPO = config("GITHUB_REPO", "")
GITHUB_USERNAME = config("GITHUB_USERNAME", "")

# -- Debugging and Logging Configurations --
LOCAL_DEBUG = config("LOCAL_DEBUG", True)
LOG_LEVEL = config("LOG_LEVEL", "DEBUG")
LOGGING_DIR = config("LOGGING_DIR", ".flock/logs")

OTEL_SERVICE_NAME = config("OTL_SERVICE_NAME", "otel-flock")
JAEGER_ENDPOINT = config(
    "JAEGER_ENDPOINT", "http://localhost:14268/api/traces"
)  # Default gRPC endpoint for Jaeger
JAEGER_TRANSPORT = config(
    "JAEGER_TRANSPORT", "http"
).lower()  # Options: "grpc" or "http"
OTEL_SQL_DATABASE_NAME = config("OTEL_SQL_DATABASE", "flock_events.db")
OTEL_FILE_NAME = config("OTEL_FILE_NAME", "flock_events.jsonl")
OTEL_ENABLE_SQL: bool = config("OTEL_ENABLE_SQL", True) == "True"
OTEL_ENABLE_FILE: bool = config("OTEL_ENABLE_FILE", True) == "True"
OTEL_ENABLE_JAEGER: bool = config("OTEL_ENABLE_JAEGER", False) == "True"
OTEL_ENABLE_OTLP: bool = config("OTEL_ENABLE_OTLP", False) == "True"
OTEL_EXPORTER_OTLP_PROTOCOL: str = config("OTEL_EXPORTER_OTLP_PROTOCOL", "grpc")
OTEL_EXPORTER_OTLP_ENDPOINT: str = config("OTEL_EXPORTER_OTLP_ENDPOINT", "http://localhost:4317")

TELEMETRY = TelemetryConfig(
    OTEL_SERVICE_NAME,
    JAEGER_ENDPOINT,
    JAEGER_TRANSPORT,
    LOGGING_DIR,
    OTEL_FILE_NAME,
    OTEL_SQL_DATABASE_NAME,
    OTEL_ENABLE_JAEGER,
    OTEL_ENABLE_FILE,
    OTEL_ENABLE_SQL,
    OTEL_ENABLE_OTLP,
    OTEL_EXPORTER_OTLP_PROTOCOL,
    OTEL_EXPORTER_OTLP_ENDPOINT,
)
```

### src\flock\core\api\__init__.py

- **Lines**: 10
- **Last modified**: 2025-05-21 19:51:15

```py
# src/flock/core/api/__init__.py
"""Flock API Server components."""


from .models import FlockAPIRequest, FlockAPIResponse

__all__ = [
    "FlockAPIRequest",
    "FlockAPIResponse",
]
```

### src\flock\core\api\custom_endpoint.py

- **Lines**: 45
- **Last modified**: 2025-05-21 19:51:15

```py
"""Lightweight helper object for declaring additional REST routes.

Developers can pass instances of :class:`FlockEndpoint` to
``Flock.start_api(custom_endpoints=[...])`` instead of the terse dictionary
syntax.  The class carries optional Pydantic request/response models plus
OpenAPI metadata so the generated docs look perfect.
"""
from __future__ import annotations

from collections.abc import Callable
from typing import Any

from pydantic import BaseModel

__all__ = [
    "FlockEndpoint",
]


class FlockEndpoint(BaseModel):
    """Declarative description of an extra API route."""

    path: str
    methods: list[str] = ["GET"]
    callback: Callable[..., Any]

    # Optional schema models
    request_model: type[BaseModel] | None = None
    response_model: type[BaseModel] | None = None
    # Query-string parameters as a Pydantic model (treated as Depends())
    query_model: type[BaseModel] | None = None

    # OpenAPI / Swagger metadata
    summary: str | None = None
    description: str | None = None
    name: str | None = None  # Route name in FastAPI
    include_in_schema: bool = True

    # FastAPI dependency injections (e.g. security)
    dependencies: list[Any] | None = None

    model_config = {
        "arbitrary_types_allowed": True,
        "validate_default": True,
    }
```

### src\flock\core\api\endpoints.py

- **Lines**: 254
- **Last modified**: 2025-05-21 19:51:15

```py
# src/flock/core/api/endpoints.py
"""FastAPI endpoints for the Flock API, using FastAPI's dependency injection."""

import uuid
from typing import TYPE_CHECKING

from fastapi import (
    APIRouter,
    BackgroundTasks,
    Depends,  # Crucial import for dependency injection
    HTTPException,
)

from flock.core.logging.logging import get_logger

# Import the dependency provider functions
# This assumes you'll create 'src/flock/webapp/app/dependencies.py' (or similar)
# with get_flock_instance and get_run_store functions.
# For now, to make this file runnable in isolation for generation,
# we might need temporary stubs or to adjust the import path later.
# Let's assume a common location for dependencies.
from flock.webapp.app.dependencies import get_flock_instance, get_run_store

from .models import (
    FlockAPIRequest,
    FlockAPIResponse,
    FlockBatchRequest,
    FlockBatchResponse,
)

if TYPE_CHECKING:
    from flock.core.flock import Flock

    from .run_store import RunStore
    # We also need the _run_flock, _run_batch, _type_convert_inputs methods.
    # These will be part of the Flock instance itself or helper functions
    # that take Flock/RunStore as arguments. For now, let's assume they
    # are methods on the Flock instance or can be called statically with it.

logger = get_logger("api.endpoints")

# The create_api_router function no longer needs to be passed the FlockAPI instance
# as dependencies will be injected directly into the route handlers.
def create_api_router() -> APIRouter:
    """Creates the APIRouter for core Flock API endpoints."""
    router = APIRouter()

    # --- API Endpoints ---
    @router.post("/run/flock", response_model=FlockAPIResponse, tags=["Flock API Core"])
    async def run_flock_workflow(
        request_model: FlockAPIRequest, # Renamed from 'request' to avoid conflict with FastAPI's Request
        background_tasks: BackgroundTasks,
        flock_instance: "Flock" = Depends(get_flock_instance), # Injected
        run_store: "RunStore" = Depends(get_run_store)          # Injected
    ):
        """Run a flock workflow starting with the specified agent."""
        run_id = None
        try:
            run_id = str(uuid.uuid4())
            run_store.create_run(run_id)
            response_data = run_store.get_run(run_id)

            if not response_data:
                logger.error(f"Failed to create run record for run_id: {run_id}")
                raise HTTPException(status_code=500, detail="Failed to initialize run.")

            processed_inputs = request_model.inputs if request_model.inputs else {}
            logger.info(
                f"API request: run flock '{request_model.agent_name}' (run_id: {run_id})",
                inputs=processed_inputs,
            )

            # The _run_flock logic now needs to be called. It could be a method on
            # flock_instance, or a static/helper function that takes flock_instance.
            # Let's assume it's a method on flock_instance for now.
            # To avoid direct calls to private-like methods from router,
            # Flock class would expose a public method that handles this internal logic.
            # For this refactoring step, let's assume a helper `_execute_flock_run` exists
            # that we can call.

            async def _execute_flock_run_task(run_id_task, agent_name_task, inputs_task):
                # This is a simplified version of what was in FlockAPI._run_flock
                try:
                    if agent_name_task not in flock_instance.agents:
                        raise ValueError(f"Starting agent '{agent_name_task}' not found")
                    # Type conversion would ideally be part of the Flock's run logic
                    # or a utility. For now, assume it's handled or simple.
                    # typed_inputs = flock_instance._type_convert_inputs(agent_name_task, inputs_task) # Example if kept on Flock
                    typed_inputs = inputs_task # Simplified for now

                    result = await flock_instance.run_async(
                        start_agent=agent_name_task, input=typed_inputs
                    )
                    run_store.update_run_result(run_id_task, result)
                except Exception as e_task:
                    logger.error(f"Error in background flock run {run_id_task}: {e_task!s}", exc_info=True)
                    run_store.update_run_status(run_id_task, "failed", str(e_task))


            if request_model.async_run:
                logger.debug(
                    f"Running flock '{request_model.agent_name}' asynchronously (run_id: {run_id})"
                )
                background_tasks.add_task(
                    _execute_flock_run_task,
                    run_id,
                    request_model.agent_name,
                    processed_inputs,
                )
                run_store.update_run_status(run_id, "running")
                response_data.status = "running"
            else:
                logger.debug(
                    f"Running flock '{request_model.agent_name}' synchronously (run_id: {run_id})"
                )
                await _execute_flock_run_task(
                     run_id, request_model.agent_name, processed_inputs
                )
                response_data = run_store.get_run(run_id)

            if not response_data:
                 logger.error(f"Run data lost for run_id: {run_id} after execution.")
                 raise HTTPException(status_code=500, detail="Run data lost after execution.")

            return response_data
        except ValueError as ve:
            logger.error(f"Value error starting run for agent '{request_model.agent_name}': {ve}", exc_info=True)
            if run_id: run_store.update_run_status(run_id, "failed", str(ve))
            raise HTTPException(status_code=400, detail=str(ve))
        except Exception as e:
            error_msg = f"Internal server error during flock run: {type(e).__name__}"
            logger.error(f"Error starting flock run: {e!s}", exc_info=True)
            if run_id: run_store.update_run_status(run_id, "failed", error_msg)
            raise HTTPException(status_code=500, detail=error_msg)

    @router.post("/run/batch", response_model=FlockBatchResponse, tags=["Flock API Core"])
    async def run_batch_workflow(
        request_model: FlockBatchRequest, # Renamed from 'request'
        background_tasks: BackgroundTasks,
        flock_instance: "Flock" = Depends(get_flock_instance),
        run_store: "RunStore" = Depends(get_run_store)
    ):
        """Run a batch of inputs through the flock workflow."""
        batch_id = None
        try:
            if request_model.agent_name not in flock_instance.agents:
                raise ValueError(f"Agent '{request_model.agent_name}' not found in current Flock.")

            if isinstance(request_model.batch_inputs, list) and not request_model.batch_inputs:
                raise ValueError("Batch inputs list cannot be empty if provided as a list.")

            batch_id = str(uuid.uuid4())
            run_store.create_batch(batch_id)
            response_data = run_store.get_batch(batch_id)

            if not response_data:
                logger.error(f"Failed to create batch record for batch_id: {batch_id}")
                raise HTTPException(status_code=500, detail="Failed to initialize batch run.")

            batch_size = (
                len(request_model.batch_inputs)
                if isinstance(request_model.batch_inputs, list)
                else "CSV/DataFrame"
            )
            logger.info(
                f"API request: run batch with '{request_model.agent_name}' (batch_id: {batch_id})",
                batch_size=batch_size,
            )

            async def _execute_flock_batch_task(batch_id_task, batch_request_task: FlockBatchRequest):
                # This is a simplified version of what was in FlockAPI._run_batch
                try:
                    # Directly use the flock_instance's run_batch_async method
                    results = await flock_instance.run_batch_async(
                        start_agent=batch_request_task.agent_name,
                        batch_inputs=batch_request_task.batch_inputs,
                        input_mapping=batch_request_task.input_mapping,
                        static_inputs=batch_request_task.static_inputs,
                        parallel=batch_request_task.parallel,
                        max_workers=batch_request_task.max_workers,
                        use_temporal=batch_request_task.use_temporal,
                        box_results=batch_request_task.box_results,
                        return_errors=batch_request_task.return_errors,
                        silent_mode=True, # API batch runs should be internally silent
                        write_to_csv=None # API handles CSV writing based on request if needed, not here
                    )
                    run_store.update_batch_result(batch_id_task, results)
                    logger.info(f"Batch run completed (batch_id: {batch_id_task})", num_results=len(results))
                except Exception as e_task:
                    logger.error(f"Error in background flock batch {batch_id_task}: {e_task!s}", exc_info=True)
                    run_store.update_batch_status(batch_id_task, "failed", str(e_task))

            logger.debug(
                f"Running batch with '{request_model.agent_name}' asynchronously (batch_id: {batch_id})"
            )
            background_tasks.add_task(
                _execute_flock_batch_task,
                batch_id,
                request_model, # Pass the Pydantic request model
            )
            run_store.update_batch_status(batch_id, "running")
            response_data.status = "running"

            return response_data
        except ValueError as ve:
            error_msg = f"Value error starting batch for agent '{request_model.agent_name}': {ve}"
            logger.error(error_msg, exc_info=True)
            if batch_id: run_store.update_batch_status(batch_id, "failed", str(ve))
            raise HTTPException(status_code=400, detail=str(ve))
        except Exception as e:
            error_msg = f"Internal server error during batch run: {type(e).__name__}: {e!s}"
            logger.error(error_msg, exc_info=True)
            if batch_id: run_store.update_batch_status(batch_id, "failed", error_msg)
            raise HTTPException(status_code=500, detail=error_msg)

    @router.get("/run/{run_id}", response_model=FlockAPIResponse, tags=["Flock API Core"])
    async def get_run_status(
        run_id: str,
        run_store: "RunStore" = Depends(get_run_store)
    ):
        """Get the status of a specific run."""
        logger.debug(f"API request: get status for run_id: {run_id}")
        run_data = run_store.get_run(run_id)
        if not run_data:
            logger.warning(f"Run ID not found: {run_id}")
            raise HTTPException(status_code=404, detail="Run not found")
        return run_data

    @router.get("/batch/{batch_id}", response_model=FlockBatchResponse, tags=["Flock API Core"])
    async def get_batch_status(
        batch_id: str,
        run_store: "RunStore" = Depends(get_run_store)
    ):
        """Get the status and results of a specific batch run."""
        logger.debug(f"API request: get status for batch_id: {batch_id}")
        batch_data = run_store.get_batch(batch_id)
        if not batch_data:
            logger.warning(f"Batch ID not found: {batch_id}")
            raise HTTPException(status_code=404, detail="Batch not found")
        return batch_data

    @router.get("/agents", tags=["Flock API Core"])
    async def list_agents(
        flock_instance: "Flock" = Depends(get_flock_instance)
    ):
        """List all available agents in the currently loaded Flock."""
        logger.debug("API request: list agents")
        agents_list = [
            {"name": agent.name, "description": agent.resolved_description or agent.name}
            for agent in flock_instance.agents.values()
        ]
        return {"agents": agents_list}

    return router
```

### src\flock\core\api\runner.py

- **Lines**: 44
- **Last modified**: 2025-05-21 19:51:15

```py
# src/flock/api/runner.py
"""Provides functionality to start the Flock API server."""

from collections.abc import Callable, Sequence
from typing import TYPE_CHECKING, Any

from flock.core.api.custom_endpoint import FlockEndpoint
from flock.core.logging.logging import get_logger

if TYPE_CHECKING:
    from flock.core.flock import Flock

logger = get_logger("api.runner")


def start_flock_api(
    flock: "Flock",
    host: str = "127.0.0.1",
    port: int = 8344,
    server_name: str = "Flock API",
    create_ui: bool = False,
    custom_endpoints: Sequence[FlockEndpoint] | dict[tuple[str, list[str] | None], Callable[..., Any]] | None = None,
) -> None:
    """Start a REST API server for the given Flock instance."""
    try:
        # Import API class locally to avoid making it a hard dependency for core flock
        from flock.core.api import FlockAPI
    except ImportError:
        logger.error(
            "API components not found. Cannot start API. "
            "Ensure 'fastapi' and 'uvicorn' are installed."
        )
        return

    logger.info(
        f"Preparing to start API server for Flock '{flock.name}' on {host}:{port} {'with UI' if create_ui else 'without UI'}"
    )
    api_instance = FlockAPI(flock, custom_endpoints=custom_endpoints)  # Pass the Flock instance to the API
    api_instance.start(
        host=host,
        port=port,
        server_name=server_name,
        create_ui=create_ui,
    )
```

### src\flock\core\api\service.py

- **Lines**: 214
- **Last modified**: 2025-05-21 19:51:15

```py
# flock/core/api/service.py
from typing import TYPE_CHECKING, Any

if TYPE_CHECKING:
    from flock.core.api.endpoints import FlockBatchRequest
    from flock.core.api.run_store import RunStore
    from flock.core.flock import Flock


from flock.core.logging.logging import get_logger

logger = get_logger("flock.api")

class FlockApiService:
    def __init__(self, flock_instance: "Flock", run_store_instance: "RunStore"):
        self.flock = flock_instance
        self.run_store = run_store_instance
    # You would move the _run_flock, _run_batch, _type_convert_inputs methods here
    # from the old FlockAPI class.
    async def _run_flock(
        self, run_id: str, agent_name: str, inputs: dict[str, Any]
    ):
        """Executes a flock workflow run (internal helper)."""
        try:
            if agent_name not in self.flock.agents:
                raise ValueError(f"Starting agent '{agent_name}' not found")

            typed_inputs = self._type_convert_inputs(agent_name, inputs)

            logger.debug(
                f"Executing flock workflow starting with '{agent_name}' (run_id: {run_id})",
                inputs=typed_inputs,
            )
            # Flock.run_async now handles context creation and execution
            result = await self.flock.run_async(
                start_agent=agent_name, input=typed_inputs
            )
            self.run_store.update_run_result(run_id, result)

            final_agent_name = (
                result.get("agent_name", "N/A") if isinstance(result, dict) else "N/A"
            ) # Handle if result is not a dict (e.g. Box)
            logger.info(
                f"Flock workflow completed (run_id: {run_id})",
                final_agent=final_agent_name,
            )
        except Exception as e:
            logger.error(
                f"Error in flock run {run_id} (started with '{agent_name}'): {e!s}",
                exc_info=True,
            )
            self.run_store.update_run_status(run_id, "failed", str(e))
            raise

    async def _run_batch(self, batch_id: str, request: "FlockBatchRequest"):
        """Executes a batch of runs (internal helper)."""
        try:
            if request.agent_name not in self.flock.agents:
                raise ValueError(f"Agent '{request.agent_name}' not found")

            logger.debug(
                f"Executing batch run starting with '{request.agent_name}' (batch_id: {batch_id})",
                batch_size=len(request.batch_inputs)
                if isinstance(request.batch_inputs, list)
                else "CSV/DataFrame",
            )

            # --- Re-integrating the threaded batch execution from Flock.run_batch_async ---
            import asyncio
            import threading
            from concurrent.futures import ThreadPoolExecutor

            def run_batch_sync_in_thread():
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                try:
                    batch_size = (
                        len(request.batch_inputs)
                        if isinstance(request.batch_inputs, list)
                        else 0 # Or attempt to get from DataFrame/CSV load
                    )
                    if batch_size > 0:
                        self.run_store.set_batch_total_items(batch_id, batch_size)

                    class ProgressTracker:
                        def __init__(self, store, b_id, total_size):
                            self.store, self.batch_id, self.total_size = store, b_id, total_size
                            self.current_count, self.partial_results, self._lock = 0, [], threading.Lock()
                        def increment(self, res=None):
                            with self._lock:
                                self.current_count += 1
                                if res is not None: self.partial_results.append(res)
                                try: self.store.update_batch_progress(self.batch_id, self.current_count, self.partial_results)
                                except Exception as e_prog: logger.error(f"Error updating progress: {e_prog}")
                                return self.current_count

                    progress_tracker = ProgressTracker(self.run_store, batch_id, batch_size)

                    async def progress_aware_worker(index, item_inputs):
                        try:
                            # Call Flock's run_async for a single item
                            item_result = await self.flock.run_async(
                                start_agent=request.agent_name,
                                input=item_inputs,
                                box_result=request.box_results,
                            )
                            progress_tracker.increment(item_result)
                            return item_result
                        except Exception as item_err:
                            logger.error(f"Error processing batch item {index}: {item_err}")
                            progress_tracker.increment(item_err if request.return_errors else None)
                            if request.return_errors: return item_err
                            return None

                    batch_inputs_list = request.batch_inputs
                    actual_results_list = []

                    if isinstance(batch_inputs_list, list):
                        tasks = []
                        for i, item_inputs in enumerate(batch_inputs_list):
                            full_inputs = {**(request.static_inputs or {}), **item_inputs}
                            tasks.append(progress_aware_worker(i, full_inputs))

                        if request.parallel and request.max_workers > 1:
                            semaphore = asyncio.Semaphore(request.max_workers)
                            async def bounded_worker(idx, inputs_item):
                                async with semaphore: return await progress_aware_worker(idx, inputs_item)
                            bounded_tasks = [bounded_worker(i, {**(request.static_inputs or {}), **item}) for i, item in enumerate(batch_inputs_list)]
                            actual_results_list = loop.run_until_complete(asyncio.gather(*bounded_tasks, return_exceptions=request.return_errors))
                        else:
                            for i, item_inputs in enumerate(batch_inputs_list):
                                full_inputs = {**(request.static_inputs or {}), **item_inputs}
                                actual_results_list.append(loop.run_until_complete(progress_aware_worker(i, full_inputs)))
                    else: # DataFrame or CSV path - let Flock's batch processor handle this directly
                        # This path relies on self.flock.run_batch_async being able to run within this new event loop.
                        # It might be simpler to always convert DataFrame/CSV to list of dicts before this point.
                        actual_results_list = loop.run_until_complete(
                            self.flock.run_batch_async(
                                start_agent=request.agent_name,
                                batch_inputs=request.batch_inputs, # DataFrame or path
                                input_mapping=request.input_mapping,
                                static_inputs=request.static_inputs,
                                parallel=request.parallel, # Will be re-evaluated by internal BatchProcessor
                                max_workers=request.max_workers,
                                use_temporal=request.use_temporal, # Will be re-evaluated
                                box_results=request.box_results,
                                return_errors=request.return_errors,
                                silent_mode=True, # Internal batch runs silently for API
                                write_to_csv=None # API handles CSV output separately if needed
                            )
                        )
                        # Progress for DataFrame/CSV would need integration into BatchProcessor or this loop
                        if actual_results_list:
                             self.run_store.set_batch_total_items(batch_id, len(actual_results_list))
                             self.run_store.update_batch_progress(batch_id, len(actual_results_list), actual_results_list)


                    self.run_store.update_batch_result(batch_id, actual_results_list)
                    logger.info(f"Batch run completed (batch_id: {batch_id})", num_results=len(actual_results_list))
                    return actual_results_list
                except Exception as thread_err:
                    logger.error(f"Error in batch run thread {batch_id}: {thread_err!s}", exc_info=True)
                    self.run_store.update_batch_status(batch_id, "failed", str(thread_err))
                    return None
                finally:
                    loop.close()
            # --- End of re-integrated threaded batch execution ---

            # Submit the synchronous function to a thread pool from the main event loop
            main_loop = asyncio.get_running_loop()
            with ThreadPoolExecutor(thread_name_prefix="flock-api-batch") as pool:
                await main_loop.run_in_executor(pool, run_batch_sync_in_thread)

        except Exception as e:
            logger.error(
                f"Error setting up batch run {batch_id} (started with '{request.agent_name}'): {e!s}",
                exc_info=True,
            )
            self.run_store.update_batch_status(batch_id, "failed", str(e))
            raise





    def _type_convert_inputs(
        self, agent_name: str, inputs: dict[str, Any]
    ) -> dict[str, Any]:
        """Converts input values (esp. from forms) to expected Python types."""
        typed_inputs = {}
        agent_def = self.flock.agents.get(agent_name)
        if not agent_def or not agent_def.input or not isinstance(agent_def.input, str):
            return inputs # Return original if no spec or spec is not a string

        parsed_fields = self._parse_input_spec(agent_def.input) # Relies on the old UI helper
        field_types = {f["name"]: f["type"] for f in parsed_fields}

        for k, v in inputs.items():
            target_type_str = field_types.get(k)
            if target_type_str:
                if target_type_str.startswith("bool"):
                    typed_inputs[k] = str(v).lower() in ["true", "on", "1", "yes"] if isinstance(v, str) else bool(v)
                elif target_type_str.startswith("int"):
                    try: typed_inputs[k] = int(v)
                    except (ValueError, TypeError): logger.warning(f"Could not convert '{k}' value '{v}' to int for agent '{agent_name}'"); typed_inputs[k] = v
                elif target_type_str.startswith("float"):
                    try: typed_inputs[k] = float(v)
                    except (ValueError, TypeError): logger.warning(f"Could not convert '{k}' value '{v}' to float for agent '{agent_name}'"); typed_inputs[k] = v
                # TODO: Add list/dict parsing (e.g., json.loads) if type_str indicates these,
                # especially if inputs come from HTML forms as strings.
                else: typed_inputs[k] = v
            else:
                typed_inputs[k] = v
        return typed_inputs
```

### src\flock\core\context\context.py

- **Lines**: 214
- **Last modified**: 2025-05-21 19:51:15

```py
import uuid
from dataclasses import asdict
from datetime import datetime
from typing import Any, Literal

from opentelemetry import trace
from pydantic import BaseModel, Field

from flock.core.context.context_vars import FLOCK_LAST_AGENT, FLOCK_LAST_RESULT
from flock.core.logging.logging import get_logger
from flock.core.serialization.serializable import Serializable

logger = get_logger("context")
tracer = trace.get_tracer(__name__)


class AgentRunRecord(BaseModel):
    id: str = Field(default="")
    agent: str = Field(default="")
    data: dict[str, Any] = Field(default_factory=dict)
    timestamp: str = Field(default="")
    hand_off: dict | None = Field(default_factory=dict)
    called_from: str | None = Field(default=None)


class AgentDefinition(BaseModel):
    agent_type: str = Field(default="")
    agent_name: str = Field(default="")
    agent_data: dict = Field(default_factory=dict)
    serializer: Literal["json", "cloudpickle", "msgpack"] = Field(
        default="cloudpickle"
    )


class FlockContext(Serializable, BaseModel):
    state: dict[str, Any] = Field(default_factory=dict)
    history: list[AgentRunRecord] = Field(default_factory=list)
    agent_definitions: dict[str, AgentDefinition] = Field(default_factory=dict)
    run_id: str = Field(default="")
    workflow_id: str = Field(default="")
    workflow_timestamp: str = Field(default="")

    def record(
        self,
        agent_name: str,
        data: dict[str, Any],
        timestamp: str,
        hand_off: str,
        called_from: str,
    ) -> None:
        record = AgentRunRecord(
            id=agent_name + "_" + uuid.uuid4().hex[:4],
            agent=agent_name,
            data=data.copy(),
            timestamp=timestamp,
            hand_off=hand_off,
            called_from=called_from,
        )
        self.history.append(record)
        for key, value in data.items():
            self.set_variable(f"{agent_name}.{key}", value)
        self.set_variable(FLOCK_LAST_RESULT, data)
        self.set_variable(FLOCK_LAST_AGENT, agent_name)
        logger.info(
            f"Agent run recorded - run_id '{record.id}'",
            agent=agent_name,
            timestamp=timestamp,
            data=data,
        )
        current_span = trace.get_current_span()
        if current_span.get_span_context().is_valid:
            current_span.add_event(
                "record",
                attributes={"agent": agent_name, "timestamp": timestamp},
            )

    def get_variable(self, key: str, default: Any = None) -> Any:
        return self.state.get(key, default)

    def set_variable(self, key: str, value: Any) -> None:
        old_value = self.state.get(key)
        self.state[key] = value
        if old_value != value:
            escaped_value = str(value).replace("{", "{{").replace("}", "}}")

            logger.info(
                "Context variable updated - {} -> {}",
                key,
                escaped_value,  # Arguments in order
            )

            current_span = trace.get_current_span()
            if current_span.get_span_context().is_valid:
                current_span.add_event(
                    "set_variable",
                    attributes={
                        "key": key,
                        "old": str(old_value),
                        "new": str(value),
                    },
                )

    def deepcopy(self) -> "FlockContext":
        return FlockContext.from_dict(self.to_dict())

    def get_agent_history(self, agent_name: str) -> list[AgentRunRecord]:
        return [record for record in self.history if record.agent == agent_name]

    def next_input_for(self, agent) -> Any:
        try:
            if hasattr(agent, "input") and isinstance(agent.input, str):
                keys = [k.strip() for k in agent.input.split(",") if k.strip()]
                if len(keys) == 1:
                    return self.get_variable(keys[0])
                else:
                    return {key: self.get_variable(key) for key in keys}
            else:
                return self.get_variable("init_input")
        except Exception as e:
            logger.error(
                "Error getting next input for agent",
                agent=agent.name,
                error=str(e),
            )
            raise

    def get_most_recent_value(self, variable_name: str) -> Any:
        for history_record in reversed(self.history):
            if variable_name in history_record.data:
                return history_record.data[variable_name]

    def get_agent_definition(self, agent_name: str) -> AgentDefinition | None:
        return self.agent_definitions.get(agent_name)

    def get_last_agent_name(self) -> str | None:
        """Returns the name of the agent from the most recent history record."""
        if not self.history:
            return None
        last_record = self.history[-1]
        # The 'called_from' field in the *next* record is the previous agent.
        # However, to get the name of the *last executed agent*, we look at the 'agent' field.
        return last_record.agent

    def add_agent_definition(
        self, agent_type: type, agent_name: str, agent_data: Any
    ) -> None:
        definition = AgentDefinition(
            agent_type=agent_type.__name__,
            agent_name=agent_name,
            agent_data=agent_data,
        )
        self.agent_definitions[agent_name] = definition

    # Use the reactive setter for dict-like access.
    def __getitem__(self, key: str) -> Any:
        return self.get_variable(key)

    def __setitem__(self, key: str, value: Any) -> None:
        self.set_variable(key, value)

    def to_dict(self) -> dict[str, Any]:
        def convert(obj):
            if isinstance(obj, datetime):
                return obj.isoformat()
            if hasattr(obj, "__dataclass_fields__"):
                return asdict(
                    obj, dict_factory=lambda x: {k: convert(v) for k, v in x}
                )
            return obj

        return convert(asdict(self))

    @classmethod
    def from_dict(cls, data: dict[str, Any]) -> "FlockContext":
        def convert(obj):
            if isinstance(obj, dict):
                if "timestamp" in obj:
                    return AgentRunRecord(
                        **{
                            **obj,
                            "timestamp": datetime.fromisoformat(
                                obj["timestamp"]
                            ),
                        }
                    )
                if "agent_type" in obj:
                    return AgentDefinition(**obj)
                return {k: convert(v) for k, v in obj.items()}
            if isinstance(obj, list):
                return [convert(v) for v in obj]
            return obj

        converted = convert(data)
        return cls(**converted)

    def resolve(self, svc_type):
        """Resolve a service from the request-scoped DI container if present.

        The bootstrap code is expected to store the active `ServiceProvider` from
        `wd.di` in the context variable key ``di.container``.  This helper
        provides a convenient façade so that Flock components can simply call
        ``context.resolve(SomeType)`` regardless of whether a container is
        available.  When the container is missing or the service cannot be
        resolved, ``None`` is returned instead of raising to keep backward
        compatibility.
        """
        container = self.get_variable("di.container")
        if container is None:
            return None
        try:
            return container.get_service(svc_type)
        except Exception:
            # Service not registered or other resolution error – fall back to None
            return None
```

### src\flock\core\context\context_manager.py

- **Lines**: 37
- **Last modified**: 2025-04-17 02:15:54

```py
"""Module for managing the FlockContext."""

from flock.core.context.context import FlockContext
from flock.core.context.context_vars import (
    FLOCK_CURRENT_AGENT,
    FLOCK_INITIAL_INPUT,
    FLOCK_LOCAL_DEBUG,
    FLOCK_MODEL,
    FLOCK_RUN_ID,
)


def initialize_context(
    context: FlockContext,
    agent_name: str,
    input_data: dict,
    run_id: str,
    local_debug: bool,
    model: str,
) -> None:
    """Initialize the FlockContext with standard variables before running an agent.

    Args:
        context: The FlockContext instance.
        agent_name: The name of the current agent.
        input_data: A dictionary of inputs for the agent.
        run_id: A unique identifier for the run.
        local_debug: Flag indicating whether local debugging is enabled.
    """
    context.set_variable(FLOCK_CURRENT_AGENT, agent_name)
    for key, value in input_data.items():
        context.set_variable("flock." + key, value)
    context.set_variable(FLOCK_INITIAL_INPUT, input_data)
    context.set_variable(FLOCK_LOCAL_DEBUG, local_debug)
    context.run_id = run_id
    context.set_variable(FLOCK_RUN_ID, run_id)
    context.set_variable(FLOCK_MODEL, model)
```

### src\flock\core\context\context_vars.py

- **Lines**: 10
- **Last modified**: 2025-04-16 00:11:15

```py
"""Context variables for Flock."""

FLOCK_CURRENT_AGENT = "flock.current_agent"
FLOCK_INITIAL_INPUT = "flock.initial_input"
FLOCK_LOCAL_DEBUG = "flock.local_debug"
FLOCK_RUN_ID = "flock.run_id"
FLOCK_LAST_AGENT = "flock.last_agent"
FLOCK_LAST_RESULT = "flock.last_result"
FLOCK_MODEL = "flock.model"
FLOCK_BATCH_SILENT_MODE = "flock.batch_silent"
```

### src\flock\core\evaluation\utils.py

- **Lines**: 395
- **Last modified**: 2025-05-25 23:17:06

```py
# src/flock/core/util/evaluation_helpers.py
import inspect
import sys
from collections.abc import Callable
from pathlib import Path
from typing import Any, Union

import pandas as pd
from box import Box
from datasets import (
    Dataset as HFDataset,
    get_dataset_config_names,
    load_dataset,
)
from opik import Opik
from opik.evaluation import evaluate

from flock.core.flock import Flock
from flock.core.flock_agent import FlockAgent
from flock.core.flock_evaluator import FlockEvaluator
from flock.core.logging.logging import get_logger

# Potentially import metrics libraries like rouge_score, nltk, sentence_transformers

logger_helpers = get_logger("util.evaluation")


def evaluate_with_opik(
    dataset: str | Path | list[dict[str, Any]] | pd.DataFrame | HFDataset,
    dataset_name: str,
    experiment_name: str,
    start_agent: FlockAgent | str,
    input_mapping: dict[str, str],
    answer_mapping: dict[str, str],
    metrics: list[
        str
        | Callable[[Any, Any], bool | float | dict[str, Any]]
        | FlockAgent
        | FlockEvaluator
    ],
):
    df = normalize_dataset(dataset)
    client = Opik()
    dataset = client.get_or_create_dataset(name=dataset_name)

    dataset.insert_from_pandas(dataframe=df, ignore_keys=["source"])

    # Create a single Flock instance outside the task function
    shared_flock = Flock(
        name="opik_eval", model="azure/gpt-4.1", show_flock_banner=False
    )
    shared_flock.add_agent(start_agent)

    def evaluation_task(dataset_item):
        agent_input = {
            value: dataset_item[key] for key, value in input_mapping.items()
        }

        # Use the shared Flock instance instead of creating a new one
        result_flock = shared_flock.run(
            start_agent=start_agent, input=agent_input, box_result=False
        )

        # agent_output = result_flock.get(answer_mapping[key], "No answer found")

        key = next(iter(answer_mapping.keys()))
        reference = dataset_item[key]
        answer = result_flock.get(answer_mapping[key], "No answer found")

        result = {
            "input": agent_input,
            "output": answer,
            "reference": reference,
        }

        return result

    eval_results = evaluate(
        experiment_name=experiment_name,
        dataset=dataset,
        task=evaluation_task,
        scoring_metrics=metrics,
    )


def load_and_merge_all_configs(dataset_name: str) -> pd.DataFrame:
    all_configs = get_dataset_config_names(dataset_name)
    all_dfs = []

    for config in all_configs:
        dataset_dict = load_dataset(dataset_name, config)
        for split_name, split_dataset in dataset_dict.items():
            df = split_dataset.to_pandas()
            df["config"] = config
            df["split"] = split_name
            all_dfs.append(df)

    merged_df = pd.concat(all_dfs, ignore_index=True)
    logger_helpers.info(f"merged_df.head(): {merged_df.head()}")
    return merged_df


def import_hf_dataset_to_opik(dataset_name: str) -> pd.DataFrame:
    df = load_and_merge_all_configs(dataset_name)
    logger_helpers.info(
        f"type(df): {type(df)}"
    )  # ➜ <class 'pandas.core.frame.DataFrame'>
    logger_helpers.info(f"df.shape: {df.shape}")  # e.g. (123456, N_COLUMNS+2)
    logger_helpers.info(
        f"df['split'].value_counts(): {df['split'].value_counts()}"
    )
    logger_helpers.info(f"df['config'].unique(): {df['config'].unique()}")
    client = Opik()
    dataset = client.get_or_create_dataset(name=dataset_name)

    dataset.insert_from_pandas(dataframe=df, ignore_keys=["source"])
    return df


def normalize_dataset(dataset: Any) -> pd.DataFrame:
    """Converts various dataset formats into a pandas DataFrame."""
    if isinstance(dataset, pd.DataFrame):
        return dataset.copy()
    elif isinstance(dataset, str | Path):
        path = Path(dataset)
        if not path.exists():
            try:
                return load_and_merge_all_configs(dataset)
            except Exception as e:
                raise FileNotFoundError(
                    f"Dataset file not found: {path}"
                ) from e
        if path.suffix.lower() == ".csv":
            return pd.read_csv(path)
        # Add support for json, jsonl etc. if needed
        else:
            raise ValueError(
                f"Unsupported file type for dataset: {path.suffix}"
            )
    elif isinstance(dataset, list):
        if not dataset or not isinstance(dataset[0], dict):
            raise ValueError("Dataset list must contain dictionaries.")
        return pd.DataFrame(dataset)
    elif "datasets" in sys.modules and isinstance(
        dataset, sys.modules["datasets"].Dataset
    ):
        # Requires 'datasets' library to be installed
        return dataset.to_pandas()
    else:
        raise TypeError(f"Unsupported dataset type: {type(dataset)}")


def extract_value_by_dot_notation(data: dict | Box, key: str) -> Any:
    """Retrieves a value from a nested dictionary or Box object using dot notation."""
    if not key:
        return None
    keys = key.split(".")
    value = data
    try:
        for k in keys:
            if isinstance(value, (dict, Box)):
                value = value.get(k)
            # Add list index handling if needed: e.g., 'results[0].field'
            # elif isinstance(value, list) and k.isdigit():
            #     value = value[int(k)]
            else:
                return None  # Cannot traverse further
            if value is None:
                return None  # Key not found at this level
        return value
    except (KeyError, IndexError, AttributeError):
        return None


def calculate_evaluation_metrics(
    metrics: list[Union[str, Callable, "FlockAgent", "FlockEvaluator"]],
    metric_configs: dict[str, dict[str, Any]],
    predicted_answers: dict[str, Any],
    expected_answers: dict[str, Any],
    agent_inputs: dict[str, Any],  # For context
    agent_output: Any,  # For context
) -> dict[str, Any]:
    """Calculates all specified metrics for a single evaluation item."""
    results = {}
    for metric in metrics:
        metric_name = ""
        metric_result = None
        try:
            if isinstance(metric, str):
                metric_name = metric
                # Find predicted/expected values relevant to this metric string
                # Simple case: metric name matches an answer_mapping key
                if (
                    metric_name in predicted_answers
                    and metric_name in expected_answers
                ):
                    predicted = predicted_answers[metric_name]
                    expected = expected_answers[metric_name]
                    metric_func = _get_metric_function(metric_name)
                    config = metric_configs.get(metric_name, {})
                    metric_result = metric_func(predicted, expected, **config)
                else:
                    logger_helpers.warning(
                        f"Could not find matching predicted/expected values for metric '{metric_name}' based on answer_mapping keys."
                    )
                    metric_result = None  # Or some error indicator

            elif isinstance(metric, Callable):
                metric_name = getattr(metric, "__name__", "custom_function")
                # Custom functions might need specific predicted/expected pairs, or all of them
                # Let's pass all for flexibility, user function needs to handle it
                config = metric_configs.get(metric_name, {})
                # Allow passing context if function signature supports it
                sig = inspect.signature(metric)
                call_kwargs = config.copy()
                if "agent_inputs" in sig.parameters:
                    call_kwargs["agent_inputs"] = agent_inputs
                if "agent_output" in sig.parameters:
                    call_kwargs["agent_output"] = agent_output

                metric_result = metric(
                    predicted_answers, expected_answers, **call_kwargs
                )

            # --- Placeholder for Agent/Evaluator based metrics ---
            elif "FlockAgent" in str(
                type(metric)
            ):  # Avoid hard import if possible
                metric_name = getattr(metric, "name", "judge_agent")
                config = metric_configs.get(metric_name, {})
                # Requires running the judge agent - needs async context
                # metric_result = asyncio.run(_run_judge_agent(metric, predicted_answers, expected_answers, config))
                logger_helpers.warning(
                    f"Agent-based metric '{metric_name}' execution not implemented in this sketch."
                )
                metric_result = "[Agent Judge Not Implemented]"

            elif "FlockEvaluator" in str(
                type(metric)
            ):  # Avoid hard import if possible
                metric_name = getattr(metric, "name", "judge_evaluator")
                config = metric_configs.get(metric_name, {})
                # Requires running the evaluator - needs async context
                # metric_result = asyncio.run(_run_judge_evaluator(metric, predicted_answers, expected_answers, config))
                logger_helpers.warning(
                    f"Evaluator-based metric '{metric_name}' execution not implemented in this sketch."
                )
                metric_result = "[Evaluator Judge Not Implemented]"
            # --- End Placeholder ---

            else:
                logger_helpers.warning(
                    f"Unsupported metric type: {type(metric)}"
                )
                continue

            # Store result - handle dict results from metrics
            if isinstance(metric_result, dict):
                for sub_key, sub_value in metric_result.items():
                    results[f"{metric_name}_{sub_key}"] = sub_value
            else:
                results[metric_name] = metric_result

        except Exception as e:
            logger_helpers.error(
                f"Error calculating metric '{metric_name}': {e}"
            )
            results[metric_name] = f"[Error: {e}]"

    return results


def _get_metric_function(metric_name: str) -> Callable:
    """Maps metric names to their implementation functions."""
    # Lazy load metric libraries
    if metric_name == "exact_match":
        return lambda pred, act, **kw: str(pred).strip() == str(act).strip()
    elif metric_name == "fuzzy_match":
        try:
            from thefuzz import fuzz

            return (
                lambda pred, act, threshold=85, **kw: fuzz.ratio(
                    str(pred), str(act)
                )
                >= threshold
            )
        except ImportError:
            logger_helpers.warning(
                "fuzzy_match requires 'thefuzz': pip install thefuzz[speedup]"
            )
            return lambda p, a, **kw: None
    elif metric_name.startswith("rouge"):  # rouge_1, rouge_2, rouge_l
        try:
            from rouge_score import rouge_scorer

            scorer = rouge_scorer.RougeScorer(
                [metric_name.replace("_", "")], use_stemmer=True
            )

            def calculate_rouge(pred, act, score_type="fmeasure", **kw):
                scores = scorer.score(str(act), str(pred))
                return (
                    scores[metric_name.replace("_", "")]
                    ._asdict()
                    .get(score_type, 0.0)
                )

            return calculate_rouge
        except ImportError:
            logger_helpers.warning(
                "rouge requires 'rouge-score': pip install rouge-score"
            )
            return lambda p, a, **kw: None
    elif metric_name == "semantic_similarity":
        try:
            from sentence_transformers import SentenceTransformer, util

            # Cache the model? Maybe pass it in via config?
            model = SentenceTransformer("all-MiniLM-L6-v2")

            def calculate_similarity(pred, act, **kw):
                emb1 = model.encode(str(pred), convert_to_tensor=True)
                emb2 = model.encode(str(act), convert_to_tensor=True)
                return util.pytorch_cos_sim(emb1, emb2).item()

            return calculate_similarity
        except ImportError:
            logger_helpers.warning(
                "semantic_similarity requires 'sentence-transformers': pip install sentence-transformers"
            )
            return lambda p, a, **kw: None
    # Add bleu, f1 etc.
    elif metric_name == "llm_judge":
        # This is handled by checking type in calculate_evaluation_metrics
        # but we need a placeholder callable here if we map by string first
        return lambda p, a, **kw: "[LLM Judge Not Implemented Directly]"
    else:
        raise ValueError(f"Unknown built-in metric: {metric_name}")


def aggregate_results(results_list: list[dict[str, Any]]) -> dict[str, Any]:
    """Aggregates evaluation results across all items."""
    summary = {"total_items": len(results_list), "errors": 0}
    metric_values: dict[str, list[float | bool]] = {}

    for item in results_list:
        if item.get("error"):
            summary["errors"] += 1
        metrics = item.get("metrics", {})
        for name, value in metrics.items():
            if isinstance(
                value, (float, int, bool)
            ):  # Only aggregate numerics/bools
                if name not in metric_values:
                    metric_values[name] = []
                metric_values[name].append(value)

    summary["metrics_summary"] = {}
    for name, values in metric_values.items():
        if not values:
            continue
        # Calculate different stats based on value type
        if all(isinstance(v, bool) for v in values):
            summary["metrics_summary"][name] = {
                "accuracy": sum(values) / len(values)
            }
        elif all(isinstance(v, (int, float)) for v in values):
            numeric_values = [v for v in values if isinstance(v, (int, float))]
            if numeric_values:
                summary["metrics_summary"][name] = {
                    "mean": sum(numeric_values) / len(numeric_values),
                    "count": len(numeric_values),
                    # Add min, max, stddev if needed
                }

    return summary


# --- Placeholder for async judge execution ---
# Need to run these within the main async context or manage loops carefully
async def _run_judge_agent(judge_agent, predicted, expected, config):
    # Prepare input for the judge agent based on its signature
    # E.g., judge_input = {"prediction": predicted_value, "reference": expected_value, "criteria": ...}
    # judge_result = await judge_agent.run_async(judge_input)
    # return judge_result # Or extract specific score/judgement
    return "[Agent Judge Not Implemented]"


async def _run_judge_evaluator(judge_evaluator, predicted, expected, config):
    # Prepare input for the judge evaluator based on its signature
    # judge_input = {"prediction": predicted_value, "reference": expected_value, **config}
    # judge_result = await judge_evaluator.evaluate(None, judge_input, []) # Agent might not be needed
    # return judge_result # Or extract specific score/judgement
    return "[Evaluator Judge Not Implemented]"
```

### src\flock\core\execution\batch_executor.py

- **Lines**: 369
- **Last modified**: 2025-05-21 19:51:15

```py
import asyncio
import concurrent.futures  # For real parallelism via threads
from pathlib import Path
from typing import TYPE_CHECKING, Any

from box import Box
from opentelemetry import trace
from pandas import DataFrame
from rich.progress import (  # Import Rich Progress
    BarColumn,
    Progress,
    SpinnerColumn,
    TextColumn,
    TimeElapsedColumn,
)

from flock.config import TELEMETRY
from flock.core.context.context import FlockContext
from flock.core.context.context_vars import FLOCK_BATCH_SILENT_MODE
from flock.core.flock_agent import FlockAgent
from flock.core.logging.logging import get_logger

try:
    import pandas as pd

    PANDAS_AVAILABLE = True
except ImportError:
    pd = None
    PANDAS_AVAILABLE = False

if TYPE_CHECKING:
    from flock.core.flock import Flock

logger = get_logger("flock")
TELEMETRY.setup_tracing()  # Setup OpenTelemetry
tracer = trace.get_tracer(__name__)


class BatchProcessor:
    def __init__(self, flock_instance: "Flock"):
        self.flock = flock_instance

    async def run_batch_async(
        self,
        start_agent: FlockAgent | str,
        batch_inputs: list[dict[str, Any]] | DataFrame | str,
        input_mapping: dict[str, str] | None = None,
        static_inputs: dict[str, Any] | None = None,
        parallel: bool = True,
        max_workers: int = 5,
        use_temporal: bool | None = None,
        box_results: bool = True,
        return_errors: bool = False,
        silent_mode: bool = False,
        write_to_csv: str | None = None,
        hide_columns: list[str] | None = None,
        delimiter: str = ",",
    ) -> list[Box | dict | None | Exception]:
        """Runs the specified agent/workflow for each item in a batch asynchronously.

        Args:
            start_agent: Agent instance or name to start each run.
            batch_inputs: Input data in one of these forms:
                - List of dictionaries, each representing inputs for one run
                - Pandas DataFrame where each row is inputs for one run
                - String path to a CSV file to load as DataFrame
            input_mapping: Maps DataFrame/CSV column names to agent input keys (required for DataFrame/CSV).
            static_inputs: Dictionary of inputs constant across all batch runs.
            parallel: Whether to run local jobs in parallel (ignored if use_temporal=True).
            max_workers: Max concurrent local workers (used if parallel=True and use_temporal=False).
            use_temporal: Override Flock's 'enable_temporal' setting for this batch.
            box_results: Wrap successful dictionary results in Box objects.
            return_errors: If True, return Exception objects for failed runs instead of raising.
            silent_mode: If True, suppress output and show progress bar instead.
            write_to_csv: Path to save results as CSV file.
            hide_columns: List of column names to hide from output.

        Returns:
            List containing results (Box/dict), None (if error and not return_errors),
            or Exception objects (if error and return_errors). Order matches input.

        Raises:
            ValueError: For invalid input combinations.
            ImportError: If DataFrame/CSV used without pandas.
            Exception: First exception from a run if return_errors is False.
        """
        effective_use_temporal = (
            use_temporal
            if use_temporal is not None
            else self.flock.enable_temporal
        )
        exec_mode = (
            "Temporal"
            if effective_use_temporal
            else ("Parallel Local" if parallel else "Sequential Local")
        )
        logger.info(
            f"Starting batch run for agent '{start_agent}'. Execution: {exec_mode}, Silent: {silent_mode}"
        )

        # --- Input Preparation ---
        prepared_batch_inputs: list[dict[str, Any]] = []

        if input_mapping == {}:
            input_mapping = None
        if static_inputs == {}:
            static_inputs = None

        if isinstance(batch_inputs, str):
            # Handle CSV file input
            try:
                df = pd.read_csv(batch_inputs)
                logger.debug(
                    f"Loaded CSV file with {len(df)} rows: {batch_inputs}"
                )
                batch_inputs = df  # Convert to DataFrame for unified handling
            except Exception as e:
                raise ValueError(
                    f"Failed to load CSV file '{batch_inputs}': {e}"
                )

        if isinstance(batch_inputs, DataFrame):
            # Handle DataFrame input
            logger.debug(
                f"Converting DataFrame ({len(batch_inputs)} rows) to batch inputs."
            )
            for _, row in batch_inputs.iterrows():
                if input_mapping:
                    item_input = {
                        agent_key: row[df_col]
                        for df_col, agent_key in input_mapping.items()
                        if df_col in row
                    }
                else:
                    item_input = row.to_dict()
                prepared_batch_inputs.append(item_input)
        else:
            # Handle list of dictionaries
            if not isinstance(batch_inputs, list):
                raise ValueError(
                    "batch_inputs must be a list of dictionaries, DataFrame, or CSV file path"
                )

            if input_mapping:
                # Apply mapping to dictionary inputs
                logger.debug("Applying input mapping to dictionary inputs")
                for item in batch_inputs:
                    mapped_input = {}
                    for df_col, agent_key in input_mapping.items():
                        if df_col in item:
                            mapped_input[agent_key] = item[df_col]
                        else:
                            logger.warning(
                                f"Input mapping key '{df_col}' not found in input dictionary"
                            )
                    prepared_batch_inputs.append(mapped_input)
            else:
                # Use dictionaries as-is if no mapping provided
                prepared_batch_inputs = batch_inputs

            logger.debug(
                f"Using provided list of {len(prepared_batch_inputs)} batch inputs."
            )

        if not prepared_batch_inputs:
            return []

        # --- Setup Progress Bar if Silent ---
        progress_context = None
        progress_task_id = None
        if silent_mode:
            progress = Progress(
                SpinnerColumn(),
                TextColumn("[progress.description]{task.description}"),
                BarColumn(),
                TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
                TextColumn("({task.completed}/{task.total})"),
                TimeElapsedColumn(),
                # transient=True # Optionally remove progress bar when done
            )
            progress_context = progress  # Use as context manager
            progress_task_id = progress.add_task(
                f"Processing Batch ({exec_mode})",
                total=len(prepared_batch_inputs),
            )
            progress.start()

        results = [None] * len(prepared_batch_inputs)  # Pre-allocate results list

        # --- Worker Definitions ---
        # We implement two flavours:
        #   * async_worker: used for Temporal or sequential runs (keeps the original behaviour)
        #   * thread_worker: executes the run in a dedicated thread via ThreadPoolExecutor for true parallelism.

        async def async_worker(index: int, item_inputs: dict[str, Any]):
            """Original coroutine worker used for non-threaded execution paths."""
            full_input = {**(static_inputs or {}), **item_inputs}
            context = FlockContext()
            context.set_variable(FLOCK_BATCH_SILENT_MODE, silent_mode)

            run_desc = f"Batch item {index + 1}"
            logger.debug(f"{run_desc} started (async).")
            try:
                result = await self.flock.run_async(
                    start_agent,
                    full_input,
                    box_result=box_results,
                    context=context,
                )
                results[index] = result
                logger.debug(f"{run_desc} finished successfully.")
            except Exception as e:
                logger.error(f"{run_desc} failed: {e}", exc_info=not return_errors)
                if return_errors:
                    results[index] = e
                else:
                    raise  # Propagate to calling gather
            finally:
                if progress_context:
                    progress.update(progress_task_id, advance=1)

        # ThreadPool worker for real parallelism (suitable for blocking I/O)
        def _thread_worker(index: int, item_inputs: dict[str, Any]):
            """Synchronous helper executed inside a worker thread."""
            full_input = {**(static_inputs or {}), **item_inputs}
            run_desc = f"Batch item {index + 1}"
            logger.debug(f"{run_desc} started (thread).")
            try:
                # Use the synchronous wrapper to avoid nested event-loop issues inside threads
                result = self.flock.run(
                    start_agent=start_agent,
                    input=full_input,
                    box_result=box_results,
                )
                logger.debug(f"{run_desc} finished successfully.")
                return index, result, None
            except Exception as e:
                logger.error(f"{run_desc} failed: {e}")
                return index, None, e

        async def thread_worker(executor, index: int, item_inputs: dict[str, Any]):
            """Coroutine wrapper that submits _thread_worker to the specified executor."""
            loop = asyncio.get_running_loop()
            idx, res, err = await loop.run_in_executor(
                executor, _thread_worker, index, item_inputs
            )
            # Handle result / error on the asyncio side
            if err:
                if return_errors:
                    results[idx] = err
                else:
                    raise err
            else:
                results[idx] = res
            if progress_context:
                progress.update(progress_task_id, advance=1)

        tasks = []
        try:
            if effective_use_temporal:
                # Temporal Batching (Simplified: sequential execution for this example)
                # A real implementation might use start_workflow or signals
                logger.info(
                    "Running batch using Temporal (executing sequentially for now)..."
                )
                for i, item_data in enumerate(prepared_batch_inputs):
                    await async_worker(i, item_data)  # Run sequentially for demo
                # TODO: Implement true parallel Temporal workflow execution if needed

            elif parallel:
                # --- Real parallelism using ThreadPoolExecutor ---
                logger.info(
                    f"Running batch in parallel (threads) with max_workers={max_workers}..."
                )
                loop = asyncio.get_running_loop()
                with concurrent.futures.ThreadPoolExecutor(
                    max_workers=max_workers, thread_name_prefix="flock-batch"
                ) as executor:
                    for i, item_data in enumerate(prepared_batch_inputs):
                        tasks.append(
                            asyncio.create_task(
                                thread_worker(executor, i, item_data)
                            )
                        )

                    # Wait for all tasks allowing exceptions to propagate as needed
                    await asyncio.gather(*tasks)

            else:  # Sequential Local
                logger.info("Running batch sequentially...")
                for i, item_data in enumerate(prepared_batch_inputs):
                    await async_worker(i, item_data)  # Already handles errors internally based on return_errors

            logger.info("Batch execution finished.")

        except Exception as batch_error:
            # This catch handles errors re-raised from workers when return_errors=False
            logger.error(f"Batch execution stopped due to error: {batch_error}")
            # No need to cancel tasks here as gather would have stopped
            if not return_errors:
                raise  # Re-raise the first error encountered if not returning errors
        finally:
            if progress_context:
                progress.stop()

        if write_to_csv:
            try:
                df = pd.DataFrame(results)
                if hide_columns:
                    df = df.drop(columns=hide_columns)
                # create write_to_csv directory if it doesn't exist
                Path(write_to_csv).parent.mkdir(parents=True, exist_ok=True)
                df.to_csv(write_to_csv, index=False, sep=delimiter)
                logger.info(f"Results written to CSV file: {write_to_csv}")
            except Exception as e:
                logger.error(f"Failed to write results to CSV: {e}")

        return results

    def run_batch(  # Synchronous wrapper
        self,
        start_agent: FlockAgent | str,
        batch_inputs: list[dict[str, Any]] | DataFrame | str,
        input_mapping: dict[str, str] | None = None,
        static_inputs: dict[str, Any] | None = None,
        parallel: bool = True,
        max_workers: int = 5,
        use_temporal: bool | None = None,
        box_results: bool = True,
        return_errors: bool = False,
        silent_mode: bool = False,
        write_to_csv: str | None = None,
        hide_columns: list[str] | None = None,
        delimiter: str = ",",
    ) -> list[Box | dict | None | Exception]:
        """Synchronous wrapper for run_batch_async."""
        # (Standard asyncio run wrapper - same as in previous suggestion)
        try:
            loop = asyncio.get_running_loop()
            if loop.is_closed():
                raise RuntimeError("Event loop is closed")
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)

        coro = self.run_batch_async(
            start_agent=start_agent,
            batch_inputs=batch_inputs,
            input_mapping=input_mapping,
            static_inputs=static_inputs,
            parallel=parallel,
            max_workers=max_workers,
            use_temporal=use_temporal,
            box_results=box_results,
            return_errors=return_errors,
            silent_mode=silent_mode,
            write_to_csv=write_to_csv,
            hide_columns=hide_columns,
            delimiter=delimiter,
        )

        if asyncio.get_event_loop() is loop and not loop.is_running():
            results = loop.run_until_complete(coro)
            # loop.close() # Avoid closing potentially shared loop
            return results
        else:
            # Run within an existing loop
            future = asyncio.ensure_future(coro)
            return loop.run_until_complete(future)
```

### src\flock\core\execution\evaluation_executor.py

- **Lines**: 438
- **Last modified**: 2025-05-23 01:55:47

```py
# src/flock/core/execution/evaluation_processor.py
"""Contains the EvaluationProcessor class responsible for evaluating Flock agents
against datasets using various metrics.
"""

import asyncio
import json
from collections.abc import Callable
from pathlib import Path
from typing import (
    TYPE_CHECKING,
    Any,
    Literal,
    Union,
)

from pandas import DataFrame

# Conditional pandas import
try:
    import pandas as pd

    PANDAS_AVAILABLE = True
except ImportError:
    pd = None  # type: ignore
    PANDAS_AVAILABLE = False

# Box for results
from box import Box
from datasets import Dataset as HFDataset

from flock.core.evaluation.utils import (
    aggregate_results,
    calculate_evaluation_metrics,
    extract_value_by_dot_notation,
    normalize_dataset,
    # Import metric calculation/aggregation helpers
)

# Flock core imports
from flock.core.logging.logging import get_logger

if TYPE_CHECKING:
    from flock.core.flock import Flock
    from flock.core.flock_agent import FlockAgent
    from flock.core.flock_evaluator import FlockEvaluator
    # Conditional types


logger = get_logger("execution.evaluation")


class EvaluationExecutor:
    """Handles the evaluation of Flock agents against datasets."""

    def __init__(self, flock_instance: "Flock"):
        """Initializes the EvaluationProcessor.

        Args:
            flock_instance: The Flock instance this processor will use.
        """
        self.flock = flock_instance

    async def evaluate_async(
        self,
        dataset: str | Path | list[dict[str, Any]] | DataFrame | HFDataset,
        start_agent: Union["FlockAgent", str],
        input_mapping: dict[str, str],
        answer_mapping: dict[str, str],
        metrics: list[
            Union[
                str,
                Callable[[Any, Any], bool | float | dict[str, Any]],
                "FlockAgent",
                "FlockEvaluator",
            ]
        ],
        metric_configs: dict[str, dict[str, Any]] | None = None,
        static_inputs: dict[str, Any] | None = None,
        parallel: bool = True,
        max_workers: int = 5,
        use_temporal: bool | None = None,
        error_handling: Literal["raise", "skip", "log"] = "log",
        output_file: str | Path | None = None,
        return_dataframe: bool = True,
        silent_mode: bool = False,
        metadata_columns: list[str] | None = None,  # Columns to pass through
        # dataset_split: Optional[str] = None # TODO: Add split support in normalize_dataset
    ) -> DataFrame | list[dict[str, Any]]:
        """Evaluates the Flock's performance against a dataset asynchronously."""
        effective_use_temporal = (
            use_temporal
            if use_temporal is not None
            else self.flock.enable_temporal
        )
        exec_mode = (
            "Temporal"
            if effective_use_temporal
            else ("Parallel Local" if parallel else "Sequential Local")
        )
        start_agent_name = (
            start_agent.name if hasattr(start_agent, "name") else start_agent
        )
        logger.info(
            f"Starting evaluation for agent '{start_agent_name}'. Execution: {exec_mode}, Silent: {silent_mode}"
        )

        # --- 1. Normalize Dataset ---
        try:
            df = normalize_dataset(dataset)  # Uses helper
            if df is None or df.empty:
                raise ValueError(
                    "Provided dataset is empty or could not be normalized."
                )
            logger.info(f"Normalized dataset with {len(df)} items.")
        except Exception as e:
            logger.error(
                f"Failed to load or normalize dataset: {e}", exc_info=True
            )
            raise ValueError(f"Dataset processing failed: {e}") from e

        # --- 2. Prepare Batch Items ---
        batch_items = []
        required_input_cols = list(input_mapping.keys())
        required_answer_cols = list(answer_mapping.values())
        required_metadata_cols = metadata_columns or []
        all_required_cols = set(
            required_input_cols + required_answer_cols + required_metadata_cols
        )

        missing_cols = all_required_cols - set(df.columns)
        if missing_cols:
            raise ValueError(
                f"Dataset missing required columns: {', '.join(missing_cols)}"
            )

        for index, row in df.iterrows():
            agent_input = {
                agent_key: row[df_col]
                for df_col, agent_key in input_mapping.items()
            }
            expected_answers = {
                agent_out_key: row[answer_col]
                for agent_out_key, answer_col in answer_mapping.items()
            }
            metadata = {col: row[col] for col in required_metadata_cols}
            batch_items.append(
                {
                    "_original_index": index,  # Store original DF index
                    "_agent_input": agent_input,
                    "_expected_answers": expected_answers,
                    "_metadata": metadata,
                }
            )

        if not batch_items:
            logger.warning("No items prepared for evaluation.")
            return pd.DataFrame() if return_dataframe else []

        # --- 3. Execute Workers ---
        results_dict = {}  # Store results keyed by original index
        tasks = []
        semaphore = asyncio.Semaphore(
            max_workers if parallel and not effective_use_temporal else 1
        )

        # --- Worker Function ---
        async def evaluate_worker(item_index: int, item_data: dict[str, Any]):
            nonlocal results_dict
            original_index = item_data["_original_index"]
            item_result_details = {
                "index": original_index,  # Use original index in result
                "inputs": item_data["_agent_input"],
                "expected_answers": item_data["_expected_answers"],
                "agent_output": None,
                "metrics": {},
                "error": None,
                **(item_data["_metadata"]),  # Include pass-through metadata
            }
            agent_inputs_with_static = {
                **(static_inputs or {}),
                **item_data["_agent_input"],
            }

            async with semaphore:  # Acquire semaphore
                run_desc = f"Evaluation item (original index: {original_index})"
                logger.debug(f"{run_desc} starting.")
                try:
                    # Run the agent/flock for this item
                    agent_output = await self.flock.run_async(
                        start_agent=start_agent,  # Name or instance
                        input=agent_inputs_with_static,
                        box_result=True,  # Use Box for easier access via dot notation
                        # context=... # Assuming isolated context for now
                    )
                    item_result_details["agent_output"] = (
                        agent_output  # Store Box or dict
                    )

                    # Extract predicted values based on answer_mapping
                    predicted_answers = {}
                    for agent_out_key in answer_mapping:
                        # Use helper to handle dot notation
                        predicted_answers[agent_out_key] = (
                            extract_value_by_dot_notation(
                                agent_output, agent_out_key
                            )
                        )

                    # Calculate metrics using helper
                    item_result_details["metrics"] = (
                        calculate_evaluation_metrics(
                            metrics=metrics,
                            metric_configs=metric_configs or {},
                            predicted_answers=predicted_answers,
                            expected_answers=item_data["_expected_answers"],
                            agent_inputs=agent_inputs_with_static,  # Pass context if needed
                            agent_output=agent_output,  # Pass context if needed
                        )
                    )
                    logger.debug(f"{run_desc} finished successfully.")

                except Exception as e:
                    logger.warning(
                        f"Error processing item {original_index}: {e}"
                    )
                    item_result_details["error"] = str(e)
                    if error_handling == "raise":
                        raise  # Re-raise to stop processing (if parallel, stops gather)
                    elif error_handling == "skip":
                        item_result_details["_skip"] = (
                            True  # Mark for filtering
                        )

                # Store result associated with original index
                results_dict[original_index] = item_result_details

                # Update progress bar if applicable (inside the worker is okay)
                if progress_context:
                    progress.update(progress_task_id, advance=1)

        # --- Setup Progress Bar if Silent ---
        progress_context = None
        progress_task_id = None
        if silent_mode:
            from rich.progress import (
                BarColumn,
                Progress,
                SpinnerColumn,
                TextColumn,
                TimeElapsedColumn,
            )

            progress = Progress(
                SpinnerColumn(),
                TextColumn("[progress.description]{task.description}"),
                BarColumn(),
                TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
                TextColumn("({task.completed}/{task.total})"),
                TimeElapsedColumn(),
            )
            progress_context = progress
            progress_task_id = progress.add_task(
                f"Evaluating {len(batch_items)} items...",
                total=len(batch_items),
            )
            progress.start()

        # --- Execute Tasks ---
        try:
            if effective_use_temporal:
                # TODO: Implement parallel Temporal evaluation
                logger.info(
                    "Running evaluation using Temporal (executing sequentially for now)..."
                )
                for i, item_data in enumerate(batch_items):
                    await evaluate_worker(i, item_data)  # Pass sequential index
            elif parallel:
                logger.info(
                    f"Running evaluation in parallel with max_workers={max_workers}..."
                )
                for i, item_data in enumerate(batch_items):
                    # Pass sequential index i, and the item_data which contains original_index
                    tasks.append(
                        asyncio.create_task(evaluate_worker(i, item_data))
                    )
                await asyncio.gather(*tasks)
            else:  # Sequential Local
                logger.info("Running evaluation sequentially...")
                for i, item_data in enumerate(batch_items):
                    await evaluate_worker(i, item_data)

            logger.info("Evaluation execution finished.")

        except Exception as batch_error:
            logger.error(
                f"Evaluation stopped due to an error in one of the items: {batch_error}"
            )
            if (
                not error_handling == "skip"
            ):  # If skipping, we continue; otherwise, re-raise if required
                if error_handling == "raise":
                    raise
        finally:
            if progress_context:
                progress.stop()

        # --- 4. Process Results ---
        # Reconstruct results list based on original order and filtering
        final_results_list = []
        for idx in df.index:  # Iterate through original DataFrame index
            res = results_dict.get(idx)
            if res:
                if error_handling == "skip" and res.get("_skip"):
                    continue  # Skip items marked for skipping
                # Remove internal skip flag if present
                res.pop("_skip", None)
                final_results_list.append(res)

        # Calculate aggregate summary using helper
        summary = aggregate_results(final_results_list)
        logger.info(
            "Evaluation Summary:", extra=summary
        )  # Log summary automatically

        # --- 5. Save and Return ---
        if output_file:
            output_path = Path(output_file)
            output_path.parent.mkdir(parents=True, exist_ok=True)
            try:
                results_df = pd.DataFrame(final_results_list)
                # Handle complex objects before saving
                if "agent_output" in results_df.columns:
                    results_df["agent_output"] = results_df[
                        "agent_output"
                    ].apply(lambda x: x.to_dict() if isinstance(x, Box) else x)
                if (
                    "expected_answers" in results_df.columns
                ):  # Flatten dicts for CSV
                    results_df = pd.concat(
                        [
                            results_df.drop(["expected_answers"], axis=1),
                            pd.json_normalize(
                                results_df["expected_answers"]
                            ).add_prefix("expected_"),
                        ],
                        axis=1,
                    )
                if "metrics" in results_df.columns:  # Flatten dicts for CSV
                    results_df = pd.concat(
                        [
                            results_df.drop(["metrics"], axis=1),
                            pd.json_normalize(results_df["metrics"]).add_prefix(
                                "metric_"
                            ),
                        ],
                        axis=1,
                    )
                if "inputs" in results_df.columns:  # Flatten dicts for CSV
                    results_df = pd.concat(
                        [
                            results_df.drop(["inputs"], axis=1),
                            pd.json_normalize(results_df["inputs"]).add_prefix(
                                "input_"
                            ),
                        ],
                        axis=1,
                    )

                # Convert lists/dicts in metadata columns for CSV saving
                for col in metadata_columns or []:
                    if col in results_df.columns:
                        # Check if column contains lists/dicts before converting
                        if (
                            results_df[col]
                            .apply(lambda x: isinstance(x, (list, dict)))
                            .any()
                        ):
                            results_df[col] = results_df[col].apply(json.dumps)

                if output_path.suffix.lower() == ".csv":
                    results_df.to_csv(output_path, index=False)
                elif output_path.suffix.lower() == ".json":
                    # Save list of dicts directly (before potential DataFrame manipulation)
                    # Need to handle non-serializable types like Box
                    serializable_results = []
                    for res_dict in final_results_list:
                        if "agent_output" in res_dict and isinstance(
                            res_dict["agent_output"], Box
                        ):
                            res_dict["agent_output"] = res_dict[
                                "agent_output"
                            ].to_dict()
                        serializable_results.append(res_dict)
                    with open(output_path, "w", encoding="utf-8") as f:
                        json.dump(
                            serializable_results, f, indent=2, default=str
                        )  # Use default=str for safety
                else:
                    logger.warning(
                        f"Unsupported output file format: {output_path.suffix}. Use .csv or .json."
                    )
                logger.info(
                    f"Detailed evaluation results saved to {output_path}"
                )
            except Exception as e:
                logger.error(
                    f"Failed to save evaluation results to {output_file}: {e}",
                    exc_info=True,
                )

        if return_dataframe:
            if not PANDAS_AVAILABLE:
                logger.error(
                    "Cannot return DataFrame: pandas library not installed."
                )
                return final_results_list  # Fallback to list
            # Ensure DataFrame is created if not done for saving
            if "results_df" not in locals():
                results_df = pd.DataFrame(final_results_list)
                # Convert Box if needed
                if "agent_output" in results_df.columns:
                    results_df["agent_output"] = results_df[
                        "agent_output"
                    ].apply(lambda x: x.to_dict() if isinstance(x, Box) else x)
            return results_df
        else:
            # Ensure Box objects are converted if returning list
            final_list = []
            for res_dict in final_results_list:
                if "agent_output" in res_dict and isinstance(
                    res_dict["agent_output"], Box
                ):
                    res_dict["agent_output"] = res_dict[
                        "agent_output"
                    ].to_dict()
                final_list.append(res_dict)
            return final_list
```

### src\flock\core\execution\local_executor.py

- **Lines**: 31
- **Last modified**: 2025-04-16 00:11:15

```py
# src/your_package/core/execution/local_executor.py
from flock.core.context.context import FlockContext
from flock.core.logging.logging import get_logger
from flock.workflow.activities import (
    run_agent,  # This should be the local activity function
)

logger = get_logger("flock")


async def run_local_workflow(
    context: FlockContext, box_result: bool = True
) -> dict:
    """Execute the agent workflow locally (for debugging).

    Args:
        context: The FlockContext instance with state and history.
        output_formatter: Formatter options for displaying results.
        box_result: If True, wraps the result in a Box for nicer display.

    Returns:
        A dictionary containing the workflow result.
    """
    logger.info("Running local debug workflow")
    result = await run_agent(context)
    if box_result:
        from box import Box

        logger.debug("Boxing result")
        return Box(result)
    return result
```

### src\flock\core\execution\temporal_executor.py

- **Lines**: 166
- **Last modified**: 2025-04-19 03:53:31

```py
# src/your_package/core/execution/temporal_executor.py

import asyncio  # Import asyncio
from typing import TYPE_CHECKING, Any

from temporalio.worker import Worker  # Import Worker

if TYPE_CHECKING:
    from flock.core.flock import Flock  # Import Flock for type hinting

from flock.core.context.context import FlockContext
from flock.core.context.context_vars import FLOCK_RUN_ID
from flock.core.logging.logging import get_logger
from flock.workflow.agent_execution_activity import (
    determine_next_agent,
    execute_single_agent,
)
from flock.workflow.temporal_config import (
    TemporalRetryPolicyConfig,
    TemporalWorkflowConfig,
)
from flock.workflow.temporal_setup import create_temporal_client, setup_worker

logger = get_logger("flock")


async def run_temporal_workflow(
    flock_instance: "Flock",  # Accept Flock instance
    context: FlockContext,
    box_result: bool = True,
    memo: dict[str, Any] | None = None,  # Add memo argument
) -> dict:
    """Execute the agent workflow via Temporal for robust, distributed processing.

    Args:
        flock_instance: The Flock instance.
        context: The FlockContext instance with state and history.
        box_result: If True, wraps the result in a Box for nicer display.
        memo: Optional dictionary of metadata to attach to the Temporal workflow.

    Returns:
        A dictionary containing the workflow result.
    """
    try:
        from flock.workflow.flock_workflow import (
            FlockWorkflow,  # Your workflow class
        )

        # Get workflow config from Flock instance or use defaults
        wf_config = flock_instance.temporal_config or TemporalWorkflowConfig()

        logger.debug("Creating Temporal client")
        flock_client = await create_temporal_client()

        # Determine if we need to manage an in-process worker
        start_worker_locally = flock_instance.temporal_start_in_process_worker

        # Setup worker instance
        worker: Worker | None = None
        worker_task: asyncio.Task | None = None

        if start_worker_locally:
            logger.info(
                f"Setting up temporary in-process worker for task queue '{wf_config.task_queue}'"
            )
            worker = await setup_worker(
                flock_client,  # Pass the client
                wf_config.task_queue,  # Pass the task queue
                FlockWorkflow,
                [execute_single_agent, determine_next_agent],
            )

            # Run the worker in the background
            worker_task = asyncio.create_task(worker.run())
            logger.info("Temporal worker started in background.")

            # Allow worker time to start polling (heuristic for local testing)
            await asyncio.sleep(2)
        else:
            logger.info(
                "Skipping in-process worker startup. Assuming dedicated workers are running."
            )

        try:
            workflow_id = context.get_variable(FLOCK_RUN_ID)
            logger.info(
                "Executing Temporal workflow",
                workflow_id=workflow_id,
                task_queue=wf_config.task_queue,
            )

            # Prepare the single workflow argument dictionary
            workflow_args_dict = {
                "context_dict": context.model_dump(mode="json"),
                "default_retry_config_dict": (
                    wf_config.default_activity_retry_policy.model_dump(
                        mode="json"
                    )
                    if wf_config.default_activity_retry_policy
                    else TemporalRetryPolicyConfig().model_dump(mode="json")
                ),
            }

            # Start the workflow using start_workflow
            handle = await flock_client.start_workflow(
                FlockWorkflow.run,
                # Pass the single dictionary as the only element in the args list
                args=[workflow_args_dict],
                id=workflow_id,
                task_queue=wf_config.task_queue,
                # Corrected timeout argument names
                execution_timeout=wf_config.workflow_execution_timeout,
                run_timeout=wf_config.workflow_run_timeout,
                memo=memo or {},  # Pass memo if provided
            )

            logger.info(
                "Workflow started, awaiting result...", workflow_id=handle.id
            )
            # Await the result from the handle
            result = await handle.result()
            logger.info("Workflow result received.")

            agent_name = context.get_variable("FLOCK_CURRENT_AGENT")
            logger.debug("Formatting Temporal result", agent=agent_name)

            if box_result:
                from box import Box

                logger.debug("Boxing Temporal result")
                return Box(result)
            return result
        except Exception as e:
            logger.error(
                "Error during Temporal workflow execution or result retrieval",
                error=e,
            )
            raise e  # Re-raise the exception after logging
        finally:
            # Ensure worker is shut down regardless of success or failure
            if (
                start_worker_locally
                and worker
                and worker_task
                and not worker_task.done()
            ):
                logger.info("Shutting down temporal worker...")
                await worker.shutdown()  # Await the shutdown coroutine
                try:
                    await asyncio.wait_for(
                        worker_task, timeout=10.0
                    )  # Wait for task to finish
                    logger.info("Temporal worker shut down gracefully.")
                except asyncio.TimeoutError:
                    logger.warning(
                        "Temporal worker shutdown timed out. Cancelling task."
                    )
                    worker_task.cancel()
                except Exception as shutdown_err:
                    logger.error(
                        f"Error during worker shutdown: {shutdown_err}",
                        exc_info=True,
                    )
    except Exception as e:
        logger.error("Error executing Temporal workflow", error=e)
        raise e
```

### src\flock\core\flock_agent.py

- **Lines**: 1152
- **Last modified**: 2025-05-25 23:17:06

```py
# src/flock/core/flock_agent.py
"""FlockAgent is the core, declarative base class for all agents in the Flock framework."""

import asyncio
import json
import os
import uuid
from abc import ABC
from collections.abc import Callable
from datetime import datetime
from typing import TYPE_CHECKING, Any, TypeVar

from flock.core.mcp.flock_mcp_server import FlockMCPServerBase
from flock.core.serialization.json_encoder import FlockJSONEncoder
from flock.workflow.temporal_config import TemporalActivityConfig

if TYPE_CHECKING:
    from flock.core.context.context import FlockContext
    from flock.core.flock_evaluator import FlockEvaluator
    from flock.core.flock_module import FlockModule
    from flock.core.flock_router import FlockRouter

from opentelemetry import trace
from pydantic import BaseModel, Field

# Core Flock components (ensure these are importable)
from flock.core.context.context import FlockContext
from flock.core.flock_evaluator import FlockEvaluator, FlockEvaluatorConfig
from flock.core.flock_module import FlockModule, FlockModuleConfig
from flock.core.flock_router import FlockRouter, FlockRouterConfig
from flock.core.logging.logging import get_logger

# Mixins and Serialization components
from flock.core.mixin.dspy_integration import DSPyIntegrationMixin
from flock.core.serialization.serializable import (
    Serializable,  # Import Serializable base
)
from flock.core.serialization.serialization_utils import (
    deserialize_component,
    serialize_item,
)

logger = get_logger("agent")
tracer = trace.get_tracer(__name__)
T = TypeVar("T", bound="FlockAgent")


SignatureType = (
    str
    | Callable[..., str]
    | type[BaseModel]
    | Callable[..., type[BaseModel]]
    | None
)


# Make FlockAgent inherit from Serializable
class FlockAgent(BaseModel, Serializable, DSPyIntegrationMixin, ABC):
    """Core, declarative base class for Flock agents, enabling serialization,
    modularity, and integration with evaluation and routing components.
    Inherits from Pydantic BaseModel, ABC, DSPyIntegrationMixin, and Serializable.
    """

    agent_id: str = Field(
        default_factory=lambda: str(uuid.uuid4()),
        description="Internal, Unique UUID4 for this agent instance. No need to set it manually. Used for MCP features.",
    )

    name: str = Field(..., description="Unique identifier for the agent.")

    model: str | None = Field(
        None,
        description="The model identifier to use (e.g., 'openai/gpt-4o'). If None, uses Flock's default.",
    )
    description: str | Callable[..., str] | None = Field(
        "",
        description="A human-readable description or a callable returning one.",
    )
    input: SignatureType = Field(
        None,
        description=(
            "Signature for input keys. Supports type hints (:) and descriptions (|). "
            "E.g., 'query: str | Search query, context: dict | Conversation context'. Can be a callable."
        ),
    )
    output: SignatureType = Field(
        None,
        description=(
            "Signature for output keys. Supports type hints (:) and descriptions (|). "
            "E.g., 'result: str | Generated result, summary: str | Brief summary'. Can be a callable."
        ),
    )
    tools: list[Callable[..., Any]] | None = (
        Field(  # Assume tools are always callable for serialization simplicity
            default=None,
            description="List of callable tools the agent can use. These must be registered.",
        )
    )
    servers: list[str | FlockMCPServerBase] | None = Field(
        default=None,
        description="List of MCP Servers the agent can use to enhance its capabilities. These must be registered.",
    )

    write_to_file: bool = Field(
        default=False,
        description="Write the agent's output to a file.",
    )
    wait_for_input: bool = Field(
        default=False,
        description="Wait for user input after the agent's output is displayed.",
    )

    # --- Components ---
    evaluator: FlockEvaluator | None = Field(  # Make optional, allow None
        default=None,
        description="The evaluator instance defining the agent's core logic.",
    )
    handoff_router: FlockRouter | None = Field(  # Make optional, allow None
        default=None,
        description="Router determining the next agent in the workflow.",
    )
    modules: dict[str, FlockModule] = Field(  # Keep as dict
        default_factory=dict,
        description="Dictionary of FlockModules attached to this agent.",
    )

    # --- Temporal Configuration (Optional) ---
    temporal_activity_config: TemporalActivityConfig | None = Field(
        default=None,
        description="Optional Temporal settings specific to this agent's activity execution.",
    )

    # --- Runtime State (Excluded from Serialization) ---
    context: FlockContext | None = Field(
        default=None,
        exclude=True,  # Exclude context from model_dump and serialization
        description="Runtime context associated with the flock execution.",
    )

    def __init__(
        self,
        name: str,
        model: str | None = None,
        description: str | Callable[..., str] | None = "",
        input: SignatureType = None,
        output: SignatureType = None,
        tools: list[Callable[..., Any]] | None = None,
        servers: list[str | FlockMCPServerBase] | None = None,
        evaluator: "FlockEvaluator | None" = None,
        handoff_router: "FlockRouter | None" = None,
        # Use dict for modules
        modules: dict[str, "FlockModule"] | None = None,
        write_to_file: bool = False,
        wait_for_input: bool = False,
        temporal_activity_config: TemporalActivityConfig | None = None,
        **kwargs,
    ):
        super().__init__(
            name=name,
            model=model,
            description=description,
            input=input,  # Store the raw input spec
            output=output,  # Store the raw output spec
            tools=tools,
            servers=servers,
            write_to_file=write_to_file,
            wait_for_input=wait_for_input,
            evaluator=evaluator,
            handoff_router=handoff_router,
            modules=modules
            if modules is not None
            else {},  # Ensure modules is a dict
            temporal_activity_config=temporal_activity_config,
            **kwargs,
        )

        if isinstance(self.input, type) and issubclass(self.input, BaseModel):
            self._input_model = self.input
        if isinstance(self.output, type) and issubclass(self.output, BaseModel):
            self._output_model = self.output

    # --- Existing Methods (add_module, remove_module, etc.) ---
    # (Keep these methods as they were, adding type hints where useful)
    def add_module(self, module: FlockModule) -> None:
        """Add a module to this agent."""
        if not module.name:
            logger.error("Module must have a name to be added.")
            return
        if module.name in self.modules:
            logger.warning(f"Overwriting existing module: {module.name}")
        self.modules[module.name] = module
        logger.debug(f"Added module '{module.name}' to agent '{self.name}'")

    def remove_module(self, module_name: str) -> None:
        """Remove a module from this agent."""
        if module_name in self.modules:
            del self.modules[module_name]
            logger.debug(
                f"Removed module '{module_name}' from agent '{self.name}'"
            )
        else:
            logger.warning(
                f"Module '{module_name}' not found on agent '{self.name}'."
            )

    def get_module(self, module_name: str) -> FlockModule | None:
        """Get a module by name."""
        return self.modules.get(module_name)

    def get_enabled_modules(self) -> list[FlockModule]:
        """Get a list of currently enabled modules attached to this agent."""
        return [m for m in self.modules.values() if m.config.enabled]

    @property
    def resolved_description(self) -> str | None:
        """Returns the resolved agent description.
        If the description is a callable, it attempts to call it.
        Returns None if the description is None or a callable that fails.
        """
        if callable(self.description):
            try:
                # Attempt to call without context first.
                # If callables consistently need context, this might need adjustment
                # or the template-facing property might need to be simpler,
                # relying on prior resolution via resolve_callables.
                return self.description()
            except TypeError:
                # Log a warning that context might be needed?
                # For now, treat as unresolvable in this simple property.
                logger.warning(
                    f"Callable description for agent '{self.name}' could not be resolved "
                    f"without context via the simple 'resolved_description' property. "
                    f"Consider calling 'agent.resolve_callables(context)' beforehand if context is required."
                )
                return None # Or a placeholder like "[Callable Description]"
            except Exception as e:
                logger.error(
                    f"Error resolving callable description for agent '{self.name}': {e}"
                )
                return None
        elif isinstance(self.description, str):
            return self.description
        return None

    # --- Lifecycle Hooks (Keep as they were) ---
    async def initialize(self, inputs: dict[str, Any]) -> None:
        """Initialize agent and run module initializers."""
        logger.debug(f"Initializing agent '{self.name}'")
        with tracer.start_as_current_span("agent.initialize") as span:
            span.set_attribute("agent.name", self.name)
            span.set_attribute("inputs", str(inputs))
            logger.info(
                f"agent.initialize",
                agent=self.name,
            )
            try:
                for module in self.get_enabled_modules():
                    await module.on_initialize(self, inputs, self.context)
            except Exception as module_error:
                logger.error(
                    "Error during initialize",
                    agent=self.name,
                    error=str(module_error),
                )
                span.record_exception(module_error)

    async def terminate(
        self, inputs: dict[str, Any], result: dict[str, Any]
    ) -> None:
        """Terminate agent and run module terminators."""
        logger.debug(f"Terminating agent '{self.name}'")
        with tracer.start_as_current_span("agent.terminate") as span:
            span.set_attribute("agent.name", self.name)
            span.set_attribute("inputs", str(inputs))
            span.set_attribute("result", str(result))
            logger.info(
                f"agent.terminate",
                agent=self.name,
            )
            try:
                current_result = result
                for module in self.get_enabled_modules():
                    tmp_result = await module.on_terminate(
                        self, inputs, self.context, current_result
                    )
                    # If the module returns a result, use it
                    if tmp_result:
                        current_result = tmp_result

                if self.write_to_file:
                    self._save_output(self.name, current_result)

                if self.wait_for_input:
                    # simple input prompt
                    input("Press Enter to continue...")

            except Exception as module_error:
                logger.error(
                    "Error during terminate",
                    agent=self.name,
                    error=str(module_error),
                )
                span.record_exception(module_error)

    async def on_error(self, error: Exception, inputs: dict[str, Any]) -> None:
        """Handle errors and run module error handlers."""
        logger.error(f"Error occurred in agent '{self.name}': {error}")
        with tracer.start_as_current_span("agent.on_error") as span:
            span.set_attribute("agent.name", self.name)
            span.set_attribute("inputs", str(inputs))
            try:
                for module in self.get_enabled_modules():
                    await module.on_error(self, inputs, self.context, error)
            except Exception as module_error:
                logger.error(
                    "Error during on_error",
                    agent=self.name,
                    error=str(module_error),
                )
                span.record_exception(module_error)

    async def evaluate(self, inputs: dict[str, Any]) -> dict[str, Any]:
        """Core evaluation logic, calling the assigned evaluator and modules."""
        if not self.evaluator:
            raise RuntimeError(
                f"Agent '{self.name}' has no evaluator assigned."
            )
        with tracer.start_as_current_span("agent.evaluate") as span:
            span.set_attribute("agent.name", self.name)
            span.set_attribute("inputs", str(inputs))
            logger.info(
                f"agent.evaluate",
                agent=self.name,
            )

            logger.debug(f"Evaluating agent '{self.name}'")
            current_inputs = inputs

            # Pre-evaluate hooks
            for module in self.get_enabled_modules():
                current_inputs = await module.on_pre_evaluate(
                    self, current_inputs, self.context
                )

            # Actual evaluation
            try:
                # Pass registered tools if the evaluator needs them
                registered_tools = []
                if self.tools:
                    # Ensure tools are actually retrieved/validated if needed by evaluator type
                    # For now, assume evaluator handles tool resolution if necessary
                    registered_tools = self.tools

                # Retrieve available mcp_tools if the evaluator needs them
                mcp_tools = []
                if self.servers:
                    from flock.core.flock_registry import get_registry

                    FlockRegistry = get_registry()  # Get the registry
                    for server in self.servers:
                        registered_server: FlockMCPServerBase | None = None
                        server_tools = []
                        if isinstance(server, FlockMCPServerBase):
                            # check if registered
                            server_name = server.config.name
                            registered_server = FlockRegistry.get_server(
                                server_name
                            )
                        else:
                            # servers must be registered.
                            registered_server = FlockRegistry.get_server(
                                name=server
                            )
                        if registered_server:
                            server_tools = await registered_server.get_tools(
                                agent_id=self.agent_id,
                                run_id=self.context.run_id,
                            )
                        else:
                            logger.warning(
                                f"No Server with name '{server}' registered! Skipping."
                            )
                        mcp_tools = mcp_tools + server_tools

                # --------------------------------------------------
                # Optional DI middleware pipeline
                # --------------------------------------------------
                container = None
                if self.context is not None:
                    container = self.context.get_variable("di.container")

                # If a MiddlewarePipeline is registered in DI, wrap the evaluator
                result: dict[str, Any] | None = None

                if container is not None:
                    try:
                        from wd.di.middleware import (
                            MiddlewarePipeline,
                        )

                        pipeline: MiddlewarePipeline | None = None
                        try:
                            pipeline = container.get_service(MiddlewarePipeline)
                        except Exception:
                            pipeline = None

                        if pipeline is not None:
                            # Build execution chain where the evaluator is the terminal handler

                            async def _final_handler():
                                return await self.evaluator.evaluate(
                                    self, current_inputs, registered_tools
                                )

                            idx = 0

                            async def _invoke_next():
                                nonlocal idx

                                if idx < len(pipeline._middleware):
                                    mw = pipeline._middleware[idx]
                                    idx += 1
                                    return await mw(self.context, _invoke_next)  # type: ignore[arg-type]
                                return await _final_handler()

                            # Execute pipeline
                            result = await _invoke_next()
                        else:
                            # No pipeline registered, direct evaluation
                            result = await self.evaluator.evaluate(
                                self, current_inputs, registered_tools
                            )
                    except ImportError:
                        # wd.di not installed – fall back
                        result = await self.evaluator.evaluate(
                            self, current_inputs, registered_tools
                        )
                else:
                    # No DI container – standard execution
                    result = await self.evaluator.evaluate(
                        self,
                    current_inputs,
                    registered_tools,
                    mcp_tools=mcp_tools,
                    )
            except Exception as eval_error:
                logger.error(
                    "Error during evaluate",
                    agent=self.name,
                    error=str(eval_error),
                )
                span.record_exception(eval_error)
                await self.on_error(
                    eval_error, current_inputs
                )  # Call error hook
                raise  # Re-raise the exception

            # Post-evaluate hooks
            current_result = result
            for module in self.get_enabled_modules():
                tmp_result = await module.on_post_evaluate(
                    self,
                    current_inputs,
                    self.context,
                    current_result,
                )
                # If the module returns a result, use it
                if tmp_result:
                    current_result = tmp_result

            logger.debug(f"Evaluation completed for agent '{self.name}'")
            return current_result

    def run(self, inputs: dict[str, Any]) -> dict[str, Any]:
        """Synchronous wrapper for run_async."""
        try:
            loop = asyncio.get_running_loop()
        except (
            RuntimeError
        ):  # 'RuntimeError: There is no current event loop...'
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        return loop.run_until_complete(self.run_async(inputs))

    def set_model(self, model: str):
        """Set the model for the agent and its evaluator."""
        self.model = model
        if self.evaluator and hasattr(self.evaluator, "config"):
            self.evaluator.config.model = model
            logger.info(
                f"Set model to '{model}' for agent '{self.name}' and its evaluator."
            )
        elif self.evaluator:
            logger.warning(
                f"Evaluator for agent '{self.name}' does not have a standard config to set model."
            )
        else:
            logger.warning(
                f"Agent '{self.name}' has no evaluator to set model for."
            )

    async def run_async(self, inputs: dict[str, Any]) -> dict[str, Any]:
        """Asynchronous execution logic with lifecycle hooks."""
        with tracer.start_as_current_span("agent.run") as span:
            span.set_attribute("agent.name", self.name)
            span.set_attribute("inputs", str(inputs))
            try:
                await self.initialize(inputs)
                result = await self.evaluate(inputs)
                await self.terminate(inputs, result)
                span.set_attribute("result", str(result))
                logger.info("Agent run completed", agent=self.name)
                return result
            except Exception as run_error:
                logger.error(
                    "Error running agent", agent=self.name, error=str(run_error)
                )
                if "evaluate" not in str(
                    run_error
                ):  # Simple check, might need refinement
                    await self.on_error(run_error, inputs)
                logger.error(
                    f"Agent '{self.name}' run failed: {run_error}",
                    exc_info=True,
                )
                span.record_exception(run_error)
                raise  # Re-raise after handling

    async def run_temporal(self, inputs: dict[str, Any]) -> dict[str, Any]:
        with tracer.start_as_current_span("agent.run_temporal") as span:
            span.set_attribute("agent.name", self.name)
            span.set_attribute("inputs", str(inputs))
            try:
                from temporalio.client import Client

                from flock.workflow.agent_activities import (
                    run_flock_agent_activity,
                )
                from flock.workflow.temporal_setup import run_activity

                client = await Client.connect(
                    "localhost:7233", namespace="default"
                )
                agent_data = self.to_dict()
                inputs_data = inputs

                result = await run_activity(
                    client,
                    self.name,
                    run_flock_agent_activity,
                    {"agent_data": agent_data, "inputs": inputs_data},
                )
                span.set_attribute("result", str(result))
                logger.info("Temporal run successful", agent=self.name)
                return result
            except Exception as temporal_error:
                logger.error(
                    "Error in Temporal workflow",
                    agent=self.name,
                    error=str(temporal_error),
                )
                span.record_exception(temporal_error)
                raise

    def add_component(
        self,
        config_instance: FlockModuleConfig
        | FlockRouterConfig
        | FlockEvaluatorConfig,
        component_name: str | None = None,
    ) -> "FlockAgent":
        """Adds or replaces a component (Evaluator, Router, Module) based on its configuration object.

        Args:
            config_instance: An instance of a config class inheriting from
                             FlockModuleConfig, FlockRouterConfig, or FlockEvaluatorConfig.
            component_name: Explicit name for the component (required for Modules if not in config).

        Returns:
            self for potential chaining.
        """
        from flock.core.flock_registry import get_registry

        config_type = type(config_instance)
        registry = get_registry()  # Get registry instance
        logger.debug(
            f"Attempting to add component via config: {config_type.__name__}"
        )

        # --- 1. Find Component Class using Registry Map ---
        ComponentClass = registry.get_component_class_for_config(config_type)

        if not ComponentClass:
            logger.error(
                f"No component class registered for config type {config_type.__name__}. Use @flock_component(config_class=...) on the component."
            )
            raise TypeError(
                f"Cannot find component class for config {config_type.__name__}"
            )

        component_class_name = ComponentClass.__name__
        logger.debug(
            f"Found component class '{component_class_name}' mapped to config '{config_type.__name__}'"
        )

        # --- 2. Determine Assignment Target and Name (Same as before) ---
        instance_name = component_name
        attribute_name: str = ""

        if issubclass(ComponentClass, FlockEvaluator):
            attribute_name = "evaluator"
            if not instance_name:
                instance_name = getattr(
                    config_instance, "name", component_class_name.lower()
                )

        elif issubclass(ComponentClass, FlockRouter):
            attribute_name = "handoff_router"
            if not instance_name:
                instance_name = getattr(
                    config_instance, "name", component_class_name.lower()
                )

        elif issubclass(ComponentClass, FlockModule):
            attribute_name = "modules"
            if not instance_name:
                instance_name = getattr(
                    config_instance, "name", component_class_name.lower()
                )
            if not instance_name:
                raise ValueError(
                    "Module name must be provided either in config or as component_name argument."
                )
            # Ensure config has name if module expects it
            if hasattr(config_instance, "name") and not getattr(
                config_instance, "name", None
            ):
                setattr(config_instance, "name", instance_name)

        else:  # Should be caught by registry map logic ideally
            raise TypeError(
                f"Class '{component_class_name}' mapped from config is not a valid Flock component."
            )

        # --- 3. Instantiate the Component (Same as before) ---
        try:
            init_args = {"config": config_instance, "name": instance_name}

            component_instance = ComponentClass(**init_args)
        except Exception as e:
            logger.error(
                f"Failed to instantiate {ComponentClass.__name__} with config {config_type.__name__}: {e}",
                exc_info=True,
            )
            raise RuntimeError(f"Component instantiation failed: {e}") from e

        # --- 4. Assign to the Agent (Same as before) ---
        if attribute_name == "modules":
            if not isinstance(self.modules, dict):
                self.modules = {}
            self.modules[instance_name] = component_instance
            logger.info(
                f"Added/Updated module '{instance_name}' (type: {ComponentClass.__name__}) to agent '{self.name}'"
            )
        else:
            setattr(self, attribute_name, component_instance)
            logger.info(
                f"Set {attribute_name} to {ComponentClass.__name__} (instance name: '{instance_name}') for agent '{self.name}'"
            )

        return self

    # resolve_callables remains useful for dynamic definitions
    def resolve_callables(self, context: FlockContext | None = None) -> None:
        """Resolves callable fields (description, input, output) using context."""
        if callable(self.description):
            self.description = self.description(
                context
            )  # Pass context if needed by callable
        if callable(self.input):
            self.input = self.input(context)
        if callable(self.output):
            self.output = self.output(context)

    # --- Serialization Implementation ---

    def _save_output(self, agent_name: str, result: dict[str, Any]) -> None:
        """Save output to file if configured."""
        if not self.write_to_file:
            return

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{agent_name}_output_{timestamp}.json"
        filepath = os.path.join(".flock/output/", filename)
        os.makedirs(".flock/output/", exist_ok=True)

        output_data = {
            "agent": agent_name,
            "timestamp": timestamp,
            "output": result,
        }

        try:
            with open(filepath, "w") as f:
                json.dump(output_data, f, indent=2, cls=FlockJSONEncoder)
        except Exception as e:
            logger.warning(f"Failed to save output to file: {e}")

    def to_dict(self) -> dict[str, Any]:
        """Convert instance to dictionary representation suitable for serialization."""
        from flock.core.flock_registry import get_registry

        FlockRegistry = get_registry()

        exclude = [
            "context",
            "evaluator",
            "modules",
            "handoff_router",
            "tools",
            "servers",
        ]

        is_descrition_callable = False
        is_input_callable = False
        is_output_callable = False

        # if self.description is a callable, exclude it
        if callable(self.description):
            is_descrition_callable = True
            exclude.append("description")
        # if self.input is a callable, exclude it
        if callable(self.input):
            is_input_callable = True
            exclude.append("input")
        # if self.output is a callable, exclude it
        if callable(self.output):
            is_output_callable = True
            exclude.append("output")

        logger.debug(f"Serializing agent '{self.name}' to dict.")
        # Use Pydantic's dump, exclude manually handled fields and runtime context
        data = self.model_dump(
            exclude=exclude,
            mode="json",  # Use json mode for better handling of standard types by Pydantic
            exclude_none=True,  # Exclude None values for cleaner output
        )
        logger.debug(f"Base agent data for '{self.name}': {list(data.keys())}")
        serialized_modules = {}

        def add_serialized_component(component: Any, field_name: str):
            if component:
                comp_type = type(component)
                type_name = FlockRegistry.get_component_type_name(
                    comp_type
                )  # Get registered name
                if type_name:
                    try:
                        serialized_component_data = serialize_item(component)

                        if not isinstance(serialized_component_data, dict):
                            logger.error(
                                f"Serialization of component {type_name} for field '{field_name}' did not result in a dictionary. Got: {type(serialized_component_data)}"
                            )
                            serialized_modules[field_name] = {
                                "type": type_name,
                                "name": getattr(component, "name", "unknown"),
                                "error": "serialization_failed_non_dict",
                            }
                        else:
                            serialized_component_data["type"] = type_name
                            serialized_modules[field_name] = (
                                serialized_component_data
                            )
                            logger.debug(
                                f"Successfully serialized component for field '{field_name}' (type: {type_name})"
                            )

                    except Exception as e:
                        logger.error(
                            f"Failed to serialize component {type_name} for field '{field_name}': {e}",
                            exc_info=True,
                        )
                        serialized_modules[field_name] = {
                            "type": type_name,
                            "name": getattr(component, "name", "unknown"),
                            "error": "serialization_failed",
                        }
                else:
                    logger.warning(
                        f"Cannot serialize unregistered component {comp_type.__name__} for field '{field_name}'"
                    )

        add_serialized_component(self.evaluator, "evaluator")
        if serialized_modules:
            data["evaluator"] = serialized_modules["evaluator"]
            logger.debug(f"Added evaluator to agent '{self.name}'")

        serialized_modules = {}
        add_serialized_component(self.handoff_router, "handoff_router")
        if serialized_modules:
            data["handoff_router"] = serialized_modules["handoff_router"]
            logger.debug(f"Added handoff_router to agent '{self.name}'")

        serialized_modules = {}
        for module in self.modules.values():
            add_serialized_component(module, module.name)

        if serialized_modules:
            data["modules"] = serialized_modules
            logger.debug(
                f"Added {len(serialized_modules)} modules to agent '{self.name}'"
            )

        # --- Serialize Servers ---
        if self.servers:
            logger.debug(
                f"Serializing {len(self.servers)} servers for agent '{self.name}'"
            )
            serialized_servers = []
            for server in self.servers:
                if isinstance(server, FlockMCPServerBase):
                    serialized_servers.append(server.config.name)
                else:
                    # Write it down as a list of server names.
                    serialized_servers.append(server)

            if serialized_servers:
                data["mcp_servers"] = serialized_servers
                logger.debug(
                    f"Added {len(serialized_servers)} servers to agent '{self.name}'"
                )

        # --- Serialize Tools (Callables) ---
        if self.tools:
            logger.debug(
                f"Serializing {len(self.tools)} tools for agent '{self.name}'"
            )
            serialized_tools = []
            for tool in self.tools:
                if callable(tool) and not isinstance(tool, type):
                    path_str = FlockRegistry.get_callable_path_string(tool)
                    if path_str:
                        # Get just the function name from the path string
                        # If it's a namespaced path like module.submodule.function_name
                        # Just use the function_name part
                        func_name = path_str.split(".")[-1]
                        serialized_tools.append(func_name)
                        logger.debug(
                            f"Added tool '{func_name}' (from path '{path_str}') to agent '{self.name}'"
                        )
                    else:
                        logger.warning(
                            f"Could not get path string for tool {tool} in agent '{self.name}'. Skipping."
                        )
                else:
                    logger.warning(
                        f"Non-callable item found in tools list for agent '{self.name}': {tool}. Skipping."
                    )
            if serialized_tools:
                data["tools"] = serialized_tools
                logger.debug(
                    f"Added {len(serialized_tools)} tools to agent '{self.name}'"
                )

        if is_descrition_callable:
            path_str = FlockRegistry.get_callable_path_string(self.description)
            if path_str:
                func_name = path_str.split(".")[-1]
                data["description_callable"] = func_name
                logger.debug(
                    f"Added description '{func_name}' (from path '{path_str}') to agent '{self.name}'"
                )
            else:
                logger.warning(
                    f"Could not get path string for description {self.description} in agent '{self.name}'. Skipping."
                )

        if is_input_callable:
            path_str = FlockRegistry.get_callable_path_string(self.input)
            if path_str:
                func_name = path_str.split(".")[-1]
                data["input_callable"] = func_name
                logger.debug(
                    f"Added input '{func_name}' (from path '{path_str}') to agent '{self.name}'"
                )
            else:
                logger.warning(
                    f"Could not get path string for input {self.input} in agent '{self.name}'. Skipping."
                )

        if is_output_callable:
            path_str = FlockRegistry.get_callable_path_string(self.output)
            if path_str:
                func_name = path_str.split(".")[-1]
                data["output_callable"] = func_name
                logger.debug(
                    f"Added output '{func_name}' (from path '{path_str}') to agent '{self.name}'"
                )
            else:
                logger.warning(
                    f"Could not get path string for output {self.output} in agent '{self.name}'. Skipping."
                )

        # No need to call _filter_none_values here as model_dump(exclude_none=True) handles it
        logger.info(
            f"Serialization of agent '{self.name}' complete with {len(data)} fields"
        )
        return data

    @classmethod
    def from_dict(cls: type[T], data: dict[str, Any]) -> T:
        """Deserialize the agent from a dictionary, including components, tools, and callables."""
        from flock.core.flock_registry import (
            get_registry,  # Import registry locally
        )

        registry = get_registry()
        logger.debug(
            f"Deserializing agent from dict. Keys: {list(data.keys())}"
        )

        # --- Separate Data ---
        component_configs = {}
        callable_configs = {}
        tool_config = []
        servers_config = []
        agent_data = {}

        component_keys = [
            "evaluator",
            "handoff_router",
            "modules",
            "temporal_activity_config",
        ]
        callable_keys = [
            "description_callable",
            "input_callable",
            "output_callable",
        ]
        tool_key = "tools"

        servers_key = "mcp_servers"

        for key, value in data.items():
            if key in component_keys and value is not None:
                component_configs[key] = value
            elif key in callable_keys and value is not None:
                callable_configs[key] = value
            elif key == tool_key and value is not None:
                tool_config = value  # Expecting a list of names
            elif key == servers_key and value is not None:
                servers_config = value  # Expecting a list of names
            elif key not in component_keys + callable_keys + [
                tool_key,
                servers_key,
            ]:  # Avoid double adding
                agent_data[key] = value
            # else: ignore keys that are None or already handled

        # --- Deserialize Base Agent ---
        # Ensure required fields like 'name' are present if needed by __init__
        if "name" not in agent_data:
            raise ValueError(
                "Agent data must include a 'name' field for deserialization."
            )
        agent_name_log = agent_data["name"]  # For logging
        logger.info(f"Deserializing base agent data for '{agent_name_log}'")

        # Pydantic should handle base fields based on type hints in __init__
        agent = cls(**agent_data)
        logger.debug(f"Base agent '{agent.name}' instantiated.")

        # --- Deserialize Components ---
        logger.debug(f"Deserializing components for '{agent.name}'")
        # Evaluator
        if "evaluator" in component_configs:
            try:
                agent.evaluator = deserialize_component(
                    component_configs["evaluator"], FlockEvaluator
                )
                logger.debug(f"Deserialized evaluator for '{agent.name}'")
            except Exception as e:
                logger.error(
                    f"Failed to deserialize evaluator for '{agent.name}': {e}",
                    exc_info=True,
                )

        # Handoff Router
        if "handoff_router" in component_configs:
            try:
                agent.handoff_router = deserialize_component(
                    component_configs["handoff_router"], FlockRouter
                )
                logger.debug(f"Deserialized handoff_router for '{agent.name}'")
            except Exception as e:
                logger.error(
                    f"Failed to deserialize handoff_router for '{agent.name}': {e}",
                    exc_info=True,
                )

        # Modules
        if "modules" in component_configs:
            agent.modules = {}  # Initialize
            for module_name, module_data in component_configs[
                "modules"
            ].items():
                try:
                    module_instance = deserialize_component(
                        module_data, FlockModule
                    )
                    if module_instance:
                        # Use add_module for potential logic within it
                        agent.add_module(module_instance)
                        logger.debug(
                            f"Deserialized and added module '{module_name}' for '{agent.name}'"
                        )
                except Exception as e:
                    logger.error(
                        f"Failed to deserialize module '{module_name}' for '{agent.name}': {e}",
                        exc_info=True,
                    )

        # Temporal Activity Config
        if "temporal_activity_config" in component_configs:
            try:
                agent.temporal_activity_config = TemporalActivityConfig(
                    **component_configs["temporal_activity_config"]
                )
                logger.debug(
                    f"Deserialized temporal_activity_config for '{agent.name}'"
                )
            except Exception as e:
                logger.error(
                    f"Failed to deserialize temporal_activity_config for '{agent.name}': {e}",
                    exc_info=True,
                )
                agent.temporal_activity_config = None

        # --- Deserialize Tools ---
        agent.tools = []  # Initialize tools list
        if tool_config:
            logger.debug(
                f"Deserializing {len(tool_config)} tools for '{agent.name}'"
            )
            # Use get_callable to find each tool
            for tool_name_or_path in tool_config:
                try:
                    found_tool = registry.get_callable(tool_name_or_path)
                    if found_tool and callable(found_tool):
                        agent.tools.append(found_tool)
                        logger.debug(
                            f"Resolved and added tool '{tool_name_or_path}' for agent '{agent.name}'"
                        )
                    else:
                        # Should not happen if get_callable returns successfully but just in case
                        logger.warning(
                            f"Registry returned non-callable for tool '{tool_name_or_path}' for agent '{agent.name}'. Skipping."
                        )
                except (
                    ValueError
                ) as e:  # get_callable raises ValueError if not found/ambiguous
                    logger.warning(
                        f"Could not resolve tool '{tool_name_or_path}' for agent '{agent.name}': {e}. Skipping."
                    )
                except Exception as e:
                    logger.error(
                        f"Unexpected error resolving tool '{tool_name_or_path}' for agent '{agent.name}': {e}. Skipping.",
                        exc_info=True,
                    )

        # --- Deserialize Servers ---
        agent.servers = []  # Initialize Servers list.
        if servers_config:
            logger.debug(
                f"Deserializing {len(servers_config)} servers for '{agent.name}'"
            )
            # Agents keep track of server by getting a list of server names.
            # The server instances will be retrieved during runtime from the registry. (default behavior)

            for server_name in servers_config:
                if isinstance(server_name, str):
                    # Case 1 (default behavior): A server name is passe.
                    agent.servers.append(server_name)
                elif isinstance(server_name, FlockMCPServerBase):
                    # Case 2 (highly unlikely): If someone somehow manages to pass
                    # an instance of a server during the deserialization step (however that might be achieved)
                    # check the registry, if the server is already registered, if not, register it
                    # and store the name in the servers list
                    FlockRegistry = get_registry()
                    server_exists = (
                        FlockRegistry.get_server(server_name.config.name)
                        is not None
                    )
                    if server_exists:
                        agent.servers.append(server_name.config.name)
                    else:
                        FlockRegistry.register_server(
                            server=server_name
                        )  # register it.
                        agent.servers.append(server_name.config.name)

        # --- Deserialize Callables ---
        logger.debug(f"Deserializing callable fields for '{agent.name}'")
        # available_callables = registry.get_all_callables() # Incorrect

        def resolve_and_assign(field_name: str, callable_key: str):
            if callable_key in callable_configs:
                callable_name = callable_configs[callable_key]
                try:
                    # Use get_callable to find the signature function
                    found_callable = registry.get_callable(callable_name)
                    if found_callable and callable(found_callable):
                        setattr(agent, field_name, found_callable)
                        logger.debug(
                            f"Resolved callable '{callable_name}' for field '{field_name}' on agent '{agent.name}'"
                        )
                    else:
                        logger.warning(
                            f"Registry returned non-callable for name '{callable_name}' for field '{field_name}' on agent '{agent.name}'. Field remains default."
                        )
                except (
                    ValueError
                ) as e:  # get_callable raises ValueError if not found/ambiguous
                    logger.warning(
                        f"Could not resolve callable '{callable_name}' in registry for field '{field_name}' on agent '{agent.name}': {e}. Field remains default."
                    )
                except Exception as e:
                    logger.error(
                        f"Unexpected error resolving callable '{callable_name}' for field '{field_name}' on agent '{agent.name}': {e}. Field remains default.",
                        exc_info=True,
                    )
            # Else: key not present, field retains its default value from __init__

        resolve_and_assign("description", "description_callable")
        resolve_and_assign("input", "input_callable")
        resolve_and_assign("output", "output_callable")

        logger.info(f"Successfully deserialized agent '{agent.name}'.")
        return agent

    # --- Pydantic v2 Configuration ---
    class Config:
        arbitrary_types_allowed = (
            True  # Important for components like evaluator, router etc.
        )
        # Might need custom json_encoders if not using model_dump(mode='json') everywhere
        # json_encoders = {
        #      FlockEvaluator: lambda v: v.to_dict() if v else None,
        #      FlockRouter: lambda v: v.to_dict() if v else None,
        #      FlockModule: lambda v: v.to_dict() if v else None,
        # }
```

### src\flock\core\flock_evaluator.py

- **Lines**: 60
- **Last modified**: 2025-05-25 23:17:06

```py
"""Base classes and implementations for Flock evaluators."""

from abc import ABC, abstractmethod
from typing import Any, TypeVar

from pydantic import BaseModel, Field, create_model

T = TypeVar("T", bound="FlockEvaluatorConfig")


class FlockEvaluatorConfig(BaseModel):
    """Base configuration class for Flock modules.

    This class serves as the base for all module-specific configurations.
    Each module should define its own config class inheriting from this one.

    Example:
        class MemoryModuleConfig(FlockModuleConfig):
            file_path: str = Field(default="memory.json")
            save_after_update: bool = Field(default=True)
    """

    model: str = Field(
        default="", description="The model to use for evaluation"
    )

    @classmethod
    def with_fields(cls: type[T], **field_definitions) -> type[T]:
        """Create a new config class with additional fields."""
        return create_model(
            f"Dynamic{cls.__name__}", __base__=cls, **field_definitions
        )


class FlockEvaluator(ABC, BaseModel):
    """Base class for all evaluators in Flock.

    An evaluator is responsible for taking inputs and producing outputs using
    some evaluation strategy (e.g., DSPy, natural language, etc.).
    """

    name: str = Field(..., description="Unique identifier for this evaluator")
    config: FlockEvaluatorConfig = Field(
        default_factory=FlockEvaluatorConfig,
        description="Evaluator configuration",
    )

    def __init__(self, **data):
        super().__init__(**data)

    @abstractmethod
    async def evaluate(
        self,
        agent: Any,
        inputs: dict[str, Any],
        tools: list[Any],
        mcp_tools: list[Any] | None = None,
    ) -> dict[str, Any]:
        """Evaluate inputs to produce outputs."""
        pass
```

### src\flock\core\flock_factory.py

- **Lines**: 383
- **Last modified**: 2025-05-26 13:19:27

```py
"""Factory for creating pre-configured Flock agents."""

import os
from collections.abc import Callable
from pathlib import Path
from typing import Any, Literal

from pydantic import AnyUrl, BaseModel, Field, FileUrl

from flock.core.flock_agent import FlockAgent, SignatureType
from flock.core.logging.formatters.themes import OutputTheme
from flock.core.mcp.flock_mcp_server import FlockMCPServerBase
from flock.core.mcp.mcp_config import (
    FlockMCPCachingConfigurationBase,
    FlockMCPCallbackConfigurationBase,
    FlockMCPFeatureConfigurationBase,
)
from flock.core.mcp.types.types import (
    FlockListRootsMCPCallback,
    FlockLoggingMCPCallback,
    FlockMessageHandlerMCPCallback,
    FlockSamplingMCPCallback,
    MCPRoot,
    SseServerParameters,
    StdioServerParameters,
    WebsocketServerParameters,
)
from flock.evaluators.declarative.declarative_evaluator import (
    DeclarativeEvaluator,
    DeclarativeEvaluatorConfig,
)
from flock.mcp.servers.sse.flock_sse_server import (
    FlockSSEConfig,
    FlockSSEConnectionConfig,
    FlockSSEServer,
)
from flock.mcp.servers.stdio.flock_stdio_server import (
    FlockMCPStdioServer,
    FlockStdioConfig,
    FlockStdioConnectionConfig,
)
from flock.mcp.servers.websockets.flock_websocket_server import (
    FlockWSConfig,
    FlockWSConnectionConfig,
    FlockWSServer,
)
from flock.modules.output.output_module import OutputModule, OutputModuleConfig
from flock.modules.performance.metrics_module import (
    MetricsModule,
    MetricsModuleConfig,
)
from flock.workflow.temporal_config import TemporalActivityConfig

LoggingLevel = Literal[
    "debug",
    "info",
    "notice",
    "warning",
    "error",
    "critical",
    "alert",
    "emergency",
]


class FlockFactory:
    """Factory for creating pre-configured Flock agents and pre-configured Flock MCPServers with common module setups."""

    # Classes for type-hints.
    class StdioParams(BaseModel):
        """Factory-Params for Stdio-Servers."""

        command: str = Field(
            ...,
            description="Command for starting the local script. (e.g. 'uvx', 'bun', 'npx', 'bunx', etc.)",
        )

        args: list[str] = Field(
            ...,
            description="Arguments for starting the local script. (e.g. ['run', './mcp-server.py'])",
        )

        env: dict[str, Any] | None = Field(
            default=None,
            description="Environment variables to pass to the server. (e.g. {'GOOGLE_API_KEY': 'MY_SUPER_SECRET_API_KEY'})",
        )

        cwd: str | Path | None = Field(
            default_factory=os.getcwd,
            description="The working directory to start the script in.",
        )

        encoding: str = Field(
            default="utf-8",
            description="The char-encoding to use when talking to a stdio server. (e.g. 'utf-8', 'ascii', etc.)",
        )

        encoding_error_handler: Literal["strict", "ignore", "replace"] = Field(
            default="strict",
            description="The text encoding error handler. See https://docs.python.org/3/library/codecs.html#codec-base-classes for explanations of possible values",
        )

    class SSEParams(BaseModel):
        """Factory-Params for SSE-Servers."""

        url: str | AnyUrl = Field(
            ...,
            description="Url the server listens at. (e.g. https://my-mcp-server.io/sse)",
        )

        headers: dict[str, Any] | None = Field(
            default=None,
            description="Additional Headers to pass to the client.",
        )

        timeout_seconds: float | int = Field(
            default=5, description="Http Timeout in Seconds."
        )

        sse_read_timeout_seconds: float | int = Field(
            default=60 * 5,
            description="How many seconds to wait for server-sent events until closing the connection. (connections will be automatically re-established.)",
        )

    class WebsocketParams(BaseModel):
        """Factory-Params for Websocket Servers."""

        url: str | AnyUrl = Field(
            ...,
            description="The url the server listens at. (e.g. ws://my-mcp-server.io/messages)",
        )

    @staticmethod
    def create_mcp_server(
        name: str,
        connection_params: SSEParams | StdioParams | WebsocketParams,
        max_retries: int = 3,
        mount_points: list[str | MCPRoot] | None = None,
        timeout_seconds: int | float = 10,
        server_logging_level: LoggingLevel = "error",
        enable_roots_feature: bool = False,
        enable_tools_feature: bool = False,
        enable_sampling_feature: bool = False,
        enable_prompts_feature: bool = False,
        sampling_callback: FlockSamplingMCPCallback | None = None,
        list_roots_callback: FlockListRootsMCPCallback | None = None,
        logging_callback: FlockLoggingMCPCallback | None = None,
        message_handler: FlockMessageHandlerMCPCallback | None = None,
        tool_cache_size: float = 100,
        tool_cache_ttl: float = 60,
        resource_contents_cache_size=10,
        resource_contents_cache_ttl=60 * 5,
        resource_list_cache_size=100,
        resource_list_cache_ttl=100,
        tool_result_cache_size=100,
        tool_result_cache_ttl=100,
        description: str | Callable[..., str] | None = None,
        alert_latency_threshold_ms: int = 30000,
    ) -> FlockMCPServerBase:
        """Create a default MCP Server with common modules.

        Allows for creating one of the three default-implementations provided
        by Flock:
        - SSE-Server (specify "sse" in type)
        - Stdio-Server (specify "stdio" in type)
        - Websockets-Server (specifiy "websockets" in type)
        """
        # infer server type from the pydantic model class
        if isinstance(connection_params, FlockFactory.StdioParams):
            server_kind = "stdio"
            concrete_server_cls = FlockMCPStdioServer
        if isinstance(connection_params, FlockFactory.SSEParams):
            server_kind = "sse"
            concrete_server_cls = FlockSSEServer
        if isinstance(connection_params, FlockFactory.WebsocketParams):
            server_kind = "websockets"
            concrete_server_cls = FlockWSServer

        # convert mount points.
        mounts: list[MCPRoot] = []
        if mount_points:
            for item in mount_points:
                if isinstance(item, MCPRoot):
                    mounts.append(item)
                elif isinstance(item, str):
                    try:
                        conv = MCPRoot(uri=FileUrl(url=item))
                        mounts.append(conv)
                    except Exception:
                        continue  # ignore
                else:
                    continue  # ignore

        # build generic configs
        feature_config = FlockMCPFeatureConfigurationBase(
            roots_enabled=enable_roots_feature,
            tools_enabled=enable_tools_feature,
            prompts_enabled=enable_prompts_feature,
            sampling_enabled=enable_sampling_feature,
        )
        callback_config = FlockMCPCallbackConfigurationBase(
            sampling_callback=sampling_callback,
            list_roots_callback=list_roots_callback,
            logging_callback=logging_callback,
            message_handler=message_handler,
        )
        caching_config = FlockMCPCachingConfigurationBase(
            tool_cache_max_size=tool_cache_size,
            tool_cache_max_ttl=tool_cache_ttl,
            resource_contents_cache_max_size=resource_contents_cache_size,
            resource_contents_cache_max_ttl=resource_contents_cache_ttl,
            resource_list_cache_max_size=resource_list_cache_size,
            resource_list_cache_max_ttl=resource_list_cache_ttl,
            tool_result_cache_max_size=tool_result_cache_size,
            tool_result_cache_max_ttl=tool_result_cache_ttl,
        )
        connection_config = None
        server_config: (
            FlockStdioConfig | FlockSSEConfig | FlockWSConfig | None
        ) = None

        # Instantiate correct server + config
        if server_kind == "stdio":
            # build stdio config
            connection_config = FlockStdioConnectionConfig(
                max_retries=max_retries,
                connection_parameters=StdioServerParameters(
                    command=connection_params.command,
                    args=connection_params.args,
                    env=connection_params.env,
                    encoding=connection_params.encoding,
                    encoding_error_handler=connection_params.encoding_error_handler,
                    cwd=connection_params.cwd,
                ),
                mount_points=mounts,
                read_timeout_seconds=timeout_seconds,
                server_logging_level=server_logging_level,
            )
            server_config = FlockStdioConfig(
                name=name,
                connection_config=connection_config,
                feature_config=feature_config,
                caching_config=caching_config,
                callback_config=callback_config,
            )
        elif server_kind == "sse":
            # build sse config
            connection_config = FlockSSEConnectionConfig(
                max_retries=max_retries,
                connection_parameters=SseServerParameters(
                    url=connection_params.url,
                    headers=connection_params.headers,
                    timeout=connection_params.timeout_seconds,
                    sse_read_timeout=connection_params.sse_read_timeout_seconds,
                ),
                mount_points=mounts,
                server_logging_level=server_logging_level,
            )

            server_config = FlockSSEConfig(
                name=name,
                connection_config=connection_config,
                feature_config=feature_config,
                caching_config=caching_config,
                callback_config=callback_config,
            )

        elif server_kind == "websockets":
            # build websocket config
            connection_config = FlockWSConnectionConfig(
                max_retries=max_retries,
                connection_parameters=WebsocketServerParameters(
                    url=connection_params.url,
                ),
                mount_points=mounts,
                server_logging_level=server_logging_level,
            )

            server_config = FlockWSConfig(
                name=name,
                connection_config=connection_config,
                feature_config=feature_config,
                caching_config=caching_config,
                callback_config=callback_config,
            )

        else:
            raise ValueError(
                f"Unsupported connection_params type: {type(connection_params)}"
            )

        if not server_config:
            raise ValueError(
                f"Unable to create server configuration for passed params."
            )

        server = concrete_server_cls(config=server_config)

        metrics_module_config = MetricsModuleConfig(
            latency_threshold_ms=alert_latency_threshold_ms
        )

        metrics_module = MetricsModule("metrics", config=metrics_module_config)

        server.add_module(metrics_module)

        return server

    @staticmethod
    def create_default_agent(
        name: str,
        description: str | Callable[..., str] | None = None,
        model: str | Callable[..., str] | None = None,
        input: SignatureType = None,
        output: SignatureType = None,
        tools: list[Callable[..., Any] | Any] | None = None,
        servers: list[str | FlockMCPServerBase] | None = None,
        use_cache: bool = True,
        enable_rich_tables: bool = False,
        output_theme: OutputTheme = OutputTheme.abernathy,
        wait_for_input: bool = False,
        temperature: float = 0.0,
        max_tokens: int = 8192,
        max_tool_calls: int = 10,
        max_retries: int = 3,
        alert_latency_threshold_ms: int = 30000,
        no_output: bool = False,
        print_context: bool = False,
        write_to_file: bool = False,
        stream: bool = False,
        include_thought_process: bool = False,
        temporal_activity_config: TemporalActivityConfig | None = None,
    ) -> FlockAgent:
        """Creates a default FlockAgent.

        The default agent includes the following modules:
        - DeclarativeEvaluator
        - OutputModule
        - MetricsModule

        It also includes direct acces to the most important configurations.
        """
        eval_config = DeclarativeEvaluatorConfig(
            model=model,
            use_cache=use_cache,
            max_tokens=max_tokens,
            temperature=temperature,
            max_tool_calls=max_tool_calls,
            max_retries=max_retries,
            stream=stream,
            include_thought_process=include_thought_process,
        )

        evaluator = DeclarativeEvaluator(name="default", config=eval_config)
        agent = FlockAgent(
            name=name,
            input=input,
            output=output,
            tools=tools,
            servers=servers,
            model=model,
            description=description,
            evaluator=evaluator,
            write_to_file=write_to_file,
            wait_for_input=wait_for_input,
            temporal_activity_config=temporal_activity_config,
        )
        output_config = OutputModuleConfig(
            render_table=enable_rich_tables,
            theme=output_theme,
            no_output=no_output,
            print_context=print_context,
        )
        output_module = OutputModule("output", config=output_config)

        metrics_config = MetricsModuleConfig(
            latency_threshold_ms=alert_latency_threshold_ms
        )
        metrics_module = MetricsModule("metrics", config=metrics_config)

        agent.add_module(output_module)
        agent.add_module(metrics_module)
        return agent
```

### src\flock\core\flock_module.py

- **Lines**: 207
- **Last modified**: 2025-05-22 21:27:37

```py
"""Base classes and implementations for the Flock module system."""

from abc import ABC
from typing import Any, TypeVar

from pydantic import BaseModel, Field, create_model

from flock.core.context.context import FlockContext

T = TypeVar("T", bound="FlockModuleConfig")


class FlockModuleConfig(BaseModel):
    """Base configuration class for Flock modules.

    This class serves as the base for all module-specific configurations.
    Each module should define its own config class inheriting from this one.

    Example:
        class MemoryModuleConfig(FlockModuleConfig):
            file_path: str = Field(default="memory.json")
            save_after_update: bool = Field(default=True)
    """

    enabled: bool = Field(
        default=True, description="Whether the module is currently enabled"
    )

    @classmethod
    def with_fields(cls: type[T], **field_definitions) -> type[T]:
        """Create a new config class with additional fields."""
        return create_model(
            f"Dynamic{cls.__name__}", __base__=cls, **field_definitions
        )


class FlockModule(BaseModel, ABC):
    """Base class for all Flock modules.

    Modules can hook into agent lifecycle events and modify or enhance agent behavior.
    They are initialized when added to an agent and can maintain their own state.

    Each module should define its configuration requirements either by:
    1. Creating a subclass of FlockModuleConfig
    2. Using FlockModuleConfig.with_fields() to create a config class
    """

    name: str = Field(
        default="", description="Unique identifier for the module"
    )
    config: FlockModuleConfig = Field(
        default_factory=FlockModuleConfig, description="Module configuration"
    )

    # (Historic) global-module registry removed – prefer DI container instead.

    def __init__(self, **data):
        super().__init__(**data)

    async def on_initialize(
        self,
        agent: Any,
        inputs: dict[str, Any],
        context: FlockContext | None = None,
    ) -> None:
        """Called when the agent starts running."""
        pass

    async def on_pre_evaluate(
        self,
        agent: Any,
        inputs: dict[str, Any],
        context: FlockContext | None = None,
    ) -> dict[str, Any]:
        """Called before agent evaluation, can modify inputs."""
        return inputs

    async def on_post_evaluate(
        self,
        agent: Any,
        inputs: dict[str, Any],
        context: FlockContext | None = None,
        result: dict[str, Any] | None = None,
    ) -> dict[str, Any] | None:
        """Called after agent evaluation, can modify results."""
        return result

    async def on_terminate(
        self,
        agent: Any,
        inputs: dict[str, Any],
        context: FlockContext | None = None,
        result: dict[str, Any] | None = None,
    ) -> dict[str, Any] | None:
        """Called when the agent finishes running."""
        return result

    async def on_error(
        self,
        agent: Any,
        inputs: dict[str, Any],
        context: FlockContext | None = None,
        error: Exception | None = None,
    ) -> None:
        """Called when an error occurs during agent execution."""
        pass

    async def on_pre_server_init(self, server: Any) -> None:
        """Called before a server initializes."""
        pass

    async def on_post_server_init(self, server: Any) -> None:
        """Called after a server initialized."""
        pass

    async def on_pre_server_terminate(self, server: Any) -> None:
        """Called before a server terminates."""
        pass

    async def on_post_server_terminate(self, server: Any) -> None:
        """Called after a server terminates."""
        pass

    async def on_server_error(self, server: Any, error: Exception) -> None:
        """Called when a server errors."""
        pass

    async def on_connect(
        self,
        server: Any,
        additional_params: dict[str, Any],
    ) -> dict[str, Any]:
        """Called before a connection is being established to a mcp server.

        use `server` (type FlockMCPServer) to modify the core behavior of the server.
        use `additional_params` to 'tack_on' additional configurations (for example additional headers for sse-clients.)

        (For example: modify the server's config)
        new_config = NewConfigObject(...)
        server.config = new_config

        Warning:
            Be very careful when modifying a server's internal state.
            If you just need to 'tack on' additional information (such as headers)
            or want to temporarily override certain configurations (such as timeouts)
            use `additional_params` instead if you can.

        (Or pass additional values downstream:)
        additional_params["headers"] = { "Authorization": "Bearer 123" }
        additional_params["read_timeout_seconds"] = 100


        Note:
            `additional_params` resets between mcp_calls.
            so there is not persistence between individual calls.
            This choice has been made to allow developers to
            dynamically switch configurations.
            (This can be used, for example, to use a module to inject oauth headers for
            individual users on a call-to-call basis. this also gives you direct control over
            managing the headers yourself. For example, checking for lifetimes on JWT-Tokens.)

        Note:
            you can access `additional_params` when you are implementing your own subclasses of
            FlockMCPClientManager and FlockMCPClient. (with self.additional_params.)

        keys which are processed for `additional_params` in the flock core code are:
        --- General ---

        "refresh_client": bool -> defaults to False. Indicates whether or not to restart a connection on a call. (can be used when headers oder api-keys change to automatically switch to a new client.)
        "read_timeout_seconds": float -> How long to wait for a connection to happen.

        --- SSE ---

        "override_headers": bool -> default False. If set to false, additional headers will be appended, if set to True, additional headers will override existing ones.
        "headers": dict[str, Any] -> Additional Headers injected in sse-clients and ws-clients
        "sse_read_timeout_seconds": float -> how long until a connection is being terminated for sse-clients.
        "url": str -> which url the server listens on (allows switching between mcp-servers with modules.)

        --- Stdio ---

        "command": str -> Command to run for stdio-servers.
        "args": list[str] -> additional paramters for stdio-servers.
        "env": dict[str, Any] -> Environment-Variables for stdio-servers.
        "encoding": str -> Encoding to use when talking to stdio-servers.
        "encoding-error-handler": str -> Encoding error handler to use when talking to stdio-servers.

        --- Websockets ---

        "url": str -> Which url the server listens on (allows switching between mcp-servers with modules.)
        """
        pass

    async def on_pre_mcp_call(
        self,
        server: Any,
        arguments: Any | None = None,
    ) -> None:
        """Called before any MCP Calls."""
        pass

    async def on_post_mcp_call(
        self,
        server: Any,
        result: Any | None = None,
    ) -> None:
        """Called after any MCP Calls."""
        pass
```

### src\flock\core\flock_registry.py

- **Lines**: 702
- **Last modified**: 2025-05-22 21:27:37

```py
# src/flock/core/flock_registry.py
"""Centralized registry for managing Agents, Callables, Types, and Component Classes
within the Flock framework to support dynamic lookup and serialization.
"""

from __future__ import annotations  # Add this at the very top

import builtins
import importlib
import inspect
import os
import pkgutil
import sys
from collections.abc import Callable, Mapping, Sequence
from dataclasses import is_dataclass
from typing import (  # Add TYPE_CHECKING
    TYPE_CHECKING,
    Any,
    Literal,
    Optional,
    TypeVar,
    Union,
    overload,
)

from pydantic import BaseModel

if TYPE_CHECKING:
    from flock.core.flock_agent import (
        FlockAgent,  # Import only for type checking
    )
    from flock.core.flock_evaluator import FlockEvaluator
    from flock.core.flock_module import FlockModule
    from flock.core.flock_router import FlockRouter
    from flock.core.mcp.flock_mcp_server import FlockMCPServerBase

    COMPONENT_BASE_TYPES = (FlockModule, FlockEvaluator, FlockRouter)

    IS_COMPONENT_CHECK_ENABLED = True
else:
    # Define dummy types or skip check if not type checking
    FlockAgent = Any  # Or define a dummy class
    COMPONENT_BASE_TYPES = ()
    IS_COMPONENT_CHECK_ENABLED = False

# Fallback if core types aren't available during setup
from flock.core.flock_module import FlockModuleConfig
from flock.core.logging.logging import get_logger

logger = get_logger("registry")
T = TypeVar("T")
ClassType = TypeVar("ClassType", bound=type)
FuncType = TypeVar("FuncType", bound=Callable)
ConfigType = TypeVar("ConfigType", bound=BaseModel)
_COMPONENT_CONFIG_MAP: dict[type[BaseModel], type[any]] = {}


class FlockRegistry:
    """Singleton registry for Agents, Callables (functions/methods) and MCP Servers.

    Types (Pydantic/Dataclasses used in signatures), and Component Classes
    (Modules, Evaluators, Routers).
    """

    _instance = None

    _agents: dict[str, FlockAgent]
    _servers: dict[str, FlockMCPServerBase]
    _callables: dict[str, Callable]
    _types: dict[str, type]
    _components: dict[str, type]  # For Module, Evaluator, Router classes

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._initialize()
            # logger.info("FlockRegistry instance created.")
        return cls._instance

    def _initialize(self):
        """Initialize the internal dictionaries."""
        self._agents = {}
        self._servers = {}
        self._callables = {}
        self._types = {}
        self._components = {}
        # logger.debug("FlockRegistry initialized internal stores.")
        # Auto-register core Python types
        self._register_core_types()

    def _register_core_types(self):
        """Registers common built-in and typing types."""
        core_types = [
            str,
            int,
            float,
            bool,
            list,
            dict,
            tuple,
            set,
            Any,
            Mapping,
            Sequence,
            TypeVar,
            Literal,
            Optional,
            Union,  # Common typing generics
        ]
        for t in core_types:
            try:
                self.register_type(t)
            except Exception as e:
                logger.error(f"Failed to auto-register core type {t}: {e}")

    @staticmethod
    def register_config_component_pair(
        config_cls: type[ConfigType], component_cls: type[ClassType]
    ):
        """Explicitly registers the mapping between a config and component class."""
        from flock.core.flock_evaluator import (
            FlockEvaluatorConfig,
        )
        from flock.core.flock_router import FlockRouterConfig

        if not issubclass(
            config_cls,
            FlockModuleConfig | FlockRouterConfig | FlockEvaluatorConfig,
        ):
            logger.warning(
                f"Config class {config_cls.__name__} does not inherit from a standard Flock config base."
            )
        # Add more checks if needed (e.g., component_cls inherits from Module/Router/Evaluator)

        if (
            config_cls in _COMPONENT_CONFIG_MAP
            and _COMPONENT_CONFIG_MAP[config_cls] != component_cls
        ):
            logger.warning(
                f"Config class {config_cls.__name__} already mapped to {_COMPONENT_CONFIG_MAP[config_cls].__name__}. Overwriting with {component_cls.__name__}."
            )

        _COMPONENT_CONFIG_MAP[config_cls] = component_cls
        logger.debug(
            f"Registered config mapping: {config_cls.__name__} -> {component_cls.__name__}"
        )

    @staticmethod
    def get_component_class_for_config(
        config_cls: type[ConfigType],
    ) -> type[ClassType] | None:
        """Looks up the Component Class associated with a Config Class."""
        return _COMPONENT_CONFIG_MAP.get(config_cls)

    # --- Path String Generation ---
    @staticmethod
    def _get_path_string(obj: Callable | type) -> str | None:
        """Generates a unique path string 'module.ClassName' or 'module.function_name'."""
        try:
            module = obj.__module__
            name = obj.__name__
            if module == "builtins":
                return name
            # Check if it's nested (basic check, might not cover all edge cases)
            if "." in name and hasattr(sys.modules[module], name.split(".")[0]):
                # Likely a nested class/method - serialization might need custom handling or pickle
                logger.warning(
                    f"Object {name} appears nested in {module}. Path string might be ambiguous."
                )
            return f"{module}.{name}"
        except AttributeError:
            logger.warning(f"Could not determine module/name for object: {obj}")
            return None

    # --- Server Registration ---
    def register_server(self, server: FlockMCPServerBase) -> None:
        """Registers a flock mcp server by its name."""
        if not hasattr(server.config, "name") or not server.config.name:
            logger.error(
                "Attempted to register a server without a valid 'name' attribute."
            )
            return
        if (
            server.config.name in self._servers
            and self._servers[server.config.name] != server
        ):
            logger.warning(
                f"Server '{server.config.name}' already registered. Overwriting."
            )
        self._servers[server.config.name] = server
        logger.debug(f"Registered server: {server.config.name}")

    def get_server(self, name: str) -> FlockMCPServerBase | None:
        """Retrieves a registered FlockMCPServer instance by name."""
        server = self._servers.get(name)
        if not server:
            logger.warning(f"Server '{name}' not found in registry.")
        return server

    def get_all_server_names(self) -> list[str]:
        """Returns a list of names for all registered servers."""
        return list(self._servers.keys())

    # --- Agent Registration ---
    def register_agent(self, agent: FlockAgent, *, force: bool = False) -> None:
        """Registers a FlockAgent instance by its name.

        Args:
            agent: The agent instance to register.
            force: If True, allow overwriting an existing **different** agent registered under the same name.
                   If False and a conflicting registration exists, a ValueError is raised.
        """
        if not hasattr(agent, "name") or not agent.name:
            logger.error(
                "Attempted to register an agent without a valid 'name' attribute."
            )
            return

        if agent.name in self._agents and self._agents[agent.name] is not agent:
            # Same agent already registered → silently ignore; different instance → error/force.
            if not force:
                raise ValueError(
                    f"Agent '{agent.name}' already registered with a different instance. "
                    "Pass force=True to overwrite the existing registration."
                )
            logger.warning(
                f"Overwriting existing agent '{agent.name}' registration due to force=True."
            )

        self._agents[agent.name] = agent
        logger.debug(f"Registered agent: {agent.name}")

    def get_agent(self, name: str) -> FlockAgent | None:
        """Retrieves a registered FlockAgent instance by name."""
        agent = self._agents.get(name)
        if not agent:
            logger.warning(f"Agent '{name}' not found in registry.")
        return agent

    def get_all_agent_names(self) -> list[str]:
        """Returns a list of names of all registered agents."""
        return list(self._agents.keys())

    # --- Callable Registration ---
    def register_callable(
        self, func: Callable, name: str | None = None
    ) -> str | None:
        """Registers a callable (function/method). Returns its path string identifier."""
        path_str = name or self._get_path_string(func)
        if path_str:
            if (
                path_str in self._callables
                and self._callables[path_str] != func
            ):
                logger.warning(
                    f"Callable '{path_str}' already registered with a different function. Overwriting."
                )
            self._callables[path_str] = func
            logger.debug(f"Registered callable: '{path_str}' ({func.__name__})")
            return path_str
        logger.warning(
            f"Could not register callable {func.__name__}: Unable to determine path string"
        )
        return None

    def get_callable(self, name_or_path: str) -> Callable:
        """Retrieves a callable by its registered name or full path string.
        Attempts dynamic import if not found directly. Prioritizes exact match,
        then searches for matches ending with '.{name}'.
        """
        # 1. Try exact match first (covers full paths and simple names if registered that way)
        if name_or_path in self._callables:
            logger.debug(
                f"Found callable '{name_or_path}' directly in registry."
            )
            return self._callables[name_or_path]

        # 2. If not found, and it looks like a simple name, search registered paths
        if "." not in name_or_path:
            matches = []
            for path_str, func in self._callables.items():
                # Check if path ends with ".{simple_name}" or exactly matches simple_name
                if path_str == name_or_path or path_str.endswith(
                    f".{name_or_path}"
                ):
                    matches.append(func)

            if len(matches) == 1:
                logger.debug(
                    f"Found unique callable for simple name '{name_or_path}' via path '{self.get_callable_path_string(matches[0])}'."
                )
                return matches[0]
            elif len(matches) > 1:
                # Ambiguous simple name - require full path
                found_paths = [
                    self.get_callable_path_string(f) for f in matches
                ]
                logger.error(
                    f"Ambiguous callable name '{name_or_path}'. Found matches: {found_paths}. Use full path string for lookup."
                )
                raise KeyError(
                    f"Ambiguous callable name '{name_or_path}'. Use full path string."
                )
            # else: Not found by simple name search in registry, proceed to dynamic import

        # 3. Attempt dynamic import if it looks like a full path
        if "." in name_or_path:
            logger.debug(
                f"Callable '{name_or_path}' not in registry cache, attempting dynamic import."
            )
            try:
                module_name, func_name = name_or_path.rsplit(".", 1)
                module = importlib.import_module(module_name)
                func = getattr(module, func_name)
                if callable(func):
                    self.register_callable(
                        func, name_or_path
                    )  # Cache dynamically imported
                    logger.info(
                        f"Successfully imported and registered module callable '{name_or_path}'"
                    )
                    return func
                else:
                    raise TypeError(
                        f"Dynamically imported object '{name_or_path}' is not callable."
                    )
            except (ImportError, AttributeError, TypeError) as e:
                logger.error(
                    f"Failed to dynamically load/find callable '{name_or_path}': {e}",
                    exc_info=False,
                )
                # Fall through to raise KeyError
        # 4. Handle built-ins if not found yet (might be redundant if simple name check worked)
        elif name_or_path in builtins.__dict__:
            func = builtins.__dict__[name_or_path]
            if callable(func):
                self.register_callable(func, name_or_path)  # Cache it
                logger.info(
                    f"Found and registered built-in callable '{name_or_path}'"
                )
                return func

        # 5. Final failure
        logger.error(
            f"Callable '{name_or_path}' not found in registry or via import."
        )
        raise KeyError(f"Callable '{name_or_path}' not found.")

    def get_callable_path_string(self, func: Callable) -> str | None:
        """Gets the path string for a callable, registering it if necessary."""
        # First try to find by direct identity
        for path_str, registered_func in self._callables.items():
            if func == registered_func:
                logger.debug(
                    f"Found existing path string for callable: '{path_str}'"
                )
                return path_str

        # If not found by identity, generate path, register, and return
        path_str = self.register_callable(func)
        if path_str:
            logger.debug(
                f"Generated and registered new path string for callable: '{path_str}'"
            )
        else:
            logger.warning(
                f"Failed to generate path string for callable {func.__name__}"
            )

        return path_str

    # --- Type Registration ---
    def register_type(
        self, type_obj: type, name: str | None = None
    ) -> str | None:
        """Registers a class/type (Pydantic, Dataclass, etc.) used in signatures."""
        type_name = name or type_obj.__name__
        if type_name:
            if type_name in self._types and self._types[type_name] != type_obj:
                logger.warning(
                    f"Type '{type_name}' already registered. Overwriting."
                )
            self._types[type_name] = type_obj
            logger.debug(f"Registered type: {type_name}")
            return type_name
        return None

    def get_type(self, type_name: str) -> type:
        """Retrieves a registered type by its name."""
        if type_name in self._types:
            return self._types[type_name]
        else:
            # Consider adding dynamic import attempts for types if needed,
            # but explicit registration is generally safer for types.
            logger.warning(f"Type '{type_name}' not found in registry. Will attempt to build it from builtins.")
            raise KeyError(
                f"Type '{type_name}' not found. Ensure it is registered."
            )

    # --- Component Class Registration ---
    def register_component(
        self, component_class: type, name: str | None = None
    ) -> str | None:
        """Registers a component class (Module, Evaluator, Router)."""
        type_name = name or component_class.__name__
        if type_name:
            # Optional: Add check if it's a subclass of expected bases
            # if COMPONENT_BASE_TYPES and not issubclass(component_class, COMPONENT_BASE_TYPES):
            #     logger.warning(f"Registering class '{type_name}' which is not a standard Flock component type.")
            if (
                type_name in self._components
                and self._components[type_name] != component_class
            ):
                logger.warning(
                    f"Component class '{type_name}' already registered. Overwriting."
                )
            self._components[type_name] = component_class
            logger.debug(f"Registered component class: {type_name}")
            return type_name
        return None

    def get_component(self, type_name: str) -> type:
        """Retrieves a component class by its type name."""
        if type_name in self._components:
            return self._components[type_name]
        else:
            # Dynamic import attempts similar to get_callable could be added here if desired,
            # targeting likely module locations based on type_name conventions.
            logger.error(
                f"Component class '{type_name}' not found in registry."
            )
            raise KeyError(
                f"Component class '{type_name}' not found. Ensure it is registered."
            )

    def get_component_type_name(self, component_class: type) -> str | None:
        """Gets the type name for a component class, registering it if necessary."""
        for type_name, registered_class in self._components.items():
            if component_class == registered_class:
                return type_name
        # If not found, register using class name and return
        return self.register_component(component_class)

    # --- Auto-Registration ---
    def register_module_components(self, module_or_path: Any) -> None:
        """Scans a module (object or path string) and automatically registers.

        - Functions as callables.
        - Pydantic Models and Dataclasses as types.
        - Subclasses of FlockModule, FlockEvaluator, FlockRouter as components.
        """
        try:
            if isinstance(module_or_path, str):
                module = importlib.import_module(module_or_path)
            elif inspect.ismodule(module_or_path):
                module = module_or_path
            else:
                logger.error(
                    f"Invalid input for auto-registration: {module_or_path}. Must be module object or path string."
                )
                return

            logger.info(
                f"Auto-registering components from module: {module.__name__}"
            )
            registered_count = {"callable": 0, "type": 0, "component": 0}

            for name, obj in inspect.getmembers(module):
                if name.startswith("_"):
                    continue  # Skip private/internal

                # Register Functions as Callables
                if (
                    inspect.isfunction(obj)
                    and obj.__module__ == module.__name__
                ):
                    if self.register_callable(obj):
                        registered_count["callable"] += 1

                # Register Classes (Types and Components)
                elif inspect.isclass(obj) and obj.__module__ == module.__name__:
                    is_component = False
                    # Register as Component if subclass of base types
                    if (
                        COMPONENT_BASE_TYPES
                        and issubclass(obj, COMPONENT_BASE_TYPES)
                        and self.register_component(obj)
                    ):
                        registered_count["component"] += 1
                        is_component = True  # Mark as component

                    # Register as Type if Pydantic Model or Dataclass
                    # A component can also be a type used in signatures
                    base_model_or_dataclass = isinstance(obj, type) and (
                        issubclass(obj, BaseModel) or is_dataclass(obj)
                    )
                    if (
                        base_model_or_dataclass
                        and self.register_type(obj)
                        and not is_component
                    ):
                        # Only increment type count if it wasn't already counted as component
                        registered_count["type"] += 1

            logger.info(
                f"Auto-registration summary for {module.__name__}: "
                f"{registered_count['callable']} callables, "
                f"{registered_count['type']} types, "
                f"{registered_count['component']} components."
            )

        except Exception as e:
            logger.error(
                f"Error during auto-registration for {module_or_path}: {e}",
                exc_info=True,
            )


# --- Initialize Singleton ---
_registry_instance = FlockRegistry()


# --- Convenience Access ---
# Provide a function to easily get the singleton instance
def get_registry() -> FlockRegistry:
    """Returns the singleton FlockRegistry instance."""
    return _registry_instance


# Type hinting for decorators to preserve signature
@overload
def flock_component(cls: ClassType) -> ClassType: ...  # Basic registration


@overload
def flock_component(
    *, name: str | None = None, config_class: type[ConfigType] | None = None
) -> Callable[[ClassType], ClassType]: ...  # With options


def flock_component(
    cls: ClassType | None = None,
    *,
    name: str | None = None,
    config_class: type[ConfigType] | None = None,
) -> Any:
    """Decorator to register a Flock Component class and optionally link its config class."""
    registry = get_registry()

    def decorator(inner_cls: ClassType) -> ClassType:
        if not inspect.isclass(inner_cls):
            raise TypeError("@flock_component can only decorate classes.")

        component_name = name or inner_cls.__name__
        registry.register_component(
            inner_cls, name=component_name
        )  # Register component by name

        # If config_class is provided, register the mapping
        if config_class:
            FlockRegistry.register_config_component_pair(
                config_class, inner_cls
            )

        return inner_cls

    if cls is None:
        # Called as @flock_component(name="...", config_class=...)
        return decorator
    else:
        # Called as @flock_component
        return decorator(cls)


# Type hinting for decorators
@overload
def flock_tool(func: FuncType) -> FuncType: ...


@overload
def flock_tool(
    *, name: str | None = None
) -> Callable[[FuncType], FuncType]: ...


def flock_tool(func: FuncType | None = None, *, name: str | None = None) -> Any:
    """Decorator to register a callable function/method as a Tool (or general callable).

    Usage:
        @flock_tool
        def my_web_search(query: str): ...

        @flock_tool(name="utils.calculate_pi")
        def compute_pi(): ...
    """
    registry = get_registry()

    def decorator(inner_func: FuncType) -> FuncType:
        if not callable(inner_func):
            raise TypeError("@flock_tool can only decorate callables.")
        # Let registry handle default name generation if None
        registry.register_callable(inner_func, name=name)
        return inner_func

    if func is None:
        # Called as @flock_tool(name="...")
        return decorator
    else:
        # Called as @flock_tool
        return decorator(func)


# Alias for clarity if desired
flock_callable = flock_tool


@overload
def flock_type(cls: ClassType) -> ClassType: ...


@overload
def flock_type(
    *, name: str | None = None
) -> Callable[[ClassType], ClassType]: ...


def flock_type(cls: ClassType | None = None, *, name: str | None = None) -> Any:
    """Decorator to register a Type (Pydantic Model, Dataclass) used in signatures.

    Usage:
        @flock_type
        class MyDataModel(BaseModel): ...

        @flock_type(name="UserInput")
        @dataclass
        class UserQuery: ...
    """
    registry = get_registry()

    def decorator(inner_cls: ClassType) -> ClassType:
        if not inspect.isclass(inner_cls):
            raise TypeError("@flock_type can only decorate classes.")
        type_name = name or inner_cls.__name__
        registry.register_type(inner_cls, name=type_name)
        return inner_cls

    if cls is None:
        # Called as @flock_type(name="...")
        return decorator
    else:
        # Called as @flock_type
        return decorator(cls)


# --- Auto-register known core components and tools ---
def _auto_register_by_path(self):
    # List of base packages to scan for components and tools
    packages_to_scan = [
        "flock.tools",
        "flock.evaluators",
        "flock.modules",
        "flock.routers",
    ]

    for package_name in packages_to_scan:
        try:
            package_spec = importlib.util.find_spec(package_name)
            if package_spec and package_spec.origin:
                package_path_list = [os.path.dirname(package_spec.origin)]
                logger.info(f"Recursively scanning for modules in package: {package_name} (path: {package_path_list[0]})")

                # Use walk_packages to recursively find all modules
                for module_loader, module_name, is_pkg in pkgutil.walk_packages(
                    path=package_path_list,
                    prefix=package_name + ".", # Ensures module_name is fully qualified
                    onerror=lambda name: logger.warning(f"Error importing module {name} during scan.")
                ):
                    if not is_pkg and not module_name.split('.')[-1].startswith("_"):
                        # We are interested in actual modules, not sub-packages themselves for registration
                        # And also skip modules starting with underscore (e.g. __main__.py)
                        try:
                            logger.debug(f"Attempting to auto-register components from module: {module_name}")
                            _registry_instance.register_module_components(module_name)
                        except ImportError as e:
                            logger.warning(
                                f"Could not auto-register from {module_name}: Module not found or import error: {e}"
                            )
                        except Exception as e: # Catch other potential errors during registration
                            logger.error(
                                f"Unexpected error during auto-registration of {module_name}: {e}",
                                exc_info=True
                            )
            else:
                logger.warning(f"Could not find package spec for '{package_name}' to auto-register components/tools.")
        except Exception as e:
            logger.error(f"Error while trying to dynamically register from '{package_name}': {e}", exc_info=True)

# Bootstrapping the registry
# _auto_register_by_path() # Commented out or removed

# Make the registration function public and rename it
FlockRegistry.discover_and_register_components = _auto_register_by_path
```

### src\flock\core\flock_router.py

- **Lines**: 83
- **Last modified**: 2025-04-16 14:12:21

```py
"""Base router class for the Flock framework."""

from abc import ABC, abstractmethod
from typing import Any, Literal

from pydantic import BaseModel, Field

from flock.core.context.context import FlockContext


class HandOffRequest(BaseModel):
    """Base class for handoff returns."""

    next_agent: str = Field(default="", description="Next agent to invoke")
    # match = use the output fields of the current agent that also exists as input field of the next agent
    # add = add the output of the current agent to the input of the next agent
    output_to_input_merge_strategy: Literal["match", "add"] = Field(
        default="match"
    )
    add_input_fields: list[str] | None = Field(
        default=None,
        description="List of input fields to add to the next agent",
    )
    add_output_fields: list[str] | None = Field(
        default=None,
        description="List of output fields to add to the next agent",
    )
    add_description: str | None = Field(
        default=None, description="Add this description to the next agent"
    )
    override_next_agent: Any | None = Field(
        default=None,
        description="Override the next agent to hand off to",
    )
    override_context: FlockContext | None = Field(
        default=None, description="Override context parameters"
    )


class FlockRouterConfig(BaseModel):
    """Configuration for a router.

    This class defines the configuration parameters for a router.
    Subclasses can extend this to add additional parameters.
    """

    enabled: bool = Field(
        default=True, description="Whether the router is enabled"
    )
    # agents: list[str] | None = Field(
    #     default=None,
    #     description="List of agents to choose from",
    # )


class FlockRouter(BaseModel, ABC):
    """Base class for all routers.

    A router is responsible for determining the next agent in a workflow
    based on the current agent's output.
    """

    name: str = Field(..., description="Name of the router")
    config: FlockRouterConfig = Field(default_factory=FlockRouterConfig)

    @abstractmethod
    async def route(
        self,
        current_agent: Any,
        result: dict[str, Any],
        context: FlockContext,
    ) -> HandOffRequest:
        """Determine the next agent to hand off to based on the current agent's output.

        Args:
            current_agent: The agent that just completed execution
            result: The output from the current agent
            context: The global execution context

        Returns:
            A HandOff object containing the next agent and input data
        """
        pass
```

### src\flock\core\flock_server_manager.py

- **Lines**: 136
- **Last modified**: 2025-05-22 21:27:37

```py
"""Manages Server-Lifecycles within the larger lifecycle of Flock."""

import asyncio
from contextlib import AsyncExitStack

from anyio import Lock
from pydantic import BaseModel, ConfigDict, Field

from flock.core.mcp.flock_mcp_server import FlockMCPServerBase


class FlockServerManager(BaseModel):
    """Async-context-manager to start/stop a set of Flock MCP servers."""

    servers: list[FlockMCPServerBase] | None = Field(
        ..., exclude=True, description="The servers to manage."
    )

    stack: AsyncExitStack | None = Field(
        default=None,
        exclude=True,
        description="Central exit stack for managing the execution context of the servers.",
    )

    lock: Lock | None = Field(
        default=None, exclude=True, description="Global lock for mutex access."
    )

    model_config = ConfigDict(
        arbitrary_types_allowed=True,
    )

    def __init__(
        self,
        servers: list[FlockMCPServerBase] | None = None,
        stack: AsyncExitStack | None = None,
        lock: asyncio.Lock | None = None,
    ) -> None:
        """Initialize the FlockServerManager with optional server, stack, and lock references."""
        super().__init__(
            servers=servers,
            stack=stack,
            lock=lock,
        )

    def add_server_sync(self, server: FlockMCPServerBase) -> None:
        """Add a server to be managed by the ServerManager.

        Note:
          IT IS CRUCIAL THAT THIS METHOD IS NOT CALLED
          WHEN THE SERVER MANAGER HAS ALREADY BEEN INTIALIZED
          (with server_manager as manager: ...)
          OTHERWISE EXECUTION WILL BREAK DOWN.
        """
        if self.servers is None:
            self.servers = []

        self.servers.append(server)

    def remove_server_sync(self, server: FlockMCPServerBase) -> None:
        """Remove a server from the list of managed servers.

        Note:
          IT IS CRUCIAL THAT THIS METHOD IS NOT CALLED
          WHEN THE SERVER MANAGER HAS ALREADY BEEN INITIALIZED
          (with server_manager as manager: ...)
          OTHERWISE EXECUTION WILL BREAK DOWN.
        """
        if self.servers and server in self.servers:
            self.servers.remove(server)

    # -- For future use: Allow adding and removal of servers during runtime ---
    async def add_server_during_runtime(
        self, server: FlockMCPServerBase
    ) -> None:
        """Add a server to the manager and, if already running, start it immediately."""
        if self.lock is None:
            self.lock = asyncio.Lock()

        async with self.lock:
            if self.servers is None:
                self.servers = []

            self.servers.append(server)

        # If we are already running in async-with, enter the context now
        if self.stack is not None:
            await self.stack.enter_async_context(server)

    async def remove_server_during_runtime(
        self, server: FlockMCPServerBase
    ) -> None:
        """Tear down and remove a server from the manager at runtime."""
        if self.lock is None:
            self.lock = asyncio.Lock()

        retrieved_server: FlockMCPServerBase | None = None

        async with self.lock:
            if not self.servers or server not in self.servers:
                return  # Skip as to not impede application flow
            else:
                try:
                    self.servers.remove(server)
                    retrieved_server = server
                except ValueError:
                    # The server is not present (a little paranoid at this point, but still...)
                    return

        # tell the server to shut down.
        if retrieved_server:
            # trigger the server's own exit hook (this closes its connection_manager, sessions, tools....)
            await retrieved_server.__aexit__(None, None, None)

    async def __aenter__(self) -> "FlockServerManager":
        """Enter the asynchronous context for the server manager."""
        if not self.stack:
            self.stack = AsyncExitStack()

        if not self.servers:
            self.servers = []

        if not self.lock:
            self.lock = asyncio.Lock()

        for srv in self.servers:
            await self.stack.enter_async_context(srv)

        return self

    async def __aexit__(self, exc_type, exc, tb) -> None:
        """Exit the asynchronous context for the server manager."""
        # Unwind the servers in LIFO order
        if self.stack is not None:
            await self.stack.aclose()
            self.stack = None
```

### src\flock\core\interpreter\python_interpreter.py

- **Lines**: 683
- **Last modified**: 2025-05-25 23:17:06

```py
# =========== Copyright 2023 @ CAMEL-AI.org. All Rights Reserved. ===========
# Licensed under the Apache License, Version 2.0 (the “License”);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an “AS IS” BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# =========== Copyright 2023 @ CAMEL-AI.org. All Rights Reserved. ===========
import ast
import builtins
import difflib
import importlib
import re
import typing
from collections.abc import Mapping
from typing import (
    Any,
)

from opentelemetry import trace

from flock.core.logging.logging import get_logger

tracer = trace.get_tracer(__name__)
logger = get_logger("interpreter")


class InterpreterError(ValueError):
    r"""An error raised when the interpreter cannot evaluate a Python
    expression, due to syntax error or unsupported operations.
    """

    pass


class PythonInterpreter:
    r"""A customized python interpreter to control the execution of
    LLM-generated codes. The interpreter makes sure the code can only execute
    functions given in action space and import white list. It also supports
    fuzzy variable matching to receive uncertain input variable name.

    [Documentation omitted for brevity]

    Args:
        action_space (Dict[str, Any]): A dictionary mapping action names to
            their corresponding functions or objects.
        import_white_list (Optional[List[str]], optional): A list of allowed modules.
        verbose (bool, optional): If True, the interpreter prints log messages
            as it executes the code. (default: False)
    """

    def __init__(
        self,
        action_space: dict[str, Any],
        import_white_list: list[str] | None = None,
        verbose: bool = False,
    ) -> None:
        self.action_space = action_space
        self.state = self.action_space.copy()
        self.fuzz_state: dict[str, Any] = {}
        self.import_white_list = import_white_list or [
            "math",
            "random",
            "datetime",
            "time",
            "string",
            "collections",
            "itertools",
            "functools",
            "typing",
            "enum",
            "json",
            "ast",
        ]  # default imports
        self.verbose = verbose

    def log(self, message: str) -> None:
        """Print a log message immediately."""
        # print(message, flush=True)
        logger.info(message, flush=True)

    def execute(
        self,
        code: str,
        state: dict[str, Any] | None = None,
        fuzz_state: dict[str, Any] | None = None,
        keep_state: bool = True,
    ) -> Any:
        r"""Execute the input python codes in a secure environment.

        [Documentation omitted for brevity]
        """
        if state is not None:
            self.state.update(state)
        if fuzz_state is not None:
            self.fuzz_state.update(fuzz_state)

        try:
            expression = ast.parse(code)
        except SyntaxError as e:
            error_line = code.splitlines()[e.lineno - 1]
            raise InterpreterError(
                f"Syntax error in code at line {e.lineno}: {error_line}\nError: {e}"
            )

        result = None
        if self.verbose:
            self.log("[Interpreter] Starting code execution...")

        for idx, node in enumerate(expression.body):
            # Log the AST node being executed (using unparse if available)
            if self.verbose:
                try:
                    node_repr = ast.unparse(node)
                except Exception:
                    node_repr = ast.dump(node)
                self.log(f"[Interpreter] Executing node {idx}: {node_repr}")

            try:
                line_result = self._execute_ast(node)
            except InterpreterError as e:
                if not keep_state:
                    self.clear_state()
                msg = f"Evaluation of the code stopped at node {idx}. See:\n{e}"
                raise InterpreterError(msg)
            if line_result is not None:
                result = line_result
                if self.verbose:
                    self.log(f"[Interpreter] Node {idx} result: {result}")

        if self.verbose:
            self.log("[Interpreter] Finished code execution.")
        if not keep_state:
            self.clear_state()

        return result

    def clear_state(self) -> None:
        r"""Initialize :obj:`state` and :obj:`fuzz_state`"""
        self.state = self.action_space.copy()
        self.fuzz_state = {}

    # ast.Index is deprecated after python 3.9, which cannot pass type check,
    # but is still necessary for older versions.
    @typing.no_type_check
    def _execute_ast(self, expression: ast.AST) -> Any:
        if isinstance(expression, ast.Assign):
            return self._execute_assign(expression)
        elif isinstance(expression, ast.Attribute):
            value = self._execute_ast(expression.value)
            return getattr(value, expression.attr)
        elif isinstance(expression, ast.AugAssign):
            return self._execute_augassign(expression)
        elif isinstance(expression, ast.BinOp):
            return self._execute_binop(expression)
        elif isinstance(expression, ast.BoolOp):
            return self._execute_condition(expression)
        elif isinstance(expression, ast.Call):
            return self._execute_call(expression)
        elif isinstance(expression, ast.Compare):
            return self._execute_condition(expression)
        elif isinstance(expression, ast.Constant):
            return expression.value
        elif isinstance(expression, ast.Dict):
            result: dict = {}
            for k, v in zip(expression.keys, expression.values):
                if k is not None:
                    result[self._execute_ast(k)] = self._execute_ast(v)
                else:
                    result.update(self._execute_ast(v))
            return result
        elif isinstance(expression, ast.Expr):
            return self._execute_ast(expression.value)
        elif isinstance(expression, ast.For):
            return self._execute_for(expression)
        elif isinstance(expression, ast.FormattedValue):
            return self._execute_ast(expression.value)
        elif isinstance(expression, ast.FunctionDef):
            self.state[expression.name] = expression
            return None
        elif isinstance(expression, ast.GeneratorExp):
            return self._execute_generatorexp(expression)
        elif isinstance(expression, ast.If):
            return self._execute_if(expression)
        elif isinstance(expression, ast.IfExp):
            return self._execute_ifexp(expression)
        elif isinstance(expression, ast.Import):
            self._execute_import(expression)
            return None
        elif isinstance(expression, ast.ImportFrom):
            self._execute_import_from(expression)
            return None
        elif hasattr(ast, "Index") and isinstance(expression, ast.Index):
            return self._execute_ast(expression.value)
        elif isinstance(expression, ast.JoinedStr):
            return "".join(
                [str(self._execute_ast(v)) for v in expression.values]
            )
        elif isinstance(expression, ast.Lambda):
            return self._execute_lambda(expression)
        elif isinstance(expression, ast.List):
            return [self._execute_ast(elt) for elt in expression.elts]
        elif isinstance(expression, ast.Name):
            return self._execute_name(expression)
        elif isinstance(expression, ast.Return):
            return self._execute_ast(expression.value)
        elif isinstance(expression, ast.Subscript):
            return self._execute_subscript(expression)
        elif isinstance(expression, ast.Tuple):
            return tuple([self._execute_ast(elt) for elt in expression.elts])
        elif isinstance(expression, ast.UnaryOp):
            return self._execute_unaryop(expression)
        elif isinstance(expression, ast.While):
            return self._execute_while(expression)
        elif isinstance(expression, ast.ListComp):
            return self._execute_listcomp(expression)
        elif isinstance(expression, ast.DictComp):
            return self._execute_dictcomp(expression)
        elif isinstance(expression, ast.SetComp):
            return self._execute_setcomp(expression)
        elif isinstance(expression, ast.Break):
            raise BreakException()
        elif isinstance(expression, ast.Continue):
            raise ContinueException()
        elif isinstance(expression, ast.Try):
            return self._execute_try(expression)
        elif isinstance(expression, ast.Raise):
            return self._execute_raise(expression)
        elif isinstance(expression, ast.Pass):
            return None
        elif isinstance(expression, ast.Assert):
            return self._execute_assert(expression)
        else:
            raise InterpreterError(
                f"{expression.__class__.__name__} is not supported."
            )

    def _execute_assign(self, assign: ast.Assign) -> Any:
        targets = assign.targets
        result = self._execute_ast(assign.value)

        for target in targets:
            self._assign(target, result)
        return result

    def _assign(self, target: ast.expr, value: Any):
        if isinstance(target, ast.Name):
            self.state[target.id] = value
        elif isinstance(target, ast.Tuple):
            if not isinstance(value, tuple):
                raise InterpreterError(
                    f"Expected type tuple, but got {value.__class__.__name__} instead."
                )
            if len(target.elts) != len(value):
                raise InterpreterError(
                    f"Expected {len(target.elts)} values but got {len(value)}."
                )
            for t, v in zip(target.elts, value):
                self.state[self._execute_ast(t)] = v
        else:
            raise InterpreterError(
                f"Unsupported variable type. Expected ast.Name or ast.Tuple, got {target.__class__.__name__} instead."
            )

    def _execute_call(self, call: ast.Call) -> Any:
        callable_func = self._execute_ast(call.func)

        args = [self._execute_ast(arg) for arg in call.args]
        kwargs = {
            keyword.arg: self._execute_ast(keyword.value)
            for keyword in call.keywords
        }
        if isinstance(callable_func, ast.FunctionDef):
            old_state = self.state.copy()
            for param_name, arg_value in zip(
                [param.arg for param in callable_func.args.args], args
            ):
                self.state[param_name] = arg_value
            result = None
            for stmt in callable_func.body:
                result = self._execute_ast(stmt)
                if isinstance(stmt, ast.Return):
                    break
            self.state = old_state
            return result
        return callable_func(*args, **kwargs)

    def _execute_augassign(self, augassign: ast.AugAssign):
        current_value = self.state[augassign.target.id]
        increment_value = self._execute_ast(augassign.value)
        if not (
            isinstance(current_value, (int, float))
            and isinstance(increment_value, (int, float))
        ):
            raise InterpreterError(
                f"Invalid types for augmented assignment: {type(current_value)}, {type(increment_value)}"
            )
        if isinstance(augassign.op, ast.Add):
            new_value = current_value + increment_value
        elif isinstance(augassign.op, ast.Sub):
            new_value = current_value - increment_value
        elif isinstance(augassign.op, ast.Mult):
            new_value = current_value * increment_value
        elif isinstance(augassign.op, ast.Div):
            new_value = current_value / increment_value
        else:
            raise InterpreterError(
                f"Augmented assignment operator {augassign.op} is not supported"
            )
        self._assign(augassign.target, new_value)
        return new_value

    def _execute_subscript(self, subscript: ast.Subscript):
        index = self._execute_ast(subscript.slice)
        value = self._execute_ast(subscript.value)
        if not isinstance(subscript.ctx, ast.Load):
            raise InterpreterError(
                f"{subscript.ctx.__class__.__name__} is not supported for subscript."
            )
        if isinstance(value, (list, tuple)):
            return value[int(index)]
        if index in value:
            return value[index]
        if isinstance(index, str) and isinstance(value, Mapping):
            close_matches = difflib.get_close_matches(index, list(value.keys()))
            if len(close_matches) > 0:
                return value[close_matches[0]]
        raise InterpreterError(f"Could not index {value} with '{index}'.")

    def _execute_name(self, name: ast.Name):
        if name.id in dir(builtins):
            return getattr(builtins, name.id)
        if isinstance(name.ctx, ast.Store):
            return name.id
        elif isinstance(name.ctx, ast.Load):
            return self._get_value_from_state(name.id)
        else:
            raise InterpreterError(f"{name.ctx} is not supported.")

    def _execute_condition(self, condition):
        if isinstance(condition, ast.BoolOp):
            if isinstance(condition.op, ast.And):
                results = [
                    self._execute_ast(value) for value in condition.values
                ]
                return all(results)
            elif isinstance(condition.op, ast.Or):
                results = [
                    self._execute_ast(value) for value in condition.values
                ]
                return any(results)
            else:
                raise InterpreterError(
                    f"Boolean operator {condition.op} is not supported"
                )
        elif isinstance(condition, ast.Compare):
            if len(condition.ops) > 1:
                raise InterpreterError(
                    "Cannot evaluate conditions with multiple operators"
                )
            left = self._execute_ast(condition.left)
            comparator = condition.ops[0]
            right = self._execute_ast(condition.comparators[0])
            if isinstance(comparator, ast.Eq):
                return left == right
            elif isinstance(comparator, ast.NotEq):
                return left != right
            elif isinstance(comparator, ast.Lt):
                return left < right
            elif isinstance(comparator, ast.LtE):
                return left <= right
            elif isinstance(comparator, ast.Gt):
                return left > right
            elif isinstance(comparator, ast.GtE):
                return left >= right
            elif isinstance(comparator, ast.Is):
                return left is right
            elif isinstance(comparator, ast.IsNot):
                return left is not right
            elif isinstance(comparator, ast.In):
                return left in right
            elif isinstance(comparator, ast.NotIn):
                return left not in right
            else:
                raise InterpreterError("Unsupported comparison operator")
        elif isinstance(condition, ast.UnaryOp):
            return self._execute_unaryop(condition)
        elif isinstance(condition, ast.Name) or isinstance(condition, ast.Call):
            return bool(self._execute_ast(condition))
        elif isinstance(condition, ast.Constant):
            return bool(condition.value)
        else:
            raise InterpreterError(
                f"Unsupported condition type: {type(condition).__name__}"
            )

    def _execute_if(self, if_statement: ast.If):
        result = None
        if self._execute_condition(if_statement.test):
            for line in if_statement.body:
                line_result = self._execute_ast(line)
                if line_result is not None:
                    result = line_result
        else:
            for line in if_statement.orelse:
                line_result = self._execute_ast(line)
                if line_result is not None:
                    result = line_result
        return result

    def _execute_ifexp(self, ifexp: ast.IfExp) -> Any:
        test_result = self._execute_condition(ifexp.test)
        if test_result:
            return self._execute_ast(ifexp.body)
        else:
            return self._execute_ast(ifexp.orelse)

    def _execute_import(self, import_module: ast.Import) -> None:
        for module in import_module.names:
            self._validate_import(module.name)
            alias = module.asname or module.name
            self.state[alias] = importlib.import_module(module.name)

    def _execute_import_from(self, import_from: ast.ImportFrom):
        if import_from.module is None:
            raise InterpreterError('"from . import" is not supported.')
        for import_name in import_from.names:
            full_name = import_from.module + f".{import_name.name}"
            self._validate_import(full_name)
            imported_module = importlib.import_module(import_from.module)
            alias = import_name.asname or import_name.name
            self.state[alias] = getattr(imported_module, import_name.name)

    # Note: Two versions of _execute_for and _execute_while appear in this file.
    # We keep both as provided, but you may wish to consolidate these in your code.

    def _execute_for(self, for_statement: ast.For):
        class BreakException(Exception):
            pass

        class ContinueException(Exception):
            pass

        result = None
        try:
            for value in self._execute_ast(for_statement.iter):
                self._assign(for_statement.target, value)
                try:
                    for line in for_statement.body:
                        line_result = self._execute_ast(line)
                        if line_result is not None:
                            result = line_result
                except ContinueException:
                    continue
        except BreakException:
            pass
        return result

    def _execute_while(self, while_statement: ast.While):
        class BreakException(Exception):
            pass

        class ContinueException(Exception):
            pass

        result = None
        try:
            while self._execute_condition(while_statement.test):
                try:
                    for line in while_statement.body:
                        line_result = self._execute_ast(line)
                        if line_result is not None:
                            result = line_result
                except ContinueException:
                    continue
        except BreakException:
            pass
        return result

    def _execute_try(self, try_statement: ast.Try):
        try:
            for line in try_statement.body:
                self._execute_ast(line)
        except Exception as e:
            handled = False
            for handler in try_statement.handlers:
                if handler.type is None or isinstance(
                    e, self._execute_ast(handler.type)
                ):
                    if handler.name:
                        self.state[handler.name.id] = e
                    for line in handler.body:
                        self._execute_ast(line)
                    handled = True
                    break
            if not handled:
                raise
        finally:
            for line in try_statement.finalbody:
                self._execute_ast(line)

    def _execute_raise(self, raise_statement: ast.Raise):
        if raise_statement.exc:
            exception = self._execute_ast(raise_statement.exc)
            raise exception
        else:
            raise

    def _execute_assert(self, assert_statement: ast.Assert):
        test_result = self._execute_condition(assert_statement.test)
        if not test_result:
            if assert_statement.msg:
                msg = self._execute_ast(assert_statement.msg)
                raise AssertionError(msg)
            else:
                raise AssertionError

    def _execute_lambda(self, lambda_node: ast.Lambda) -> Any:
        def lambda_function(*args):
            old_state = self.state.copy()
            for param, arg in zip(lambda_node.args.args, args):
                self.state[param.arg] = arg
            result = self._execute_ast(lambda_node.body)
            self.state = old_state  # Restore the state
            return result

        return lambda_function

    def _validate_import(self, full_name: str):
        tmp_name = ""
        found_name = False
        for name in full_name.split("."):
            tmp_name += name if tmp_name == "" else f".{name}"
            if tmp_name in self.import_white_list:
                found_name = True
                return
        if not found_name:
            raise InterpreterError(
                f"It is not permitted to import modules "
                f"than module white list (try to import {full_name})."
            )

    def _execute_binop(self, binop: ast.BinOp):
        left = self._execute_ast(binop.left)
        operator = binop.op
        right = self._execute_ast(binop.right)

        if isinstance(operator, ast.Add):
            return left + right
        elif isinstance(operator, ast.Sub):
            return left - right
        elif isinstance(operator, ast.Mult):
            return left * right
        elif isinstance(operator, ast.Div):
            return left / right
        elif isinstance(operator, ast.FloorDiv):
            return left // right
        elif isinstance(operator, ast.Mod):
            return left % right
        elif isinstance(operator, ast.Pow):
            return left**right
        elif isinstance(operator, ast.LShift):
            return left << right
        elif isinstance(operator, ast.RShift):
            return left >> right
        elif isinstance(operator, ast.BitAnd):
            return left & right
        elif isinstance(operator, ast.BitOr):
            return left | right
        elif isinstance(operator, ast.BitXor):
            return left ^ right
        elif isinstance(operator, ast.MatMult):
            return left @ right
        else:
            raise InterpreterError(f"Operator not supported: {operator}")

    def _execute_unaryop(self, unaryop: ast.UnaryOp):
        operand = self._execute_ast(unaryop.operand)
        operator = unaryop.op

        if isinstance(operator, ast.UAdd):
            return +operand
        elif isinstance(operator, ast.USub):
            return -operand
        elif isinstance(operator, ast.Not):
            return not operand
        elif isinstance(operator, ast.Invert):
            return ~operand
        else:
            raise InterpreterError(f"Operator not supported: {operator}")

    def _execute_listcomp(self, comp: ast.ListComp):
        return [self._execute_comp(comp.elt, comp.generators)]

    def _execute_dictcomp(self, comp: ast.DictComp):
        return {
            self._execute_comp(comp.key, comp.generators): self._execute_comp(
                comp.value, comp.generators
            )
        }

    def _execute_setcomp(self, comp: ast.SetComp):
        return {self._execute_comp(comp.elt, comp.generators)}

    def _execute_comp(self, elt, generators):
        if not generators:
            return self._execute_ast(elt)
        gen = generators[0]
        result = []
        for value in self._execute_ast(gen.iter):
            self._assign(gen.target, value)
            if all(self._execute_condition(if_cond) for if_cond in gen.ifs):
                result.extend(self._execute_comp(elt, generators[1:]))
        return result

    def _execute_generatorexp(self, genexp: ast.GeneratorExp):
        def generator():
            for value in self._execute_comp(genexp.elt, genexp.generators):
                yield value

        return generator()

    def _get_value_from_state(self, key: str) -> Any:
        if key in self.state:
            return self.state[key]
        elif key in self.fuzz_state:
            return self.fuzz_state[key]
        else:
            raise InterpreterError(f"The variable `{key}` is not defined.")


class TextPrompt(str):
    r"""A class that represents a text prompt. The :obj:`TextPrompt` class
    extends the built-in :obj:`str` class to provide a property for retrieving
    the set of keywords in the prompt.
    """

    @property
    def key_words(self) -> set[str]:
        pattern = re.compile(r"\{([^{}]+)\}")
        found = pattern.findall(self)
        return set(found)

    def format(self, *args: Any, **kwargs: Any) -> "TextPrompt":
        default_kwargs = {key: "{" + f"{key}" + "}" for key in self.key_words}
        default_kwargs.update(kwargs)
        return TextPrompt(super().format(*args, **default_kwargs))


class CodePrompt(TextPrompt):
    r"""A class that represents a code prompt. It extends the :obj:`TextPrompt`
    class with a :obj:`code_type` property.
    """

    def __new__(cls, *args: Any, **kwargs: Any) -> "CodePrompt":
        code_type = kwargs.pop("code_type", None)
        instance = super().__new__(cls, *args, **kwargs)
        instance._code_type = code_type
        return instance

    @property
    def code_type(self) -> str | None:
        return self._code_type

    def set_code_type(self, code_type: str) -> None:
        self._code_type = code_type

    def execute(
        self,
        interpreter: PythonInterpreter | None = None,
        user_variable: dict[str, Any] | None = None,
    ) -> tuple[Any, PythonInterpreter]:
        if not interpreter:
            interpreter = PythonInterpreter(action_space=globals())
        execution_res = interpreter.execute(
            self, fuzz_state=user_variable, keep_state=True
        )
        return execution_res, interpreter
```

### src\flock\core\logging\__init__.py

- **Lines**: 6
- **Last modified**: 2025-05-22 21:27:37

```py
"""Flock logging system with Rich integration and structured logging support."""

from .logging import configure_logging

__all__ = ["configure_logging"]

```

### src\flock\core\logging\formatters\enum_builder.py

- **Lines**: 38
- **Last modified**: 2025-04-16 00:11:15

```py
"""Enum Builder."""

import os
import pathlib
import re

theme_folder = pathlib.Path(__file__).parent.parent.parent.parent / "themes"

if not theme_folder.exists():
    raise FileNotFoundError(f"Theme folder not found: {theme_folder}")

theme_files = [
    pathlib.Path(f.path).stem for f in os.scandir(theme_folder) if f.is_file()
]

theme_enum_entries = {}
for theme in theme_files:
    safe_name = (
        theme.replace("-", "_")
        .replace(" ", "_")
        .replace("(", "_")
        .replace(")", "_")
        .replace("+", "_")
        .replace(".", "_")
    )

    if re.match(r"^\d", safe_name):
        safe_name = f"_{safe_name}"

    theme_enum_entries[safe_name] = theme

with open("theme_enum.py", "w") as f:
    f.write("from enum import Enum\n\n")
    f.write("class OutputOptionsTheme(Enum):\n")
    for safe_name, original_name in theme_enum_entries.items():
        f.write(f'    {safe_name} = "{original_name}"\n')

print("Generated theme_enum.py ✅")
```

### src\flock\core\logging\formatters\theme_builder.py

- **Lines**: 476
- **Last modified**: 2025-04-16 00:11:15

```py
#!/usr/bin/env python
"""A simple interactive theme builder.

Steps:
1. Load theme files from a folder (or pick N random ones).
2. Display each theme’s color palette (colors only).
3. Let the user choose a palette.
4. Generate a number of sample tables using that palette (with randomized non-color settings).
5. Let the user select one sample table and save its configuration to a TOML file.
"""

import pathlib
import random
import re
from typing import Any

import toml
from rich import box
from rich.console import Console, Group
from rich.panel import Panel
from rich.table import Table
from rich.text import Text


def resolve_style_string(style_str: str, theme: dict) -> str:
    """Replace tokens of the form "color.<section>.<key>" in style_str with
    the value from theme["colors"][<section>][<key>].
    """
    pattern = r"color\.(\w+)\.(\w+)"

    def repl(match):
        section = match.group(1)
        key = match.group(2)
        try:
            return theme["colors"][section][key]
        except KeyError:
            return match.group(0)

    return re.sub(pattern, repl, style_str)


def generate_default_rich_block(theme: dict | None = None) -> dict[str, Any]:
    """Generate a default [rich] block that includes:
    - Color properties computed from the theme's [colors] blocks.
    - Extra color tokens (so tokens like "color.bright.green" can be used).
    - Non-color table layout properties, randomly chosen.
    """

    def random_background():
        return random.choice(
            [
                f"{normal_black}",
                f"{normal_blue}",
                f"{primary_background}",
                f"{selection_background}",
                f"{cursor_cursor}",
            ]
        )

    if theme is not None:
        bright = theme["colors"].get("bright", {})
        normal = theme["colors"].get("normal", {})
        cursor = theme["colors"].get("cursor", {})
        primary = theme["colors"].get("primary", {})
        selection = theme["colors"].get("selection", {})

        bright_black = bright.get("black", "#000000")
        bright_blue = bright.get("blue", "#96cbfe")
        bright_cyan = bright.get("cyan", "#85befd")
        bright_green = bright.get("green", "#94fa36")
        bright_magenta = bright.get("magenta", "#b9b6fc")
        bright_red = bright.get("red", "#fd5ff1")
        bright_white = bright.get("white", "#e0e0e0")
        bright_yellow = bright.get("yellow", "#f5ffa8")

        normal_black = normal.get("black", "#000000")
        normal_blue = normal.get("blue", "#85befd")
        normal_cyan = normal.get("cyan", "#85befd")
        normal_green = normal.get("green", "#87c38a")
        normal_magenta = normal.get("magenta", "#b9b6fc")
        normal_red = normal.get("red", "#fd5ff1")
        normal_white = normal.get("white", "#e0e0e0")
        normal_yellow = normal.get("yellow", "#ffd7b1")

        cursor_cursor = cursor.get("cursor", "#d0d0d0")
        cursor_text = cursor.get("text", "#151515")

        primary_background = primary.get("background", "#161719")
        primary_foreground = primary.get("foreground", "#c5c8c6")
        selection_background = selection.get("background", "#444444")
        selection_text = selection.get("text", primary_foreground)
    else:
        # Fallback default values.
        bright_black = "black"
        bright_blue = "blue"
        bright_cyan = "cyan"
        bright_green = "green"
        bright_magenta = "magenta"
        bright_red = "red"
        bright_white = "white"
        bright_yellow = "yellow"
        normal_black = "black"
        normal_blue = "blue"
        normal_cyan = "cyan"
        normal_green = "green"
        normal_magenta = "magenta"
        normal_red = "red"
        normal_white = "white"
        normal_yellow = "yellow"
        cursor_cursor = "gray"
        cursor_text = "white"
        primary_background = "black"
        primary_foreground = "white"
        selection_background = "gray"
        selection_text = "white"

    # Color properties.
    default_color_props = {
        "panel_style": f"on {random_background()}",
        "table_header_style": f"bold {selection_text} on {selection_background}",
        "table_title_style": f"bold {primary_foreground}",
        "table_border_style": bright_blue,
        "panel_border_style": bright_blue,
        "column_output": f"bold {primary_foreground}",
        "column_value": primary_foreground,
    }
    # Extra color tokens.
    extra_color_props = {
        "bright_black": bright_black,
        "bright_blue": bright_blue,
        "bright_cyan": bright_cyan,
        "bright_green": bright_green,
        "bright_magenta": bright_magenta,
        "bright_red": bright_red,
        "bright_white": bright_white,
        "bright_yellow": bright_yellow,
        "normal_black": normal_black,
        "normal_blue": normal_blue,
        "normal_cyan": normal_cyan,
        "normal_green": normal_green,
        "normal_magenta": normal_magenta,
        "normal_red": normal_red,
        "normal_white": normal_white,
        "normal_yellow": normal_yellow,
        "cursor_cursor": cursor_cursor,
        "cursor_text": cursor_text,
    }
    # Non-color layout properties, randomly chosen.
    default_non_color_props = {
        "table_show_lines": random.choice([True, False]),
        "table_box": random.choice(
            ["ROUNDED", "SIMPLE", "SQUARE", "MINIMAL", "HEAVY", "DOUBLE_EDGE"]
        ),
        "panel_padding": random.choice([[1, 2], [1, 1], [2, 2], [0, 2]]),
        "panel_title_align": random.choice(["left", "center", "right"]),
        "table_row_styles": random.choice(
            [["", "dim"], ["", "italic"], ["", "underline"]]
        ),
    }
    # Extra table layout properties (non-content).
    default_extra_table_props = {
        "table_safe_box": True,
        "table_padding": [0, 1],
        "table_collapse_padding": False,
        "table_pad_edge": True,
        "table_expand": False,
        "table_show_footer": False,
        "table_show_edge": True,
        "table_leading": 0,
        "table_style": "none",
        "table_footer_style": "none",
        "table_caption": "",
        "table_caption_style": "none",
        "table_title_justify": "center",
        "table_caption_justify": "center",
        "table_highlight": False,
    }
    defaults = {
        **default_color_props,
        **extra_color_props,
        **default_non_color_props,
        **default_extra_table_props,
    }
    return defaults


def load_theme_from_file(filepath: str) -> dict:
    """Load a theme from a TOML file.

    If the file does not contain a [rich] block, one is generated and saved.
    """
    with open(filepath) as f:
        theme = toml.load(f)
    if "rich" not in theme:
        theme["rich"] = generate_default_rich_block(theme)
        with open(filepath, "w") as f:
            toml.dump(theme, f)
    return theme


def get_default_styles(theme: dict | None) -> dict[str, Any]:
    """Build a style mapping from the theme by merging defaults with any overrides
    in the [rich] block. Also resolves any color tokens.
    """
    if theme is None:
        final_styles = generate_default_rich_block(None)
    else:
        defaults = generate_default_rich_block(theme)
        rich_props = theme.get("rich", {})
        final_styles = {
            key: rich_props.get(key, defaults[key]) for key in defaults
        }
    # Ensure tuple for padding properties.
    final_styles["panel_padding"] = tuple(final_styles["panel_padding"])
    if "table_padding" in final_styles:
        final_styles["table_padding"] = tuple(final_styles["table_padding"])
    # Resolve tokens.
    if theme is not None:
        for key, value in final_styles.items():
            if isinstance(value, str):
                final_styles[key] = resolve_style_string(value, theme)
    return final_styles


def create_rich_renderable(
    value: Any,
    level: int = 0,
    theme: dict | None = None,
    styles: dict[str, Any] | None = None,
) -> Any:
    """Recursively creates a Rich renderable.

    - If value is a dict, renders it as a Table.
    - If a list/tuple, renders each item.
    - Otherwise, returns the string representation.
    """
    if styles is None:
        styles = get_default_styles(theme)

    if isinstance(value, dict):
        box_style = (
            getattr(box, styles["table_box"])
            if isinstance(styles["table_box"], str)
            else styles["table_box"]
        )
        table_kwargs = {
            "show_header": True,
            "header_style": styles["table_header_style"],
            "title": f"Subtable (Level {level})" if level > 0 else None,
            "title_style": styles["table_title_style"],
            "border_style": styles["table_border_style"],
            "show_lines": styles["table_show_lines"],
            "box": box_style,
            "row_styles": styles["table_row_styles"],
            "safe_box": styles.get("table_safe_box"),
            "padding": styles.get("table_padding"),
            "collapse_padding": styles.get("table_collapse_padding"),
            "pad_edge": styles.get("table_pad_edge"),
            "expand": styles.get("table_expand"),
            "show_footer": styles.get("table_show_footer"),
            "show_edge": styles.get("table_show_edge"),
            "leading": styles.get("table_leading"),
            "style": styles.get("table_style"),
            "footer_style": styles.get("table_footer_style"),
            "caption": styles.get("table_caption"),
            "caption_style": styles.get("table_caption_style"),
            "title_justify": styles.get("table_title_justify"),
            "caption_justify": styles.get("table_caption_justify"),
            "highlight": styles.get("table_highlight"),
        }
        table = Table(**table_kwargs)
        table.add_column("Key", style=styles["column_output"])
        table.add_column("Value", style=styles["column_value"])
        for k, v in value.items():
            table.add_row(
                str(k), create_rich_renderable(v, level + 1, theme, styles)
            )
        return table

    elif isinstance(value, (list, tuple)):
        if all(isinstance(item, dict) for item in value):
            sub_tables = []
            for i, item in enumerate(value):
                sub_tables.append(f"[bold]Item {i + 1}[/bold]")
                sub_tables.append(
                    create_rich_renderable(item, level + 1, theme, styles)
                )
            return Group(*sub_tables)
        else:
            rendered_items = [
                create_rich_renderable(item, level + 1, theme, styles)
                for item in value
            ]
            if all(isinstance(item, str) for item in rendered_items):
                return "\n".join(rendered_items)
            else:
                return Group(*rendered_items)
    else:
        if isinstance(value, str) and "\n" in value:
            return f"\n{value}\n"
        return str(value)


# --- Theme Builder Functions --- #


def load_theme_files(theme_dir: pathlib.Path) -> list[pathlib.Path]:
    """Return a list of .toml theme files in the given directory."""
    return list(theme_dir.glob("*.toml"))


def display_color_palette(theme: dict) -> None:
    """Display the color palette from a theme's [colors] sections with a color preview."""
    console = Console()
    palette_table = Table(
        title="Color Palette", show_header=True, header_style="bold"
    )
    palette_table.add_column("Section", style="bold")
    palette_table.add_column("Key", style="italic")
    palette_table.add_column("Value", style="bold")
    palette_table.add_column("Preview", justify="center")

    # Iterate over the colors in each section.
    for section, colors in theme.get("colors", {}).items():
        for key, value in colors.items():
            # Create a Text object with a fixed-width string (here, six spaces)
            # styled with a background color of the actual color value.
            preview = Text("      ", style=f"on {value}")
            palette_table.add_row(section, key, value, preview)

    console.print(palette_table)


def generate_sample_rich_blocks(
    chosen_theme: dict, count: int
) -> list[dict[str, Any]]:
    """Generate a list of sample rich blocks (randomized layout) using the chosen theme's colors."""
    samples = []
    for _ in range(count):
        samples.append(generate_default_rich_block(chosen_theme))
    return samples


def generate_sample_table(sample_theme: dict, dummy_data: dict) -> Panel:
    """Generate a sample table using the given theme dictionary (which includes a [rich] block)
    and some dummy data.
    """
    # Here we use our create_rich_renderable to build a table for dummy_data.
    # For simplicity, we create our own panel.
    styles = get_default_styles(sample_theme)
    # Build a basic table (using our earlier functions)
    table = create_rich_renderable(
        dummy_data, theme=sample_theme, styles=styles
    )
    return Panel(
        table,
        title="Sample Table",
        title_align=styles["panel_title_align"],
        border_style=styles["panel_border_style"],
        padding=styles["panel_padding"],
        style=styles["panel_style"],
    )


def save_theme(theme: dict, filename: pathlib.Path) -> None:
    """Save the given theme dictionary to the specified TOML file."""
    with open(filename, "w") as f:
        toml.dump(theme, f)


# --- Main Interactive Loop --- #


def theme_builder():
    console = Console(force_terminal=True, color_system="truecolor")
    themes_dir = pathlib.Path(__file__).parent.parent.parent.parent / "themes"
    theme_files = load_theme_files(themes_dir)

    if not theme_files:
        console.print("[red]No theme files found in the themes folder.[/red]")
        return

    # Ask the user: load all themes or N random themes?
    console.print("[bold]Theme Builder[/bold]")
    choice = console.input(
        "Load [bold](a)ll[/bold] themes or [bold](n)[/bold] random ones? (a/n): "
    )
    if choice.lower() == "n":
        n = console.input("How many random themes? ")
        try:
            n = int(n)
        except ValueError:
            n = len(theme_files)
        theme_files = random.sample(theme_files, min(n, len(theme_files)))

    # Display palettes for each theme file.
    console.print("\n[underline]Available Color Palettes:[/underline]")
    palettes = []
    for idx, tf in enumerate(theme_files):
        theme_dict = load_theme_from_file(str(tf))
        palettes.append((tf, theme_dict))
        console.print(f"\n[bold]Theme #{idx} - {tf.name}[/bold]")
        display_color_palette(theme_dict)

    # Let the user choose a palette by index.
    sel = console.input("\nEnter the number of the palette to use: ")
    try:
        sel = int(sel)
        chosen_theme = palettes[sel][1]
    except (ValueError, IndexError):
        console.print("[red]Invalid selection. Exiting.[/red]")
        return

    console.print("\n[underline]Selected Palette:[/underline]")
    display_color_palette(chosen_theme)

    # Ask the user how many sample tables to generate.
    count = console.input("\nHow many sample tables to generate? (default 3): ")
    try:
        count = int(count)
    except ValueError:
        count = 3

    # Generate sample rich blocks from the chosen theme.
    sample_rich_blocks = generate_sample_rich_blocks(chosen_theme, count)

    # For each sample, create a new theme dict that uses the chosen palette and the sample rich block.
    dummy_data = {
        "Agent": "Test Agent",
        "Status": "Running",
        "Metrics": {
            "CPU": "20%",
            "Memory": "512MB",
            "Nested": {"value1": 1, "value2": 2},
        },
        "Logs": [
            "Initialization complete",
            "Running process...",
            {"Step": "Completed", "Time": "2025-02-07T12:00:00Z"},
        ],
    }

    samples = []
    for i, rich_block in enumerate(sample_rich_blocks):
        # Build a sample theme: copy the chosen theme and override its [rich] block.
        sample_theme = dict(
            chosen_theme
        )  # shallow copy (good enough if colors remain unchanged)
        sample_theme["rich"] = rich_block
        sample_table = generate_sample_table(sample_theme, dummy_data)
        samples.append((sample_theme, sample_table))
        console.print(f"\n[bold]Sample Table #{i}[/bold]")
        console.print(sample_table)

    # Let the user choose one sample or regenerate.
    sel2 = console.input(
        "\nEnter the number of the sample table you like, or type [bold]r[/bold] to regenerate: "
    )
    if sel2.lower() == "r":
        console.print("Regenerating samples...")
        theme_builder()  # restart the builder
        return
    try:
        sel2 = int(sel2)
        chosen_sample_theme = samples[sel2][0]
    except (ValueError, IndexError):
        console.print("[red]Invalid selection. Exiting.[/red]")
        return

    # Ask for file name to save the chosen theme.
    filename = console.input(
        "\nEnter a filename to save the chosen theme (e.g. mytheme.toml): "
    )
    save_path = themes_dir / filename
    save_theme(chosen_sample_theme, save_path)
    console.print(f"\n[green]Theme saved as {save_path}.[/green]")
```

### src\flock\core\logging\formatters\themed_formatter.py

- **Lines**: 546
- **Last modified**: 2025-05-25 23:17:06

```py
"""A Rich-based formatter for agent results with theme support."""

import pathlib
import random
import re
from typing import Any

from temporalio import workflow

from flock.core.logging.formatters.themes import OutputTheme

with workflow.unsafe.imports_passed_through():
    from pygments.style import Style
    from pygments.token import Token
    from rich import box
    from rich.console import Console, Group
    from rich.panel import Panel
    from rich.syntax import PygmentsSyntaxTheme, Syntax
    from rich.table import Table
    from rich.theme import Theme

import toml  # install with: pip install toml


def resolve_style_string(style_str: str, theme: dict) -> str:
    """Replace tokens in a style string of the form.

        color.<section>.<key>

    with the corresponding value from theme["colors"][<section>][<key>].
    If the token cannot be resolved, it is left unchanged.
    """
    pattern = r"color\.(\w+)\.(\w+)"

    def repl(match):
        section = match.group(1)
        key = match.group(2)
        try:
            return theme["colors"][section][key]
        except KeyError:
            return match.group(0)  # leave token unchanged if not found

    return re.sub(pattern, repl, style_str)


def generate_default_rich_block(theme: dict | None = None) -> dict[str, Any]:
    """Generate a default [rich] block with *all* styling properties.

    For the color mapping properties the defaults are computed from the
    theme's [colors] blocks (if available). This includes colors from the
    "bright", "normal", and "cursor" sections.

    Non color properties (layout and table specific properties) are randomly
    chosen from a set of sensible alternatives.
    """
    if theme is not None:
        # Retrieve colors from the theme.
        bright_black = theme["colors"]["bright"].get("black", "#000000")
        bright_blue = theme["colors"]["bright"].get("blue", "#96cbfe")
        bright_cyan = theme["colors"]["bright"].get("cyan", "#85befd")
        bright_green = theme["colors"]["bright"].get("green", "#94fa36")
        bright_magenta = theme["colors"]["bright"].get("magenta", "#b9b6fc")
        bright_red = theme["colors"]["bright"].get("red", "#fd5ff1")
        bright_white = theme["colors"]["bright"].get("white", "#e0e0e0")
        bright_yellow = theme["colors"]["bright"].get("yellow", "#f5ffa8")

        normal_black = theme["colors"]["normal"].get("black", "#000000")
        normal_blue = theme["colors"]["normal"].get("blue", "#85befd")
        normal_cyan = theme["colors"]["normal"].get("cyan", "#85befd")
        normal_green = theme["colors"]["normal"].get("green", "#87c38a")
        normal_magenta = theme["colors"]["normal"].get("magenta", "#b9b6fc")
        normal_red = theme["colors"]["normal"].get("red", "#fd5ff1")
        normal_white = theme["colors"]["normal"].get("white", "#e0e0e0")
        normal_yellow = theme["colors"]["normal"].get("yellow", "#ffd7b1")

        cursor_cursor = theme["colors"]["cursor"].get("cursor", "#d0d0d0")
        cursor_text = theme["colors"]["cursor"].get("text", "#151515")

        primary_background = theme["colors"]["primary"].get(
            "background", "#161719"
        )
        primary_foreground = theme["colors"]["primary"].get(
            "foreground", "#c5c8c6"
        )
        selection_background = theme["colors"]["selection"].get(
            "background", "#444444"
        )
        selection_text = theme["colors"]["selection"].get(
            "text", primary_foreground
        )
    else:
        bright_black = "black"
        bright_blue = "blue"
        bright_cyan = "cyan"
        bright_green = "green"
        bright_magenta = "magenta"
        bright_red = "red"
        bright_white = "white"
        bright_yellow = "yellow"

        normal_black = "black"
        normal_blue = "blue"
        normal_cyan = "cyan"
        normal_green = "green"
        normal_magenta = "magenta"
        normal_red = "red"
        normal_white = "white"
        normal_yellow = "yellow"

        cursor_cursor = "gray"
        cursor_text = "white"

        primary_background = "black"
        primary_foreground = "white"
        selection_background = "gray"
        selection_text = "white"

    # Color properties computed from the theme.
    default_color_props = {
        "panel_style": f"on {primary_background}",
        "table_header_style": f"bold {selection_text} on {selection_background}",
        "table_title_style": f"bold {primary_foreground}",
        "table_border_style": bright_blue,
        "panel_border_style": bright_blue,
        "column_output": f"bold {primary_foreground}",
        "column_value": primary_foreground,
    }
    # Extra color tokens so they can be used via tokens like color.bright.black, etc.
    extra_color_props = {
        "bright_black": bright_black,
        "bright_blue": bright_blue,
        "bright_cyan": bright_cyan,
        "bright_green": bright_green,
        "bright_magenta": bright_magenta,
        "bright_red": bright_red,
        "bright_white": bright_white,
        "bright_yellow": bright_yellow,
        "normal_black": normal_black,
        "normal_blue": normal_blue,
        "normal_cyan": normal_cyan,
        "normal_green": normal_green,
        "normal_magenta": normal_magenta,
        "normal_red": normal_red,
        "normal_white": normal_white,
        "normal_yellow": normal_yellow,
        "cursor_cursor": cursor_cursor,
        "cursor_text": cursor_text,
    }
    # Randomly choose non color properties.
    default_non_color_props = {
        "table_show_lines": random.choice([True, False]),
        "table_box": random.choice(
            ["ROUNDED", "SIMPLE", "SQUARE", "MINIMAL", "HEAVY", "DOUBLE_EDGE"]
        ),
        "panel_padding": random.choice([[1, 2], [1, 1], [2, 2], [0, 2]]),
        "panel_title_align": random.choice(["left", "center", "right"]),
        # Add table_row_styles property.
        "table_row_styles": random.choice(
            [["", "dim"], ["", "italic"], ["", "underline"]]
        ),
    }
    # Extra table layout properties (non content properties).
    default_extra_table_props = {
        "table_safe_box": True,
        "table_padding": [0, 1],
        "table_collapse_padding": False,
        "table_pad_edge": True,
        "table_expand": False,
        "table_show_footer": False,
        "table_show_edge": True,
        "table_leading": 0,
        "table_style": "none",
        "table_footer_style": "none",
        "table_caption": None,
        "table_caption_style": "none",
        "table_title_justify": "center",
        "table_caption_justify": "center",
        "table_highlight": False,
    }
    # Combine all defaults.
    defaults = {
        **default_color_props,
        **extra_color_props,
        **default_non_color_props,
        **default_extra_table_props,
    }
    return defaults


def load_theme_from_file(filepath: str) -> dict:
    """Load a theme from a TOML file.

    The theme is expected to contain color blocks like [colors.primary],
    [colors.selection], [colors.normal], [colors.cursor], etc.
    If the file does not contain a [rich] block for styling properties,
    one is generated (with all properties including color mappings) and
    written back into the file.
    """
    with open(filepath) as f:
        theme = toml.load(f)

    if "rich" not in theme:
        theme["rich"] = generate_default_rich_block(theme)
        # Write the updated theme back into the file.
        with open(filepath, "w") as f:
            toml.dump(theme, f)

    return theme


def get_default_styles(theme: dict | None) -> dict[str, Any]:
    """Build a style mapping from the theme.

    It first computes defaults from the [colors] block (via generate_default_rich_block)
    and then overrides any property found in the [rich] block.
    Finally, for every property that is a string, tokens of the form
    "color.<section>.<key>" are resolved.
    """
    if theme is None:
        final_styles = generate_default_rich_block(None)
    else:
        defaults = generate_default_rich_block(theme)
        rich_props = theme.get("rich", {})
        final_styles = {
            key: rich_props.get(key, defaults[key]) for key in defaults
        }

    # Ensure that panel_padding and table_padding are tuples.
    final_styles["panel_padding"] = tuple(final_styles["panel_padding"])
    if "table_padding" in final_styles:
        final_styles["table_padding"] = tuple(final_styles["table_padding"])

    # Resolve tokens in every string value.
    if theme is not None:
        for key, value in final_styles.items():
            if isinstance(value, str):
                final_styles[key] = resolve_style_string(value, theme)

    return final_styles


def create_rich_renderable(
    value: Any,
    level: int = 0,
    theme: dict | None = None,
    styles: dict[str, Any] | None = None,
    max_length: int = -1,
) -> Any:
    """Recursively creates a Rich renderable for a given value.

    - For dicts: creates a Table with headers styled via the computed properties.
    - For lists/tuples: if every item is a dict, returns a Group of subtables;
      otherwise, renders each item recursively.
    - Other types: returns a string (adding extra newlines for multi-line strings).
    """
    if styles is None:
        styles = get_default_styles(theme)

    # If the value is a dictionary, render it as a table.
    if isinstance(value, dict):
        # Convert table_box string into an actual box style.
        box_style = (
            getattr(box, styles["table_box"])
            if isinstance(styles["table_box"], str)
            else styles["table_box"]
        )
        # Gather all table-related keyword arguments.
        table_kwargs = {
            "show_header": True,
            "header_style": styles["table_header_style"],
            "title": f"Subtable (Level {level})" if level > 0 else None,
            "title_style": styles["table_title_style"],
            "border_style": styles["table_border_style"],
            "show_lines": styles["table_show_lines"],
            "box": box_style,
            "row_styles": styles["table_row_styles"],
            "safe_box": styles.get("table_safe_box"),
            "padding": styles.get("table_padding"),
            "collapse_padding": styles.get("table_collapse_padding"),
            "pad_edge": styles.get("table_pad_edge"),
            "expand": styles.get("table_expand"),
            "show_footer": styles.get("table_show_footer"),
            "show_edge": styles.get("table_show_edge"),
            "leading": styles.get("table_leading"),
            "style": styles.get("table_style"),
            "footer_style": styles.get("table_footer_style"),
            "caption": styles.get("table_caption"),
            "caption_style": styles.get("table_caption_style"),
            "title_justify": styles.get("table_title_justify"),
            "caption_justify": styles.get("table_caption_justify"),
            "highlight": styles.get("table_highlight"),
        }
        table = Table(**table_kwargs)
        table.add_column("Key", style=styles["column_output"])
        table.add_column("Value", style=styles["column_value"])
        for k, v in value.items():
            table.add_row(
                str(k),
                create_rich_renderable(v, level + 1, theme, styles, max_length),
            )
        return table

    # If the value is a list or tuple, render each item.
    elif isinstance(value, list | tuple):
        if all(isinstance(item, dict) for item in value):
            sub_tables = []
            for i, item in enumerate(value):
                sub_tables.append(f"[bold]Item {i + 1}[/bold]")
                sub_tables.append(
                    create_rich_renderable(
                        item, level + 1, theme, styles, max_length=max_length
                    )
                )
            return Group(*sub_tables)
        else:
            rendered_items = [
                create_rich_renderable(
                    item, level + 1, theme, styles, max_length=max_length
                )
                for item in value
            ]
            if all(isinstance(item, str) for item in rendered_items):
                return "\n".join(rendered_items)
            else:
                return Group(*rendered_items)

    # Otherwise, return a string representation.
    else:
        s = str(value).strip()
        if max_length > 0 and len(s) > max_length:
            omitted = len(s) - max_length
            s = (
                s[:max_length]
                + f"[bold bright_yellow]...(+{omitted}chars)[/bold bright_yellow]"
            )
        if isinstance(value, str) and "\n" in value:
            return f"\n{s}\n"
        return s


def load_syntax_theme_from_file(filepath: str) -> dict:
    """Load a syntax highlighting theme from a TOML file and map it to Rich styles."""
    with open(filepath) as f:
        theme = toml.load(f)

    if "colors" not in theme:
        raise ValueError(
            f"Theme file {filepath} does not contain a 'colors' section."
        )

    # Map theme colors to syntax categories
    syntax_theme = {
        "background": theme["colors"]["primary"].get("background", "#161719"),
        "text": theme["colors"]["primary"].get("foreground", "#c5c8c6"),
        "comment": theme["colors"]["normal"].get("black", "#666666"),
        "keyword": theme["colors"]["bright"].get("magenta", "#ff79c6"),
        "builtin": theme["colors"]["bright"].get("cyan", "#8be9fd"),
        "string": theme["colors"]["bright"].get("green", "#50fa7b"),
        "name": theme["colors"]["bright"].get("blue", "#6272a4"),
        "number": theme["colors"]["bright"].get("yellow", "#f1fa8c"),
        "operator": theme["colors"]["bright"].get("red", "#ff5555"),
        "punctuation": theme["colors"]["normal"].get("white", "#bbbbbb"),
        "error": theme["colors"]["bright"].get("red", "#ff5555"),
    }

    return syntax_theme


def create_rich_syntax_theme(syntax_theme: dict) -> Theme:
    """Convert a syntax theme dict to a Rich-compatible Theme."""
    return Theme(
        {
            "background": f"on {syntax_theme['background']}",
            "text": syntax_theme["text"],
            "keyword": f"bold {syntax_theme['keyword']}",
            "builtin": f"bold {syntax_theme['builtin']}",
            "string": syntax_theme["string"],
            "name": syntax_theme["name"],
            "number": syntax_theme["number"],
            "operator": syntax_theme["operator"],
            "punctuation": syntax_theme["punctuation"],
            "error": f"bold {syntax_theme['error']}",
        }
    )


def create_pygments_syntax_theme(syntax_theme: dict) -> PygmentsSyntaxTheme:
    """Convert a syntax theme dict to a Pygments-compatible Rich syntax theme."""

    class CustomSyntaxStyle(Style):
        """Dynamically generated Pygments style based on the loaded theme."""

        background_color = syntax_theme["background"]
        styles = {
            Token.Text: syntax_theme["text"],
            Token.Comment: f"italic {syntax_theme['comment']}",
            Token.Keyword: f"bold {syntax_theme['keyword']}",
            Token.Name.Builtin: f"bold {syntax_theme['builtin']}",
            Token.String: syntax_theme["string"],
            Token.Name: syntax_theme["name"],
            Token.Number: syntax_theme["number"],
            Token.Operator: syntax_theme["operator"],
            Token.Punctuation: syntax_theme["punctuation"],
            Token.Error: f"bold {syntax_theme['error']}",
        }

    return PygmentsSyntaxTheme(CustomSyntaxStyle)


class ThemedAgentResultFormatter:
    """Formats agent results in a Rich table with nested subtables and theme support."""

    def __init__(
        self,
        theme: OutputTheme = OutputTheme.afterglow,
        max_length: int = -1,
        render_table: bool = True,
    ):
        """Initialize the formatter with a theme and optional max length."""
        self.theme = theme
        self.styles = None
        self.max_length = max_length
        self.render_table = render_table

    def format_result(
        self,
        result: dict[str, Any],
        agent_name: str,
        theme,
        styles,
    ) -> Panel:
        from devtools import pformat

        """Format an agent's result as a Rich Panel containing a table."""
        box_style = (
            getattr(box, styles["table_box"])
            if isinstance(styles["table_box"], str)
            else styles["table_box"]
        )

        # Gather table properties for the main table.
        table_kwargs = {
            "show_header": True,
            "header_style": styles["table_header_style"],
            "title": f"Agent Results: {agent_name}",
            "title_style": styles["table_title_style"],
            "border_style": styles["table_border_style"],
            "show_lines": styles["table_show_lines"],
            "box": box_style,
            "row_styles": styles["table_row_styles"],
            "safe_box": styles.get("table_safe_box"),
            "padding": styles.get("table_padding"),
            "collapse_padding": styles.get("table_collapse_padding"),
            "pad_edge": styles.get("table_pad_edge"),
            "expand": styles.get("table_expand"),
            "show_footer": styles.get("table_show_footer"),
            "show_edge": styles.get("table_show_edge"),
            "leading": styles.get("table_leading"),
            "style": styles.get("table_style"),
            "footer_style": styles.get("table_footer_style"),
            "caption": styles.get("table_caption"),
            "caption_style": styles.get("table_caption_style"),
            "title_justify": styles.get("table_title_justify"),
            "caption_justify": styles.get("table_caption_justify"),
            "highlight": styles.get("table_highlight"),
        }

        table = Table(**table_kwargs)
        table.add_column("Output", style=styles["column_output"])
        table.add_column("Value", style=styles["column_value"])
        for key, value in result.items():
            rich_renderable = create_rich_renderable(
                value,
                level=0,
                theme=theme,
                styles=styles,
                max_length=self.max_length,
            )
            table.add_row(key, rich_renderable)

        s = pformat(result, highlight=False)

        if self.render_table:
            return Panel(
                table,
                title="🐤🐧🐓🦆",
                title_align=styles["panel_title_align"],
                border_style=styles["panel_border_style"],
                padding=styles["panel_padding"],
                style=styles["panel_style"],
            )
        else:
            syntax = Syntax(
                s,  # The formatted string
                "python",  # Highlight as Python (change this for other formats)
                theme=self.syntax_style,  # Choose a Rich theme (matches your color setup)
                line_numbers=False,
            )
            return Panel(
                syntax,
                title=agent_name,
                title_align=styles["panel_title_align"],
                border_style=styles["panel_border_style"],
                padding=styles["panel_padding"],
                style=styles["panel_style"],
            )

    def display_result(self, result: dict[str, Any], agent_name: str) -> None:
        """Print an agent's result using Rich formatting."""
        theme = self.theme
        themes_dir = (
            pathlib.Path(__file__).parent.parent.parent.parent / "themes"
        )
        all_themes = list(themes_dir.glob("*.toml"))
        theme = (
            theme.value + ".toml"
            if not theme.value.endswith(".toml")
            else theme.value
        )
        theme = (
            pathlib.Path(__file__).parent.parent.parent.parent
            / "themes"
            / theme
        )

        if pathlib.Path(theme) not in all_themes:
            raise ValueError(
                f"Invalid theme: {theme}\nAvailable themes: {all_themes}"
            )

        theme_dict = load_theme_from_file(theme)

        styles = get_default_styles(theme_dict)
        self.styles = styles
        self.syntax_style = create_pygments_syntax_theme(
            load_syntax_theme_from_file(theme)
        )

        console = Console()
        panel = self.format_result(
            result=result,
            agent_name=agent_name,
            theme=theme_dict,
            styles=styles,
        )
        console.print(panel)
```

### src\flock\core\logging\formatters\themes.py

- **Lines**: 340
- **Last modified**: 2025-04-16 00:11:15

```py
from enum import Enum


class OutputTheme(str, Enum):
    tomorrow_night_eighties = "tomorrow-night-eighties"
    builtin_light = "builtin-light"
    iterm2_dark_background = "iterm2-dark-background"
    zenbones = "zenbones"
    iterm2_tango_dark = "iterm2-tango-dark"
    gruber_darker = "gruber-darker"
    scarlet_protocol = "scarlet-protocol"
    purplepeter = "purplepeter"
    seashells = "seashells"
    monokai_soda = "monokai-soda"
    wildcherry = "wildcherry"
    builtin_solarized_light = "builtin-solarized-light"
    firewatch = "firewatch"
    builtin_tango_dark = "builtin-tango-dark"
    spacedust = "spacedust"
    paraiso_dark = "paraiso-dark"
    nightlion_v2 = "nightlion-v2"
    misterioso = "misterioso"
    shades_of_purple = "shades-of-purple"
    red_planet = "red-planet"
    flat = "flat"
    terafox = "terafox"
    crayonponyfish = "crayonponyfish"
    elementary = "elementary"
    blulocolight = "blulocolight"
    blazer = "blazer"
    purple_rain = "purple-rain"
    aurora = "aurora"
    neutron = "neutron"
    alienblood = "alienblood"
    symfonic = "symfonic"
    pro = "pro"
    highway = "highway"
    grape = "grape"
    hax0r_blue = "hax0r-blue"
    zenwritten_light = "zenwritten-light"
    spacegray = "spacegray"
    everblush = "everblush"
    popping_and_locking = "popping-and-locking"
    zenburn = "zenburn"
    monalisa = "monalisa"
    deep = "deep"
    ir_black = "ir-black"
    wombat = "wombat"
    zenbones_light = "zenbones-light"
    darkermatrix = "darkermatrix"
    wez = "wez"
    matrix = "matrix"
    farmhouse_light = "farmhouse-light"
    sublette = "sublette"
    nocturnal_winter = "nocturnal-winter"
    ryuuko = "ryuuko"
    jackie_brown = "jackie-brown"
    framer = "framer"
    _3024_day = "3024-day"
    lovelace = "lovelace"
    teerb = "teerb"
    fairyfloss = "fairyfloss"
    tokyonight = "tokyonight"
    xcodelighthc = "xcodelighthc"
    iceberg_light = "iceberg-light"
    gruvboxlight = "gruvboxlight"
    tomorrow = "tomorrow"
    sleepyhollow = "sleepyhollow"
    monokai_vivid = "monokai-vivid"
    synthwave_everything = "synthwave-everything"
    tomorrow_night_burns = "tomorrow-night-burns"
    hurtado = "hurtado"
    dotgov = "dotgov"
    adventure = "adventure"
    tomorrow_night = "tomorrow-night"
    arthur = "arthur"
    fahrenheit = "fahrenheit"
    oxocarbon = "oxocarbon"
    violet_dark = "violet-dark"
    adventuretime = "adventuretime"
    vesper = "vesper"
    overnight_slumber = "overnight-slumber"
    japanesque = "japanesque"
    encom = "encom"
    brogrammer = "brogrammer"
    _3024_night = "3024-night"
    hivacruz = "hivacruz"
    darkmatrix = "darkmatrix"
    synthwavealpha = "synthwavealpha"
    aardvark_blue = "aardvark-blue"
    xcodewwdc = "xcodewwdc"
    chester = "chester"
    flatland = "flatland"
    n0tch2k = "n0tch2k"
    molokai = "molokai"
    violet_light = "violet-light"
    solarized_darcula = "solarized-darcula"
    espresso = "espresso"
    darkside = "darkside"
    flexoki_light = "flexoki-light"
    bright_lights = "bright-lights"
    clrs = "clrs"
    firefly_traditional = "firefly-traditional"
    forestblue = "forestblue"
    batman = "batman"
    snazzy = "snazzy"
    wryan = "wryan"
    kurokula = "kurokula"
    iterm2_pastel_dark_background = "iterm2-pastel-dark-background"
    afterglow = "afterglow"
    seoulbones_light = "seoulbones-light"
    ollie = "ollie"
    shaman = "shaman"
    liquidcarbontransparent = "liquidcarbontransparent"
    ayu_mirage = "ayu-mirage"
    kolorit = "kolorit"
    red_sands = "red-sands"
    funforrest = "funforrest"
    unikitty = "unikitty"
    espresso_libre = "espresso-libre"
    ultraviolent = "ultraviolent"
    ayu_light = "ayu-light"
    terminal_basic = "terminal-basic"
    paulmillr = "paulmillr"
    github = "github"
    hacktober = "hacktober"
    ayu_copy = "ayu copy"
    material = "material"
    vimbones = "vimbones"
    arcoiris = "arcoiris"
    wilmersdorf = "wilmersdorf"
    desert = "desert"
    rouge_2 = "rouge-2"
    doom_peacock = "doom-peacock"
    smyck = "smyck"
    cutiepro = "cutiepro"
    nvimlight = "nvimlight"
    hipster_green = "hipster-green"
    spiderman = "spiderman"
    nvimdark = "nvimdark"
    sugarplum = "sugarplum"
    catppuccin_latte = "catppuccin-latte"
    dayfox = "dayfox"
    seafoam_pastel = "seafoam-pastel"
    peppermint = "peppermint"
    tokyonight_storm = "tokyonight-storm"
    mariana = "mariana"
    novel = "novel"
    argonaut_copy = "argonaut copy"
    twilight = "twilight"
    xcodelight = "xcodelight"
    homebrew = "homebrew"
    ateliersulphurpool = "ateliersulphurpool"
    thayer_bright = "thayer-bright"
    konsolas = "konsolas"
    iterm2_solarized_light = "iterm2-solarized-light"
    midnight_in_mojave = "midnight-in-mojave"
    materialdarker = "materialdarker"
    royal = "royal"
    builtin_tango_light = "builtin-tango-light"
    idletoes = "idletoes"
    operator_mono_dark = "operator-mono-dark"
    cyberdyne = "cyberdyne"
    atom = "atom"
    hybrid = "hybrid"
    slate = "slate"
    duckbones = "duckbones"
    tinacious_design__dark_ = "tinacious-design-(dark)"
    kibble = "kibble"
    sakura = "sakura"
    lab_fox = "lab-fox"
    blue_matrix = "blue-matrix"
    materialdesigncolors = "materialdesigncolors"
    seoulbones_dark = "seoulbones-dark"
    seti = "seti"
    solarized_dark_higher_contrast = "solarized-dark-higher-contrast"
    chalkboard = "chalkboard"
    mathias = "mathias"
    neobones_dark = "neobones-dark"
    alabaster = "alabaster"
    djangorebornagain = "djangorebornagain"
    ayu = "ayu"
    iterm2_default = "iterm2-default"
    mirage = "mirage"
    firefoxdev = "firefoxdev"
    nightfox = "nightfox"
    grey_green = "grey-green"
    broadcast = "broadcast"
    solarized_dark___patched = "solarized-dark---patched"
    flexoki_dark = "flexoki-dark"
    challengerdeep = "challengerdeep"
    onehalflight = "onehalflight"
    earthsong = "earthsong"
    kanagawabones = "kanagawabones"
    gruvboxdarkhard = "gruvboxdarkhard"
    abernathy = "abernathy"
    oceanicmaterial = "oceanicmaterial"
    medallion = "medallion"
    pnevma = "pnevma"
    birdsofparadise = "birdsofparadise"
    toychest = "toychest"
    dimidium = "dimidium"
    cyberpunk = "cyberpunk"
    duotone_dark = "duotone-dark"
    whimsy = "whimsy"
    nord_light = "nord-light"
    belafonte_day = "belafonte-day"
    square = "square"
    retro = "retro"
    pandora = "pandora"
    galaxy = "galaxy"
    the_hulk = "the-hulk"
    rose_pine_moon = "rose-pine-moon"
    coffee_theme = "coffee-theme"
    tomorrow_night_bright = "tomorrow-night-bright"
    blulocodark = "blulocodark"
    sundried = "sundried"
    rippedcasts = "rippedcasts"
    glacier = "glacier"
    zenwritten_dark = "zenwritten-dark"
    xcodedarkhc = "xcodedarkhc"
    iterm2_solarized_dark = "iterm2-solarized-dark"
    softserver = "softserver"
    jubi = "jubi"
    fishtank = "fishtank"
    spacegray_eighties_dull = "spacegray-eighties-dull"
    raycast_light = "raycast-light"
    tinacious_design__light_ = "tinacious-design-(light)"
    gruvboxdark = "gruvboxdark"
    piatto_light = "piatto-light"
    grass = "grass"
    catppuccin_mocha = "catppuccin-mocha"
    hardcore = "hardcore"
    tokyonight_day = "tokyonight-day"
    underthesea = "underthesea"
    guezwhoz = "guezwhoz"
    borland = "borland"
    argonaut = "argonaut"
    farmhouse_dark = "farmhouse-dark"
    rapture = "rapture"
    zenbones_dark = "zenbones-dark"
    iceberg_dark = "iceberg-dark"
    pro_light = "pro-light"
    jellybeans = "jellybeans"
    later_this_evening = "later-this-evening"
    blueberrypie = "blueberrypie"
    vibrantink = "vibrantink"
    dimmedmonokai = "dimmedmonokai"
    catppuccin_macchiato = "catppuccin-macchiato"
    ocean = "ocean"
    banana_blueberry = "banana-blueberry"
    dark_ = "dark+"
    neopolitan = "neopolitan"
    relaxed = "relaxed"
    galizur = "galizur"
    liquidcarbon = "liquidcarbon"
    hax0r_gr33n = "hax0r-gr33n"
    ic_orange_ppl = "ic-orange-ppl"
    niji = "niji"
    liquidcarbontransparentinverse = "liquidcarbontransparentinverse"
    github_dark = "github-dark"
    zenburned = "zenburned"
    django = "django"
    rose_pine_dawn = "rose-pine-dawn"
    builtin_dark = "builtin-dark"
    iterm2_smoooooth = "iterm2-smoooooth"
    neon = "neon"
    raycast_dark = "raycast-dark"
    palenighthc = "palenighthc"
    laser = "laser"
    builtin_solarized_dark = "builtin-solarized-dark"
    cobalt2 = "cobalt2"
    breeze = "breeze"
    apple_classic = "apple-classic"
    c64 = "c64"
    calamity = "calamity"
    onehalfdark = "onehalfdark"
    neobones_light = "neobones-light"
    dracula = "dracula"
    spring = "spring"
    monokai_remastered = "monokai-remastered"
    lavandula = "lavandula"
    night_owlish_light = "night-owlish-light"
    builtin_pastel_dark = "builtin-pastel-dark"
    frontenddelight = "frontenddelight"
    tango_adapted = "tango-adapted"
    ubuntu = "ubuntu"
    oceanic_next = "oceanic-next"
    primary = "primary"
    materialdark = "materialdark"
    doomone = "doomone"
    rose_pine = "rose-pine"
    chalk = "chalk"
    andromeda = "andromeda"
    djangosmooth = "djangosmooth"
    red_alert = "red-alert"
    warmneon = "warmneon"
    man_page = "man-page"
    hopscotch = "hopscotch"
    urple = "urple"
    tomorrow_night_blue = "tomorrow-night-blue"
    atomonelight = "atomonelight"
    pencillight = "pencillight"
    ciapre = "ciapre"
    dracula_ = "dracula+"
    hopscotch_256 = "hopscotch.256"
    fideloper = "fideloper"
    treehouse = "treehouse"
    ic_green_ppl = "ic-green-ppl"
    tango_half_adapted = "tango-half-adapted"
    belafonte_night = "belafonte-night"
    iterm2_light_background = "iterm2-light-background"
    harper = "harper"
    mellifluous = "mellifluous"
    rebecca = "rebecca"
    cga = "cga"
    cobalt_neon = "cobalt-neon"
    synthwave = "synthwave"
    pencildark = "pencildark"
    cyberpunkscarletprotocol = "cyberpunkscarletprotocol"
    iterm2_tango_light = "iterm2-tango-light"
    subliminal = "subliminal"
    idea = "idea"
    xcodedark = "xcodedark"
    apple_system_colors = "apple-system-colors"
    hax0r_r3d = "hax0r-r3d"
    atom_test = "atom_test"
    floraverse = "floraverse"
    materialocean = "materialocean"
    nord = "nord"
    vaughn = "vaughn"
    obsidian = "obsidian"
    jetbrains_darcula = "jetbrains-darcula"
    elemental = "elemental"
    spacegray_eighties = "spacegray-eighties"
    nightlion_v1 = "nightlion-v1"
    bluedolphin = "bluedolphin"
    catppuccin_frappe = "catppuccin-frappe"
    dark_pastel = "dark-pastel"
    ultradark = "ultradark"
```

### src\flock\core\logging\span_middleware\baggage_span_processor.py

- **Lines**: 31
- **Last modified**: 2025-04-16 00:11:15

```py
from opentelemetry.baggage import get_baggage
from opentelemetry.sdk.trace import SpanProcessor


class BaggageAttributeSpanProcessor(SpanProcessor):
    """A custom span processor that, on span start, inspects the baggage items from the parent context
    and attaches specified baggage keys as attributes on the span.
    """

    def __init__(self, baggage_keys=None):
        # baggage_keys: list of baggage keys to attach to spans (e.g. ["session_id", "run_id"])
        if baggage_keys is None:
            baggage_keys = []
        self.baggage_keys = baggage_keys

    def on_start(self, span, parent_context):
        # For each desired key, look up its value in the parent context baggage and set it as an attribute.
        for key in self.baggage_keys:
            value = get_baggage(key, context=parent_context)
            if value is not None:
                span.set_attribute(key, value)

    def on_end(self, span):
        # No action required on span end for this processor.
        pass

    def shutdown(self):
        pass

    def force_flush(self, timeout_millis: int = 30000):
        pass
```

### src\flock\core\logging\telemetry.py

- **Lines**: 170
- **Last modified**: 2025-05-22 21:27:37

```py
"""This module sets up OpenTelemetry tracing for a service."""

import sys

from opentelemetry import trace
from opentelemetry.sdk.resources import Resource
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import SimpleSpanProcessor
from temporalio import workflow

from flock.core.logging.span_middleware.baggage_span_processor import (
    BaggageAttributeSpanProcessor,
)

with workflow.unsafe.imports_passed_through():
    from flock.core.logging.telemetry_exporter.file_exporter import (
        FileSpanExporter,
    )
    from flock.core.logging.telemetry_exporter.sqlite_exporter import (
        SqliteTelemetryExporter,
    )


class TelemetryConfig:
    """This configuration class sets up OpenTelemetry tracing.

      - Export spans to a Jaeger collector using gRPC.
      - Write spans to a file.
      - Save spans in a SQLite database.

    Only exporters with a non-None configuration will be activated.
    """

    def __init__(
        self,
        service_name: str,
        jaeger_endpoint: str | None = None,
        jaeger_transport: str = "grpc",
        local_logging_dir: str | None = None,
        file_export_name: str | None = None,
        sqlite_db_name: str | None = None,
        enable_jaeger: bool = True,
        enable_file: bool = True,
        enable_sql: bool = True,
        enable_otlp: bool = True,
        otlp_protocol: str = "grpc",
        otlp_endpoint: str = "http://localhost:4317",
        batch_processor_options: dict | None = None,
    ):
        """:param service_name: Name of your service.

        :param jaeger_endpoint: The Jaeger collector gRPC endpoint (e.g., "localhost:14250").
        :param file_export_path: If provided, spans will be written to this file.
        :param sqlite_db_path: If provided, spans will be stored in this SQLite DB.
        :param batch_processor_options: Dict of options for BatchSpanProcessor (e.g., {"max_export_batch_size": 10}).
        """
        self.service_name = service_name
        self.jaeger_endpoint = jaeger_endpoint
        self.jaeger_transport = jaeger_transport
        self.file_export_name = file_export_name
        self.sqlite_db_name = sqlite_db_name
        self.local_logging_dir = local_logging_dir
        self.batch_processor_options = batch_processor_options or {}
        self.enable_jaeger = enable_jaeger
        self.enable_file = enable_file
        self.enable_sql = enable_sql
        self.enable_otlp = enable_otlp
        self.otlp_protocol = otlp_protocol
        self.otlp_endpoint = otlp_endpoint
        self.global_tracer = None

    def setup_tracing(self):
        """Set up OpenTelemetry tracing with the specified exporters."""
        # Create a Resource with the service name.
        resource = Resource(attributes={"service.name": self.service_name})
        provider = TracerProvider(resource=resource)
        trace.set_tracer_provider(provider)

        # List to collect our span processors.
        span_processors = []

        # If a Jaeger endpoint is specified, add the Jaeger exporter.
        if self.jaeger_endpoint and self.enable_jaeger:
            if self.jaeger_transport == "grpc":
                from opentelemetry.exporter.jaeger.proto.grpc import (
                    JaegerExporter,
                )

                jaeger_exporter = JaegerExporter(
                    endpoint=self.jaeger_endpoint,
                    insecure=True,
                )
            elif self.jaeger_transport == "http":
                from opentelemetry.exporter.jaeger.thrift import JaegerExporter

                jaeger_exporter = JaegerExporter(
                    collector_endpoint=self.jaeger_endpoint,
                )
            else:
                raise ValueError(
                    "Invalid JAEGER_TRANSPORT specified. Use 'grpc' or 'http'."
                )

            span_processors.append(SimpleSpanProcessor(jaeger_exporter))


        if self.enable_otlp:
            if self.otlp_protocol == "grpc":
                from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import (
                    OTLPSpanExporter,
                )

                otlp_exporter = OTLPSpanExporter(
                    endpoint=self.otlp_endpoint,
                    insecure=True,
                )
            elif self.otlp_protocol == "http":
                from opentelemetry.exporter.otlp.proto.http.trace_exporter import (
                    OTLPSpanExporter,
                )

                otlp_exporter = OTLPSpanExporter(
                    endpoint=self.otlp_endpoint,
                )
            else:
                raise ValueError(
                    "Invalid OTEL_EXPORTER_OTLP_PROTOCOL specified. Use 'grpc' or 'http'."
                )

            span_processors.append(SimpleSpanProcessor(otlp_exporter))

        # If a file path is provided, add the custom file exporter.
        if self.file_export_name and self.enable_file:
            file_exporter = FileSpanExporter(
                self.local_logging_dir, self.file_export_name
            )
            span_processors.append(SimpleSpanProcessor(file_exporter))

        # If a SQLite database path is provided, ensure the DB exists and add the SQLite exporter.
        if self.sqlite_db_name and self.enable_sql:
            sqlite_exporter = SqliteTelemetryExporter(
                self.local_logging_dir, self.sqlite_db_name
            )
            span_processors.append(SimpleSpanProcessor(sqlite_exporter))

        # Register all span processors with the provider.
        for processor in span_processors:
            provider.add_span_processor(processor)

        provider.add_span_processor(
            BaggageAttributeSpanProcessor(baggage_keys=["session_id", "run_id"])
        )
        # self.global_tracer = trace.get_tracer("flock")
        sys.excepthook = self.log_exception_to_otel

    def log_exception_to_otel(self, exc_type, exc_value, exc_traceback):
        """Log unhandled exceptions to OpenTelemetry."""
        if issubclass(exc_type, KeyboardInterrupt):
            # Allow normal handling of KeyboardInterrupt
            sys.__excepthook__(exc_type, exc_value, exc_traceback)
            return

        # Use OpenTelemetry to record the exception
        with self.global_tracer.start_as_current_span(
            "UnhandledException"
        ) as span:
            span.record_exception(exc_value)
            span.set_status(
                trace.Status(trace.StatusCode.ERROR, str(exc_value))
            )
```

### src\flock\core\logging\telemetry_exporter\base_exporter.py

- **Lines**: 38
- **Last modified**: 2025-04-16 00:11:15

```py
"""Base class for custom OpenTelemetry exporters."""

from abc import ABC, abstractmethod

from opentelemetry.sdk.trace.export import SpanExporter, SpanExportResult


class TelemetryExporter(SpanExporter, ABC):
    """Base class for custom OpenTelemetry exporters."""

    def __init__(self):
        """Base class for custom OpenTelemetry exporters."""
        super().__init__()

    def _export(self, spans):
        """Forward spans to the Jaeger exporter."""
        try:
            result = self.export(spans)
            if result is None:
                return SpanExportResult.SUCCESS
            return result
        except Exception:
            return SpanExportResult.FAILURE
        finally:
            self.shutdown()

    @abstractmethod
    def export(self, spans) -> SpanExportResult | None:
        """Export spans to the configured backend.

        To be implemented by subclasses.
        """
        raise NotImplementedError("Subclasses must implement the export method")

    @abstractmethod
    def shutdown(self):
        """Cleanup resources, if any. Optional for subclasses."""
        pass
```

### src\flock\core\logging\telemetry_exporter\file_exporter.py

- **Lines**: 85
- **Last modified**: 2025-04-16 00:11:15

```py
"""A simple exporter that writes span data as JSON lines into a file."""

import json

from opentelemetry.sdk.trace.export import SpanExportResult
from opentelemetry.trace import Status, StatusCode
from temporalio import workflow

from flock.core.logging.telemetry_exporter.base_exporter import (
    TelemetryExporter,
)

with workflow.unsafe.imports_passed_through():
    from pathlib import Path


class FileSpanExporter(TelemetryExporter):
    """A simple exporter that writes span data as JSON lines into a file."""

    def __init__(self, dir: str, file_path: str = "flock_events.jsonl"):
        """Initialize the exporter with a file path."""
        super().__init__()
        self.telemetry_path = Path(dir)
        self.telemetry_path.mkdir(parents=True, exist_ok=True)
        self.file_path = self.telemetry_path.joinpath(file_path).__str__()

    def _span_to_json(self, span):
        """Convert a ReadableSpan to a JSON-serializable dict."""
        context = span.get_span_context()
        status = span.status or Status(StatusCode.UNSET)

        return {
            "name": span.name,
            "context": {
                "trace_id": format(context.trace_id, "032x"),
                "span_id": format(context.span_id, "016x"),
                "trace_flags": context.trace_flags,
                "trace_state": str(context.trace_state),
            },
            "kind": span.kind.name if span.kind else None,
            "start_time": span.start_time,
            "end_time": span.end_time,
            "status": {
                "status_code": status.status_code.name,
                "description": status.description,
            },
            "attributes": dict(span.attributes or {}),
            "events": [
                {
                    "name": event.name,
                    "timestamp": event.timestamp,
                    "attributes": dict(event.attributes or {}),
                }
                for event in span.events
            ],
            "links": [
                {
                    "context": {
                        "trace_id": format(link.context.trace_id, "032x"),
                        "span_id": format(link.context.span_id, "016x"),
                    },
                    "attributes": dict(link.attributes or {}),
                }
                for link in span.links
            ],
            "resource": {
                attr_key: attr_value
                for attr_key, attr_value in span.resource.attributes.items()
            },
        }

    def export(self, spans):
        """Write spans to a log file."""
        try:
            with open(self.file_path, "a") as f:
                for span in spans:
                    json_span = self._span_to_json(span)
                    f.write(f"{json.dumps(json_span)}\n")
            return SpanExportResult.SUCCESS
        except Exception:
            return SpanExportResult.FAILURE

    def shutdown(self) -> None:
        # Nothing special needed on shutdown.
        pass
```

### src\flock\core\logging\telemetry_exporter\sqlite_exporter.py

- **Lines**: 103
- **Last modified**: 2025-04-16 00:11:15

```py
"""Exporter for storing OpenTelemetry spans in SQLite."""

import json
import sqlite3
from pathlib import Path
from typing import Any

from opentelemetry.sdk.trace.export import SpanExportResult

from flock.core.logging.telemetry_exporter.base_exporter import (
    TelemetryExporter,
)


class SqliteTelemetryExporter(TelemetryExporter):
    """Exporter for storing OpenTelemetry spans in SQLite."""

    def __init__(self, dir: str, db_path: str = "flock_events.db"):
        """Initialize the SQLite exporter.

        Args:
            db_path: Path to the SQLite database file
        """
        super().__init__()
        self.telemetry_path = Path(dir)
        self.telemetry_path.mkdir(parents=True, exist_ok=True)
        # Create an absolute path to the database file:
        self.db_path = self.telemetry_path.joinpath(db_path).resolve().__str__()
        # Use the absolute path when connecting:
        self.conn = sqlite3.connect(self.db_path, check_same_thread=False)
        self._initialize_database()

    def _initialize_database(self):
        """Set up the SQLite database schema."""
        cursor = self.conn.cursor()
        cursor.execute(
            """
            CREATE TABLE IF NOT EXISTS spans (
                id TEXT PRIMARY KEY,
                name TEXT,
                trace_id TEXT,
                span_id TEXT,
                start_time INTEGER,
                end_time INTEGER,
                attributes TEXT,
                status TEXT
            )
            """
        )
        self.conn.commit()

    def _convert_attributes(self, attributes: dict[str, Any]) -> str:
        """Convert span attributes to a JSON string.

        Args:
            attributes: Dictionary of span attributes

        Returns:
            JSON string representation of attributes
        """
        # Convert attributes to a serializable format
        serializable_attrs = {}
        for key, value in attributes.items():
            # Convert complex types to strings if needed
            if isinstance(value, dict | list | tuple):
                serializable_attrs[key] = json.dumps(value)
            else:
                serializable_attrs[key] = str(value)
        return json.dumps(serializable_attrs)

    def export(self, spans) -> SpanExportResult:
        """Export spans to SQLite."""
        try:
            cursor = self.conn.cursor()
            for span in spans:
                span_id = format(span.context.span_id, "016x")
                trace_id = format(span.context.trace_id, "032x")
                cursor.execute(
                    """
                    INSERT OR REPLACE INTO spans 
                    (id, name, trace_id, span_id, start_time, end_time, attributes, status)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                    """,
                    (
                        span_id,
                        span.name,
                        trace_id,
                        span_id,
                        span.start_time,
                        span.end_time,
                        self._convert_attributes(span.attributes),
                        str(span.status),
                    ),
                )
            self.conn.commit()
            return SpanExportResult.SUCCESS
        except Exception as e:
            print("Error exporting spans to SQLite:", e)
            return SpanExportResult.FAILURE

    def shutdown(self) -> None:
        """Cleanup resources."""
        pass
```

### src\flock\core\logging\trace_and_logged.py

- **Lines**: 59
- **Last modified**: 2025-04-16 00:11:15

```py
"""A decorator that wraps a function in an OpenTelemetry span and logs its inputs, outputs, and exceptions."""

import functools
import inspect

from opentelemetry import trace

from flock.core.logging.logging import get_logger

logger = get_logger("tools")
tracer = trace.get_tracer(__name__)


def traced_and_logged(func):
    """A decorator that wraps a function in an OpenTelemetry span.

    and logs its inputs,
    outputs, and exceptions. Supports both synchronous and asynchronous functions.
    """
    if inspect.iscoroutinefunction(func):

        @functools.wraps(func)
        async def async_wrapper(*args, **kwargs):
            with tracer.start_as_current_span(func.__name__) as span:
                span.set_attribute("args", str(args))
                span.set_attribute("kwargs", str(kwargs))
                try:
                    result = await func(*args, **kwargs)
                    span.set_attribute("result", str(result))
                    logger.debug(
                        f"{func.__name__} executed successfully", result=result
                    )
                    return result
                except Exception as e:
                    logger.error(f"Error in {func.__name__}", error=str(e))
                    span.record_exception(e)
                    raise

        return async_wrapper
    else:

        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            with tracer.start_as_current_span(func.__name__) as span:
                span.set_attribute("args", str(args))
                span.set_attribute("kwargs", str(kwargs))
                try:
                    result = func(*args, **kwargs)
                    span.set_attribute("result", str(result))
                    logger.debug(
                        f"{func.__name__} executed successfully", result=result
                    )
                    return result
                except Exception as e:
                    logger.error(f"Error in {func.__name__}", error=str(e))
                    span.record_exception(e)
                    raise

        return wrapper
```

### src\flock\core\mcp\__init__.py

- **Lines**: 1
- **Last modified**: 2025-05-22 21:27:37

```py
"""Flock MCP package."""
```

### src\flock\core\mcp\flock_mcp_server.py

- **Lines**: 614
- **Last modified**: 2025-05-26 13:19:27

```py
"""FlockMCPServer is the core, declarative base class for all types of MCP-Servers in the Flock framework."""

import asyncio
import importlib
import inspect
import os
from abc import ABC, abstractmethod
from typing import Any, Literal, TypeVar

from dspy import Tool as DSPyTool
from opentelemetry import trace
from pydantic import (
    BaseModel,
    ConfigDict,
    Field,
)

from flock.core.flock_module import FlockModule
from flock.core.logging.logging import get_logger
from flock.core.mcp.flock_mcp_tool_base import FlockMCPToolBase
from flock.core.mcp.mcp_client_manager import FlockMCPClientManagerBase
from flock.core.mcp.mcp_config import FlockMCPConfigurationBase
from flock.core.serialization.serializable import Serializable
from flock.core.serialization.serialization_utils import (
    deserialize_component,
    serialize_item,
)

logger = get_logger("core.mcp.server_base")
tracer = trace.get_tracer(__name__)
T = TypeVar("T", bound="FlockMCPServerBase")

LoggingLevel = Literal[
    "debug",
    "info",
    "notice",
    "warning",
    "error",
    "critical",
    "alert",
    "emergency",
]


class FlockMCPServerBase(BaseModel, Serializable, ABC):
    """Base class for all Flock MCP Server Types.

    Servers serve as an abstraction-layer between the underlying MCPClientSession
    which is the actual connection between Flock and a (remote) MCP-Server.

    Servers hook into the lifecycle of their assigned agents and take care
    of establishing sessions, getting and converting tools and other functions
    without agents having to worry about the details.

    Tools (if provided) will be injected into the list of tools of any attached
    agent automatically.

    Servers provide lifecycle-hooks (`initialize`, `get_tools`, `get_prompts`, `list_resources`, `get_resource_contents`, `set_roots`, etc)
    which allow modules to hook into them. This can be used to modify data or
    pass headers from authentication-flows to a server.

    Each Server should define its configuration requirements either by:
    1. Creating a subclass of FlockMCPServerConfig
    2. Using FlockMCPServerConfig.with_fields() to create a config class.
    """

    config: FlockMCPConfigurationBase = Field(
        ..., description="Config for clients connecting to the server."
    )

    initialized: bool = Field(
        default=False,
        exclude=True,
        description="Whether or not this Server has already initialized.",
    )

    modules: dict[str, FlockModule] = Field(
        default={},
        description="Dictionary of FlockModules attached to this Server.",
    )

    # --- Underlying ConnectionManager ---
    # (Manages a pool of ClientConnections and does the actual talking to the MCP Server)
    # (Excluded from Serialization)
    client_manager: FlockMCPClientManagerBase | None = Field(
        default=None,
        exclude=True,
        description="Underlying Connection Manager. Handles the actual underlying connections to the server.",
    )

    condition: asyncio.Condition = Field(
        default_factory=asyncio.Condition,
        exclude=True,
        description="Condition for asynchronous operations.",
    )

    model_config = ConfigDict(
        arbitrary_types_allowed=True,
    )

    def add_module(self, module: FlockModule) -> None:
        """Add a module to this server."""
        if not module.name:
            logger.error("Module must have a name to be added.")
            return
        if self.modules and module.name in self.modules:
            logger.warning(f"Overwriting existing module: {module.name}")

        self.modules[module.name] = module
        logger.debug(
            f"Added module '{module.name}' to server {self.config.name}"
        )
        return

    def remove_module(self, module_name: str) -> None:
        """Remove a module from this server."""
        if module_name in self.modules:
            del self.modules[module_name]
            logger.debug(
                f"Removed module '{module_name}' from server '{self.config.name}'"
            )
        else:
            logger.warning(
                f"Module '{module_name}' not found on server '{self.config.name}'"
            )
        return

    def get_module(self, module_name: str) -> FlockModule | None:
        """Get a module by name."""
        return self.modules.get(module_name)

    def get_enabled_modules(self) -> list[FlockModule]:
        """Get a list of currently enabled modules attached to this server."""
        return [m for m in self.modules.values() if m.config.enabled]

    @abstractmethod
    async def initialize(self) -> FlockMCPClientManagerBase:
        """Called when initializing the server."""
        pass

    async def call_tool(
        self, agent_id: str, run_id: str, name: str, arguments: dict[str, Any]
    ) -> Any:
        """Call a tool via the MCP Protocol on the client's server."""
        with tracer.start_as_current_span("server.call_tool") as span:
            span.set_attribute("agent_id", agent_id)
            span.set_attribute("run_id", run_id)
            span.set_attribute("tool.name", name)
            span.set_attribute("arguments", str(arguments))
            if not self.initialized or not self.client_manager:
                async with self.condition:
                    await self.pre_init()
                    self.client_manager = await self.initialize()
                    self.initialized = True
                    await self.post_init()
            async with self.condition:
                try:
                    additional_params: dict[str, Any] = {
                        "refresh_client": False,
                        "override_headers": False,
                    }  # initialize the additional params as an empty dict.

                    await self.before_connect(
                        additional_params=additional_params
                    )
                    pre_call_args = {
                        "agent_id": agent_id,
                        "run_id": run_id,
                        "tool_name": name,
                        "arguments": arguments,
                    }
                    pre_call_args.update(additional_params)
                    await self.pre_mcp_call(pre_call_args)
                    result = await self.client_manager.call_tool(
                        agent_id=agent_id,
                        run_id=run_id,
                        name=name,
                        arguments=arguments,
                        additional_params=additional_params,
                    )
                    # re-set addtional-params, just to be sure.
                    await self.post_mcp_call(result=result)
                    return result
                except Exception as mcp_error:
                    logger.error(
                        "Error during server.call_tool",
                        server=self.config.name,
                        error=str(mcp_error),
                    )
                    span.record_exception(mcp_error)
                    return None

    async def get_tools(self, agent_id: str, run_id: str) -> list[DSPyTool]:
        """Retrieves a list of available tools from this server."""
        with tracer.start_as_current_span("server.get_tools") as span:
            span.set_attribute("server.name", self.config.name)
            span.set_attribute("agent_id", agent_id)
            span.set_attribute("run_id", run_id)
            if not self.initialized or not self.client_manager:
                async with self.condition:
                    await self.pre_init()
                    self.client_manager = await self.initialize()
                    self.initialized = True
                    await self.post_init()

            async with self.condition:
                try:
                    await self.pre_mcp_call()
                    # TODO: inject additional params here.
                    additional_params: dict[str, Any] = {}
                    additional_params = await self.before_connect(
                        additional_params=additional_params
                    )
                    result: list[
                        FlockMCPToolBase
                    ] = await self.client_manager.get_tools(
                        agent_id=agent_id,
                        run_id=run_id,
                        additional_params=additional_params,
                    )
                    converted_tools = [
                        t.as_dspy_tool(server=self) for t in result
                    ]
                    await self.post_mcp_call(result=converted_tools)
                    return converted_tools
                except Exception as e:
                    logger.error(
                        f"Unexpected Exception ocurred while trying to get tools from server '{self.config.name}': {e}"
                    )
                    await self.on_error(error=e)
                    span.record_exception(e)
                    return []
                finally:
                    self.condition.notify()

    async def before_connect(
        self, additional_params: dict[str, Any]
    ) -> dict[str, Any]:
        """Run before_connect hooks on modules."""
        logger.debug(
            f"Running before_connect hooks for modules in server '{self.config.name}'."
        )
        with tracer.start_as_current_span("server.before_connect") as span:
            span.set_attribute("server.name", self.config.name)
            try:
                if not additional_params:
                    additional_params = {}
                for module in self.get_enabled_modules():
                    additional_params = await module.on_connect(
                        server=self, additional_params=additional_params
                    )
            except Exception as module_error:
                logger.error(
                    "Error during before_connect",
                    server=self.config.name,
                    error=str(module_error),
                )
                span.record_exception(module_error)

    async def pre_init(self) -> None:
        """Run pre-init hooks on modules."""
        logger.debug(
            f"Running pre-init hooks for modules in server '{self.config.name}'"
        )
        with tracer.start_as_current_span("server.pre_init") as span:
            span.set_attribute("server.name", self.config.name)
            try:
                for module in self.get_enabled_modules():
                    await module.on_pre_server_init(self)
            except Exception as module_error:
                logger.error(
                    "Error during pre_init",
                    server=self.config.name,
                    error=str(module_error),
                )
                span.record_exception(module_error)

    async def post_init(self) -> None:
        """Run post-init hooks on modules."""
        logger.debug(
            f"Running post_init hooks for modules in server '{self.config.name}'"
        )
        with tracer.start_as_current_span("server.post_init") as span:
            span.set_attribute("server.name", self.config.name)
            try:
                for module in self.get_enabled_modules():
                    await module.on_post_server_init(self)
            except Exception as module_error:
                logger.error(
                    "Error during post_init",
                    server=self.config.name,
                    error=str(module_error),
                )
                span.record_exception(module_error)

    async def pre_terminate(self) -> None:
        """Run pre-terminate hooks on modules."""
        logger.debug(
            f"Running post_init hooks for modules in server: '{self.config.name}'"
        )
        with tracer.start_as_current_span("server.pre_terminate") as span:
            span.set_attribute("server.name", self.config.name)
            try:
                for module in self.get_enabled_modules():
                    await module.on_pre_server_terminate(self)
            except Exception as module_error:
                logger.error(
                    "Error during pre_terminate",
                    server=self.config.name,
                    error=str(module_error),
                )
                span.record_exception(module_error)

    async def post_terminate(self) -> None:
        """Run post-terminate hooks on modules."""
        logger.debug(
            f"Running post_terminat hooks for modules in server: '{self.config.name}'"
        )
        with tracer.start_as_current_span("server.post_terminate") as span:
            span.set_attribute("server.name", self.config.name)
            try:
                for module in self.get_enabled_modules():
                    await module.on_post_server_terminate(server=self)
            except Exception as module_error:
                logger.error(
                    "Error during post_terminate",
                    server=self.config.name,
                    error=str(module_error),
                )
                span.record_exception(module_error)

    async def on_error(self, error: Exception) -> None:
        """Run on_error hooks on modules."""
        logger.debug(
            f"Running on_error hooks for modules in server '{self.config.name}'"
        )
        with tracer.start_as_current_span("server.on_error") as span:
            span.set_attribute("server.name", self.config.name)
            try:
                for module in self.get_enabled_modules():
                    await module.on_server_error(server=self, error=error)
            except Exception as module_error:
                logger.error(
                    "Error during on_error",
                    server=self.config.name,
                    error=str(module_error),
                )
                span.record_exception(module_error)

    async def pre_mcp_call(self, arguments: Any | None = None) -> None:
        """Run pre_mcp_call-hooks on modules."""
        logger.debug(
            f"Running pre_mcp_call hooks for modules in server '{self.config.name}'"
        )
        with tracer.start_as_current_span("server.pre_mcp_call") as span:
            span.set_attribute("server.name", self.config.name)
            try:
                for module in self.get_enabled_modules():
                    await module.on_pre_mcp_call(
                        server=self, arguments=arguments
                    )
            except Exception as module_error:
                logger.error(
                    f"Error during pre_mcp_call: {module_error}",
                    server=self.config.name,
                    error=str(module_error),
                )
                span.record_exception(module_error)

    async def post_mcp_call(self, result: Any) -> None:
        """Run Post MCP_call hooks on modules."""
        logger.debug(
            f"Running post_mcp_call hooks for modules in server '{self.config.name}'"
        )
        with tracer.start_as_current_span("server.post_mcp_call") as span:
            span.set_attribute("server.name", self.config.name)
            try:
                for module in self.get_enabled_modules():
                    await module.on_post_mcp_call(server=self, result=result)
            except Exception as module_error:
                logger.error(
                    "Error during post_mcp_call",
                    server=self.config.name,
                    error=str(module_error),
                )
                span.record_exception(module_error)

    # --- Async Methods ---
    async def __aenter__(self) -> "FlockMCPServerBase":
        """Enter the asynchronous context for the server."""
        # Spin up the client-manager
        with tracer.start_as_current_span("server.__aenter__") as span:
            span.set_attribute("server.name", self.config.name)
            logger.info(f"server.__aenter__", server=self.config.name)
            try:
                await self.pre_init()
                self.client_manager = await self.initialize()
                await self.post_init()
                self.initialized = True
            except Exception as server_error:
                logger.error(
                    f"Error during __aenter__ for server '{self.config.name}'",
                    server=self.config.name,
                    error=server_error,
                )
                span.record_exception(server_error)

    async def __aexit__(self, exc_type, exc, tb) -> None:
        """Exit the asynchronous context for the server."""
        # tell the underlying client-manager to terminate connections
        # and unwind the clients.
        with tracer.start_as_current_span("server.__aexit__") as span:
            span.set_attribute("server.name", self.config.name)
            try:
                await self.pre_terminate()
                if self.initialized and self.client_manager:
                    # means we ran through the initialize()-method
                    # and the client manager is present
                    await self.client_manager.close_all()
                    self.client_manager = None
                    self.initialized = False
                await self.post_terminate()
                return
            except Exception as server_error:
                logger.error(
                    f"Error during __aexit__ for server '{self.config.name}'",
                    server=self.config.name,
                    error=server_error,
                )
                await self.on_error(error=server_error)
                span.record_exception(server_error)

    # --- Serialization Implementation ---
    def to_dict(self, path_type: str = "relative") -> dict[str, Any]:
        """Convert instance to dictionary representation suitable for serialization."""
        from flock.core.flock_registry import get_registry

        FlockRegistry = get_registry()

        exclude = ["modules"]

        logger.debug(f"Serializing server '{self.config.name}' to dict.")
        # Use Pydantic's dump, exclued manually handled fields.
        data = self.model_dump(
            exclude=exclude,
            mode="json",  # Use json mode for better handling of standard types by Pydantic
            exclude_none=True,  # Exclude None values for cleaner output
        )

        builtin_by_transport = {}

        try:
            from flock.mcp.servers.sse.flock_sse_server import FlockSSEServer
            from flock.mcp.servers.stdio.flock_stdio_server import (
                FlockMCPStdioServer,
            )
            from flock.mcp.servers.websockets.flock_websocket_server import (
                FlockWSServer,
            )

            builtin_by_transport = {
                "stdio": FlockMCPStdioServer,
                "sse": FlockSSEServer,
                "websockets": FlockWSServer,
            }
        except ImportError:
            builtin_by_transport = {}

        # --- Only emit full impl for non-builtins ---
        transport = getattr(
            self.config.connection_config, "transport_type", None
        )
        builtin_cls = builtin_by_transport.get(transport)

        if type(self) is not builtin_cls:
            file_path = inspect.getsourcefile(type(self))
            if path_type == "relative":
                file_path = os.path.relpath(file_path)
            data["implementation"] = {
                "class_name": type(self).__name__,
                "module_path": type(self).__module__,
                "file_path": file_path,
            }

        logger.debug(
            f"Base server data for '{self.config.name}': {list(data.keys())}"
        )
        serialized_modules = {}

        def add_serialized_component(component: Any, field_name: str):
            if component:
                comp_type = type(component)
                type_name = FlockRegistry.get_component_type_name(
                    comp_type
                )  # Get registered name

                if type_name:
                    try:
                        serialized_component_data = serialize_item(component)

                        if not isinstance(serialized_component_data, dict):
                            logger.error(
                                f"Serialization of component {type_name} for field '{field_name}' did not result in a dictionary. Got: {type(serialized_component_data)}"
                            )
                            serialized_modules[field_name] = {
                                "type": type_name,
                                "name": getattr(component, "name", "unknown"),
                                "error": "serialization_failed_non_dict",
                            }
                        else:
                            serialized_component_data["type"] = type_name
                            serialized_modules[field_name] = (
                                serialized_component_data
                            )
                            logger.debug(
                                f"Successfully serialized component for field '{field_name}' (type: {type_name})"
                            )
                    except Exception as e:
                        logger.error(
                            f"Failed to serialize component {type_name} for field '{field_name}': {e}",
                            exc_info=True,
                        )

                else:
                    logger.warning(
                        f"Cannot serialize unregistered component {comp_type.__name__} for field '{field_name}'"
                    )

        serialized_modules = {}
        for module in self.modules.values():
            add_serialized_component(module, module.name)

        if serialized_modules:
            data["modules"] = serialized_modules
            logger.debug(
                f"Added {len(serialized_modules)} modules to server '{self.config.name}'"
            )

        def _clean(obj: Any) -> Any:
            if isinstance(obj, dict):
                return {
                    k: _clean(v)
                    for k, v in obj.items()
                    if v is not None
                    and not (isinstance(v, list | dict) and len(v) == 0)
                }
            if isinstance(obj, list):
                return [
                    _clean(v)
                    for v in obj
                    if v is not None
                    and not (isinstance(v, dict | list) and len(v) == 0)
                ]
            return obj

        data = _clean(data)
        return data

    @classmethod
    def from_dict(cls: type[T], data: dict[str, Any]) -> T:
        """Deserialize the server from a dictionary, including components."""
        logger.debug(
            f"Deserializing server from dict. Keys: {list(data.keys())}"
        )

        builtin_by_transport = {}

        try:
            from flock.mcp.servers.sse.flock_sse_server import FlockSSEServer
            from flock.mcp.servers.stdio.flock_stdio_server import (
                FlockMCPStdioServer,
            )
            from flock.mcp.servers.websockets.flock_websocket_server import (
                FlockWSServer,
            )

            builtin_by_transport = {
                "stdio": FlockMCPStdioServer,
                "sse": FlockSSEServer,
                "websockets": FlockWSServer,
            }
        except ImportError:
            builtin_by_transport = {}

        # find custom impl or built-in
        impl = data.pop("implementation", None)
        if impl:
            mod = importlib.import_module(impl["module_path"])
            real_cls = getattr(mod, impl["class_name"])
        else:
            # built-in: inspect transport_type in data["config"]
            transport = data["config"]["connection_config"]["transport_type"]
            real_cls = builtin_by_transport.get(transport, cls)

        # now construct
        server = real_cls(**{k: v for k, v in data.items() if k != "modules"})

        # re-hydrate modules
        for mname, mdata in data.get("modules", {}).items():
            server.add_module(deserialize_component(mdata, FlockModule))

        # --- Separate Data ---
        component_configs = {}
        server_data = {}
        component_keys = ["modules"]

        for key, value in data.items():
            if key in component_keys and value is not None:
                component_configs[key] = value
            else:
                server_data[key] = value

        logger.info(f"Successfully deserialized server '{server.config.name}'")
        return server
```

### src\flock\core\mcp\flock_mcp_tool_base.py

- **Lines**: 201
- **Last modified**: 2025-05-22 21:27:37

```py
"""Represents a MCP Tool in a format which is compatible with Flock's ecosystem."""

from typing import Any, TypeVar

from dspy import Tool as DSPyTool
from mcp import Tool
from mcp.types import CallToolResult, TextContent, ToolAnnotations
from opentelemetry import trace
from pydantic import BaseModel, Field

from flock.core.logging.logging import get_logger

logger = get_logger("core.mcp.tool_base")
tracer = trace.get_tracer(__name__)

T = TypeVar("T", bound="FlockMCPToolBase")

TYPE_MAPPING = {
    "string": str,
    "integer": int,
    "number": float,
    "boolean": bool,
    "array": list,
    "object": dict,
}


class FlockMCPToolBase(BaseModel):
    """Base Class for MCP Tools for Flock."""

    name: str = Field(..., description="Name of the tool")

    agent_id: str = Field(
        ..., description="Associated agent_id. Used for internal tracking."
    )

    run_id: str = Field(
        ..., description="Associated run_id. Used for internal tracking."
    )

    description: str | None = Field(
        ..., description="A human-readable description of the tool"
    )

    input_schema: dict[str, Any] = Field(
        ...,
        description="A JSON Schema object defining the expected parameters for the tool.",
    )

    annotations: ToolAnnotations | None = Field(
        ..., description="Optional additional tool information."
    )

    @classmethod
    def from_mcp_tool(
        cls: type[T], tool: Tool, agent_id: str, run_id: str
    ) -> T:
        """Convert MCP Tool to Flock Tool."""
        return cls(
            name=tool.name,
            agent_id=agent_id,
            run_id=run_id,
            description=tool.description,
            input_schema=tool.inputSchema,
            annotations=tool.annotations,
        )

    @classmethod
    def to_mcp_tool(cls: type[T], instance: T) -> Tool | None:
        """Convert a flock mcp tool into a mcp tool."""
        return Tool(
            name=instance.name,
            description=instance.description,
            inputSchema=instance.input_schema,
            annotations=instance.annotations,
        )

    def resolve_json_schema_reference(self, schema: dict) -> dict:
        """Recursively resolve json model schema, expanding all references."""
        if "$defs" not in schema and "definitions" not in schema:
            return schema

        def resolve_refs(obj: Any) -> Any:
            if not isinstance(obj, dict[list, list]):
                return obj
            if isinstance(obj, dict) and "$ref" in obj:
                # ref_path = obj["$ref"].split("/")[-1]
                return {resolve_refs(v) for k, v in obj.items()}

            return [resolve_refs(item) for item in obj]

        resolved_schema = resolve_refs(schema)

        resolved_schema.pop("$defs", None)
        return resolved_schema

    def _convert_input_schema_to_tool_args(
        self, input_schema: dict[str, Any]
    ) -> tuple[dict[str, Any], dict[str, type], dict[str, str]]:
        """Convert an input schema to tool arguments compatible with Dspy Tool.

        Args:
            schema: an input schema describing the tool's input parameters

        Returns:
            A tuple of (args, arg_types, arg_desc) for Dspy Tool definition
        """
        args, arg_types, arg_desc = {}, {}, {}
        properties = input_schema.get("properties")
        if properties is None:
            return args, arg_types, arg_desc

        required = input_schema.get("required", [])

        defs = input_schema.get("$defs", {})

        for name, prop in properties.items():
            if len(defs) > 0:
                prop = self.resolve_json_schema_reference(
                    {"$defs": defs, **prop}
                )

            args[name] = prop

            arg_types[name] = TYPE_MAPPING.get(prop.get("type"), Any)
            arg_desc[name] = prop.get("description", "No description provided")
            if name in required:
                arg_desc[name] += " (Required)"

        return args, arg_types, arg_desc

    def _convert_mcp_tool_result(
        self, call_tool_result: CallToolResult
    ) -> str | list[Any]:
        text_contents: list[TextContent] = []
        non_text_contents = []

        for content in call_tool_result.content:
            if isinstance(content, TextContent):
                text_contents.append(content)
            else:
                non_text_contents.append(content)

        tool_content = [content.text for content in text_contents]
        if len(text_contents) == 1:
            tool_content = tool_content[0]

        if call_tool_result.isError:
            logger.error(f"MCP Tool '{self.name}' returned an error.")

        return tool_content or non_text_contents

    def on_error(self, res: CallToolResult, **kwargs) -> None:
        """Optional on error hook."""
        # leave it for now, might be useful for more sophisticated processing.
        logger.error(f"Tool: '{self.name}' on_error: Tool returned error.")
        return res

    def as_dspy_tool(self, server: Any) -> DSPyTool:
        """Wrap this tool as a DSPyTool for downstream."""
        args, arg_type, args_desc = self._convert_input_schema_to_tool_args(
            self.input_schema
        )

        async def func(*args, **kwargs):
            with tracer.start_as_current_span(f"tool.{self.name}.call") as span:
                span.set_attribute("tool.name", self.name)
                try:
                    logger.debug(f"Tool: {self.name}: getting client.")

                    server_name = server.config.name
                    logger.debug(
                        f"Tool: {self.name}: got client for server '{server_name}' for agent {self.agent_id} on run {self.run_id}"
                    )
                    logger.debug(
                        f"Tool: {self.name}: calling server '{server_name}'"
                    )
                    result = await server.call_tool(
                        agent_id=self.agent_id,
                        run_id=self.run_id,
                        name=self.name,
                        arguments=kwargs,
                    )
                    logger.debug(
                        f"Tool: Called Tool: {self.name} on server '{server_name}'. Returning result to LLM."
                    )
                    return self._convert_mcp_tool_result(result)
                except Exception as e:
                    logger.error(
                        f"Tool: Exception ocurred when calling tool '{self.name}': {e}"
                    )
                    span.record_exception(e)

        return DSPyTool(
            func=func,
            name=self.name,
            desc=self.description,
            args=args,
            arg_types=arg_type,
            arg_desc=args_desc,
        )
```

### src\flock\core\mcp\types\__init__.py

- **Lines**: 1
- **Last modified**: 2025-05-22 21:27:37

```py
"""MCP Types package."""
```

### src\flock\core\mcp\types\callbacks.py

- **Lines**: 86
- **Last modified**: 2025-05-26 13:19:27

```py
"""MCP Callbacks."""

from mcp.shared.context import RequestContext
from mcp.shared.session import RequestResponder
from mcp.types import (
    INVALID_REQUEST,
    ClientResult,
    CreateMessageRequestParams,
    ErrorData,
    ListRootsResult,
    ServerRequest,
)

from flock.core.logging.logging import FlockLogger
from flock.core.mcp.mcp_client import Any
from flock.core.mcp.types.handlers import (
    handle_incoming_exception,
    handle_incoming_request,
    handle_incoming_server_notification,
    handle_logging_message,
)
from flock.core.mcp.types.types import (
    FlockLoggingMessageNotificationParams,
    ServerNotification,
)


async def default_sampling_callback(
    ctx: RequestContext, params: CreateMessageRequestParams, logger: FlockLogger
) -> ErrorData:
    """Default Callback for Sampling."""
    logger.info(f"Rejecting Sampling Request.")
    return ErrorData(code=INVALID_REQUEST, message="Sampling not supported.")


async def default_message_handler(
    req: RequestResponder[ServerRequest, ClientResult]
    | ServerNotification
    | Exception,
    logger: FlockLogger,
    associated_client: Any,
) -> None:
    """Default Message Handler."""
    if isinstance(req, Exception):
        await handle_incoming_exception(
            e=req,
            logger_to_use=logger,
            associated_client=associated_client,
        )
    elif isinstance(req, ServerNotification):
        await handle_incoming_server_notification(
            n=req,
            logger=logger,
            client=associated_client,
        )
    elif isinstance(req, RequestResponder[ServerRequest, ClientResult]):
        await handle_incoming_request(
            req=req,
            logger_to_use=logger,
            associated_client=associated_client,
        )


async def default_list_roots_callback(
    associated_client: Any,
    logger: FlockLogger,
) -> ListRootsResult | ErrorData:
    """Default List Roots Callback."""
    if associated_client.config.feature_config.roots_enabled:
        current_roots = await associated_client.get_current_roots()
        return ListRootsResult(roots=current_roots)
    else:
        return ErrorData(
            code=INVALID_REQUEST, message="List roots not supported."
        )


async def default_logging_callback(
    params: FlockLoggingMessageNotificationParams,
    logger: FlockLogger,
    server_name: str,
) -> None:
    """Default Logging Handling Callback."""
    await handle_logging_message(
        params=params, logger=logger, server_name=server_name
    )
```

### src\flock\core\mcp\types\factories.py

- **Lines**: 111
- **Last modified**: 2025-05-26 13:19:27

```py
"""Factories for default MCP Callbacks."""

from typing import TYPE_CHECKING, Any

from mcp.shared.context import RequestContext
from mcp.types import (
    CreateMessageRequestParams,
)

from flock.core.logging.logging import FlockLogger, get_logger
from flock.core.mcp.types.types import (
    FlockListRootsMCPCallback,
    FlockLoggingMCPCallback,
    FlockLoggingMessageNotificationParams,
    FlockMessageHandlerMCPCallback,
    FlockSamplingMCPCallback,
    ServerNotification,
)

if TYPE_CHECKING:
    from flock.core.mcp.types.callbacks import (
        default_list_roots_callback,
        default_logging_callback,
        default_message_handler,
        default_sampling_callback,
    )

default_logging_callback_logger = get_logger("core.mcp.callback.logging")
default_sampling_callback_logger = get_logger("core.mcp.callback.sampling")
default_list_roots_callback_logger = get_logger("core.mcp.callback.sampling")
default_message_handler_logger = get_logger("core.mcp.callback.message")


def default_flock_mcp_logging_callback_factory(
    associated_client: Any,
    logger: FlockLogger | None = None,
) -> FlockLoggingMCPCallback:
    """Creates a fallback for handling incoming logging requests."""
    logger_to_use = logger if logger else default_logging_callback_logger

    async def _method(
        params: FlockLoggingMessageNotificationParams,
    ) -> None:
        return await default_logging_callback(
            params=params,
            logger=logger_to_use,
            server_name=associated_client.config.name,
        )


def default_flock_mcp_sampling_callback_factory(
    associated_client: Any,
    logger: FlockLogger | None = None,
) -> FlockSamplingMCPCallback:
    """Creates a fallback for handling incoming sampling requests."""
    logger_to_use = logger if logger else default_sampling_callback_logger

    async def _method(
        ctx: RequestContext,
        params: CreateMessageRequestParams,
    ):
        logger_to_use.info(
            f"SAMPLING_REQUEST: server '{associated_client.config.name}' sent a sampling request: {params}"
        )
        await default_sampling_callback(
            ctx=ctx, params=params, logger=logger_to_use
        )

    return _method


def default_flock_mcp_message_handler_callback_factory(
    associated_client: Any,
    logger: FlockLogger | None = None,
) -> FlockMessageHandlerMCPCallback:
    """Creates a fallback for handling incoming messages.

    Note:
        Incoming Messages differ from incoming requests.
        Requests can do things like list roots, create messages through sampling etc.
        While Incoming Messages mainly consist of miscellanious information
        sent by the server.
    """
    logger_to_use = logger if logger else default_message_handler_logger

    async def _method(
        n: ServerNotification,
    ) -> None:
        await default_message_handler(
            req=n,
            logger_to_use=logger_to_use,
            associated_client=associated_client,
        )

    return _method


def default_flock_mcp_list_roots_callback_factory(
    associated_client: Any,
    logger: FlockLogger | None = None,
) -> FlockListRootsMCPCallback:
    """Creates a fallback for a list roots callback for a client."""
    logger_to_use = logger or default_list_roots_callback_logger

    async def _method(*args, **kwargs):
        return await default_list_roots_callback(
            associated_client=associated_client,
            logger=logger_to_use,
        )

    return _method
```

### src\flock\core\mcp\types\handlers.py

- **Lines**: 240
- **Last modified**: 2025-05-26 13:19:27

```py
"""Handler functions."""

from collections.abc import Callable

from mcp import CreateMessageRequest
from mcp.client.session import ClientResponse
from mcp.shared.context import RequestContext
from mcp.shared.session import RequestResponder
from mcp.types import (
    INTERNAL_ERROR,
    ClientResult,
    ErrorData,
    ListRootsRequest,
    ServerNotification as _MCPServerNotification,
    ServerRequest,
)

from flock.core.logging.logging import FlockLogger
from flock.core.mcp.mcp_client import Any
from flock.core.mcp.types.types import (
    CancelledNotification,
    FlockLoggingMessageNotificationParams,
    LoggingMessageNotification,
    ProgressNotification,
    ResourceListChangedNotification,
    ResourceUpdatedNotification,
    ServerNotification,
    ToolListChangedNotification,
)


async def handle_incoming_exception(
    e: Exception,
    logger_to_use: FlockLogger,
    associated_client: Any,
) -> None:
    """Process an incoming exception Message."""
    server_name = await associated_client.config.name

    # For now, simply log it
    logger_to_use.error(
        f"Encountered Exception while communicating with server '{server_name}': {e}"
    )


async def handle_progress_notification(
    n: ProgressNotification,
    logger_to_use: FlockLogger,
    server_name: str,
) -> None:
    """Process an incoming progress Notification."""
    params = n.params
    progress = params.progress
    total = params.total or "Unknown"
    progress_token = params.progressToken
    metadata = params.meta or {}

    message = f"PROGRESS_NOTIFICATION: Server '{server_name}' reports Progress: {progress}/{total}. (Token: {progress_token}) (Meta Data: {metadata})"

    logger_to_use.info(message)


async def handle_cancellation_notification(
    n: CancelledNotification,
    logger_to_use: FlockLogger,
    server_name: str,
) -> None:
    """Process an incoming Cancellation Notification."""
    params = n.params
    request_id_to_cancel = params.requestId
    reason = params.reason or "no reason given"
    metadata = params.meta or {}

    message = f"CANCELLATION_REQUEST: Server '{server_name}' requests to cancel request with id: {request_id_to_cancel}. Reason: {reason}. (Metadata: {metadata})"

    logger_to_use.warning(message)


async def handle_resource_update_notification(
    n: ResourceUpdatedNotification,
    logger_to_use: FlockLogger,
    associated_client: Any,
) -> None:
    """Handle an incoming ResourceUpdatedNotification."""
    # This also means that the associated client needs to invalidate
    # its resource_contents_cache

    params = n.params
    metadata = params.meta or {}
    uri = params.uri

    message = f"RESOURCE_UPDATE: Server '{associated_client.config.name}' reports change on resoure at: {uri}. (Meta Data: {metadata})"

    logger_to_use.info(message)

    await associated_client.invalidate_resource_contents_cache_entry(key=uri)


async def handle_resource_list_changed_notification(
    n: ResourceListChangedNotification,
    logger_to_use: FlockLogger,
    associated_client: Any,
) -> None:
    """Handle an incoming ResourecListChangedNotification."""
    # This also means that the associated client needs to invalidate
    # its resource_contents_cache

    params = n.params or {}
    metadata = params.meta or {}

    message = f"TOOLS_LIST_CHANGED: Server '{associated_client.config.name}' reports a change in their tools list: {metadata}. Resetting Tools Cache for associated clients."

    logger_to_use.info(message)
    await associated_client.invalidate_resource_list_cache()


async def handle_tool_list_changed_notification(
    n: ToolListChangedNotification,
    logger_to_use: FlockLogger,
    associated_client: Any,
) -> None:
    """Handle an incoming ToolListChangedNotification."""
    params = n.params or {}
    metadata = params.meta or {}

    message = f"TOOLS_LIST_CHANGED: Server '{associated_client.config.name}' reports a change in their tools list: {metadata}. Resetting Tools Cache for associated clients."

    logger_to_use.info(message)
    await associated_client.invalidate_tool_cache()


_SERVER_NOTIFICATION_MAP: dict[type[_MCPServerNotification], Callable] = {
    ResourceListChangedNotification: handle_resource_list_changed_notification,
    ResourceUpdatedNotification: handle_resource_update_notification,
    LoggingMessageNotification: lambda n, log, client: handle_logging_message(
        params=n,
        logger=log,
        server_name=client.config.name,
    ),
    ProgressNotification: handle_progress_notification,
    CancelledNotification: handle_cancellation_notification,
}


async def handle_incoming_server_notification(
    n: ServerNotification,
    logger: FlockLogger,
    client: Any,
) -> None:
    """Process an incoming server notification."""
    handler = _SERVER_NOTIFICATION_MAP.get(type(n.root))
    if handler:
        await handler(n.root, logger, client)


async def handle_logging_message(
    params: FlockLoggingMessageNotificationParams,
    logger: FlockLogger,
    server_name: str,
) -> None:
    """Handle a logging request."""
    level = params.level
    method = logger.debug
    logger_name = params.logger if params.logger else "unknown_remote_logger"
    metadata = params.meta or {}

    str_level = "DEBUG"
    prefix = f"Message from Remote MCP Logger '{logger_name}' for server '{server_name}': "

    match level:
        case "info":
            method = logger.info
            str_level = "INFO: "
        case "notice":
            method = logger.info
            str_level = "NOTICE: "
        case "alert":
            method = logger.warning
            str_level = "WARNING: "
        case "critical":
            method = logger.error
            str_level = "CRITICAL: "
        case "error":
            method = logger.error
            str_level = "ERROR: "
        case "emergency":
            method = logger.error
            str_level = "EMERGENCY: "

    full_msg = f"{prefix}{str_level}{params.data} (Meta Data: {metadata})"
    method(full_msg)


async def handle_incoming_request(
    req: RequestResponder[ServerRequest, ClientResult],
    logger_to_use: FlockLogger,
    associated_client: Any,
) -> None:
    """Handle generic request."""
    ctx = RequestContext(
        request_id=req.request_id,
        meta=req.request_meta,
        session=req._session,
        lifespan_context=None,
    )

    try:
        match req.request.root:
            case CreateMessageRequest(params=req.request.root.params):
                with req:
                    # invoke user's sampling callback
                    # type: ignore
                    response = await associated_client.sampling_callback(
                        ctx, req.request.root.params
                    )
                    client_resp = ClientResponse.validate_python(response)
                    await req.respond(client_resp)
            case ListRootsRequest():
                with req:
                    # type: ignore
                    response = await associated_client.list_roots_callback(ctx)
                    client_resp = ClientResponse.validate_python(response)
                    await req.respond(client_resp)
            case _:
                # unrecognized -> no-op
                return
    except Exception as e:
        # 1) Log the error and stacktrace
        logger_to_use.error(
            f"Error in fallback handle_incoming_request (id={req.request_id}): {e}"
        )
        # 2) If the request wasn't already completed, send a JSON-RPC error back
        if not getattr(req, "_completed", False):
            with req:
                err = ErrorData(
                    code=INTERNAL_ERROR, message=f"Client-side error: {e}"
                )
                client_err = ClientResponse.validate_python(err)
                await req.respond(client_err)
    return
```

### src\flock\core\mcp\types\types.py

- **Lines**: 157
- **Last modified**: 2025-05-26 13:19:27

```py
"""Types for Flock's MCP functionality."""

from collections.abc import Awaitable, Callable
from contextlib import AbstractAsyncContextManager
from typing import Any

from anyio.streams.memory import (
    MemoryObjectReceiveStream,
    MemoryObjectSendStream,
)
from mcp import (
    ClientSession,
    CreateMessageResult,
    StdioServerParameters as _MCPStdioServerParameters,
)
from mcp.shared.context import RequestContext
from mcp.shared.session import RequestResponder
from mcp.types import (
    CancelledNotification as _MCPCancelledNotification,
    ClientResult,
    CreateMessageRequestParams,
    ErrorData,
    JSONRPCMessage,
    ListRootsResult,
    LoggingMessageNotification as _MCPLoggingMessageNotification,
    LoggingMessageNotificationParams as _MCPLoggingMessageNotificationParams,
    ProgressNotification as _MCPProgressNotification,
    PromptListChangedNotification as _MCPPromptListChangedNotification,
    ResourceListChangedNotification as _MCPResourceListChangedNotification,
    ResourceUpdatedNotification as _MCPResourceUpdateNotification,
    Root as _MCPRoot,
    ServerNotification as _MCPServerNotification,
    ServerRequest,
    ToolListChangedNotification as _MCPToolListChangedNotification,
)
from pydantic import AnyUrl, BaseModel, ConfigDict, Field

from flock.core.mcp.util.helpers import get_default_env


class ServerNotification(_MCPServerNotification):
    """A notification message sent by the server side."""


class CancelledNotification(_MCPCancelledNotification):
    """Notification, which can be sent bei either side to indicate that it is cancelling a previously issued request."""


class ProgressNotification(_MCPProgressNotification):
    """An out-of band notification used to inform the receiver of a progress update for a long-running request."""


class LoggingMessageNotification(_MCPLoggingMessageNotification):
    """A notification message sent by the server side containing a logging message."""


class ResourceUpdatedNotification(_MCPResourceUpdateNotification):
    """A notification message sent by the server side informing a client about a change in a resource."""


class ResourceListChangedNotification(_MCPResourceListChangedNotification):
    """A notification message sent by the server side informing a client about a change in the list of resources."""


class ToolListChangedNotification(_MCPToolListChangedNotification):
    """A notification message sent by the server side informing a client about a change in the offered tools."""


class PromptListChangedNotification(_MCPPromptListChangedNotification):
    """A notification message sent by the server side informing a client about a change in the list of offered Prompts."""


class FlockLoggingMessageNotificationParams(
    _MCPLoggingMessageNotificationParams
):
    """Parameters contained within a Logging Message Notification."""


class MCPRoot(_MCPRoot):
    """Wrapper for mcp.types.Root."""


class ServerParameters(BaseModel):
    """Base Type for server parameters."""

    model_config = ConfigDict(
        arbitrary_types_allowed=True,
    )


class StdioServerParameters(_MCPStdioServerParameters, ServerParameters):
    """Base Type for Stdio Server parameters."""

    env: dict[str, str] | None = Field(
        default_factory=get_default_env,
        description="Environment for the MCP Server.",
    )


class WebsocketServerParameters(ServerParameters):
    """Base Type for Websocket Server params."""

    url: str | AnyUrl = Field(..., description="Url the server listens at.")


class SseServerParameters(ServerParameters):
    """Base Type for SSE Server params."""

    url: str | AnyUrl = Field(..., description="The url the server listens at.")

    headers: dict[str, Any] | None = Field(
        default=None, description="Additional Headers to pass to the client."
    )

    timeout: float | int = Field(default=5, description="Http Timeout")

    sse_read_timeout: float | int = Field(
        default=60 * 5,
        description="How long the client will wait before disconnecting from the server.",
    )


MCPCLientInitFunction = Callable[
    ...,
    AbstractAsyncContextManager[
        tuple[
            MemoryObjectReceiveStream[JSONRPCMessage | Exception],
            MemoryObjectSendStream[JSONRPCMessage],
        ]
    ],
]


FlockSamplingMCPCallback = Callable[
    [RequestContext, CreateMessageRequestParams],
    Awaitable[CreateMessageResult | ErrorData],
]


FlockListRootsMCPCallback = Callable[
    [RequestContext[ClientSession, Any]],
    Awaitable[ListRootsResult | ErrorData],
]

FlockLoggingMCPCallback = Callable[
    [FlockLoggingMessageNotificationParams],
    Awaitable[None],
]

FlockMessageHandlerMCPCallback = Callable[
    [
        RequestResponder[ServerRequest, ClientResult]
        | ServerNotification
        | Exception
    ],
    Awaitable[None],
]
```

### src\flock\core\mcp\util\__init__.py

- **Lines**: 0
- **Last modified**: 2025-05-22 21:27:37

```py

```

### src\flock\core\mcp\util\helpers.py

- **Lines**: 23
- **Last modified**: 2025-05-22 21:27:37

```py
"""Helper functions for Flock MCP Functionality."""

import hashlib
import json

from mcp.client.stdio import get_default_environment


def get_default_env() -> dict[str, str]:
    """Returns a default environment object.

    Including only environment-variables
    deemed safe to inherit.
    """
    return get_default_environment()


def cache_key_generator(agent_id: str, run_id: str, *args, **kwargs) -> str:
    """Helper function to generate cache keys for Flock MCP caches."""
    args_digest = hashlib.md5(
        json.dumps(kwargs, sort_keys=True).encode()
    ).hexdigest()
    return f"{agent_id}:{run_id}:{args_digest}"
```

### src\flock\core\mixin\dspy_integration.py

- **Lines**: 445
- **Last modified**: 2025-05-25 23:17:06

```py
# src/flock/core/mixin/dspy_integration.py
"""Mixin class for integrating with the dspy library."""

import ast
import re  # Import re for parsing
import typing
from typing import Any, Literal

from dspy import Tool

from flock.core.logging.logging import get_logger
from flock.core.util.spliter import split_top_level

# Import split_top_level (assuming it's moved or copied appropriately)
# Option 1: If moved to a shared util
# from flock.core.util.parsing_utils import split_top_level
# Option 2: If kept within this file (as in previous example)
# Define split_top_level here or ensure it's imported

logger = get_logger("mixin.dspy")

# Type definition for agent type override
AgentType = Literal["ReAct", "Completion", "ChainOfThought"] | None


# Helper function to resolve type strings (can be static or module-level)
def _resolve_type_string(type_str: str) -> type:
    """Resolves a type string into a Python type object.
    Handles built-ins, registered types, and common typing generics like
    List, Dict, Optional, Union, Literal.
    """
    # Import registry here to avoid circular imports
    from flock.core.flock_registry import get_registry

    FlockRegistry = get_registry()

    type_str = type_str.strip()
    logger.debug(f"Attempting to resolve type string: '{type_str}'")

    # 1. Check built-ins and registered types directly
    try:
        # This covers str, int, bool, Any, and types registered by name
        resolved_type = FlockRegistry.get_type(type_str)
        logger.debug(f"Resolved '{type_str}' via registry to: {resolved_type}")
        return resolved_type
    except KeyError:
        logger.debug(
            f"'{type_str}' not found directly in registry, attempting generic parsing."
        )
        pass  # Not found, continue parsing generics

    # 2. Handle typing generics (List, Dict, Optional, Union, Literal)
    # Use regex to match pattern like Generic[InnerType1, InnerType2, ...]
    generic_match = re.fullmatch(r"(\w+)\s*\[(.*)\]", type_str)
    if generic_match:
        base_name = generic_match.group(1).strip()
        args_str = generic_match.group(2).strip()
        logger.debug(
            f"Detected generic pattern: Base='{base_name}', Args='{args_str}'"
        )

        try:
            # Get the base generic type (e.g., list, dict, Optional) from registry/builtins
            BaseType = FlockRegistry.get_type(
                base_name
            )  # Expects List, Dict etc. to be registered
            logger.debug(
                f"Resolved base generic type '{base_name}' to: {BaseType}"
            )

            # Special handling for Literal
            if BaseType is typing.Literal:
                # Split literal values, remove quotes, strip whitespace
                def parse_literal_args(args_str: str) -> tuple[str, ...]:
                    try:
                        return tuple(ast.literal_eval(f"[{args_str}]"))
                    except (SyntaxError, ValueError) as exc:
                        raise ValueError(
                            f"Cannot parse {args_str!r} as literals"
                        ) from exc

                literal_args = parse_literal_args(args_str)
                logger.debug(
                    f"Parsing Literal arguments: {args_str} -> {literal_args}"
                )
                resolved_type = typing.Literal[literal_args]  # type: ignore
                logger.debug(f"Constructed Literal type: {resolved_type}")
                return resolved_type

            # Recursively resolve arguments for other generics
            logger.debug(f"Splitting generic arguments: '{args_str}'")
            arg_strs = split_top_level(args_str)
            logger.debug(f"Split arguments: {arg_strs}")
            if not arg_strs:
                raise ValueError("Generic type has no arguments.")

            resolved_arg_types = tuple(
                _resolve_type_string(arg) for arg in arg_strs
            )
            logger.debug(f"Resolved generic arguments: {resolved_arg_types}")

            # Construct the generic type hint
            if BaseType is typing.Optional:
                if len(resolved_arg_types) != 1:
                    raise ValueError("Optional requires exactly one argument.")
                # type: ignore
                resolved_type = typing.Union[resolved_arg_types[0], type(None)]
                logger.debug(
                    f"Constructed Optional type as Union: {resolved_type}"
                )
                return resolved_type
            elif BaseType is typing.Union:
                if not resolved_arg_types:
                    raise ValueError("Union requires at least one argument.")
                # type: ignore
                resolved_type = typing.Union[resolved_arg_types]
                logger.debug(f"Constructed Union type: {resolved_type}")
                return resolved_type
            elif hasattr(
                BaseType, "__getitem__"
            ):  # Check if subscriptable (like list, dict, List, Dict)
                resolved_type = BaseType[resolved_arg_types]  # type: ignore
                logger.debug(
                    f"Constructed subscripted generic type: {resolved_type}"
                )
                return resolved_type
            else:
                # Base type found but cannot be subscripted
                logger.warning(
                    f"Base type '{base_name}' found but is not a standard subscriptable generic. Returning base type."
                )
                return BaseType

        except (KeyError, ValueError, IndexError, TypeError) as e:
            logger.warning(
                f"Failed to parse generic type '{type_str}': {e}. Falling back."
            )
            # Fall through to raise KeyError below if base type itself wasn't found or parsing failed

    # 3. If not resolved by now, raise error
    logger.error(f"Type string '{type_str}' could not be resolved.")
    raise KeyError(f"Type '{type_str}' could not be resolved.")


class DSPyIntegrationMixin:
    """Mixin class for integrating with the dspy library."""

    def create_dspy_signature_class(
        self, agent_name, description_spec, fields_spec
    ) -> Any:
        """Creates a dynamic DSPy Signature class from string specifications,
        resolving types using the FlockRegistry.
        """
        try:
            import dspy
        except ImportError:
            logger.error(
                "DSPy library is not installed. Cannot create DSPy signature. "
                "Install with: pip install dspy-ai"
            )
            raise ImportError("DSPy is required for this functionality.")

        base_class = dspy.Signature
        class_dict = {"__doc__": description_spec, "__annotations__": {}}

        if "->" in fields_spec:
            inputs_spec, outputs_spec = fields_spec.split("->", 1)
        else:
            inputs_spec, outputs_spec = (
                fields_spec,
                "",
            )  # Assume only inputs if no '->'

        def parse_field(field_str):
            """Parses 'name: type_str | description' using _resolve_type_string."""
            field_str = field_str.strip()
            if not field_str:
                return None

            parts = field_str.split("|", 1)
            main_part = parts[0].strip()
            desc = parts[1].strip() if len(parts) > 1 else None

            if ":" in main_part:
                name, type_str = [s.strip() for s in main_part.split(":", 1)]
            else:
                name = main_part
                type_str = "str"  # Default type

            try:
                field_type = _resolve_type_string(type_str)
            except Exception as e:  # Catch resolution errors
                logger.error(
                    f"Failed to resolve type '{type_str}' for field '{name}': {e}. Defaulting to str."
                )
                field_type = str

            return name, field_type, desc

        def process_fields(fields_string, field_kind):
            """Process fields and add to class_dict."""
            if not fields_string or not fields_string.strip():
                return

            split_fields = split_top_level(fields_string)
            for field in split_fields:
                if field.strip():
                    parsed = parse_field(field)
                    if not parsed:
                        continue
                    name, field_type, desc = parsed
                    class_dict["__annotations__"][name] = (
                        field_type  # Use resolved type
                    )

                    FieldClass = (
                        dspy.InputField
                        if field_kind == "input"
                        else dspy.OutputField
                    )
                    # DSPy Fields use 'desc' for description
                    class_dict[name] = (
                        FieldClass(desc=desc)
                        if desc is not None
                        else FieldClass()
                    )

        try:
            process_fields(inputs_spec, "input")
            process_fields(outputs_spec, "output")
        except Exception as e:
            logger.error(
                f"Error processing fields for DSPy signature '{agent_name}': {e}",
                exc_info=True,
            )
            raise ValueError(
                f"Could not process fields for signature: {e}"
            ) from e

        # Create and return the dynamic class
        try:
            DynamicSignature = type(
                "dspy_" + agent_name, (base_class,), class_dict
            )
            logger.info(
                f"Successfully created DSPy Signature: {DynamicSignature.__name__} "
                f"with fields: {DynamicSignature.__annotations__}"
            )
            return DynamicSignature
        except Exception as e:
            logger.error(
                f"Failed to create dynamic type 'dspy_{agent_name}': {e}",
                exc_info=True,
            )
            raise TypeError(f"Could not create DSPy signature type: {e}") from e

    def _configure_language_model(
        self,
        model: str | None,
        use_cache: bool,
        temperature: float,
        max_tokens: int,
    ) -> None:
        """Initialize and configure the language model using dspy."""
        if model is None:
            logger.warning(
                "No model specified for DSPy configuration. Using DSPy default."
            )
            # Rely on DSPy's global default or raise error if none configured
            # import dspy
            # if dspy.settings.lm is None:
            #      raise ValueError("No model specified for agent and no global DSPy LM configured.")
            return

        try:
            import dspy
        except ImportError:
            logger.error(
                "DSPy library is not installed. Cannot configure language model."
            )
            return  # Or raise

        try:
            # Ensure 'cache' parameter is handled correctly (might not exist on dspy.LM directly)
            # DSPy handles caching globally or via specific optimizers typically.
            # We'll configure the LM without explicit cache control here.
            lm_instance = dspy.LM(
                model=model,
                temperature=temperature,
                max_tokens=max_tokens,
                cache=use_cache,
                # Add other relevant parameters if needed, e.g., API keys via dspy.settings
            )
            dspy.settings.configure(lm=lm_instance)
            logger.info(
                f"DSPy LM configured with model: {model}, temp: {temperature}, max_tokens: {max_tokens}"
            )
            # Note: DSPy caching is usually configured globally, e.g., dspy.settings.configure(cache=...)
            # or handled by optimizers. Setting `cache=use_cache` on dspy.LM might not be standard.
        except Exception as e:
            logger.error(
                f"Failed to configure DSPy language model '{model}': {e}",
                exc_info=True,
            )
            # We need to raise this exception, otherwise Flock will trundle on until it needs dspy.settings.lm and can't find it.
            raise

    def _select_task(
        self,
        signature: Any,
        override_evaluator_type: AgentType,
        max_tool_calls: int = 10,
        tools: list[Any] | None = None,
        mcp_tools: list[Any] | None = None,
        kwargs: dict[str, Any] = {},
    ) -> Any:
        """Select and instantiate the appropriate DSPy Program/Module."""
        try:
            import dspy
        except ImportError:
            logger.error(
                "DSPy library is not installed. Cannot select DSPy task."
            )
            raise ImportError("DSPy is required for this functionality.")

        processed_tools = []
        if tools:
            for tool in tools:
                if callable(tool):  # Basic check
                    processed_tools.append(tool)
                # Could add more sophisticated tool wrapping/validation here if needed
                else:
                    logger.warning(
                        f"Item '{tool}' in tools list is not callable, skipping."
                    )

        processed_mcp_tools = []
        if mcp_tools:
            for mcp_tool in mcp_tools:
                if isinstance(mcp_tool, Tool):  # Basic check
                    processed_mcp_tools.append(mcp_tool)
                else:
                    logger.warning(
                        f"Item '{mcp_tool}' is not a dspy.primitives.Tool, skipping."
                    )

        dspy_program = None
        selected_type = override_evaluator_type

        # Determine type if not overridden
        if not selected_type:
            selected_type = (
                "ReAct" if processed_tools or processed_mcp_tools else "Predict"
            )  # Default logic

        logger.debug(
            f"Selecting DSPy program type: {selected_type} (Tools provided: {bool(processed_tools)}) (MCP Tools: {bool(processed_mcp_tools)}"
        )

        # Merge list of native tools and processed tools.
        # This makes mcp tools appear as native code functions to the llm of the agent.
        merged_tools = []

        if processed_tools:
            merged_tools = merged_tools + processed_tools

        if processed_mcp_tools:
            merged_tools = merged_tools + processed_mcp_tools

        try:
            if selected_type == "ChainOfThought":
                dspy_program = dspy.ChainOfThought(signature, **kwargs)
            elif selected_type == "ReAct":
                if not kwargs:
                    kwargs = {"max_iters": max_tool_calls}
                dspy_program = dspy.ReAct(
                    signature, tools=merged_tools or [], **kwargs
                )
            elif selected_type == "Predict":  # Default or explicitly Completion
                dspy_program = dspy.Predict(signature)
            else:  # Fallback or handle unknown type
                logger.warning(
                    f"Unknown or unsupported agent_type_override '{selected_type}'. Defaulting to dspy.Predict."
                )
                dspy_program = dspy.Predict(signature)

            logger.info(
                f"Instantiated DSPy program: {type(dspy_program).__name__}"
            )
            return dspy_program
        except Exception as e:
            logger.error(
                f"Failed to instantiate DSPy program of type '{selected_type}': {e}",
                exc_info=True,
            )
            raise RuntimeError(f"Could not create DSPy program: {e}") from e

    def _process_result(
        self, result: Any, inputs: dict[str, Any]
    ) -> tuple[dict[str, Any], float, list]:
        """Convert the DSPy result object to a dictionary."""
        import dspy

        if result is None:
            logger.warning("DSPy program returned None result.")
            return {}
        try:
            # DSPy Prediction objects often behave like dicts or have .keys() / items()
            if hasattr(result, "items") and callable(result.items):
                output_dict = dict(result.items())
            elif hasattr(result, "__dict__"):  # Fallback for other object types
                output_dict = {
                    k: v
                    for k, v in result.__dict__.items()
                    if not k.startswith("_")
                }
            else:
                # If it's already a dict (less common for DSPy results directly)
                if isinstance(result, dict):
                    output_dict = result
                else:  # Final fallback
                    logger.warning(
                        f"Could not reliably convert DSPy result of type {type(result)} to dict. Returning as is."
                    )
                    output_dict = {"raw_result": result}

            logger.debug(f"Processed DSPy result to dict: {output_dict}")
            # Optionally merge inputs back if desired (can make result dict large)
            final_result = {**inputs, **output_dict}

            lm = dspy.settings.get("lm")
            cost = sum([x["cost"] for x in lm.history if x["cost"] is not None])
            lm_history = lm.history

            return final_result, cost, lm_history

        except Exception as conv_error:
            logger.error(
                f"Failed to process DSPy result into dictionary: {conv_error}",
                exc_info=True,
            )
            return {
                "error": "Failed to process result",
                "raw_result": str(result),
            }
```

### src\flock\core\mixin\prompt_parser.py

- **Lines**: 125
- **Last modified**: 2025-05-25 23:17:06

```py
"""A mixin class for parsing agent prompts and building clean signatures for DSPy."""

# DEPRECATED! This mixin is no longer used in the current version of Flock. It was used to parse agent prompts and build clean signatures for DSPy.
# TODO: DELETE THIS FILE!

from flock.core.util.input_resolver import split_top_level


class PromptParserMixin:
    """A mixin class for parsing agent prompts and building clean signatures for DSPy."""

    def _parse_key_descriptions(self, keys_str: str) -> list[tuple[str, str]]:
        """Parse a comma-separated string into a list of (key, description) tuples.

        This function processes a configuration string that defines one or more keys, where each key may
        include a type hint and an optional human-readable description. The expected format for each key is:

            key: type_hint | description

        If the pipe symbol ("|") is absent, the description is set to an empty string.

        The splitting is performed using split_top_level() so that commas inside type hints are preserved.

        For example, given:
            "query: str | The search query, context: dict | The full conversation context"
        it returns:
            [("query", "The search query"), ("context", "The full conversation context")]

        Args:
            keys_str (str): A comma-separated string of key definitions.

        Returns:
            List[Tuple[str, str]]: A list of (key, description) tuples.
        """
        key_descs = []
        for part in split_top_level(keys_str):
            if not part:
                continue
            if "|" in part:
                key_type_part, desc = part.split("|", 1)
                desc = desc.strip()
            else:
                key_type_part = part
                desc = ""
            key = key_type_part.split(":", 1)[0].strip()
            key_descs.append((key, desc))
        return key_descs

    def _build_clean_signature(self, keys_str: str) -> str:
        """Build a clean signature string from the configuration string by removing the description parts.

        Given a string like:
            "query: str | The search query, context: dict | The full conversation context"
        this method returns:
            "query: str, context: dict"

        This function uses split_top_level() to avoid splitting on commas that are inside type hints.

        Args:
            keys_str (str): The configuration string containing keys, type hints, and optional descriptions.

        Returns:
            str: A clean signature string with only keys and type hints.
        """
        parts = []
        for part in split_top_level(keys_str):
            if not part:
                continue
            if "|" in part:
                clean_part = part.split("|", 1)[0].strip()
            else:
                clean_part = part.strip()
            parts.append(clean_part)
        return ", ".join(parts)

    def _build_descriptions(self) -> tuple[dict[str, str], dict[str, str]]:
        """Build dictionaries of input and output descriptions from the agent's configuration.

        Returns:
            A tuple containing:
            - input_desc: A dictionary mapping each input key (without type hints) to its description.
            - output_desc: A dictionary mapping each output key (without type hints) to its description.
        """
        input_desc: dict[str, str] = {}
        if self.input:
            for key, desc in self._parse_key_descriptions(self.input):
                input_desc[key] = desc

        output_desc: dict[str, str] = {}
        if self.output:
            for key, desc in self._parse_key_descriptions(self.output):
                output_desc[key] = desc

        return input_desc, output_desc

    def _build_prompt(
        self, input_desc: dict[str, str], output_desc: dict[str, str]
    ) -> str:
        """Build a clean signature prompt from the agent's configuration.

        This method uses the original input and output strings (removing the description parts)
        to create a signature string that is passed to DSPy. For example, if:
        - self.input is "query: str | The search query, context: dict | The full conversation context"
        - self.output is "result: str | The result"
        then the prompt will be:
        "query: str, context: dict -> result: str"

        **Note:** The descriptive metadata is preserved in the dictionaries obtained from _build_descriptions,
        which are passed separately to DSPy.

        Args:
            input_desc: Dictionary of input key descriptions (for metadata only).
            output_desc: Dictionary of output key descriptions (for metadata only).

        Returns:
            A clean signature string for DSPy.
        """
        clean_input = (
            self._build_clean_signature(self.input) if self.input else ""
        )
        clean_output = (
            self._build_clean_signature(self.output) if self.output else ""
        )
        # Combine the clean input and output signatures using "->"
        return f"{clean_input} -> {clean_output}"
```

### src\flock\core\serialization\__init__.py

- **Lines**: 13
- **Last modified**: 2025-04-16 00:11:15

```py
"""Serialization utilities for Flock objects."""

from flock.core.serialization.callable_registry import CallableRegistry
from flock.core.serialization.json_encoder import FlockJSONEncoder
from flock.core.serialization.secure_serializer import SecureSerializer
from flock.core.serialization.serializable import Serializable

__all__ = [
    "CallableRegistry",
    "FlockJSONEncoder",
    "SecureSerializer",
    "Serializable",
]
```

### src\flock\core\serialization\callable_registry.py

- **Lines**: 52
- **Last modified**: 2025-04-16 00:11:15

```py
"""Registry system for callable objects to support serialization."""

from collections.abc import Callable


class CallableRegistry:
    """Registry for callable objects.

    This class serves as a central registry for callable objects (functions, methods)
    that can be referenced by name in serialized formats.

    This is a placeholder implementation that will be fully implemented in task US007-T004.
    """

    _registry: dict[str, Callable] = {}

    @classmethod
    def register(cls, name: str, callable_obj: Callable) -> None:
        """Register a callable object with the given name.

        Args:
            name: Unique name for the callable
            callable_obj: Function or method to register
        """
        cls._registry[name] = callable_obj

    @classmethod
    def get(cls, name: str) -> Callable:
        """Get a callable object by name.

        Args:
            name: Name of the callable to retrieve

        Returns:
            The registered callable

        Raises:
            KeyError: If no callable with the given name is registered
        """
        return cls._registry[name]

    @classmethod
    def contains(cls, name: str) -> bool:
        """Check if a callable with the given name is registered.

        Args:
            name: Name to check

        Returns:
            True if registered, False otherwise
        """
        return name in cls._registry
```

### src\flock\core\serialization\flock_serializer.py

- **Lines**: 828
- **Last modified**: 2025-05-22 21:27:37

```py
# src/flock/core/serialization/flock_serializer.py
"""Handles serialization and deserialization logic for Flock instances."""

import builtins
import importlib
import importlib.util
import inspect
import os
import re
import sys
from dataclasses import is_dataclass
from typing import TYPE_CHECKING, Any, Literal

from pydantic import BaseModel, create_model

# Need registry access
from flock.core.flock_registry import get_registry
from flock.core.logging.logging import get_logger
from flock.core.serialization.serialization_utils import (
    # Assuming this handles basic serialization needs
    extract_pydantic_models_from_type_string,
)

if TYPE_CHECKING:
    from flock.core.flock import Flock


logger = get_logger("serialization.flock")
FlockRegistry = get_registry()


class FlockSerializer:
    """Provides static methods for serializing and deserializing Flock instances."""

    @staticmethod
    def serialize(
        flock_instance: "Flock",
        path_type: Literal["absolute", "relative"] = "relative",
    ) -> dict[str, Any]:
        """Convert Flock instance to dictionary representation.

        Args:
            flock_instance: The Flock instance to serialize.
            path_type: How file paths should be formatted ('absolute' or 'relative').
        """
        logger.debug(
            f"Serializing Flock instance '{flock_instance.name}' to dict."
        )
        # Use Pydantic's dump for base fields defined in Flock's model
        data = flock_instance.model_dump(mode="json", exclude_none=True)
        logger.info(
            f"Serializing Flock '{flock_instance.name}' with {len(flock_instance._agents)} agents"
        )

        data["agents"] = {}
        data["mcp_servers"] = {}
        custom_types = {}
        components = {}

        for name, server_instance in flock_instance._servers.items():
            try:
                # Servers handle their own serialization via their to_dict method
                server_data = server_instance.to_dict(path_type=path_type)
                data["mcp_servers"][name] = server_data

                # --- Extract Component Information ---

                # Modules
                if "modules" in server_data:
                    for module_name, module_data in server_data[
                        "modules"
                    ].items():
                        if module_data and "type" in module_data:
                            component_type = module_data["type"]
                            if component_type not in components:
                                logger.debug(
                                    f"Adding module component '{component_type}' from module '{module_name}' in server '{name}'"
                                )
                                components[component_type] = (
                                    FlockSerializer._get_component_definition(
                                        component_type, path_type
                                    )
                                )
            except Exception as e:
                logger.error(
                    f"Failed to serialize server '{name}' within Flock: {e}",
                    exc_info=True,
                )

        for name, agent_instance in flock_instance._agents.items():
            try:
                logger.debug(f"Serializing agent '{name}'")
                # Agents handle their own serialization via their to_dict
                agent_data = (
                    agent_instance.to_dict()
                )  # This now uses the agent's refined to_dict
                data["agents"][name] = agent_data

                # --- Extract Types from Agent Signatures ---
                input_types = []
                if agent_instance.input:
                    input_types = FlockSerializer._extract_types_from_signature(
                        agent_instance.input
                    )
                    if input_types:
                        logger.debug(
                            f"Found input types in agent '{name}': {input_types}"
                        )

                output_types = []
                if agent_instance.output:
                    output_types = (
                        FlockSerializer._extract_types_from_signature(
                            agent_instance.output
                        )
                    )
                    if output_types:
                        logger.debug(
                            f"Found output types in agent '{name}': {output_types}"
                        )

                all_types = set(input_types + output_types)
                if all_types:
                    custom_types.update(
                        FlockSerializer._get_type_definitions(list(all_types))
                    )

                # --- Extract Component Information ---
                # Evaluator
                if (
                    "evaluator" in agent_data
                    and agent_data["evaluator"]
                    and "type" in agent_data["evaluator"]
                ):
                    component_type = agent_data["evaluator"]["type"]
                    if component_type not in components:
                        logger.debug(
                            f"Adding evaluator component '{component_type}' from agent '{name}'"
                        )
                        components[component_type] = (
                            FlockSerializer._get_component_definition(
                                component_type, path_type
                            )
                        )

                # Modules
                if "modules" in agent_data:
                    for module_name, module_data in agent_data[
                        "modules"
                    ].items():
                        if module_data and "type" in module_data:
                            component_type = module_data["type"]
                            if component_type not in components:
                                logger.debug(
                                    f"Adding module component '{component_type}' from module '{module_name}' in agent '{name}'"
                                )
                                components[component_type] = (
                                    FlockSerializer._get_component_definition(
                                        component_type, path_type
                                    )
                                )

                # Router
                if (
                    "handoff_router" in agent_data
                    and agent_data["handoff_router"]
                    and "type" in agent_data["handoff_router"]
                ):
                    component_type = agent_data["handoff_router"]["type"]
                    if component_type not in components:
                        logger.debug(
                            f"Adding router component '{component_type}' from agent '{name}'"
                        )
                        components[component_type] = (
                            FlockSerializer._get_component_definition(
                                component_type, path_type
                            )
                        )

                # Description (Callables)
                if agent_data.get("description_callable"):
                    logger.debug(
                        f"Adding description callable '{agent_data['description_callable']}' from agent '{name}'"
                    )
                    description_callable_name = agent_data[
                        "description_callable"
                    ]
                    description_callable = agent_instance.description
                    path_str = FlockRegistry.get_callable_path_string(
                        description_callable
                    )
                    if path_str:
                        logger.debug(
                            f"Adding description callable '{description_callable_name}' (from path '{path_str}') to components"
                        )
                        components[description_callable_name] = (
                            FlockSerializer._get_callable_definition(
                                path_str, description_callable_name, path_type
                            )
                        )

                if agent_data.get("input_callable"):
                    logger.debug(
                        f"Adding input callable '{agent_data['input_callable']}' from agent '{name}'"
                    )
                    input_callable_name = agent_data["input_callable"]
                    input_callable = agent_instance.input
                    path_str = FlockRegistry.get_callable_path_string(
                        input_callable
                    )
                    if path_str:
                        logger.debug(
                            f"Adding input callable '{input_callable_name}' (from path '{path_str}') to components"
                        )
                        components[input_callable_name] = (
                            FlockSerializer._get_callable_definition(
                                path_str, input_callable_name, path_type
                            )
                        )

                if agent_data.get("output_callable"):
                    logger.debug(
                        f"Adding output callable '{agent_data['output_callable']}' from agent '{name}'"
                    )
                    output_callable_name = agent_data["output_callable"]
                    output_callable = agent_instance.output
                    path_str = FlockRegistry.get_callable_path_string(
                        output_callable
                    )
                    if path_str:
                        logger.debug(
                            f"Adding output callable '{output_callable_name}' (from path '{path_str}') to components"
                        )
                        components[output_callable_name] = (
                            FlockSerializer._get_callable_definition(
                                path_str, output_callable_name, path_type
                            )
                        )

                # Tools (Callables)
                if agent_data.get("tools"):
                    logger.debug(
                        f"Extracting tool information from agent '{name}': {agent_data['tools']}"
                    )
                    tool_objs = (
                        agent_instance.tools if agent_instance.tools else []
                    )
                    for i, tool_name in enumerate(agent_data["tools"]):
                        if tool_name not in components and i < len(tool_objs):
                            tool = tool_objs[i]
                            if callable(tool) and not isinstance(tool, type):
                                path_str = (
                                    FlockRegistry.get_callable_path_string(tool)
                                )
                                if path_str:
                                    logger.debug(
                                        f"Adding tool '{tool_name}' (from path '{path_str}') to components"
                                    )
                                    components[tool_name] = (
                                        FlockSerializer._get_callable_definition(
                                            path_str, tool_name, path_type
                                        )
                                    )

            except Exception as e:
                logger.error(
                    f"Failed to serialize agent '{name}' within Flock: {e}",
                    exc_info=True,
                )

        if custom_types:
            logger.info(f"Adding {len(custom_types)} custom type definitions")
            data["types"] = custom_types
        if components:
            logger.info(
                f"Adding {len(components)} component/callable definitions"
            )
            data["components"] = components

        data["dependencies"] = FlockSerializer._get_dependencies()
        data["metadata"] = {
            "path_type": path_type,
            "flock_version": "0.4.0",
        }  # Example version

        logger.debug(f"Flock '{flock_instance.name}' serialization complete.")
        return data

    @staticmethod
    def deserialize(cls: type["Flock"], data: dict[str, Any]) -> "Flock":
        """Create Flock instance from dictionary representation."""
        # Import concrete types needed for instantiation
        from flock.core.flock import Flock  # Import the actual class
        from flock.core.flock_agent import FlockAgent as ConcreteFlockAgent
        from flock.core.mcp.flock_mcp_server import (
            FlockMCPServerBase as ConcreteFlockMCPServer,
        )

        logger.debug(
            f"Deserializing Flock from dict. Provided keys: {list(data.keys())}"
        )

        metadata = data.pop("metadata", {})
        path_type = metadata.get(
            "path_type", "relative"
        )  # Default to relative for loading flexibility
        logger.debug(
            f"Using path_type '{path_type}' from metadata for component loading"
        )

        if "types" in data:
            logger.info(f"Processing {len(data['types'])} type definitions")
            FlockSerializer._register_type_definitions(data.pop("types"))

        if "components" in data:
            logger.info(
                f"Processing {len(data['components'])} component/callable definitions"
            )
            FlockSerializer._register_component_definitions(
                data.pop("components"), path_type
            )

        if "dependencies" in data:
            logger.debug(f"Checking {len(data['dependencies'])} dependencies")
            FlockSerializer._check_dependencies(data.pop("dependencies"))

        agents_data = data.pop("agents", {})
        server_data = data.pop("mcp_servers", {})
        logger.info(f"Found {len(server_data)} servers to deserialize")
        logger.info(f"Found {len(agents_data)} agents to deserialize")

        try:
            # Pass only fields defined in Flock's Pydantic model to constructor
            init_data = {
                k: v for k, v in data.items() if k in Flock.model_fields
            }
            logger.debug(
                f"Creating Flock instance with fields: {list(init_data.keys())}"
            )
            flock_instance = cls(**init_data)  # Use cls which is Flock
        except Exception as e:
            logger.error(
                f"Pydantic validation/init failed for Flock: {e}", exc_info=True
            )
            raise ValueError(
                f"Failed to initialize Flock from dict: {e}"
            ) from e

        # Deserialize and add server AFTER Flock instance exists and BEFORE Agents have been added
        for name, server_data in server_data.items():
            try:
                logger.debug(f"Deserializing server '{name}'")
                server_data.setdefault("name", name)
                server_instance = ConcreteFlockMCPServer.from_dict(server_data)
                flock_instance.add_server(server_instance)
                logger.debug(f"Successfully added server '{name}' to Flock")
            except Exception as e:
                logger.error(
                    f"Failed to deserialize/add server '{name}': {e}",
                    exc_info=True,
                )

        # Deserialize and add agents AFTER Flock instance exists
        for name, agent_data in agents_data.items():
            try:
                logger.debug(f"Deserializing agent '{name}'")
                agent_data.setdefault("name", name)
                agent_instance = ConcreteFlockAgent.from_dict(agent_data)
                flock_instance.add_agent(agent_instance)
                logger.debug(f"Successfully added agent '{name}' to Flock")
            except Exception as e:
                logger.error(
                    f"Failed to deserialize/add agent '{name}': {e}",
                    exc_info=True,
                )

        logger.info(
            f"Successfully deserialized Flock '{flock_instance.name}' with {len(flock_instance._agents)} agents"
        )
        return flock_instance

    # --- Helper methods moved from Flock ---
    # (Keep all the _extract..., _get..., _register..., _create... methods here)
    # Ensure they use FlockSerializer._... or are standalone functions called directly.
    # Make static if they don't need instance state (which they shouldn't here).

    @staticmethod
    def _extract_types_from_signature(signature: str) -> list[str]:
        """Extract type names from an input/output signature string."""
        if not signature:
            return []
        from flock.core.util.input_resolver import (
            split_top_level,  # Import locally if needed
        )

        type_names = set()
        try:
            parts = split_top_level(signature)
            for part in parts:
                if ":" in part:
                    type_str = part.split(":", 1)[1].split("|", 1)[0].strip()
                    # Use the more robust extractor
                    models = extract_pydantic_models_from_type_string(type_str)
                    for model in models:
                        type_names.add(model.__name__)
        except Exception as e:
            logger.warning(
                f"Could not fully parse types from signature '{signature}': {e}"
            )
        return list(type_names)

    @staticmethod
    def _get_type_definitions(type_names: list[str]) -> dict[str, Any]:
        """Get definitions for the specified custom types from the registry."""
        type_definitions = {}
        for type_name in type_names:
            try:
                type_obj = FlockRegistry.get_type(
                    type_name
                )  # Throws KeyError if not found
                type_def = FlockSerializer._extract_type_definition(
                    type_name, type_obj
                )
                if type_def:
                    type_definitions[type_name] = type_def
            except KeyError:
                logger.warning(
                    f"Type '{type_name}' requested but not found in registry."
                )
            except Exception as e:
                logger.warning(
                    f"Could not extract definition for type {type_name}: {e}"
                )
        return type_definitions

    @staticmethod
    def _extract_type_definition(
        type_name: str, type_obj: type
    ) -> dict[str, Any] | None:
        """Extract a definition for a custom type (Pydantic or Dataclass)."""
        # Definition includes module path and schema/fields
        module_path = getattr(type_obj, "__module__", "unknown")
        type_def = {"module_path": module_path}
        try:
            if issubclass(type_obj, BaseModel):
                type_def["type"] = "pydantic.BaseModel"
                schema = type_obj.model_json_schema()
                if "title" in schema and schema["title"] == type_name:
                    del schema["title"]
                type_def["schema"] = schema
                return type_def
            elif is_dataclass(type_obj):
                type_def["type"] = "dataclass"
                fields = {}
                for field_name, field in getattr(
                    type_obj, "__dataclass_fields__", {}
                ).items():
                    # Attempt to get a string representation of the type
                    try:
                        type_repr = str(field.type)
                    except Exception:
                        type_repr = "unknown"
                    fields[field_name] = {
                        "type": type_repr,
                        "default": str(field.default)
                        if field.default is not inspect.Parameter.empty
                        else None,
                    }
                type_def["fields"] = fields
                return type_def
            else:
                logger.debug(
                    f"Type '{type_name}' is not Pydantic or Dataclass, skipping detailed definition."
                )
                return (
                    None  # Don't include non-data types in the 'types' section
                )
        except Exception as e:
            logger.warning(f"Error extracting definition for {type_name}: {e}")
            return None

    @staticmethod
    def _get_component_definition(
        component_type_name: str, path_type: Literal["absolute", "relative"]
    ) -> dict[str, Any]:
        """Get definition for a component type from the registry."""
        component_def = {
            "type": "flock_component",
            "module_path": "unknown",
            "file_path": None,
        }
        try:
            component_class = FlockRegistry.get_component(
                component_type_name
            )  # Raises KeyError if not found
            component_def["module_path"] = getattr(
                component_class, "__module__", "unknown"
            )
            component_def["description"] = (
                inspect.getdoc(component_class)
                or f"{component_type_name} component"
            )

            # Get file path
            try:
                file_path_abs = inspect.getfile(component_class)
                component_def["file_path"] = (
                    os.path.relpath(file_path_abs)
                    if path_type == "relative"
                    else file_path_abs
                )
            except (TypeError, ValueError) as e:
                logger.debug(
                    f"Could not determine file path for component {component_type_name}: {e}"
                )

        except KeyError:
            logger.warning(
                f"Component class '{component_type_name}' not found in registry."
            )
            component_def["description"] = (
                f"{component_type_name} component (class not found in registry)"
            )
        except Exception as e:
            logger.warning(
                f"Could not extract full definition for component {component_type_name}: {e}"
            )
        return component_def

    @staticmethod
    def _get_callable_definition(
        callable_path: str,
        func_name: str,
        path_type: Literal["absolute", "relative"],
    ) -> dict[str, Any]:
        """Get definition for a callable using its registry path."""
        callable_def = {
            "type": "flock_callable",
            "module_path": "unknown",
            "file_path": None,
        }
        try:
            func = FlockRegistry.get_callable(
                callable_path
            )  # Raises KeyError if not found
            callable_def["module_path"] = getattr(func, "__module__", "unknown")
            callable_def["description"] = (
                inspect.getdoc(func) or f"Callable function {func_name}"
            )
            # Get file path
            try:
                file_path_abs = inspect.getfile(func)
                callable_def["file_path"] = (
                    os.path.relpath(file_path_abs)
                    if path_type == "relative"
                    else file_path_abs
                )
            except (TypeError, ValueError) as e:
                logger.debug(
                    f"Could not determine file path for callable {callable_path}: {e}"
                )

        except KeyError:
            logger.warning(
                f"Callable '{callable_path}' (for tool '{func_name}') not found in registry."
            )
            callable_def["description"] = (
                f"Callable {func_name} (function not found in registry)"
            )
        except Exception as e:
            logger.warning(
                f"Could not extract full definition for callable {callable_path}: {e}"
            )
        return callable_def

    @staticmethod
    def _get_dependencies() -> list[str]:
        """Get list of core dependencies required by Flock."""
        # Basic static list for now
        return [
            "pydantic>=2.0.0",
            "flock-core>=0.4.0",
        ]  # Update version as needed

    @staticmethod
    def _register_type_definitions(type_defs: dict[str, Any]) -> None:
        """Register type definitions from serialized data."""
        # (Logic remains largely the same as original, ensure it uses FlockRegistry)
        for type_name, type_def in type_defs.items():
            logger.debug(f"Registering type definition for: {type_name}")
            # Prioritize direct import
            module_path = type_def.get("module_path")
            registered = False
            if module_path and module_path != "unknown":
                try:
                    module = importlib.import_module(module_path)
                    if hasattr(module, type_name):
                        type_obj = getattr(module, type_name)
                        FlockRegistry.register_type(type_obj, type_name)
                        logger.info(
                            f"Registered type '{type_name}' from module '{module_path}'"
                        )
                        registered = True
                except ImportError:
                    logger.debug(
                        f"Could not import module {module_path} for type {type_name}"
                    )
                except Exception as e:
                    logger.warning(
                        f"Error registering type {type_name} from module: {e}"
                    )

            if registered:
                continue

            # Attempt dynamic creation if direct import failed or wasn't possible
            type_kind = type_def.get("type")
            if type_kind == "pydantic.BaseModel" and "schema" in type_def:
                FlockSerializer._create_pydantic_model(type_name, type_def)
            elif type_kind == "dataclass" and "fields" in type_def:
                FlockSerializer._create_dataclass(type_name, type_def)
            else:
                logger.warning(
                    f"Cannot dynamically register type '{type_name}' with kind '{type_kind}'"
                )

    @staticmethod
    def _create_pydantic_model(
        type_name: str, type_def: dict[str, Any]
    ) -> None:
        """Dynamically create and register a Pydantic model from schema."""
        # (Logic remains the same, ensure it uses FlockRegistry.register_type)
        schema = type_def.get("schema", {})
        try:
            fields = {}
            properties = schema.get("properties", {})
            required = schema.get("required", [])
            for field_name, field_schema in properties.items():
                field_type = FlockSerializer._get_type_from_schema(field_schema)
                default = ... if field_name in required else None
                fields[field_name] = (field_type, default)

            DynamicModel = create_model(type_name, **fields)
            FlockRegistry.register_type(DynamicModel, type_name)
            logger.info(
                f"Dynamically created and registered Pydantic model: {type_name}"
            )
        except Exception as e:
            logger.error(f"Failed to create Pydantic model {type_name}: {e}")

    @staticmethod
    def _get_type_from_schema(field_schema: dict[str, Any]) -> Any:
        """Convert JSON schema type to Python type."""
        # (Logic remains the same)
        schema_type = field_schema.get("type")
        type_mapping = {
            "string": str,
            "integer": int,
            "number": float,
            "boolean": bool,
            "array": list,
            "object": dict,
        }
        if schema_type in type_mapping:
            return type_mapping[schema_type]
        if "enum" in field_schema:
            from typing import Literal

            return Literal[tuple(field_schema["enum"])]  # type: ignore
        return Any

    @staticmethod
    def _create_dataclass(type_name: str, type_def: dict[str, Any]) -> None:
        """Dynamically create and register a dataclass."""
        # (Logic remains the same, ensure it uses FlockRegistry.register_type)
        from dataclasses import make_dataclass

        fields_def = type_def.get("fields", {})
        try:
            fields = []
            for field_name, field_props in fields_def.items():
                # Safely evaluate type string - requires care!
                field_type_str = field_props.get("type", "str")
                try:
                    field_type = eval(
                        field_type_str,
                        {"__builtins__": builtins.__dict__},
                        {"List": list, "Dict": dict},
                    )  # Allow basic types
                except Exception:
                    field_type = Any
                fields.append((field_name, field_type))

            DynamicDataclass = make_dataclass(type_name, fields)
            FlockRegistry.register_type(DynamicDataclass, type_name)
            logger.info(
                f"Dynamically created and registered dataclass: {type_name}"
            )
        except Exception as e:
            logger.error(f"Failed to create dataclass {type_name}: {e}")

    @staticmethod
    def _register_component_definitions(
        component_defs: dict[str, Any],
        path_type: Literal["absolute", "relative"],
    ) -> None:
        """Register component/callable definitions from serialized data."""
        # (Logic remains the same, ensure it uses FlockRegistry.register_component/register_callable)
        # Key change: Ensure file_path is handled correctly based on path_type from metadata
        for name, comp_def in component_defs.items():
            logger.debug(
                f"Registering component/callable definition for: {name}"
            )
            kind = comp_def.get("type")
            module_path = comp_def.get("module_path")
            file_path = comp_def.get("file_path")
            registered = False

            # Resolve file path if relative
            if (
                path_type == "relative"
                and file_path
                and not os.path.isabs(file_path)
            ):
                abs_file_path = os.path.abspath(file_path)
                logger.debug(
                    f"Resolved relative path '{file_path}' to absolute '{abs_file_path}'"
                )
                file_path = abs_file_path  # Use absolute path for loading

            # 1. Try importing from module_path
            if module_path and module_path != "unknown":
                try:
                    module = importlib.import_module(module_path)
                    if hasattr(module, name):
                        obj = getattr(module, name)
                        if kind == "flock_callable" and callable(obj):
                            FlockRegistry.register_callable(
                                obj, name
                            )  # Register by simple name
                            # Also register by full path if possible
                            full_path = f"{module_path}.{name}"
                            if full_path != name:
                                FlockRegistry.register_callable(obj, full_path)
                            logger.info(
                                f"Registered callable '{name}' from module '{module_path}'"
                            )
                            registered = True
                        elif kind == "flock_component" and isinstance(
                            obj, type
                        ):
                            FlockRegistry.register_component(obj, name)
                            logger.info(
                                f"Registered component '{name}' from module '{module_path}'"
                            )
                            registered = True
                except (ImportError, AttributeError):
                    logger.debug(
                        f"Could not import '{name}' from module '{module_path}', trying file path."
                    )
                except Exception as e:
                    logger.warning(
                        f"Error registering '{name}' from module '{module_path}': {e}"
                    )

            if registered:
                continue

            # 2. Try importing from file_path if module import failed or wasn't possible
            if file_path and os.path.exists(file_path):
                logger.debug(
                    f"Attempting to load '{name}' from file: {file_path}"
                )
                try:
                    mod_name = f"flock_dynamic_{name}"  # Unique module name
                    spec = importlib.util.spec_from_file_location(
                        mod_name, file_path
                    )
                    if spec and spec.loader:
                        module = importlib.util.module_from_spec(spec)
                        sys.modules[spec.name] = (
                            module  # Important for pickle/cloudpickle
                        )
                        spec.loader.exec_module(module)
                        if hasattr(module, name):
                            obj = getattr(module, name)
                            if kind == "flock_callable" and callable(obj):
                                FlockRegistry.register_callable(obj, name)
                                logger.info(
                                    f"Registered callable '{name}' from file '{file_path}'"
                                )
                            elif kind == "flock_component" and isinstance(
                                obj, type
                            ):
                                FlockRegistry.register_component(obj, name)
                                logger.info(
                                    f"Registered component '{name}' from file '{file_path}'"
                                )
                        else:
                            logger.warning(
                                f"'{name}' not found in loaded file '{file_path}'"
                            )
                    else:
                        logger.warning(
                            f"Could not create import spec for file '{file_path}'"
                        )
                except Exception as e:
                    logger.error(
                        f"Error loading '{name}' from file '{file_path}': {e}",
                        exc_info=True,
                    )
            elif not registered:
                logger.warning(
                    f"Could not register '{name}'. No valid module or file path found."
                )

    @staticmethod
    def _check_dependencies(dependencies: list[str]) -> None:
        """Check if required dependencies are available (basic check)."""
        # (Logic remains the same)
        for dep in dependencies:
            match = re.match(r"([^>=<]+)", dep)
            if match:
                pkg_name = match.group(1).replace("-", "_")
                try:
                    importlib.import_module(pkg_name)
                except ImportError:
                    logger.warning(f"Dependency '{dep}' might be missing.")
```

### src\flock\core\serialization\json_encoder.py

- **Lines**: 41
- **Last modified**: 2025-04-16 00:11:15

```py
"""JSON encoder utilities for Flock objects."""

import json
from datetime import datetime
from typing import Any


class FlockJSONEncoder(json.JSONEncoder):
    """Custom JSON encoder for handling Pydantic models and other non-serializable objects."""

    def default(self, obj: Any) -> Any:
        from pydantic import BaseModel

        # Handle Pydantic models
        if isinstance(obj, BaseModel):
            return obj.model_dump()

        # Handle datetime objects
        if isinstance(obj, datetime):
            return obj.isoformat()

        # Handle sets, convert to list
        if isinstance(obj, set):
            return list(obj)

        # Handle objects with a to_dict method
        if hasattr(obj, "to_dict") and callable(getattr(obj, "to_dict")):
            return obj.to_dict()

        # Handle objects with a __dict__ attribute
        if hasattr(obj, "__dict__"):
            return {
                k: v for k, v in obj.__dict__.items() if not k.startswith("_")
            }

        # Let the parent class handle it or raise TypeError
        try:
            return super().default(obj)
        except TypeError:
            # If all else fails, convert to string
            return str(obj)
```

### src\flock\core\serialization\secure_serializer.py

- **Lines**: 175
- **Last modified**: 2025-04-16 00:11:15

```py
import cloudpickle


class SecureSerializer:
    """Security-focused serialization system with capability controls for Flock objects."""

    # Define capability levels for different modules
    MODULE_CAPABILITIES = {
        # Core Python - unrestricted
        "builtins": "unrestricted",
        "datetime": "unrestricted",
        "re": "unrestricted",
        "math": "unrestricted",
        "json": "unrestricted",
        # Framework modules - unrestricted
        "flock": "unrestricted",
        # System modules - restricted but allowed
        "os": "restricted",
        "io": "restricted",
        "sys": "restricted",
        "subprocess": "high_risk",
        # Network modules - high risk
        "socket": "high_risk",
        "requests": "high_risk",
    }

    # Functions that should never be serialized
    BLOCKED_FUNCTIONS = {
        "os.system",
        "os.popen",
        "os.spawn",
        "os.exec",
        "subprocess.call",
        "subprocess.run",
        "subprocess.Popen",
        "eval",
        "exec",
        "__import__",
    }

    @staticmethod
    def _get_module_capability(module_name):
        """Get the capability level for a module."""
        for prefix, level in SecureSerializer.MODULE_CAPABILITIES.items():
            if module_name == prefix or module_name.startswith(f"{prefix}."):
                return level
        return "unknown"  # Default to unknown for unlisted modules

    @staticmethod
    def _is_safe_callable(obj):
        """Check if a callable is safe to serialize."""
        if not callable(obj) or isinstance(obj, type):
            return True, "Not a callable function"

        module = obj.__module__
        func_name = (
            f"{module}.{obj.__name__}"
            if hasattr(obj, "__name__")
            else "unknown"
        )

        # Check against blocked functions
        if func_name in SecureSerializer.BLOCKED_FUNCTIONS:
            return False, f"Function {func_name} is explicitly blocked"

        # Check module capability level
        capability = SecureSerializer._get_module_capability(module)
        if capability == "unknown":
            return False, f"Module {module} has unknown security capability"

        return True, capability

    @staticmethod
    def serialize(obj, allow_restricted=True, allow_high_risk=False):
        """Serialize an object with capability checks."""
        if callable(obj) and not isinstance(obj, type):
            is_safe, capability = SecureSerializer._is_safe_callable(obj)

            if not is_safe:
                raise ValueError(
                    f"Cannot serialize unsafe callable: {capability}"
                )

            if capability == "high_risk" and not allow_high_risk:
                raise ValueError(
                    f"High risk callable {obj.__module__}.{obj.__name__} requires explicit permission"
                )

            if capability == "restricted" and not allow_restricted:
                raise ValueError(
                    f"Restricted callable {obj.__module__}.{obj.__name__} requires explicit permission"
                )

            # Store metadata about the callable for verification during deserialization
            metadata = {
                "module": obj.__module__,
                "name": getattr(obj, "__name__", "unknown"),
                "capability": capability,
            }

            return {
                "__serialized_callable__": True,
                "data": cloudpickle.dumps(obj).hex(),
                "metadata": metadata,
            }

        if isinstance(obj, list):
            return [
                SecureSerializer.serialize(
                    item, allow_restricted, allow_high_risk
                )
                for item in obj
            ]

        if isinstance(obj, dict):
            return {
                k: SecureSerializer.serialize(
                    v, allow_restricted, allow_high_risk
                )
                for k, v in obj.items()
            }

        return obj

    @staticmethod
    def deserialize(obj, allow_restricted=True, allow_high_risk=False):
        """Deserialize an object with capability enforcement."""
        if isinstance(obj, dict) and obj.get("__serialized_callable__") is True:
            # Validate the capability level during deserialization
            metadata = obj.get("metadata", {})
            capability = metadata.get("capability", "unknown")

            if capability == "high_risk" and not allow_high_risk:
                raise ValueError(
                    f"Cannot deserialize high risk callable {metadata.get('module')}.{metadata.get('name')}"
                )

            if capability == "restricted" and not allow_restricted:
                raise ValueError(
                    f"Cannot deserialize restricted callable {metadata.get('module')}.{metadata.get('name')}"
                )

            try:
                callable_obj = cloudpickle.loads(bytes.fromhex(obj["data"]))

                # Additional verification that the deserialized object matches its metadata
                if callable_obj.__module__ != metadata.get("module") or (
                    hasattr(callable_obj, "__name__")
                    and callable_obj.__name__ != metadata.get("name")
                ):
                    raise ValueError(
                        "Callable metadata mismatch - possible tampering detected"
                    )

                return callable_obj
            except Exception as e:
                raise ValueError(f"Failed to deserialize callable: {e!s}")

        if isinstance(obj, list):
            return [
                SecureSerializer.deserialize(
                    item, allow_restricted, allow_high_risk
                )
                for item in obj
            ]

        if isinstance(obj, dict) and "__serialized_callable__" not in obj:
            return {
                k: SecureSerializer.deserialize(
                    v, allow_restricted, allow_high_risk
                )
                for k, v in obj.items()
            }

        return obj
```

### src\flock\core\serialization\serializable.py

- **Lines**: 342
- **Last modified**: 2025-04-16 00:11:15

```py
# src/flock/core/serialization/serializable.py
import json
from abc import ABC, abstractmethod
from pathlib import Path
from typing import Any, Literal, TypeVar

# Use yaml if available, otherwise skip yaml methods
try:
    import yaml

    YAML_AVAILABLE = True
except ImportError:
    YAML_AVAILABLE = False

# Use msgpack if available
try:
    import msgpack

    MSGPACK_AVAILABLE = True
except ImportError:
    MSGPACK_AVAILABLE = False

# Use cloudpickle
try:
    import cloudpickle

    PICKLE_AVAILABLE = True
except ImportError:
    PICKLE_AVAILABLE = False


T = TypeVar("T", bound="Serializable")


class Serializable(ABC):
    """Base class for all serializable objects in the system.

    Provides methods for serializing/deserializing objects to various formats.
    Subclasses MUST implement to_dict and from_dict.
    """

    @abstractmethod
    def to_dict(self) -> dict[str, Any]:
        """Convert instance to a dictionary representation suitable for serialization.
        This method should handle converting nested Serializable objects and callables.
        """
        pass

    @classmethod
    @abstractmethod
    def from_dict(cls: type[T], data: dict[str, Any]) -> T:
        """Create instance from a dictionary representation.
        This method should handle reconstructing nested Serializable objects and callables.
        """
        pass

    # --- JSON Methods ---
    def to_json(self, indent: int | None = 2) -> str:
        """Serialize to JSON string."""
        # Import encoder locally to avoid making it a hard dependency if JSON isn't used
        from .json_encoder import FlockJSONEncoder

        try:
            # Note: to_dict should ideally prepare the structure fully.
            # FlockJSONEncoder is a fallback for types missed by to_dict.
            return json.dumps(
                self.to_dict(), cls=FlockJSONEncoder, indent=indent
            )
        except Exception as e:
            raise RuntimeError(
                f"Failed to serialize {self.__class__.__name__} to JSON: {e}"
            ) from e

    @classmethod
    def from_json(cls: type[T], json_str: str) -> T:
        """Create instance from JSON string."""
        try:
            data = json.loads(json_str)
            return cls.from_dict(data)
        except json.JSONDecodeError as e:
            raise ValueError(f"Invalid JSON string: {e}") from e
        except Exception as e:
            raise RuntimeError(
                f"Failed to deserialize {cls.__name__} from JSON: {e}"
            ) from e

    # --- YAML Methods ---
    def to_yaml(
        self,
        path_type: Literal["absolute", "relative"] = "relative",
        sort_keys=False,
        default_flow_style=False,
    ) -> str:
        """Serialize to YAML string.

        Args:
            path_type: How file paths should be formatted ('absolute' or 'relative')
            sort_keys: Whether to sort dictionary keys
            default_flow_style: YAML flow style setting
        """
        if not YAML_AVAILABLE:
            raise NotImplementedError(
                "YAML support requires PyYAML: pip install pyyaml"
            )
        try:
            # If to_dict supports path_type, pass it; otherwise use standard to_dict
            if "path_type" in self.to_dict.__code__.co_varnames:
                dict_data = self.to_dict(path_type=path_type)
            else:
                dict_data = self.to_dict()

            return yaml.dump(
                dict_data,
                sort_keys=sort_keys,
                default_flow_style=default_flow_style,
                allow_unicode=True,
            )
        except Exception as e:
            raise RuntimeError(
                f"Failed to serialize {self.__class__.__name__} to YAML: {e}"
            ) from e

    @classmethod
    def from_yaml(cls: type[T], yaml_str: str) -> T:
        """Create instance from YAML string."""
        if not YAML_AVAILABLE:
            raise NotImplementedError(
                "YAML support requires PyYAML: pip install pyyaml"
            )
        try:
            data = yaml.safe_load(yaml_str)
            if not isinstance(data, dict):
                raise TypeError(
                    f"YAML did not yield a dictionary for {cls.__name__}"
                )
            return cls.from_dict(data)
        except yaml.YAMLError as e:
            raise ValueError(f"Invalid YAML string: {e}") from e
        except Exception as e:
            raise RuntimeError(
                f"Failed to deserialize {cls.__name__} from YAML: {e}"
            ) from e

    def to_yaml_file(
        self,
        path: Path | str,
        path_type: Literal["absolute", "relative"] = "relative",
        **yaml_dump_kwargs,
    ) -> None:
        """Serialize to YAML file.

        Args:
            path: File path to write to
            path_type: How file paths should be formatted ('absolute' or 'relative')
            **yaml_dump_kwargs: Additional arguments to pass to yaml.dump
        """
        if not YAML_AVAILABLE:
            raise NotImplementedError(
                "YAML support requires PyYAML: pip install pyyaml"
            )
        path = Path(path)
        try:
            path.parent.mkdir(parents=True, exist_ok=True)
            yaml_str = self.to_yaml(path_type=path_type, **yaml_dump_kwargs)
            path.write_text(yaml_str, encoding="utf-8")
        except Exception as e:
            raise RuntimeError(
                f"Failed to write {self.__class__.__name__} to YAML file {path}: {e}"
            ) from e

    @classmethod
    def from_yaml_file(cls: type[T], path: Path | str) -> T:
        """Create instance from YAML file."""
        if not YAML_AVAILABLE:
            raise NotImplementedError(
                "YAML support requires PyYAML: pip install pyyaml"
            )
        path = Path(path)
        try:
            yaml_str = path.read_text(encoding="utf-8")
            return cls.from_yaml(yaml_str)
        except FileNotFoundError:
            raise
        except Exception as e:
            raise RuntimeError(
                f"Failed to read {cls.__name__} from YAML file {path}: {e}"
            ) from e

    # --- MsgPack Methods ---
    def to_msgpack(self) -> bytes:
        """Serialize to msgpack bytes."""
        if not MSGPACK_AVAILABLE:
            raise NotImplementedError(
                "MsgPack support requires msgpack: pip install msgpack"
            )
        try:
            # Use default hook for complex types if needed, or rely on to_dict
            return msgpack.packb(self.to_dict(), use_bin_type=True)
        except Exception as e:
            raise RuntimeError(
                f"Failed to serialize {self.__class__.__name__} to MsgPack: {e}"
            ) from e

    @classmethod
    def from_msgpack(cls: type[T], msgpack_bytes: bytes) -> T:
        """Create instance from msgpack bytes."""
        if not MSGPACK_AVAILABLE:
            raise NotImplementedError(
                "MsgPack support requires msgpack: pip install msgpack"
            )
        try:
            # Use object_hook if custom deserialization is needed beyond from_dict
            data = msgpack.unpackb(msgpack_bytes, raw=False)
            if not isinstance(data, dict):
                raise TypeError(
                    f"MsgPack did not yield a dictionary for {cls.__name__}"
                )
            return cls.from_dict(data)
        except Exception as e:
            raise RuntimeError(
                f"Failed to deserialize {cls.__name__} from MsgPack: {e}"
            ) from e

    def to_msgpack_file(self, path: Path | str) -> None:
        """Serialize to msgpack file."""
        if not MSGPACK_AVAILABLE:
            raise NotImplementedError(
                "MsgPack support requires msgpack: pip install msgpack"
            )
        path = Path(path)
        try:
            path.parent.mkdir(parents=True, exist_ok=True)
            msgpack_bytes = self.to_msgpack()
            path.write_bytes(msgpack_bytes)
        except Exception as e:
            raise RuntimeError(
                f"Failed to write {self.__class__.__name__} to MsgPack file {path}: {e}"
            ) from e

    @classmethod
    def from_msgpack_file(cls: type[T], path: Path | str) -> T:
        """Create instance from msgpack file."""
        if not MSGPACK_AVAILABLE:
            raise NotImplementedError(
                "MsgPack support requires msgpack: pip install msgpack"
            )
        path = Path(path)
        try:
            msgpack_bytes = path.read_bytes()
            return cls.from_msgpack(msgpack_bytes)
        except FileNotFoundError:
            raise
        except Exception as e:
            raise RuntimeError(
                f"Failed to read {cls.__name__} from MsgPack file {path}: {e}"
            ) from e

    # --- Pickle Methods (Use with caution due to security risks) ---
    def to_pickle(self) -> bytes:
        """Serialize to pickle bytes using cloudpickle."""
        if not PICKLE_AVAILABLE:
            raise NotImplementedError(
                "Pickle support requires cloudpickle: pip install cloudpickle"
            )
        try:
            return cloudpickle.dumps(self)
        except Exception as e:
            raise RuntimeError(
                f"Failed to serialize {self.__class__.__name__} to Pickle: {e}"
            ) from e

    @classmethod
    def from_pickle(cls: type[T], pickle_bytes: bytes) -> T:
        """Create instance from pickle bytes using cloudpickle."""
        if not PICKLE_AVAILABLE:
            raise NotImplementedError(
                "Pickle support requires cloudpickle: pip install cloudpickle"
            )
        try:
            instance = cloudpickle.loads(pickle_bytes)
            if not isinstance(instance, cls):
                raise TypeError(
                    f"Deserialized object is not of type {cls.__name__}"
                )
            return instance
        except Exception as e:
            raise RuntimeError(
                f"Failed to deserialize {cls.__name__} from Pickle: {e}"
            ) from e

    def to_pickle_file(self, path: Path | str) -> None:
        """Serialize to pickle file using cloudpickle."""
        if not PICKLE_AVAILABLE:
            raise NotImplementedError(
                "Pickle support requires cloudpickle: pip install cloudpickle"
            )
        path = Path(path)
        try:
            path.parent.mkdir(parents=True, exist_ok=True)
            pickle_bytes = self.to_pickle()
            path.write_bytes(pickle_bytes)
        except Exception as e:
            raise RuntimeError(
                f"Failed to write {self.__class__.__name__} to Pickle file {path}: {e}"
            ) from e

    @classmethod
    def from_pickle_file(cls: type[T], path: Path | str) -> T:
        """Create instance from pickle file using cloudpickle."""
        if not PICKLE_AVAILABLE:
            raise NotImplementedError(
                "Pickle support requires cloudpickle: pip install cloudpickle"
            )
        path = Path(path)
        try:
            pickle_bytes = path.read_bytes()
            return cls.from_pickle(pickle_bytes)
        except FileNotFoundError:
            raise
        except Exception as e:
            raise RuntimeError(
                f"Failed to read {cls.__name__} from Pickle file {path}: {e}"
            ) from e

    # _filter_none_values remains unchanged
    @staticmethod
    def _filter_none_values(data: Any) -> Any:
        """Filter out None values from dictionaries and lists recursively."""
        if isinstance(data, dict):
            return {
                k: Serializable._filter_none_values(v)
                for k, v in data.items()
                if v is not None
            }
        elif isinstance(data, list):
            # Filter None from list items AND recursively filter within items
            return [
                Serializable._filter_none_values(item)
                for item in data
                if item is not None
            ]
        return data
```

### src\flock\core\serialization\serialization_utils.py

- **Lines**: 409
- **Last modified**: 2025-04-18 00:51:45

```py
# src/flock/core/serialization/serialization_utils.py
"""Utilities for recursive serialization/deserialization with callable handling."""

import ast
import builtins
import importlib
import sys
import types
import typing
from collections.abc import Mapping, Sequence
from enum import Enum
from typing import (
    TYPE_CHECKING,
    Any,
    Literal,
    Union,
    get_args,
    get_origin,
)

from pydantic import BaseModel

# Use TYPE_CHECKING to avoid circular imports
if TYPE_CHECKING:
    pass

from flock.core.flock_registry import get_registry
from flock.core.logging.logging import get_logger

logger = get_logger("serialization.utils")

# Remove this line to avoid circular import at module level
# FlockRegistry = get_registry()  # Get singleton instance

# --- Serialization Helper ---

# src/flock/util/hydrator.py (or import from serialization_utils)


def _format_type_to_string(type_hint: type) -> str:
    """Converts a Python type object back into its string representation."""
    # This needs to handle various typing scenarios (List, Dict, Union, Optional, Literal, custom types)
    origin = typing.get_origin(type_hint)
    args = typing.get_args(type_hint)

    # Handle common cases first
    if origin is list or origin is list:
        if args:
            return f"list[{_format_type_to_string(args[0])}]"
        return "list[Any]"  # Or just "list"
    elif origin is dict or origin is dict:
        if args and len(args) == 2:
            return f"dict[{_format_type_to_string(args[0])}, {_format_type_to_string(args[1])}]"
        return "dict[Any, Any]"  # Or just "dict"
    elif origin is Union or origin is types.UnionType:
        # Handle Optional[T] as Union[T, NoneType]
        if len(args) == 2 and type(None) in args:
            inner_type = next(t for t in args if t is not type(None))
            return _format_type_to_string(inner_type)
        # return f"Optional[{_format_type_to_string(inner_type)}]"
        return (
            f"Union[{', '.join(_format_type_to_string(arg) for arg in args)}]"
        )
    elif origin is Literal:
        formatted_args = []
        for arg in args:
            if isinstance(arg, str):
                formatted_args.append(f"'{arg}'")
            else:
                formatted_args.append(str(arg))
        return f"Literal[{', '.join(formatted_args)}]"
    elif hasattr(
        type_hint, "__forward_arg__"
    ):  # Handle ForwardRefs if necessary
        return type_hint.__forward_arg__
    elif hasattr(type_hint, "__name__"):
        # Handle custom types registered in registry (get preferred name)
        registry = get_registry()
        for (
            name,
            reg_type,
        ) in registry._types.items():  # Access internal for lookup
            if reg_type == type_hint:
                return name  # Return registered name
        return type_hint.__name__  # Fallback to class name if not registered
    else:
        # Fallback for complex types or types not handled above
        type_repr = str(type_hint).replace("typing.", "")  # Basic cleanup
        type_repr = str(type_hint).replace("| None", "")
        type_repr = type_repr.strip()
        logger.debug(
            f"Using fallback string representation for type: {type_repr}"
        )
        return type_repr


def extract_identifiers_from_type_str(type_str: str) -> set[str]:
    """Extract all identifiers from a type annotation string using the AST."""
    tree = ast.parse(type_str, mode="eval")
    identifiers = set()

    class IdentifierVisitor(ast.NodeVisitor):
        def visit_Name(self, node):
            identifiers.add(node.id)

        def visit_Attribute(self, node):
            # Optionally support dotted names like mymodule.MyModel
            full_name = []
            while isinstance(node, ast.Attribute):
                full_name.append(node.attr)
                node = node.value
            if isinstance(node, ast.Name):
                full_name.append(node.id)
                identifiers.add(".".join(reversed(full_name)))

    IdentifierVisitor().visit(tree)
    return identifiers


def resolve_name(name: str):
    """Resolve a name to a Python object from loaded modules."""
    # Try dotted names first
    parts = name.split(".")
    obj = None

    if len(parts) == 1:
        # Search globals and builtins
        if parts[0] in globals():
            return globals()[parts[0]]
        if parts[0] in builtins.__dict__:
            return builtins.__dict__[parts[0]]
    else:
        try:
            obj = sys.modules[parts[0]]
            for part in parts[1:]:
                obj = getattr(obj, part)
            return obj
        except Exception:
            return None

    # Try all loaded modules' symbols
    for module in list(sys.modules.values()):
        if module is None or not hasattr(module, "__dict__"):
            continue
        if parts[0] in module.__dict__:
            return module.__dict__[parts[0]]

    return None


def extract_pydantic_models_from_type_string(
    type_str: str,
) -> list[type[BaseModel]]:
    identifiers = extract_identifiers_from_type_str(type_str)
    models = []
    for name in identifiers:
        resolved = resolve_name(name)
        if (
            isinstance(resolved, type)
            and issubclass(resolved, BaseModel)
            and resolved is not BaseModel
        ):
            models.append(resolved)
    return models


def collect_pydantic_models(
    type_hint, seen: set[type[BaseModel]] | None = None
) -> set[type[BaseModel]]:
    if seen is None:
        seen = set()

    origin = get_origin(type_hint)
    args = get_args(type_hint)

    # Direct BaseModel
    if isinstance(type_hint, type) and issubclass(type_hint, BaseModel):
        seen.add(type_hint)
        return seen

    # For Unions, Lists, Dicts, Tuples, etc.
    if origin is not None:
        for arg in args:
            collect_pydantic_models(arg, seen)

    return seen


def serialize_item(item: Any) -> Any:
    """Recursively prepares an item for serialization (e.g., to dict for YAML/JSON).
    Converts known callables to their path strings using FlockRegistry.
    Converts Pydantic models using model_dump.
    """
    # Import the registry lazily when needed
    from flock.core.flock_registry import get_registry

    FlockRegistry = get_registry()

    if callable(item) and not isinstance(item, type):
        path_str = FlockRegistry.get_callable_path_string(
            item
        )  # Use registry helper
        if path_str:
            # Store the simple name (last part of the path) for cleaner YAML/JSON
            simple_name = path_str.split(".")[-1]
            logger.debug(
                f"Serializing callable '{getattr(item, '__name__', 'unknown')}' as reference: '{simple_name}' (from path '{path_str}')"
            )
            return {
                "__callable_ref__": simple_name
            }  # Use simple name convention
        else:
            # Handle unregistered callables (e.g., lambdas defined inline)
            # Option 1: Raise error (stricter)
            # raise ValueError(f"Cannot serialize unregistered callable: {getattr(item, '__name__', item)}")
            # Option 2: Store string representation with warning (more lenient)
            logger.warning(
                f"Cannot serialize unregistered callable {getattr(item, '__name__', item)}, storing as string."
            )
            return str(item)
    elif isinstance(item, BaseModel):
        logger.debug(
            f"Serializing Pydantic model instance: {item.__class__.__name__}"
        )
        serialized_dict = {}
        # Iterate through defined fields in the model
        fields_to_iterate = {}
        if hasattr(item, "model_fields"):  # Pydantic v2
            fields_to_iterate = item.model_fields

        for field_name in fields_to_iterate:
            # Get the value *from the instance*
            try:
                value = getattr(item, field_name)
                if value is not None:  # Exclude None values
                    # Recursively serialize the field's value
                    serialized_dict[field_name] = serialize_item(value)
            except AttributeError:
                # Should not happen if iterating model_fields/__fields__ but handle defensively
                logger.warning(
                    f"Attribute '{field_name}' not found on instance of {item.__class__.__name__} during serialization."
                )
        return serialized_dict
    elif isinstance(item, Mapping):
        return {key: serialize_item(value) for key, value in item.items()}
    elif isinstance(item, Sequence) and not isinstance(item, str):
        return [serialize_item(sub_item) for sub_item in item]
    elif isinstance(
        item, type
    ):  # Handle type objects themselves (e.g. if stored directly)
        type_name = FlockRegistry.get_component_type_name(
            item
        )  # Check components first
        if type_name:
            return {"__component_ref__": type_name}
        type_name = FlockRegistry._get_path_string(
            item
        )  # Check regular types/classes by path
        if type_name:
            return {"__type_ref__": type_name}
        logger.warning(
            f"Could not serialize type object {item}, storing as string."
        )
        return str(item)
    elif isinstance(item, Enum):
        return item.value
    else:
        # Return basic types as is
        return item


# --- Deserialization Helper ---


def deserialize_item(item: Any) -> Any:
    """Recursively processes a deserialized item (e.g., from YAML/JSON dict).
    Converts reference dicts back to actual callables or types using FlockRegistry.
    Handles nested lists and dicts.
    """
    # Import the registry lazily when needed
    from flock.core.flock_registry import get_registry

    FlockRegistry = get_registry()

    if isinstance(item, Mapping):
        if "__callable_ref__" in item and len(item) == 1:
            ref_name = item["__callable_ref__"]
            try:
                # The registry's get_callable needs to handle lookup by simple name OR full path
                # Or we assume get_callable handles finding the right function from the simple name
                resolved_callable = FlockRegistry.get_callable(ref_name)
                logger.debug(
                    f"Deserialized callable reference '{ref_name}' to {resolved_callable}"
                )
                return resolved_callable
            except KeyError:
                logger.error(
                    f"Callable reference '{ref_name}' not found during deserialization."
                )
                return None  # Or raise?
            except Exception as e:
                logger.error(
                    f"Error resolving callable reference '{ref_name}': {e}",
                    exc_info=True,
                )
                return None
        elif "__component_ref__" in item and len(item) == 1:
            type_name = item["__component_ref__"]
            try:
                return FlockRegistry.get_component(type_name)
            except KeyError:
                logger.error(
                    f"Component reference '{type_name}' not found during deserialization."
                )
                return None
        elif "__type_ref__" in item and len(item) == 1:
            type_name = item["__type_ref__"]
            try:
                # For general types, use get_type or fallback to dynamic import like get_callable
                # Using get_type for now, assuming it needs registration
                return FlockRegistry.get_type(type_name)
            except KeyError:
                # Attempt dynamic import as fallback if get_type fails (similar to get_callable)
                try:
                    if "." not in type_name:  # Builtins?
                        mod = importlib.import_module("builtins")
                    else:
                        module_name, class_name = type_name.rsplit(".", 1)
                        mod = importlib.import_module(module_name)
                    type_obj = getattr(mod, class_name)
                    if isinstance(type_obj, type):
                        FlockRegistry.register_type(
                            type_obj, type_name
                        )  # Cache it
                        return type_obj
                    else:
                        raise TypeError()
                except Exception:
                    logger.error(
                        f"Type reference '{type_name}' not found in registry or via dynamic import."
                    )
                    return None

        else:
            # Recursively deserialize dictionary values
            return {key: deserialize_item(value) for key, value in item.items()}
    elif isinstance(item, Sequence) and not isinstance(item, str):
        return [deserialize_item(sub_item) for sub_item in item]
    else:
        # Return basic types as is
        return item


# --- Component Deserialization Helper ---
def deserialize_component(
    data: dict | None, expected_base_type: type
) -> Any | None:
    """Deserializes a component (Module, Evaluator, Router) from its dict representation.
    Uses the 'type' field to find the correct class via FlockRegistry.
    """
    # Import the registry and COMPONENT_BASE_TYPES lazily when needed
    from flock.core.flock_registry import COMPONENT_BASE_TYPES, get_registry

    FlockRegistry = get_registry()

    if data is None:
        return None
    if not isinstance(data, dict):
        logger.error(
            f"Expected dict for component deserialization, got {type(data)}"
        )
        return None

    type_name = data.get(
        "type"
    )  # Assuming 'type' key holds the class name string
    if not type_name:
        logger.error(f"Component data missing 'type' field: {data}")
        return None

    try:
        ComponentClass = FlockRegistry.get_component(type_name)  # Use registry
        # Optional: Keep the base type check
        if COMPONENT_BASE_TYPES and not issubclass(
            ComponentClass, expected_base_type
        ):
            raise TypeError(
                f"Deserialized class {type_name} is not a subclass of {expected_base_type.__name__}"
            )

        # Recursively deserialize the data *before* passing to Pydantic constructor
        # This handles nested callables/types within the component's config/data
        deserialized_data_for_init = {}
        for k, v in data.items():
            # Don't pass the 'type' field itself to the constructor if it matches class name
            if k == "type" and v == ComponentClass.__name__:
                continue
            deserialized_data_for_init[k] = deserialize_item(v)

        # Use Pydantic constructor directly. Assumes keys match field names.
        # from_dict could be added to components for more complex logic if needed.
        return ComponentClass(**deserialized_data_for_init)

    except (KeyError, TypeError, Exception) as e:
        logger.error(
            f"Failed to deserialize component of type '{type_name}': {e}",
            exc_info=True,
        )
        return None
```

### src\flock\core\util\file_path_utils.py

- **Lines**: 223
- **Last modified**: 2025-04-16 00:11:15

```py
"""Utility functions for handling file paths in Flock.

This module provides utilities for working with file paths,
especially for components that may be loaded from file system paths
rather than module imports.
"""

import importlib.util
import inspect
import os
import sys
from pathlib import Path
from typing import Any


def get_file_path(obj: Any) -> str | None:
    """Get the file path for a Python object.

    Args:
        obj: The object to get the file path for

    Returns:
        The file path if it can be determined, None otherwise
    """
    try:
        if inspect.ismodule(obj):
            return obj.__file__
        elif inspect.isclass(obj) or inspect.isfunction(obj):
            return inspect.getfile(obj)
        return None
    except (TypeError, ValueError):
        return None


def normalize_path(path: str) -> str:
    """Normalize a path for consistent representation.

    Args:
        path: The path to normalize

    Returns:
        The normalized path
    """
    return os.path.normpath(path)


def is_same_path(path1: str, path2: str) -> bool:
    """Check if two paths point to the same file.

    Args:
        path1: The first path
        path2: The second path

    Returns:
        True if the paths point to the same file, False otherwise
    """
    return os.path.normpath(os.path.abspath(path1)) == os.path.normpath(
        os.path.abspath(path2)
    )


def get_relative_path(path: str, base_path: str | None = None) -> str:
    """Get a path relative to a base path.

    Args:
        path: The path to make relative
        base_path: The base path (defaults to current working directory)

    Returns:
        The relative path
    """
    if base_path is None:
        base_path = os.getcwd()

    return os.path.relpath(path, base_path)


def load_class_from_file(file_path: str, class_name: str) -> type | None:
    """Load a class from a file.

    Args:
        file_path: The path to the file
        class_name: The name of the class to load

    Returns:
        The loaded class, or None if it could not be loaded
    """
    try:
        # Generate a unique module name to avoid conflicts
        module_name = f"flock_dynamic_import_{hash(file_path)}"

        # Create a spec for the module
        spec = importlib.util.spec_from_file_location(module_name, file_path)
        if not spec or not spec.loader:
            return None

        # Create and load the module
        module = importlib.util.module_from_spec(spec)
        sys.modules[module_name] = module
        spec.loader.exec_module(module)

        # Get the class from the module
        if not hasattr(module, class_name):
            return None

        return getattr(module, class_name)
    except Exception:
        return None


def get_project_root() -> Path:
    """Get the project root directory.

    Returns:
        The project root path
    """
    # Try to find the directory containing pyproject.toml or setup.py
    current_dir = Path(os.getcwd())

    # Walk up the directory tree looking for project markers
    for path in [current_dir, *current_dir.parents]:
        if (path / "pyproject.toml").exists() or (path / "setup.py").exists():
            return path

    # Default to current directory if no project markers found
    return current_dir


def component_path_to_file_path(component_path: str) -> str | None:
    """Convert a component path (module.ClassName) to a file path.

    Args:
        component_path: The component path in the form module.ClassName

    Returns:
        The file path if it can be determined, None otherwise
    """
    try:
        # Split into module path and class name
        if "." not in component_path:
            return None

        module_path, class_name = component_path.rsplit(".", 1)

        # Import the module
        module = importlib.import_module(module_path)

        # Get the file path
        if hasattr(module, "__file__"):
            return module.__file__

        return None
    except (ImportError, AttributeError):
        return None


def file_path_to_component_path(file_path: str, class_name: str) -> str | None:
    """Convert a file path and class name to a component path (module.ClassName).

    This is approximate and may not work in all cases, especially for non-standard
    module structures.

    Args:
        file_path: The file path to the module
        class_name: The name of the class

    Returns:
        The component path if it can be determined, None otherwise
    """
    try:
        # Convert the file path to an absolute path
        abs_path = os.path.abspath(file_path)

        # Get the project root
        root = get_project_root()

        # Get the relative path from the project root
        rel_path = os.path.relpath(abs_path, root)

        # Convert to a module path
        module_path = os.path.splitext(rel_path)[0].replace(os.sep, ".")

        # Remove 'src.' prefix if present (common in Python projects)
        if module_path.startswith("src."):
            module_path = module_path[4:]

        # Combine with the class name
        return f"{module_path}.{class_name}"
    except Exception:
        return None


def register_file_paths_in_registry(
    component_paths: dict[str, str], registry: Any | None = None
) -> bool:
    """Register file paths in the registry.

    Args:
        component_paths: Dictionary mapping component paths to file paths
        registry: The registry to register in (defaults to the global registry)

    Returns:
        True if all paths were registered, False otherwise
    """
    try:
        # Get the global registry if none provided
        if registry is None:
            from flock.core.flock_registry import get_registry

            registry = get_registry()

        # Initialize component_file_paths if needed
        if not hasattr(registry, "_component_file_paths"):
            registry._component_file_paths = {}

        # Register each path
        for component_name, file_path in component_paths.items():
            if component_name in registry._components:
                registry._component_file_paths[component_name] = file_path

        return True
    except Exception:
        return False
```

### src\flock\core\util\hydrator.py

- **Lines**: 309
- **Last modified**: 2025-05-22 21:27:37

```py
# src/flock/core/util/hydrator.py (Revised - Simpler)

import asyncio
import json
from typing import (
    Any,
    TypeVar,
    get_type_hints,
)

from pydantic import BaseModel

# Import necessary Flock components
from flock.core import Flock, FlockFactory
from flock.core.logging.logging import get_logger

# Import helper to format type hints back to strings
from flock.core.serialization.serialization_utils import _format_type_to_string

logger = get_logger("hydrator")
T = TypeVar("T", bound=BaseModel)


def flockclass(model: str = "openai/gpt-4o", agent_description: str | None = None):
    """Decorator to add a .hydrate() method to a Pydantic class.
    Leverages a dynamic Flock agent to fill missing (None) fields.

    Args:
        model: The default LLM model identifier to use for hydration.
        agent_description: An optional description for the dynamically created agent.
    """

    def decorator(cls: type[T]) -> type[T]:
        if not issubclass(cls, BaseModel):
            raise TypeError(
                "@flockclass can only decorate Pydantic BaseModel subclasses."
            )

        # Store metadata on the class
        setattr(cls, "__flock_model__", model)
        setattr(cls, "__flock_agent_description__", agent_description)

        # --- Attach the async hydrate method directly ---
        async def hydrate_async(self) -> T:
            """Hydrates the object by filling None fields using a dynamic Flock agent.
            Uses existing non-None fields as input context.
            Returns the hydrated object (self).
            """
            class_name = self.__class__.__name__
            logger.info(f"Starting hydration for instance of {class_name}")

            # Get field information
            all_fields, type_hints = _get_model_fields(self, class_name)
            if all_fields is None or type_hints is None:
                return self  # Return early if field introspection failed

            # Identify existing and missing fields
            existing_data, missing_fields = _identify_fields(self, all_fields)

            if not missing_fields:
                logger.info(f"No fields to hydrate for {class_name} instance.")
                return self

            logger.debug(f"{class_name}: Fields to hydrate: {missing_fields}")
            logger.debug(
                f"{class_name}: Existing data for context: {json.dumps(existing_data, default=str)}"
            )

            # Create agent signatures
            input_str, output_str, input_parts = _build_agent_signatures(
                existing_data,
                missing_fields,
                type_hints,
                all_fields,
                class_name,
            )

            # Create and run agent
            result = await _run_hydration_agent(
                self,
                input_str,
                output_str,
                input_parts,
                existing_data,
                class_name,
            )
            if result is None:
                return self  # Return early if agent run failed

            # Update object fields with results
            _update_fields_with_results(self, result, missing_fields, class_name)

            return self

        # --- Attach the sync hydrate method directly ---
        def hydrate(self) -> T:
            """Synchronous wrapper for the async hydrate method."""
            try:
                # Try to get the current running loop
                loop = asyncio.get_running_loop()

                # If we reach here, there is a running loop
                if loop.is_running():
                    # This runs the coroutine in the existing loop from a different thread
                    import concurrent.futures

                    with concurrent.futures.ThreadPoolExecutor() as executor:
                        future = executor.submit(asyncio.run, hydrate_async(self))
                        return future.result()
                else:
                    # There's a loop but it's not running
                    return loop.run_until_complete(hydrate_async(self))

            except RuntimeError:  # No running loop
                # If no loop is running, create a new one and run our coroutine
                return asyncio.run(hydrate_async(self))

        # Attach the methods to the class
        setattr(cls, "hydrate_async", hydrate_async)
        setattr(cls, "hydrate", hydrate)
        setattr(cls, "hydrate_sync", hydrate)  # Alias for backward compatibility

        logger.debug(f"Attached hydrate methods to class {cls.__name__}")
        return cls

    return decorator


def _get_model_fields(
    obj: BaseModel, class_name: str
) -> tuple[dict | None, dict | None]:
    """Extracts field information from a Pydantic model, handling v1/v2 compatibility."""
    try:
        if hasattr(obj, "model_fields"):  # Pydantic v2
            all_fields = obj.model_fields
            type_hints = {name: field.annotation for name, field in all_fields.items()}
        else:  # Pydantic v1 fallback
            type_hints = get_type_hints(obj.__class__)
            all_fields = getattr(obj, "__fields__", {name: None for name in type_hints})
        return all_fields, type_hints
    except Exception as e:
        logger.error(
            f"Could not get fields/type hints for {class_name}: {e}",
            exc_info=True,
        )
        return None, None


def _identify_fields(
    obj: BaseModel, all_fields: dict
) -> tuple[dict[str, Any], list[str]]:
    """Identifies existing (non-None) and missing fields in the object."""
    existing_data: dict[str, Any] = {}
    missing_fields: list[str] = []

    for field_name in all_fields:
        if hasattr(obj, field_name):  # Check if attribute exists
            value = getattr(obj, field_name)
            if value is not None:
                existing_data[field_name] = value
            else:
                missing_fields.append(field_name)

    return existing_data, missing_fields


def _build_agent_signatures(
    existing_data: dict[str, Any],
    missing_fields: list[str],
    type_hints: dict,
    all_fields: dict,
    class_name: str,
) -> tuple[str, str, list]:
    """Builds input and output signatures for the dynamic agent."""
    # Input signature based on existing data
    input_parts = []
    for name in existing_data:
        field_type = type_hints.get(name, Any)
        type_str = _format_type_to_string(field_type)
        field_info = all_fields.get(name)
        field_desc = getattr(field_info, "description", "")
        if field_desc:
            input_parts.append(f"{name}: {type_str} | {field_desc}")
        else:
            input_parts.append(f"{name}: {type_str}")

    input_str = (
        ", ".join(input_parts)
        if input_parts
        else "context_info: dict | Optional context if no fields have values"
    )

    # Output signature based on missing fields
    output_parts = []
    for name in missing_fields:
        field_type = type_hints.get(name, Any)
        type_str = _format_type_to_string(field_type)
        field_info = all_fields.get(name)
        field_desc = getattr(field_info, "description", "")
        if field_desc:
            output_parts.append(f"{name}: {type_str} | {field_desc}")
        else:
            output_parts.append(f"{name}: {type_str}")

    output_str = ", ".join(output_parts)

    return input_str, output_str, input_parts


async def _run_hydration_agent(
    obj: BaseModel,
    input_str: str,
    output_str: str,
    input_parts: list,
    existing_data: dict[str, Any],
    class_name: str,
) -> dict[str, Any] | None:
    """Creates and runs a dynamic Flock agent to hydrate the object."""
    # Agent configuration
    agent_name = f"hydrator_{class_name}_{id(obj)}"
    description = (
        getattr(obj, "__flock_agent_description__", None)
        or f"Agent that completes missing data for a {class_name} object."
    )
    hydration_model = getattr(obj, "__flock_model__", "openai/gpt-4o")

    logger.debug(f"Creating dynamic agent '{agent_name}' for {class_name}")
    logger.debug(f"  Input Schema: {input_str}")
    logger.debug(f"  Output Schema: {output_str}")

    try:
        # Create agent
        dynamic_agent = FlockFactory.create_default_agent(
            name=agent_name,
            description=description,
            input=input_str,
            output=output_str,
            model=hydration_model,
            no_output=True,
            use_cache=False,
        )

        # Create temporary Flock
        temp_flock = Flock(
            name=f"temp_hydrator_flock_{agent_name}",
            model=hydration_model,
            show_flock_banner=False,
        )
        temp_flock.add_agent(dynamic_agent)

        # Prepare input data
        agent_input_data = (
            existing_data
            if input_parts
            else {"context_info": {"object_type": class_name}}
        )

        logger.info(f"Running hydration agent '{agent_name}' for {class_name}...")

        # Run agent
        result = await temp_flock.run_async(
            start_agent=agent_name,
            input=agent_input_data,
            box_result=False,
        )
        logger.info(f"Hydration agent returned for {class_name}: {list(result.keys())}")

        return result

    except Exception as e:
        logger.error(
            f"Hydration agent creation or run failed for {class_name}: {e}",
            exc_info=True,
        )
        return None


def _update_fields_with_results(
    obj: BaseModel,
    result: dict[str, Any],
    missing_fields: list[str],
    class_name: str,
) -> None:
    """Updates object fields with results from the hydration agent."""
    updated_count = 0
    for field_name in missing_fields:
        if field_name in result:
            try:
                setattr(obj, field_name, result[field_name])
                logger.debug(
                    f"Hydrated field '{field_name}' in {class_name} with value: {getattr(obj, field_name)}"
                )
                updated_count += 1
            except Exception as e:
                logger.warning(
                    f"Failed to set hydrated value for '{field_name}' in {class_name}: {e}. Value received: {result[field_name]}"
                )
        else:
            logger.warning(
                f"Hydration result missing expected field for {class_name}: '{field_name}'"
            )

    logger.info(
        f"Hydration complete for {class_name}. Updated {updated_count}/{len(missing_fields)} fields."
    )


# Ensure helper functions are available
# from flock.core.serialization.serialization_utils import _format_type_to_string
```

### src\flock\core\util\input_resolver.py

- **Lines**: 127
- **Last modified**: 2025-05-25 23:17:06

```py
"""Utility functions for resolving input keys to their corresponding values."""

from flock.core.context.context import FlockContext
from flock.core.util.spliter import split_top_level


def get_callable_members(obj):
    """Extract all callable (methods/functions) members from a module or class.
    Returns a list of callable objects.
    """
    import inspect

    # Get all members of the object
    members = inspect.getmembers(obj)

    # Filter for callable members that don't start with underscore (to exclude private/special methods)
    callables = [
        member[1]
        for member in members
        if inspect.isroutine(member[1]) and not member[0].startswith("_")
    ]

    return callables


def _parse_keys(keys: list[str]) -> list[str]:
    """Split a comma‐separated string and strip any type annotations.

    For example, "a, b: list[str]" becomes ["a", "b"].
    """
    res_keys = []
    for key in keys:
        if "|" in key:
            key = key.split("|")[0].strip()
        if ":" in key:
            key = key.split(":")[0].strip()
        res_keys.append(key)
    return res_keys


def top_level_to_keys(s: str) -> list[str]:
    """Convert a top-level comma-separated string to a list of keys."""
    top_level_split = split_top_level(s)
    return _parse_keys(top_level_split)


def resolve_inputs(
    input_spec: str, context: FlockContext, previous_agent_name: str
) -> dict:
    """Build a dictionary of inputs based on the input specification string and the provided context.

    The lookup rules are:
      - "context" (case-insensitive): returns the entire context.
      - "context.property": returns an attribute from the context.
      - "def.agent_name": returns the agent definition for the given agent.
      - "agent_name": returns the most up2date record from the given agent's history.
      - "agent_name.property": returns the value of a property from the state variable keyed by "agent_name.property".
      - "property": searches the history for the most recent value of a property.
      - Otherwise, if no matching value is found, fallback to the FLOCK_INITIAL_INPUT.

    -> Recommendations:
        - prefix your agent variables with the agent name or a short handle to avoid conflicts.
        eg. agent name: "idea_agent", variable: "ia_idea" (ia = idea agent)
        - or set hand off mode to strict to avoid conflicts.
        with strict mode, the agent will only accept inputs from the previous agent.

    Args:
        input_spec: Comma-separated input keys (e.g., "query" or "agent_name.property").
        context: A FlockContext instance.

    Returns:
        A dictionary mapping each input key to its resolved value.
    """
    split_input = split_top_level(input_spec)
    keys = _parse_keys(split_input)
    inputs = {}

    for key in keys:
        split_key = key.split(".")

        # Case 1: A single key
        if len(split_key) == 1:
            # Special keyword: "context"
            if key.lower() == "context":
                inputs[key] = context
                continue

            # Try to get a historic record for an agent (if any)
            historic_records = context.get_agent_history(key)
            if historic_records:
                # You may choose to pass the entire record or just its data.
                inputs[key] = historic_records[0].data
                continue

            # Fallback to the most recent value in the state
            historic_value = context.get_most_recent_value(key)
            if historic_value is not None:
                inputs[key] = historic_value
                continue

            # Fallback to the initial input
            var_value = context.get_variable(key)
            if var_value is not None:
                inputs[key] = var_value
                continue

            inputs[key] = context.get_variable("flock." + key)

        # Case 2: A compound key (e.g., "agent_name.property" or "context.property")
        elif len(split_key) == 2:
            entity_name, property_name = split_key

            if entity_name.lower() == "context":
                # Try to fetch the attribute from the context
                inputs[key] = getattr(context, property_name, None)
                continue

            if entity_name.lower() == "def":
                # Return the agent definition for the given property name
                inputs[key] = context.get_agent_definition(property_name)
                continue

            # Otherwise, attempt to look up a state variable with the key "agent_name.property"
            inputs[key] = context.get_variable(f"{entity_name}.{property_name}")
            continue

    return inputs
```

### src\flock\core\util\loader.py

- **Lines**: 59
- **Last modified**: 2025-04-16 00:11:15

```py
# src/flock/core/loader.py
"""Provides functionality to load Flock instances from files."""

from pathlib import Path
from typing import TYPE_CHECKING

# Use TYPE_CHECKING to avoid runtime circular import if Flock imports this module indirectly
if TYPE_CHECKING:
    from flock.core.flock import Flock

# Import locally within the function to ensure Serializable methods are available
# from .serialization.serializable import Serializable # Serializable defines the file methods

# Cloudpickle check needs to be top-level
try:
    import cloudpickle

    PICKLE_AVAILABLE = True
except ImportError:
    PICKLE_AVAILABLE = False


def load_flock_from_file(file_path: str) -> "Flock":
    """Load a Flock instance from various file formats (detects type)."""
    # Import Flock locally within the function to avoid circular dependency at module level
    from flock.core.flock import Flock

    p = Path(file_path)
    if not p.exists():
        raise FileNotFoundError(f"Flock file not found: {file_path}")

    try:
        if p.suffix.lower() in [".yaml", ".yml"]:
            return Flock.from_yaml_file(p)
        elif p.suffix.lower() == ".json":
            # Assuming from_json is available via Serializable or directly on Flock
            return Flock.from_json(p.read_text())
        elif p.suffix.lower() == ".msgpack":
            # Assuming from_msgpack_file is available via Serializable or directly on Flock
            return Flock.from_msgpack_file(p)
        elif p.suffix.lower() == ".pkl":
            if PICKLE_AVAILABLE:
                # Assuming from_pickle_file is available via Serializable or directly on Flock
                return Flock.from_pickle_file(p)
            else:
                raise RuntimeError(
                    "Cannot load Pickle file: cloudpickle not installed."
                )
        else:
            raise ValueError(f"Unsupported file extension: {p.suffix}")
    except Exception as e:
        # Add specific error logging if helpful
        from flock.core.logging.logging import get_logger

        logger = get_logger("loader")
        logger.error(
            f"Error loading Flock from {file_path}: {e}", exc_info=True
        )
        raise  # Re-raise the original exception
```

### src\flock\core\util\spliter.py

- **Lines**: 219
- **Last modified**: 2025-05-25 23:17:06

```py
import re


def split_top_level(spec: str) -> list[str]:
    """Return raw field strings, split on *top-level* commas."""
    fields: list[str] = []
    start = 0
    depth = 0
    quote_char: str | None = None
    i = 0
    ident_re = re.compile(r"[A-Za-z_]\w*")  # cheap identifier

    while i < len(spec):
        ch = spec[i]

        # ---------- string handling ----------
        if quote_char:
            if ch == "\\":
                i += 2  # skip escaped char
                continue
            if ch == quote_char:
                quote_char = None
            i += 1
            continue

        if ch in {"'", '"'}:
            prev = spec[i - 1] if i else " "
            if (
                depth or prev.isspace() or prev in "=([{,:"
            ):  # looks like a quote
                quote_char = ch
            i += 1
            continue

        # ---------- bracket / brace / paren ----------
        if ch in "([{":
            depth += 1
            i += 1
            continue
        if ch in ")]}":
            depth = max(depth - 1, 0)
            i += 1
            continue

        # ---------- field separators ----------
        if ch == "," and depth == 0:
            j = i + 1
            while j < len(spec) and spec[j].isspace():
                j += 1
            if j >= len(spec):  # comma at end – split
                fields.append(spec[start:i].strip())
                start = i + 1
                i += 1
                continue

            id_match = ident_re.match(spec, j)
            if id_match:
                k = id_match.end()
                while k < len(spec) and spec[k].isspace():
                    k += 1
                if k >= len(spec) or spec[k] in {":", "|", ","}:
                    # confirmed: comma separates two fields
                    fields.append(spec[start:i].strip())
                    start = i + 1
                    i += 1
                    continue

        i += 1

    fields.append(spec[start:].strip())
    return [f for f in fields if f]  # prune empties


def parse_schema(spec: str) -> list[tuple[str, str, str]]:
    """Turn *spec* into a list of (name, python_type, description)."""
    result: list[tuple[str, str, str]] = []

    for field in split_top_level(spec):
        name = ""
        type_str = "str"  # default type
        description = ""

        name_part, _, desc_part = field.partition("|")
        description = desc_part.strip()
        main_part = name_part.strip()

        if ":" in main_part:
            name, type_candidate = main_part.split(":", 1)
            name = name.strip()
            type_candidate = type_candidate.strip()
            if type_candidate:
                type_str = type_candidate
        else:
            name = main_part  # keeps default type

        if name:  # skip broken pieces
            result.append((name, type_str, description))

    return result


# ------------------------------ demo ------------------------------
if __name__ == "__main__":
    SAMPLE_1 = (
        " name: str | The character's full name,"
        "race: str | The character's fantasy race,"
        "class: Literal['mage','thief'] | The character's profession,"
        "background: str | A brief backstory for the character"
    )

    SAMPLE_2 = (
        "field_with_internal_quotes: Literal['type_A', "
        '"type_B_with_\'_apostrophe"] | A literal with mixed quotes,'
        " another_field :str| A field with a description"
    )

    SAMPLE_3 = (
        "field_with_internal_quotes: Literal['type_A', "
        '"type_B_with_\'_apostrophe"] | A literal with mixed quotes,'
        " another_field | A field with a description"
    )

    SAMPLE_4 = "input, query, output"

    SAMPLE_5 = (
        "name: str | The character's full name,"
        "race: str | The character's fantasy race,"
        "class: Literal['mage','thief'] | The character's profession, which can be either mage or thief,"
        "background: str | A brief backstory for the character"
    )

    SAMPLE_6 = (
        "summary: str | A short blurb, e.g. key:value pairs that appear in logs"
    )
    # ➜ [('summary', 'str',
    #     'A short blurb, e.g. key:value pairs that appear in logs')]

    SAMPLE_7 = "path: str | The literal string 'C:\\\\Program Files\\\\My,App'"

    # ➜ [('path', 'str',
    #     "The literal string 'C:\\Program Files\\My,App'")]

    SAMPLE_8 = (
        "transform: Callable[[int, str], bool] | Function that returns True on success,"
        "retries: int | How many times to retry"
    )
    # ➜ ('transform', 'Callable[[int, str], bool]', 'Function that returns True on success')
    #    ('retries',   'int',                         'How many times to retry')

    SAMPLE_9 = (
        r"regex: str | Pattern such as r'^[A-Z\|a-z]+$',"
        "flags: int | re flags to use"
    )
    # ➜ ('regex', 'str', "Pattern such as r'^[A-Z\\|a-z]+$'")
    #    ('flags', 'int', 're flags to use')

    SAMPLE_10 = "id:int, name:str,"  # note the final comma!
    # ➜ ('id', 'int', '')
    #    ('name', 'str', '')

    SAMPLE_11 = "id:int | Primary key\nname:str | Display name\nactive:bool"
    # ➜ should not work!

    SAMPLE_12 = (
        'comment:str | The text "done | failed" goes here,'
        'status:Literal["done","failed"]'
    )
    # ➜ ('comment', 'str',    'The text "done | failed" goes here')
    #    ('status',  'Literal["done","failed"]', '')

    SAMPLE_13 = "choice: Literal['He said \\'yes\\'', 'no'] | User response"
    # ➜ ('choice', "Literal['He said \\'yes\\'', 'no']", 'User response')

    SAMPLE_14 = ""
    # ➜ []

    SAMPLE_15 = "username"
    # ➜ [('username', 'str', '')]

    SAMPLE_16 = (
        "payload: dict[str, list[dict[str, tuple[int,str]]]] "
        "| Arbitrarily complex structure"
    )
    # ➜ ('payload', 'dict[str, list[dict[str, tuple[int,str]]]]',
    #     'Arbitrarily complex structure')

    SAMPLE_17 = "münze: str | Deutsche Münzbezeichnung, engl. 'coin'"


    SAMPLE_18 = "ticket_info : str, reasoning : str, search_queries : list[str], relevant_documents: dict[str, float] | dict of pdf_ids as keys and scores as values"


    SAMPLE_19 = "title, headings: list[str], entities_and_metadata: list[dict[str, str]], type:Literal['news', 'blog', 'opinion piece', 'tweet']"
    # ➜ [('münze', 'str', "Deutsche Münzbezeichnung, engl. 'coin'")]

    for title, spec in [
        ("Sample-1", SAMPLE_1),
        ("Sample-2", SAMPLE_2),
        ("Sample-3", SAMPLE_3),
        ("Sample-4", SAMPLE_4),
        ("Sample-5", SAMPLE_5),
        ("Sample-6", SAMPLE_6),
        ("Sample-7", SAMPLE_7),
        ("Sample-8", SAMPLE_8),
        ("Sample-9", SAMPLE_9),
        ("Sample-10", SAMPLE_10),
        ("Sample-11", SAMPLE_11),
        ("Sample-12", SAMPLE_12),
        ("Sample-13", SAMPLE_13),
        ("Sample-14", SAMPLE_14),
        ("Sample-15", SAMPLE_15),
        ("Sample-16", SAMPLE_16),
        ("Sample-17", SAMPLE_17),
        ("Sample-18", SAMPLE_18),
        ("Sample-19", SAMPLE_19),
    ]:
        print(f"\n{title}")
        for row in parse_schema(spec):
            print(row)
```

### src\flock\di.py

- **Lines**: 41
- **Last modified**: 2025-05-21 19:51:15

```py
from __future__ import annotations

"""Flock – Dependency-Injection helpers.

This module provides a very small façade over the `wd.di` container so that
other parts of the codebase do not need to know where the active container is
stored.  The bootstrap code – usually located in the runner initialisation –
should store the `ServiceProvider` instance (returned by ``ServiceCollection.
build()``) on the `FlockContext` under the key ``di.container``.

Example
-------
>>> from wd.di import ServiceCollection
>>> sc = ServiceCollection()
>>> sc.add_singleton(str, lambda _: "hello")
>>> container = sc.build()
>>> ctx = FlockContext()
>>> ctx.set_variable("di.container", container)
>>> from flock.di import get_current_container
>>> assert get_current_container(ctx).get_service(str) == "hello"
"""

from typing import TYPE_CHECKING

if TYPE_CHECKING:  # pragma: no cover
    from wd.di.container import (
        ServiceProvider,  # noqa: F401 – import only for typing
    )

    from flock.core.context.context import FlockContext


def get_current_container(context: FlockContext | None = None):
    """Return the active `wd.di` container from *context* if present.

    If *context* is ``None`` or no container has been attached to it the
    function returns ``None``.
    """
    if context is None:
        return None
    return context.get_variable("di.container")
```

### src\flock\evaluators\__init__.py

- **Lines**: 1
- **Last modified**: 2025-05-21 19:51:15

```py
# Package for modules
```

### src\flock\evaluators\declarative\__init__.py

- **Lines**: 1
- **Last modified**: 2025-05-21 19:51:15

```py
# Package for modules
```

### src\flock\evaluators\declarative\declarative_evaluator.py

- **Lines**: 194
- **Last modified**: 2025-05-25 23:17:06

```py
from collections.abc import Generator
from typing import Any

from temporalio import workflow

with workflow.unsafe.imports_passed_through():
    import dspy

from pydantic import Field, PrivateAttr

from flock.core.flock_agent import FlockAgent
from flock.core.flock_evaluator import FlockEvaluator, FlockEvaluatorConfig
from flock.core.flock_registry import flock_component
from flock.core.logging.logging import get_logger
from flock.core.mixin.dspy_integration import DSPyIntegrationMixin
from flock.core.mixin.prompt_parser import PromptParserMixin

logger = get_logger("evaluators.declarative")


class DeclarativeEvaluatorConfig(FlockEvaluatorConfig):
    """Configuration for the DeclarativeEvaluator."""

    override_evaluator_type: str | None = None
    model: str | None = "openai/gpt-4o"
    use_cache: bool = True
    temperature: float = 0.0
    max_tokens: int = 4096
    max_retries: int = 3
    max_tool_calls: int = 10
    stream: bool = Field(
        default=False,
        description="Enable streaming output from the underlying DSPy program.",
    )
    include_thought_process: bool = Field(
        default=False,
        description="Include the thought process in the output.",
    )
    kwargs: dict[str, Any] = Field(default_factory=dict)


@flock_component(config_class=DeclarativeEvaluatorConfig)
class DeclarativeEvaluator(
    FlockEvaluator, DSPyIntegrationMixin, PromptParserMixin
):
    """Evaluator that uses DSPy for generation."""

    config: DeclarativeEvaluatorConfig = Field(
        default_factory=DeclarativeEvaluatorConfig,
        description="Evaluator configuration",
    )

    _cost: float = PrivateAttr(default=0.0)
    _lm_history: list = PrivateAttr(default_factory=list)

    # def __init__(self, name: str, config: DeclarativeEvaluatorConfig) -> None:
    #     super().__init__(name=name, config=config)
    # self._configure_language_model(
    #     model=config.model,
    #     use_cache=config.use_cache,
    #     temperature=config.temperature,
    #     max_tokens=config.max_tokens,
    # )

    async def evaluate(
        self,
        agent: FlockAgent,
        inputs: dict[str, Any],
        tools: list[Any],
        mcp_tools: list[Any] | None = None,
    ) -> dict[str, Any]:
        """Evaluate using DSPy, with optional asynchronous streaming."""
        # --- Setup Signature and LM ---

        with dspy.context(
            lm=dspy.LM(
                model=self.config.model or agent.model,
                cache=self.config.use_cache,
                temperature=self.config.temperature,
                max_tokens=self.config.max_tokens,
                num_retries=self.config.max_retries,
            )
        ):
            try:
                from rich.console import Console

                console = Console()
                _dspy_signature = self.create_dspy_signature_class(
                    agent.name,
                    agent.description,
                    f"{agent.input} -> {agent.output}",
                )
                # --- Get output field names ---
                # dspy.Signature holds fields in .output_fields attribute
                output_field_names = list(_dspy_signature.output_fields.keys())
                if not output_field_names:
                    logger.warning(
                        f"DSPy signature for agent '{agent.name}' has no defined output fields. Streaming might not produce text."
                    )
                # -----------------------------

                agent_task = self._select_task(
                    _dspy_signature,
                    override_evaluator_type=self.config.override_evaluator_type,
                    tools=tools,
                    max_tool_calls=self.config.max_tool_calls,
                    mcp_tools=mcp_tools,
                    kwargs=self.config.kwargs,
                )
            except Exception as setup_error:
                logger.error(
                    f"Error setting up DSPy task for agent '{agent.name}': {setup_error}",
                    exc_info=True,
                )
                raise RuntimeError(
                    f"DSPy task setup failed: {setup_error}"
                ) from setup_error

            # --- Conditional Evaluation (Stream vs No Stream) ---
            if self.config.stream:
                logger.info(
                    f"Evaluating agent '{agent.name}' with async streaming."
                )
                if not callable(agent_task):
                    logger.error("agent_task is not callable, cannot stream.")
                    raise TypeError(
                        "DSPy task could not be created or is not callable."
                    )

                streaming_task = dspy.streamify(
                    agent_task, is_async_program=True
                )
                stream_generator: Generator = streaming_task(**inputs)
                delta_content = ""

                console.print("\n")
                async for chunk in stream_generator:
                    if (
                        hasattr(chunk, "choices")
                        and chunk.choices
                        and hasattr(chunk.choices[0], "delta")
                        and chunk.choices[0].delta
                        and hasattr(chunk.choices[0].delta, "content")
                    ):
                        delta_content = chunk.choices[0].delta.content

                    if delta_content:
                        console.print(delta_content, end="")

                    result_dict, cost, lm_history = self._process_result(
                        chunk, inputs
                    )
                    self._cost = cost
                    self._lm_history = lm_history

                console.print("\n")
                return self.filter_thought_process(
                    result_dict, self.config.include_thought_process
                )

            else:  # Non-streaming path
                logger.info(
                    f"Evaluating agent '{agent.name}' without streaming."
                )
                try:
                    # Ensure the call is awaited if the underlying task is async
                    result_obj = await agent_task.acall(**inputs)
                    result_dict, cost, lm_history = self._process_result(
                        result_obj, inputs
                    )
                    self._cost = cost
                    self._lm_history = lm_history
                    return self.filter_thought_process(
                        result_dict, self.config.include_thought_process
                    )
                except Exception as e:
                    logger.error(
                        f"Error during non-streaming evaluation for agent '{agent.name}': {e}",
                        exc_info=True,
                    )
                    raise RuntimeError(f"Evaluation failed: {e}") from e

    def filter_thought_process(
        self, result_dict: dict[str, Any], include_thought_process: bool
    ) -> dict[str, Any]:
        """Filter out thought process from the result dictionary."""
        if include_thought_process:
            return result_dict
        else:
            return {
                k: v
                for k, v in result_dict.items()
                if not (k.startswith("reasoning") or k.startswith("trajectory"))
            }
```

### src\flock\evaluators\memory\memory_evaluator.py

- **Lines**: 90
- **Last modified**: 2025-04-19 00:19:04

```py
from typing import Any, Literal

from pydantic import Field

from flock.core.flock_agent import FlockAgent
from flock.core.flock_evaluator import FlockEvaluator, FlockEvaluatorConfig
from flock.core.flock_registry import flock_component
from flock.core.mixin.dspy_integration import DSPyIntegrationMixin
from flock.core.mixin.prompt_parser import PromptParserMixin
from flock.modules.memory.memory_module import MemoryModule, MemoryModuleConfig


class MemoryEvaluatorConfig(FlockEvaluatorConfig):
    folder_path: str = Field(
        default="concept_memory/",
        description="Directory where memory file and concept graph will be saved",
    )
    concept_graph_file: str = Field(
        default="concept_graph.png",
        description="Base filename for the concept graph image",
    )

    file_path: str | None = Field(
        default="agent_memory.json", description="Path to save memory file"
    )
    memory_mapping: str | None = Field(
        default=None, description="Memory mapping configuration"
    )
    similarity_threshold: float = Field(
        default=0.5, description="Threshold for semantic similarity"
    )
    max_length: int = Field(
        default=1000, description="Max length of memory entry before splitting"
    )
    save_after_update: bool = Field(
        default=True, description="Whether to save memory after each update"
    )
    splitting_mode: Literal["summary", "semantic", "characters", "none"] = (
        Field(default="none", description="Mode to split memory content")
    )
    enable_read_only_mode: bool = Field(
        default=False, description="Whether to enable read only mode"
    )
    number_of_concepts_to_extract: int = Field(
        default=3, description="Number of concepts to extract from the memory"
    )


@flock_component(config_class=MemoryEvaluatorConfig)
class MemoryEvaluator(FlockEvaluator, DSPyIntegrationMixin, PromptParserMixin):
    """Evaluator that uses DSPy for generation."""

    config: MemoryEvaluatorConfig = Field(
        default_factory=MemoryEvaluatorConfig,
        description="Evaluator configuration",
    )

    async def evaluate(
        self, agent: FlockAgent, inputs: dict[str, Any], tools: list[Any]
    ) -> dict[str, Any]:
        """Simple evaluator that uses a memory concept graph.

        if inputs contain "query", it searches memory for the query and returns the facts.
        if inputs contain "data", it adds the data to memory
        """
        result = {}
        memory_module = MemoryModule(
            name=self.name,
            config=MemoryModuleConfig(
                folder_path=self.config.folder_path,
                concept_graph_file=self.config.concept_graph_file,
                file_path=self.config.file_path,
                memory_mapping=self.config.memory_mapping,
                similarity_threshold=self.config.similarity_threshold,
                max_length=self.config.max_length,
                save_after_update=self.config.save_after_update,
                splitting_mode=self.config.splitting_mode,
                enable_read_only_mode=self.config.enable_read_only_mode,
                number_of_concepts_to_extract=self.config.number_of_concepts_to_extract,
            ),
        )

        if "query" in inputs:
            facts = await memory_module.search_memory(agent, inputs)
            result = {"facts": facts}

        if "data" in inputs:
            await memory_module.add_to_memory(agent, inputs)
            result = {"message": "Data added to memory"}
        return result
```

### src\flock\evaluators\test\test_case_evaluator.py

- **Lines**: 38
- **Last modified**: 2025-04-19 00:19:04

```py
from typing import Any

from pydantic import Field

from flock.core.flock_agent import FlockAgent
from flock.core.flock_evaluator import FlockEvaluator, FlockEvaluatorConfig
from flock.core.flock_registry import flock_component
from flock.core.mixin.dspy_integration import DSPyIntegrationMixin


class TestCaseEvaluatorConfig(FlockEvaluatorConfig):
    """Configuration for the TestCaseEvaluator."""

    pass


@flock_component(config_class=TestCaseEvaluatorConfig)
class TestCaseEvaluator(FlockEvaluator, DSPyIntegrationMixin):
    """Evaluator for test cases."""

    config: TestCaseEvaluatorConfig = Field(
        default_factory=TestCaseEvaluatorConfig,
        description="Evaluator configuration",
    )

    async def evaluate(
        self, agent: FlockAgent, inputs: dict[str, Any], tools: list[Any]
    ) -> dict[str, Any]:
        _dspy_signature = self.create_dspy_signature_class(
            agent.name,
            agent.description,
            f"{agent.input} -> {agent.output}",
        )
        output_field_names = list(_dspy_signature.output_fields.keys())
        result = {}
        for output_field_name in output_field_names:
            result[output_field_name] = "Test Result"
        return result
```

### src\flock\evaluators\zep\zep_evaluator.py

- **Lines**: 59
- **Last modified**: 2025-04-19 00:19:04

```py
from typing import Any

from pydantic import Field

from flock.core.flock_agent import FlockAgent
from flock.core.flock_evaluator import FlockEvaluator, FlockEvaluatorConfig
from flock.core.flock_registry import flock_component
from flock.core.mixin.dspy_integration import DSPyIntegrationMixin
from flock.core.mixin.prompt_parser import PromptParserMixin
from flock.modules.zep.zep_module import ZepModule, ZepModuleConfig


class ZepEvaluatorConfig(FlockEvaluatorConfig):
    zep_url: str = "http://localhost:8000"
    zep_api_key: str = "apikey"
    min_fact_rating: float = Field(
        default=0.7, description="Minimum rating for facts to be considered"
    )


@flock_component(config_class=ZepEvaluatorConfig)
class ZepEvaluator(FlockEvaluator, DSPyIntegrationMixin, PromptParserMixin):
    """Evaluator that uses DSPy for generation."""

    config: ZepEvaluatorConfig = Field(
        default_factory=ZepEvaluatorConfig,
        description="Evaluator configuration",
    )

    async def evaluate(
        self, agent: FlockAgent, inputs: dict[str, Any], tools: list[Any]
    ) -> dict[str, Any]:
        """Simple evaluator that uses Zep.

        if inputs contain "query", it searches memory for the query and returns the facts.
        if inputs contain "data", it adds the data to memory
        """
        result = {}
        zep = ZepModule(
            name=self.name,
            config=ZepModuleConfig(
                zep_api_key=self.config.zep_api_key,
                zep_url=self.config.zep_url,
                min_fact_rating=self.config.min_fact_rating,
                enable_read=True,
                enable_write=True,
            ),
        )
        client = zep.get_client()
        if "query" in inputs:
            query = inputs["query"]
            facts = zep.search_memory(query, client)
            result = {"facts": facts}

        if "data" in inputs:
            data = inputs["data"]
            zep.add_to_memory(data, client)
            result = {"message": "Data added to memory"}
        return result
```

### src\flock\mcp\servers\sse\__init__.py

- **Lines**: 1
- **Last modified**: 2025-05-22 21:27:37

```py
"""Default SSE Server Implementation for Flock."""
```

### src\flock\mcp\servers\sse\flock_sse_server.py

- **Lines**: 139
- **Last modified**: 2025-05-26 13:19:27

```py
"""This module provides the Flock SSE Server functionality."""

import copy
from contextlib import AbstractAsyncContextManager
from typing import Any, Literal

from anyio.streams.memory import (
    MemoryObjectReceiveStream,
    MemoryObjectSendStream,
)
from mcp.client.sse import sse_client
from mcp.types import JSONRPCMessage
from opentelemetry import trace
from pydantic import Field

from flock.core.logging.logging import get_logger
from flock.core.mcp.flock_mcp_server import FlockMCPServerBase
from flock.core.mcp.mcp_client import FlockMCPClientBase
from flock.core.mcp.mcp_client_manager import FlockMCPClientManagerBase
from flock.core.mcp.mcp_config import (
    FlockMCPConfigurationBase,
    FlockMCPConnectionConfigurationBase,
)
from flock.core.mcp.types.types import SseServerParameters

logger = get_logger("mcp.sse.server")
tracer = trace.get_tracer(__name__)


class FlockSSEConnectionConfig(FlockMCPConnectionConfigurationBase):
    """Concrete ConnectionConfig for an SSEClient."""

    # Only thing we need to override here is the concrete transport_type
    # and connection_parameters fields.
    transport_type: Literal["sse"] = Field(
        default="sse", description="Use the sse transport type."
    )

    connection_parameters: SseServerParameters = Field(
        ..., description="SSE Server Connection Parameters."
    )


class FlockSSEConfig(FlockMCPConfigurationBase):
    """Configuration for SSE Clients."""

    # The only thing we need to override here is the concrete
    # connection config. The rest is generic enough to handle
    # everything else.
    connection_config: FlockSSEConnectionConfig = Field(
        ..., description="Concrete SSE Connection Configuration."
    )


class FlockSSEClient(FlockMCPClientBase):
    """Client for SSE Servers."""

    config: FlockSSEConfig = Field(..., description="Client configuration.")

    async def create_transport(
        self,
        params: SseServerParameters,
        additional_params: dict[str, Any] | None = None,
    ) -> AbstractAsyncContextManager[
        tuple[
            MemoryObjectReceiveStream[JSONRPCMessage | Exception],
            MemoryObjectSendStream[JSONRPCMessage],
        ]
    ]:
        """Return an async context manager whose __aenter__ method yields (read_stream, send_stream)."""
        # avoid modifying the config of the client as a side-effect.
        param_copy = copy.deepcopy(params)

        if self.additional_params:
            override_headers = bool(
                self.additional_params.get("override_headers", False)
            )
            if "headers" in self.additional_params:
                if override_headers:
                    param_copy.headers = self.additional_params.get(
                        "headers", params.headers
                    )
                else:
                    param_copy.headers.update(
                        self.additional_params.get("headers", {})
                    )
            if "read_timeout_seconds" in self.additional_params:
                param_copy.timeout = self.additional_params.get(
                    "read_timeout_seconds", params.timeout
                )

            if "sse_read_timeout" in self.additional_params:
                param_copy.sse_read_timeout = self.additional_params.get(
                    "sse_read_timeout",
                    params.sse_read_timeout,
                )
            if "url" in self.additional_params:
                param_copy.url = self.additional_params.get(
                    "url",
                    params.url,
                )

        return sse_client(
            url=param_copy.url,
            headers=param_copy.headers,
            timeout=float(param_copy.timeout),
            sse_read_timeout=float(param_copy.sse_read_timeout),
        )


class FlockSSEClientManager(FlockMCPClientManagerBase):
    """Manager for handling SSE Clients."""

    client_config: FlockSSEConfig = Field(
        ..., description="Configuration for clients."
    )

    async def make_client(
        self, additional_params: dict[str, Any]
    ) -> FlockSSEClient:
        """Create a new client instance."""
        new_client = FlockSSEClient(
            config=self.client_config, additional_params=additional_params
        )
        return new_client


class FlockSSEServer(FlockMCPServerBase):
    """Class which represents a MCP Server using the SSE Transport type."""

    config: FlockSSEConfig = Field(..., description="Config for the server.")

    async def initialize(self) -> FlockSSEClientManager:
        """Called when initializing the server."""
        client_manager = FlockSSEClientManager(
            client_config=self.config,
        )

        return client_manager
```

### src\flock\mcp\servers\stdio\__init__.py

- **Lines**: 1
- **Last modified**: 2025-05-22 21:27:37

```py
"""Default Stdio Server Implementation for Flock."""
```

### src\flock\mcp\servers\stdio\flock_stdio_server.py

- **Lines**: 138
- **Last modified**: 2025-05-22 21:27:37

```py
"""This module provides the default implementation for MCP servers using the stdio transport."""

import copy
from contextlib import AbstractAsyncContextManager
from typing import Any, Literal

from anyio.streams.memory import (
    MemoryObjectReceiveStream,
    MemoryObjectSendStream,
)
from mcp import stdio_client
from mcp.types import JSONRPCMessage
from opentelemetry import trace
from pydantic import Field

from flock.core.logging.logging import get_logger
from flock.core.mcp.flock_mcp_server import FlockMCPServerBase
from flock.core.mcp.mcp_client import FlockMCPClientBase
from flock.core.mcp.mcp_client_manager import FlockMCPClientManagerBase
from flock.core.mcp.mcp_config import (
    FlockMCPConfigurationBase,
    FlockMCPConnectionConfigurationBase,
)
from flock.core.mcp.types.types import StdioServerParameters

logger = get_logger("mcp.stdio.server")
tracer = trace.get_tracer(__name__)


class FlockStdioConnectionConfig(FlockMCPConnectionConfigurationBase):
    """Concrete ConnectionConfig for an StdioClient."""

    # Only thing we need to override here is the concrete transport_type
    # and connection_parameters fields.
    transport_type: Literal["stdio"] = Field(
        default="stdio", description="Use the stdio transport type."
    )

    connection_parameters: StdioServerParameters = Field(
        ...,
        description="StdioServerParameters to be used for the stdio transport.",
    )


class FlockStdioConfig(FlockMCPConfigurationBase):
    """Configuration for Stdio Clients."""

    # The only thing we need to override here is the
    # concrete connection config. The rest is generic
    # enough to handle everything else.
    connection_config: FlockStdioConnectionConfig = Field(
        ..., description="Concrete Stdio Connection Configuration."
    )


class FlockStdioClient(FlockMCPClientBase):
    """Client for Stdio Servers."""

    config: FlockStdioConfig = Field(..., description="Client Configuration.")

    async def create_transport(
        self,
        params: StdioServerParameters,
        additional_params: dict[str, Any] | None = None,
    ) -> AbstractAsyncContextManager[
        tuple[
            MemoryObjectReceiveStream[JSONRPCMessage | Exception],
            MemoryObjectSendStream[JSONRPCMessage],
        ]
    ]:
        """Return an async context manager whose __aenter__ method yields (read_stream, send_stream)."""
        # additional_params take precedence over passed config, as modules can influence
        # how to connect to a stdio server.

        # avoid modifying the config of the client as a side-effect.
        param_copy = copy.deepcopy(params)

        if additional_params:
            # If it is present, then modify server parameters based on certain keys.
            if "command" in additional_params:
                param_copy.command = additional_params.get(
                    "command", params.command
                )
            if "args" in additional_params:
                param_copy.args = additional_params.get("args", params.command)
            if "env" in additional_params:
                param_copy.env = additional_params.get("env", params.env)

            if "cwd" in additional_params:
                param_copy.cwd = additional_params.get("cwd", params.env)

            if "encoding" in additional_params:
                param_copy.encoding = additional_params.get(
                    "encoding", params.encoding
                )

            if "encoding_error_handler" in additional_params:
                param_copy.encoding_error_handler = additional_params.get(
                    "encoding_error_handler", params.encoding_error_handler
                )

        # stdio_client already is an AsyncContextManager
        return stdio_client(server=param_copy)


# Not really needed but kept here as an example.
class FlockStdioClientManager(FlockMCPClientManagerBase):
    """Manager for handling Stdio Clients."""

    client_config: FlockStdioConfig = Field(
        ..., description="Configuration for clients."
    )

    async def make_client(
        self, additional_params: dict[str, Any] | None = None
    ):
        """Create a new client instance with any additional parameters."""
        new_client = FlockStdioClient(
            config=self.client_config,
            additional_params=additional_params,
        )
        return new_client


class FlockMCPStdioServer(FlockMCPServerBase):
    """Class which represents a MCP Server using the Stdio Transport type.

    This means (most likely) that the server is a locally
    executed script.
    """

    config: FlockStdioConfig = Field(..., description="Config for the server.")

    async def initialize(self) -> FlockStdioClientManager:
        """Called when initializing the server."""
        client_manager = FlockStdioClientManager(client_config=self.config)

        return client_manager
```

### src\flock\mcp\servers\websockets\__init__.py

- **Lines**: 1
- **Last modified**: 2025-05-22 21:27:37

```py
"""Default Websocket Server Implementation."""
```

### src\flock\mcp\servers\websockets\flock_websocket_server.py

- **Lines**: 119
- **Last modified**: 2025-05-26 13:19:27

```py
"""This module provides the default implementation for MCP servers using the websocket transport."""

import copy
from contextlib import AbstractAsyncContextManager
from typing import Any, Literal

from anyio.streams.memory import (
    MemoryObjectReceiveStream,
    MemoryObjectSendStream,
)
from mcp.client.websocket import websocket_client
from mcp.types import JSONRPCMessage
from opentelemetry import trace
from pydantic import Field

from flock.core.logging.logging import get_logger
from flock.core.mcp.flock_mcp_server import FlockMCPServerBase
from flock.core.mcp.mcp_client import FlockMCPClientBase
from flock.core.mcp.mcp_client_manager import FlockMCPClientManagerBase
from flock.core.mcp.mcp_config import (
    FlockMCPConfigurationBase,
    FlockMCPConnectionConfigurationBase,
)
from flock.core.mcp.types.types import WebsocketServerParameters

logger = get_logger("mcp.ws.server")
tracer = trace.get_tracer(__name__)


# Optional to provide type hints.
class FlockWSConnectionConfig(FlockMCPConnectionConfigurationBase):
    """Concrete ConnectionConfig for a WS Client."""

    # Only thing we need to override here is the concrete transport_type
    # and connection_parameters fields.
    transport_type: Literal["websockets"] = Field(
        default="websockets", description="Use the websockets transport type."
    )

    connection_parameters: WebsocketServerParameters = Field(
        ...,
        description="WebsocketServer parameters to be used for the websocket transport.",
    )


# Optional to provide type hints.
class FlockWSConfig(FlockMCPConfigurationBase):
    """Configuration for Websocket clients."""

    # The only thing we need to override here is the concrete
    # connection config. The rest is generic enough to handle
    # everything else. (This is just here so that type hints work for the
    # rest of the implementation, we could just omit this override entirely.)
    connection_config: FlockWSConnectionConfig = Field(
        ..., description="Concrete WS connection configuration"
    )


class FlockWSClient(FlockMCPClientBase):
    """Client for Websocket servers."""

    config: FlockWSConfig = Field(..., description="Client Configuration")

    # This one we HAVE to specify. This tells Flock
    # how to create the underlying connection.
    async def create_transport(
        self,
        params: WebsocketServerParameters,
        additional_params: dict[str, Any] | None = None,
    ) -> AbstractAsyncContextManager[
        tuple[
            MemoryObjectReceiveStream[JSONRPCMessage | Exception],
            MemoryObjectSendStream[JSONRPCMessage],
        ]
    ]:
        """Return an async context manager whose __aenter__ method yields a read_stream and a send_stream."""
        # additional_params take precedence over passed config, as modules
        # can influece how to connect to a ws server.

        # avoid modifying the underlying config directly
        param_copy = copy.deepcopy(params)

        if additional_params and "url" in additional_params:
            # If present, then apply the changes in "url" to the create_transport logic.
            param_copy.url = additional_params.get("url", params.url)

        return websocket_client(
            url=param_copy.url
        )  # return the async context manager


# not really needed, but kept for type hints and as an example.
class FlockWSClientManager(FlockMCPClientManagerBase):
    """Manager for handling websocket clients."""

    client_config: FlockWSConfig = Field(
        ..., description="Configuration for clients."
    )

    async def make_client(self, additional_params=None):
        """Create a new client instance."""
        new_client = FlockWSClient(
            config=self.client_config,
            additional_params=additional_params,
        )
        return new_client


class FlockWSServer(FlockMCPServerBase):
    """Class which represents an MCP Server using the websocket transport type."""

    config: FlockWSConfig = Field(..., description="Config for the server.")

    # Specify the concrete type for the server.
    async def initialize(self) -> FlockWSClientManager:
        """Called when initializing the server."""
        client_manager = FlockWSClientManager(client_config=self.config)

        return client_manager
```

### src\flock\modules\__init__.py

- **Lines**: 1
- **Last modified**: 2025-05-21 19:51:15

```py
# Package for modules
```

### src\flock\modules\assertion\__init__.py

- **Lines**: 1
- **Last modified**: 2025-05-21 19:51:15

```py
# Package for modules
```

### src\flock\modules\assertion\assertion_module.py

- **Lines**: 286
- **Last modified**: 2025-05-06 21:09:36

```py
# src/flock/modules/asserts/assertion_module.py (New File)

import json
from collections.abc import Callable
from typing import Any, Literal

import dspy  # For potential LLM-based rule checking
from pydantic import BaseModel, Field, PrivateAttr, ValidationError

from flock.core.context.context import FlockContext
from flock.core.flock_agent import FlockAgent
from flock.core.flock_module import FlockModule, FlockModuleConfig

# Need registry access if rules are callables defined elsewhere
from flock.core.flock_registry import flock_component, get_registry
from flock.core.logging.logging import get_logger

logger = get_logger("module.assertion")

# --- Rule Definition ---
# Rules can be defined in several ways:
# 1. Python lambda/function: (result: Dict, inputs: Dict, context: FlockContext) -> bool | Tuple[bool, str]
# 2. String referencing a registered callable: "my_validation_function"
# 3. Natural language rule string: "The summary must contain the keyword 'Flock'." (requires LLM judge)
# 4. Pydantic Model: The output must conform to this Pydantic model.

RuleType = (
    Callable[[dict, dict, FlockContext | None], bool | tuple[bool, str]]
    | str
    | type[BaseModel]
)


class Rule(BaseModel):
    """Container for a single assertion rule."""

    condition: RuleType = Field(
        ...,
        description="""
# --- Rule Definition ---
# Rules can be defined in several ways:
# 1. Python lambda/function: (result: Dict, inputs: Dict, context: FlockContext) -> bool | Tuple[bool, str]
# 2. String referencing a registered callable: "my_validation_function"
# 3. Natural language rule string: "The summary must contain the keyword 'Flock'." (requires LLM judge)
# 4. Pydantic Model: The output must conform to this Pydantic model.
                                """,
    )
    fail_message: str  # Message to provide as feedback on failure
    name: str | None = None  # Optional name for clarity

    def __post_init__(self):
        # Basic validation of fail_message
        if not isinstance(self.fail_message, str) or not self.fail_message:
            raise ValueError("Rule fail_message must be a non-empty string.")


class AssertionModuleConfig(FlockModuleConfig):
    """--- Rule Definition ---
    Rules can be defined in several ways:
    1. Python lambda/function: (result: Dict, inputs: Dict, context: FlockContext) -> bool | Tuple[bool, str]
    2. String referencing a registered callable: "my_validation_function"
    3. Natural language rule string: "The summary must contain the keyword 'Flock'." (requires LLM judge)
    4. Pydantic Model: The output must conform to this Pydantic model.
    """

    rules: list[Rule] = Field(
        default_factory=list,
        description="List of rules to check against the agent's output.",
    )
    # Optional LLM for evaluating natural language rules
    judge_lm_model: str | None = Field(
        None, description="LLM model to use for judging natural language rules."
    )
    # How to handle failure
    on_failure: Literal["add_feedback", "raise_error", "log_warning"] = Field(
        default="add_feedback",
        description="Action on rule failure: 'add_feedback' to context, 'raise_error', 'log_warning'.",
    )
    feedback_context_key: str = Field(
        default="flock.assertion_feedback",
        description="Context key to store failure messages for retry loops.",
    )
    clear_feedback_on_success: bool = Field(
        default=True,
        description="Clear the feedback key from context if all assertions pass.",
    )


@flock_component(config_class=AssertionModuleConfig)
class AssertionCheckerModule(FlockModule):
    """Checks the output of an agent against a set of defined rules.

    Can trigger different actions on failure, including adding feedback
    to the context to enable self-correction loops via routing.
    """

    name: str = "assertion_checker"
    config: AssertionModuleConfig = Field(default_factory=AssertionModuleConfig)
    _judge_lm: dspy.LM | None = PrivateAttr(None)  # Initialize lazily

    def _get_judge_lm(self) -> dspy.LM | None:
        """Initializes the judge LM if needed."""
        if self.config.judge_lm_model and self._judge_lm is None:
            try:
                self._judge_lm = dspy.LM(self.config.judge_lm_model)
            except Exception as e:
                logger.error(
                    f"Failed to initialize judge LM '{self.config.judge_lm_model}': {e}"
                )
                # Proceed without judge LM for other rule types
        return self._judge_lm

    async def on_post_evaluate(
        self,
        agent: FlockAgent,
        inputs: dict[str, Any],
        context: FlockContext | None = None,
        result: dict[str, Any] | None = None,
    ) -> dict[str, Any]:
        """Checks rules after the main evaluator runs."""
        if not self.config.rules:
            return result  # No rules to check

        logger.debug(f"Running assertion checks for agent '{agent.name}'...")
        all_passed = True
        failed_messages = []
        registry = get_registry()  # Needed for callable lookup

        for i, rule in enumerate(self.config.rules):
            rule_name = rule.name or f"Rule_{i + 1}"
            passed = False
            eval_result = None
            feedback_msg = rule.fail_message

            try:
                condition = rule.condition
                if callable(condition):
                    # Rule is a Python function/lambda
                    logger.debug(f"Checking callable rule: {rule_name}")
                    eval_result = condition(result, inputs, context)
                elif isinstance(condition, str) and registry.contains(
                    condition
                ):
                    # Rule is a string referencing a registered callable
                    logger.debug(
                        f"Checking registered callable rule: '{condition}'"
                    )
                    rule_func = registry.get_callable(condition)
                    eval_result = rule_func(result, inputs, context)
                elif isinstance(condition, str):
                    # Rule is a natural language string (requires judge LLM)
                    logger.debug(
                        f"Checking natural language rule: '{condition}'"
                    )
                    judge_lm = self._get_judge_lm()
                    if judge_lm:
                        # Define a simple judge signature dynamically or use a predefined one
                        class JudgeSignature(dspy.Signature):
                            """Evaluate if the output meets the rule based on input and output."""

                            program_input: str = dspy.InputField(
                                desc="Input provided to the agent."
                            )
                            program_output: str = dspy.InputField(
                                desc="Output generated by the agent."
                            )
                            rule_to_check: str = dspy.InputField(
                                desc="The rule to verify."
                            )
                            is_met: bool = dspy.OutputField(
                                desc="True if the rule is met, False otherwise."
                            )
                            reasoning: str = dspy.OutputField(
                                desc="Brief reasoning for the decision."
                            )

                        judge_predictor = dspy.Predict(
                            JudgeSignature, llm=judge_lm
                        )
                        # Convert complex dicts/lists to strings for the judge prompt
                        input_str = json.dumps(inputs, default=str, indent=2)
                        result_str = json.dumps(result, default=str, indent=2)
                        judge_pred = judge_predictor(
                            program_input=input_str,
                            program_output=result_str,
                            rule_to_check=condition,
                        )
                        passed = judge_pred.is_met
                        feedback_msg = f"{rule.fail_message} (Reason: {judge_pred.reasoning})"
                        logger.debug(
                            f"LLM Judge result for rule '{condition}': {passed} ({judge_pred.reasoning})"
                        )
                    else:
                        logger.warning(
                            f"Cannot evaluate natural language rule '{condition}' - no judge_lm_model configured."
                        )
                        passed = True  # Default to pass if no judge available? Or fail? Let's pass.

                elif isinstance(condition, type) and issubclass(
                    condition, BaseModel
                ):
                    # Rule is a Pydantic model for validation
                    logger.debug(
                        f"Checking Pydantic validation rule: {condition.__name__}"
                    )
                    try:
                        # Assumes the *entire* result dict should match the model
                        # More specific logic might be needed (e.g., validate only a specific key)
                        condition.model_validate(result)
                        passed = True
                    except ValidationError as e:
                        passed = False
                        feedback_msg = (
                            f"{rule.fail_message} (Validation Error: {e})"
                        )
                else:
                    logger.warning(
                        f"Unsupported rule type for rule '{rule_name}': {type(condition)}"
                    )
                    continue  # Skip rule

                # Process result if it was a callable returning bool or (bool, msg)
                if eval_result is not None:
                    if (
                        isinstance(eval_result, tuple)
                        and len(eval_result) == 2
                        and isinstance(eval_result[0], bool)
                    ):
                        passed, custom_msg = eval_result
                        if not passed and custom_msg:
                            feedback_msg = (
                                custom_msg  # Use custom message on failure
                            )
                    elif isinstance(eval_result, bool):
                        passed = eval_result
                    else:
                        logger.warning(
                            f"Rule callable '{rule_name}' returned unexpected type: {type(eval_result)}. Rule skipped."
                        )
                        continue

                # Handle failure
                if not passed:
                    all_passed = False
                    failed_messages.append(feedback_msg)
                    logger.warning(
                        f"Assertion Failed for agent '{agent.name}': {feedback_msg}"
                    )
                    # Optionally break early? For now, check all rules.

            except Exception as e:
                logger.error(
                    f"Error executing rule '{rule_name}' for agent '{agent.name}': {e}",
                    exc_info=True,
                )
                all_passed = False
                failed_messages.append(
                    f"Error checking rule '{rule_name}': {e}"
                )
                # Treat error during check as failure

        # --- Take action based on results ---
        if not all_passed:
            logger.warning(f"Agent '{agent.name}' failed assertion checks.")
            if self.config.on_failure == "add_feedback" and context:
                context.set_variable(
                    self.config.feedback_context_key, "\n".join(failed_messages)
                )
                logger.debug(
                    f"Added assertion feedback to context key '{self.config.feedback_context_key}'"
                )
            elif self.config.on_failure == "raise_error":
                # Maybe wrap in a specific FlockAssertionError
                raise AssertionError(
                    f"Agent '{agent.name}' failed assertions: {'; '.join(failed_messages)}"
                )
            # else "log_warning" is default behavior
        elif context and self.config.clear_feedback_on_success:
            # Clear feedback key if all rules passed and key exists
            if self.config.feedback_context_key in context.state:
                del context.state[self.config.feedback_context_key]
                logger.debug(
                    f"Cleared assertion feedback key '{self.config.feedback_context_key}' on success."
                )

        return result  # Return the original result unmodified
```

### src\flock\modules\callback\__init__.py

- **Lines**: 1
- **Last modified**: 2025-05-21 19:51:15

```py
# Package for modules
```

### src\flock\modules\callback\callback_module.py

- **Lines**: 91
- **Last modified**: 2025-05-06 21:09:36

```py
"""Callback module for handling agent lifecycle hooks."""

from collections.abc import Awaitable, Callable
from typing import Any

from pydantic import Field

from flock.core import FlockModule, FlockModuleConfig
from flock.core.context.context import FlockContext
from flock.core.flock_registry import flock_component


class CallbackModuleConfig(FlockModuleConfig):
    """Configuration for callback module."""

    initialize_callback: (
        Callable[[Any, dict[str, Any]], Awaitable[None]] | None
    ) = Field(
        default=None,
        description="Optional callback function for initialization",
    )
    evaluate_callback: (
        Callable[[Any, dict[str, Any]], Awaitable[dict[str, Any]]] | None
    ) = Field(
        default=None, description="Optional callback function for evaluate"
    )
    terminate_callback: (
        Callable[[Any, dict[str, Any], dict[str, Any]], Awaitable[None]] | None
    ) = Field(
        default=None, description="Optional callback function for termination"
    )
    on_error_callback: (
        Callable[[Any, Exception, dict[str, Any]], Awaitable[None]] | None
    ) = Field(
        default=None,
        description="Optional callback function for error handling",
    )


@flock_component(config_class=CallbackModuleConfig)
class CallbackModule(FlockModule):
    """Module that provides callback functionality for agent lifecycle events."""

    name: str = "callbacks"
    config: CallbackModuleConfig = Field(
        default_factory=CallbackModuleConfig,
        description="Callback module configuration",
    )

    async def pre_initialize(
        self,
        agent: Any,
        inputs: dict[str, Any],
        context: FlockContext | None = None,
    ) -> None:
        """Run initialize callback if configured."""
        if self.config.initialize_callback:
            await self.config.initialize_callback(agent, inputs)

    async def on_pre_evaluate(
        self,
        agent: Any,
        inputs: dict[str, Any],
        context: FlockContext | None = None,
    ) -> dict[str, Any]:
        """Run evaluate callback if configured."""
        if self.config.evaluate_callback:
            return await self.config.evaluate_callback(agent, inputs)
        return inputs

    async def pre_terminate(
        self,
        agent: Any,
        inputs: dict[str, Any],
        context: FlockContext | None = None,
        result: dict[str, Any] | None = None,
    ) -> None:
        """Run terminate callback if configured."""
        if self.config.terminate_callback:
            await self.config.terminate_callback(agent, inputs, result)

    async def on_error(
        self,
        agent: Any,
        error: Exception,
        inputs: dict[str, Any],
        context: FlockContext | None = None,
    ) -> None:
        """Run error callback if configured."""
        if self.config.on_error_callback:
            await self.config.on_error_callback(agent, error, inputs)
```

### src\flock\modules\enterprise_memory\enterprise_memory_module.py

- **Lines**: 526
- **Last modified**: 2025-05-21 19:51:15

```py
from __future__ import annotations

"""Enterprise-grade memory module for Flock.

This module persists:
• vector embeddings in a Chroma collection (or any collection that
  implements the same API)
• a concept graph in Neo4j/Memgraph (Cypher-compatible)

It follows the same life-cycle callbacks as the standard MemoryModule but
is designed for large-scale, concurrent deployments.
"""

import asyncio
import json
import time
import uuid
from pathlib import Path
from typing import Any, Literal

from neo4j import AsyncGraphDatabase
from opentelemetry import trace
from pydantic import Field
from sentence_transformers import SentenceTransformer

from flock.adapter.azure_adapter import AzureSearchAdapter
from flock.adapter.chroma_adapter import ChromaAdapter
from flock.adapter.faiss_adapter import FAISSAdapter
from flock.adapter.pinecone_adapter import PineconeAdapter

# Adapter imports
from flock.adapter.vector_base import VectorAdapter
from flock.core.context.context import FlockContext
from flock.core.flock_agent import FlockAgent
from flock.core.flock_module import FlockModule, FlockModuleConfig
from flock.core.flock_registry import flock_component
from flock.core.logging.logging import get_logger
from flock.modules.performance.metrics_module import MetricsModule

logger = get_logger("enterprise_memory")
tracer = trace.get_tracer(__name__)


# ---------------------------------------------------------------------------
# Configuration
# ---------------------------------------------------------------------------
class EnterpriseMemoryModuleConfig(FlockModuleConfig):
    """Configuration for EnterpriseMemoryModule."""

    # ---------------------
    # Vector store settings
    # ---------------------

    vector_backend: Literal["chroma", "pinecone", "azure"] = Field(
        default="chroma",
        description="Which vector backend to use (chroma | pinecone | azure)",
    )

    # --- Chroma ---
    chroma_path: str | None = Field(
        default="./vector_store",
        description="Disk path for Chroma persistent storage (if running embedded).",
    )
    chroma_collection: str = Field(
        default="flock_memories", description="Chroma collection name"
    )
    chroma_host: str | None = Field(
        default=None,
        description="If provided, connect to a remote Chroma HTTP server at this host",
    )
    chroma_port: int = Field(default=8000, description="Remote Chroma HTTP port")

    # --- Pinecone ---
    pinecone_api_key: str | None = Field(default=None, description="Pinecone API key")
    pinecone_env: str | None = Field(default=None, description="Pinecone environment")
    pinecone_index: str | None = Field(default=None, description="Pinecone index name")

    # --- Azure Cognitive Search ---
    azure_search_endpoint: str | None = Field(default=None, description="Azure search endpoint (https://<service>.search.windows.net)")
    azure_search_key: str | None = Field(default=None, description="Azure search admin/key")
    azure_search_index_name: str | None = Field(default=None, description="Azure search index name")

    # Graph DB (Neo4j / Memgraph) settings
    cypher_uri: str = Field(
        default="bolt://localhost:7687", description="Bolt URI for the graph DB"
    )
    cypher_username: str = Field(default="neo4j", description="Username for DB")
    cypher_password: str = Field(default="password", description="Password for DB")

    similarity_threshold: float = Field(
        default=0.5, description="Cosine-similarity threshold for retrieval"
    )
    max_results: int = Field(default=10, description="Maximum retrieved memories")
    number_of_concepts_to_extract: int = Field(
        default=3, description="Number of concepts extracted per chunk"
    )
    save_interval: int = Field(
        default=10,
        description="Persist to disk after this many new chunks (0 disables auto-save)",
    )

    export_graph_image: bool = Field(
        default=False,
        description="If true, exports a PNG image of the concept graph each time it is updated.",
    )
    graph_image_dir: str = Field(
        default="./concept_graphs",
        description="Directory where graph images will be stored when export_graph_image is true.",
    )


# ---------------------------------------------------------------------------
# Storage Abstraction
# ---------------------------------------------------------------------------
class EnterpriseMemoryStore:
    """Persistence layer that wraps Chroma + Cypher graph."""

    def __init__(self, cfg: EnterpriseMemoryModuleConfig, metrics_module: MetricsModule | None = None):
        self.cfg = cfg
        # Metrics module (DI-resolved or fallback)
        self._metrics = metrics_module or MetricsModule  # can be either instance or class exposing .record
        # Lazy initialise expensive resources
        self._embedding_model: SentenceTransformer | None = None
        self._adapter: VectorAdapter | None = None
        self._driver = None  # Neo4j driver
        self._pending_writes: list[tuple[str, dict[str, Any]]] = []
        self._write_lock = asyncio.Lock()
        self._concept_cache: set[str] | None = None  # names of known concepts

    # ---------------------------------------------------------------------
    # Connections
    # ---------------------------------------------------------------------
    def _ensure_embedding_model(self) -> SentenceTransformer:
        if self._embedding_model is None:
            logger.debug("Loading embedding model 'all-MiniLM-L6-v2'")
            with tracer.start_as_current_span("memory.load_embedding_model") as span:
                try:
                    self._embedding_model = SentenceTransformer("all-MiniLM-L6-v2")
                    span.set_attribute("model", "all-MiniLM-L6-v2")
                except Exception as e:
                    span.record_exception(e)
                    raise
        return self._embedding_model

    def _ensure_adapter(self) -> VectorAdapter:
        if self._adapter is not None:
            return self._adapter

        backend = self.cfg.vector_backend

        if backend == "chroma":
            self._adapter = ChromaAdapter(
                collection=self.cfg.chroma_collection,
                host=self.cfg.chroma_host,
                port=self.cfg.chroma_port,
                path=self.cfg.chroma_path,
            )
        elif backend == "pinecone":
            self._adapter = PineconeAdapter(
                api_key=self.cfg.pinecone_api_key,
                environment=self.cfg.pinecone_env,
                index=self.cfg.pinecone_index,
            )
        elif backend == "azure":
            self._adapter = AzureSearchAdapter(
                endpoint=self.cfg.azure_search_endpoint,
                key=self.cfg.azure_search_key,
                index_name=self.cfg.azure_search_index_name,
            )
        elif backend == "faiss":
            self._adapter = FAISSAdapter(index_path="./faiss.index")
        else:
            raise ValueError(f"Unsupported vector backend: {backend}")

        return self._adapter

    def _ensure_graph_driver(self):
        if self._driver is None:
            self._driver = AsyncGraphDatabase.driver(
                self.cfg.cypher_uri,
                auth=(self.cfg.cypher_username, self.cfg.cypher_password),
                encrypted=False,
            )
        return self._driver

    # ---------------------------------------------------------------------
    # Public API
    # ---------------------------------------------------------------------
    async def add_entry(
        self,
        content: str,
        concepts: set[str],
        metadata: dict[str, Any] | None = None,
    ) -> str:
        """Store a chunk in both vector store and graph DB and return its id."""
        with tracer.start_as_current_span("memory.add_entry") as span:
            span.set_attribute("entry_id", str(uuid.uuid4()))

            # Embed
            embedding = self._ensure_embedding_model().encode(content).tolist()
            span.set_attribute("embedding_length", len(embedding))

            # Vector store write
            adapter = self._ensure_adapter()
            span.set_attribute("vector_backend", self.cfg.vector_backend)

            start_t = time.perf_counter()
            try:
                adapter.add(
                    id=span.get_attribute("entry_id"),
                    content=content,
                    embedding=embedding,
                    metadata=metadata,
                )
            except Exception as e:
                span.record_exception(e)
                raise
            finally:
                elapsed = (time.perf_counter() - start_t) * 1000  # ms
                self._metrics.record(
                    "memory_add_latency_ms",
                    elapsed,
                    {"backend": self.cfg.vector_backend},
                )

        # Schedule graph writes (batched)
        async with self._write_lock:
            self._pending_writes.append((span.get_attribute("entry_id"), {"concepts": concepts}))
            if self.cfg.save_interval and len(self._pending_writes) >= self.cfg.save_interval:
                await self._flush_pending_graph_writes()
        return span.get_attribute("entry_id")

    async def search(
        self, query_text: str, threshold: float, k: int
    ) -> list[dict[str, Any]]:
        """Vector similarity search followed by graph enrichment."""
        with tracer.start_as_current_span("memory.search") as span:
            span.set_attribute("vector_backend", self.cfg.vector_backend)
            embedding = (
                self._ensure_embedding_model().encode(query_text).tolist()
            )
            span.set_attribute("embedding_length", len(embedding))
            adapter = self._ensure_adapter()
            backend = self.cfg.vector_backend
            results: list[dict[str, Any]] = []

            search_start = time.perf_counter()
            vector_hits = adapter.query(embedding=embedding, k=k)
            search_elapsed = (time.perf_counter() - search_start) * 1000
            self._metrics.record(
                "memory_search_hits", len(vector_hits), {"backend": backend}
            )
            for hit in vector_hits:
                if hit.score < threshold:
                    continue
                results.append(
                    {
                        "id": hit.id,
                        "content": hit.content,
                        "metadata": hit.metadata,
                        "score": hit.score,
                    }
                )

            span.set_attribute("results_count", len(results))
            self._metrics.record(
                "memory_search_latency_ms", search_elapsed, {"backend": backend}
            )
            return results

    # ------------------------------------------------------------------
    # Graph persistence helpers
    # ------------------------------------------------------------------
    async def _flush_pending_graph_writes(self):
        """Commit queued node/edge creations to the Cypher store."""
        if not self._pending_writes:
            return
        driver = self._ensure_graph_driver()
        async with driver.session() as session:
            tx_commands: list[str] = []
            params: dict[str, Any] = {}
            # Build Cypher in one transaction
            for idx, (entry_id, extra) in enumerate(self._pending_writes):
                concept_param = f"concepts_{idx}"
                tx_commands.append(
                    f"MERGE (e:Memory {{id: '{entry_id}'}}) "
                    f"SET e.created = datetime() "
                )
                if extra.get("concepts"):
                    tx_commands.append(
                        f"WITH e UNWIND ${concept_param} AS c "
                        "MERGE (co:Concept {name: c}) "
                        "MERGE (e)-[:MENTIONS]->(co)"
                    )
                    params[concept_param] = list(extra["concepts"])
            cypher = "\n".join(tx_commands)
            await session.run(cypher, params)
            # Export graph image if requested
            if self.cfg.export_graph_image:
                await self._export_graph_image(session)
        self._pending_writes.clear()

    async def _export_graph_image(self, session):
        """Generate and save a PNG of the concept graph."""
        try:
            import matplotlib
            matplotlib.use("Agg")
            import matplotlib.pyplot as plt
            import networkx as nx

            records = await session.run(
                "MATCH (c1:Concept)<-[:MENTIONS]-(:Memory)-[:MENTIONS]->(c2:Concept) "
                "RETURN DISTINCT c1.name AS source, c2.name AS target"
            )
            edges = [(r["source"], r["target"]) for r in await records.values("source", "target")]
            if not edges:
                return

            G = nx.Graph()
            G.add_edges_from(edges)

            pos = nx.spring_layout(G, k=0.4)
            plt.figure(figsize=(12, 9), dpi=100)
            nx.draw_networkx_nodes(G, pos, node_color="#8fa8d6", node_size=500, edgecolors="white")
            nx.draw_networkx_edges(G, pos, alpha=0.5, width=1.5)
            nx.draw_networkx_labels(G, pos, font_size=8)
            plt.axis("off")

            img_dir = Path(self.cfg.graph_image_dir)
            img_dir.mkdir(parents=True, exist_ok=True)
            filename = img_dir / f"concept_graph_{uuid.uuid4().hex[:8]}.png"
            plt.savefig(filename, bbox_inches="tight", facecolor="white")
            plt.close()
            logger.info("Concept graph image exported to %s", filename)
        except Exception as e:
            logger.warning("Failed to export concept graph image: %s", e)

    async def close(self):
        if self._pending_writes:
            await self._flush_pending_graph_writes()
        if self._driver:
            await self._driver.close()
        if self._adapter and hasattr(self._adapter, "close"):
            self._adapter.close()


# ---------------------------------------------------------------------------
# Module
# ---------------------------------------------------------------------------
@flock_component(config_class=EnterpriseMemoryModuleConfig)
class EnterpriseMemoryModule(FlockModule):
    """Enterprise-ready memory module using real datastores."""

    name: str = "enterprise_memory"
    config: EnterpriseMemoryModuleConfig = Field(default_factory=EnterpriseMemoryModuleConfig)

    _store: EnterpriseMemoryStore | None = None
    _container: Any | None = None  # DI container if supplied
    _metrics_module: MetricsModule | None = None

    # ----------------------------------------------------------
    # DI-enabled constructor
    # ----------------------------------------------------------
    def __init__(
        self,
        name: str = "enterprise_memory",
        config: EnterpriseMemoryModuleConfig | None = None,
        *,
        container: object | None = None,
        **kwargs,
    ):
        """Create a new EnterpriseMemoryModule instance.

        Parameters
        ----------
        container : ServiceProvider | None
            Optional DI container used to resolve shared services.  When
            provided, the module will attempt to resolve
            :class:`flock.modules.performance.metrics_module.MetricsModule` from
            it.  Falling back to the global singleton when not available keeps
            backward-compatibility.
        """
        from wd.di.container import (
            ServiceProvider,  # Local import to avoid hard dependency if wd.di is absent
        )

        if config is None:
            config = EnterpriseMemoryModuleConfig()

        super().__init__(name=name, config=config, **kwargs)

        self._container = container if isinstance(container, ServiceProvider) else None

        # Attempt to resolve MetricsModule via DI, then via FlockModule registry
        resolved_metrics: MetricsModule | None = None
        if self._container is not None:
            try:
                resolved_metrics = self._container.get_service(MetricsModule)
            except Exception:
                resolved_metrics = None

        if resolved_metrics is None:
            resolved_metrics = MetricsModule._INSTANCE

        self._metrics_module = resolved_metrics

    # ----------------------------------------------------------
    # Life-cycle hooks
    # ----------------------------------------------------------
    async def on_initialize(
        self,
        agent: FlockAgent,
        inputs: dict[str, Any],
        context: FlockContext | None = None,
    ) -> None:
        self._store = EnterpriseMemoryStore(self.config, self._metrics_module)
        logger.info("EnterpriseMemoryModule initialised", agent=agent.name)

    async def on_pre_evaluate(
        self,
        agent: FlockAgent,
        inputs: dict[str, Any],
        context: FlockContext | None = None,
    ) -> dict[str, Any]:
        if not self._store:
            return inputs
        try:
            query_str = json.dumps(inputs)
            matches = await self._store.search(
                query_str,
                threshold=self.config.similarity_threshold,
                k=self.config.max_results,
            )
            if matches:
                inputs = {**inputs, "context": matches}
                # Advertise new input key to DSPy signature if needed
                if isinstance(agent.input, str) and "context:" not in agent.input:
                    agent.input += ", context: list | retrieved memories"
        except Exception as e:
            logger.warning("Enterprise memory retrieval failed: %s", e, agent=agent.name)
        return inputs

    async def on_post_evaluate(
        self,
        agent: FlockAgent,
        inputs: dict[str, Any],
        context: FlockContext | None = None,
        result: dict[str, Any] | None = None,
    ) -> dict[str, Any] | None:
        if not self._store:
            return result
        try:
            full_text = json.dumps(inputs) + (json.dumps(result) if result else "")
            concepts = await self._extract_concepts(agent, full_text)
            if self._store:
                concepts = await self._store._deduplicate_concepts(concepts)
            await self._store.add_entry(full_text, concepts)
        except Exception as e:
            logger.warning("Enterprise memory store failed: %s", e, agent=agent.name)
        return result

    async def on_terminate(
        self,
        agent: FlockAgent,
        inputs: dict[str, Any],
        result: dict[str, Any],
        context: FlockContext | None = None,
    ) -> None:
        if self._store:
            await self._store.close()

    # ----------------------------------------------------------
    # Helpers (mostly copied from original module but simplified)
    # ----------------------------------------------------------
    async def _extract_concepts(self, agent: FlockAgent, text: str) -> set[str]:
        """Use the LLM to extract concept tokens."""
        concept_signature = agent.create_dspy_signature_class(
            f"{agent.name}_concept_extractor_enterprise",
            "Extract key concepts from text",
            "text: str | Input text -> concepts: list[str] | key concepts lower case",
        )
        agent._configure_language_model(agent.model, True, 0.0, 8192)
        predictor = agent._select_task(concept_signature, "Completion")
        res = predictor(text=text)
        return set(getattr(res, "concepts", []))

    # --------------------------------------------------------------
    # Concept helpers
    # --------------------------------------------------------------
    async def _ensure_concept_cache(self):
        if self._concept_cache is not None:
            return
        driver = self._ensure_graph_driver()
        async with driver.session() as session:
            records = await session.run("MATCH (c:Concept) RETURN c.name AS name")
            self._concept_cache = {r["name"] for r in await records.values("name")}

    async def _deduplicate_concepts(self, new_concepts: set[str]) -> set[str]:
        """Return a set of concept names that merges with existing ones to avoid duplicates.

        Strategy: case-insensitive equality first, then fuzzy match via difflib with cutoff 0.85.
        """
        await self._ensure_concept_cache()
        assert self._concept_cache is not None

        from difflib import get_close_matches

        unified: set[str] = set()
        for concept in new_concepts:
            # Exact (case-insensitive) match
            lower = concept.lower()
            exact = next((c for c in self._concept_cache if c.lower() == lower), None)
            if exact:
                unified.add(exact)
                continue

            # Fuzzy match (>=0.85 similarity)
            close = get_close_matches(concept, list(self._concept_cache), n=1, cutoff=0.85)
            if close:
                unified.add(close[0])
                continue

            # No match – treat as new
            unified.add(concept)
            self._concept_cache.add(concept)
        return unified
```

### src\flock\modules\mem0\__init__.py

- **Lines**: 1
- **Last modified**: 2025-05-21 19:51:15

```py
# Package for modules
```

### src\flock\modules\mem0\mem0_module.py

- **Lines**: 126
- **Last modified**: 2025-05-21 19:51:15

```py
from typing import Any

# from mem0.client.main import MemoryClient
# from mem0.memory.main import Memory
from mem0 import Memory, MemoryClient
from pydantic import Field

from flock.core.context.context import FlockContext
from flock.core.flock_agent import FlockAgent
from flock.core.flock_module import FlockModule, FlockModuleConfig
from flock.core.flock_registry import flock_component
from flock.core.logging.logging import get_logger

logger = get_logger("module.mem0")


config = {
    "vector_store": {
        "provider": "chroma",
        "config": {
            "collection_name": "flock_memory",
            "path": ".flock/memory",
        }
    }
}


class Mem0ModuleConfig(FlockModuleConfig):
    top_k: int = Field(default=10, description="Number of memories to retrieve")
    user_id: str = Field(default="flock", description="User ID the memories will be associated with")
    agent_id: str = Field(default="flock", description="Agent ID the memories will be associated with")
    memory_input_key: str | None = Field(default=None, description="Input key to use for memory, if none the description of the agent will be used")
    api_key: str | None = Field(default=None, description="API key for mem0 Platform")
    config: dict[str, Any] = Field(default=config, description="Configuration for mem0")


@flock_component(config_class=Mem0ModuleConfig)
class Mem0Module(FlockModule):

    name: str = "mem0"
    config: Mem0ModuleConfig = Mem0ModuleConfig()


    def __init__(self, name, config: Mem0ModuleConfig) -> None:
        global memory
        """Initialize Mem0 module."""
        super().__init__(name=name, config=config)
        logger.debug("Initializing Mem0 module")




    def dict_to_str_repr(self,d: dict) -> str:
        return repr(d)


    async def on_post_evaluate(
        self,
        agent: FlockAgent,
        inputs: dict[str, Any],
        context: FlockContext | None = None,
        result: dict[str, Any] | None = None,
    ) -> dict[str, Any]:
        if self.config.api_key:
            memory = MemoryClient(api_key=self.config.api_key)
        else:
            memory = Memory.from_config(config_dict=self.config.config)

        agent_id = self.config.agent_id if self.config.agent_id else agent.name

        # get the result without the inputs
        filtered_result = {k: v for k, v in result.items() if k not in inputs}
        # get the inputs without memory
        filtered_inputs = {k: v for k, v in inputs.items() if k not in [self.config.memory_input_key]}

        # add memories about the user inputs
        added_user_memory =  memory.add(self.dict_to_str_repr(filtered_inputs), user_id=self.config.user_id)
        logger.info(f"Added caller memory: {added_user_memory}")

        # add memories about the agent result
        added_agent_memory =  memory.add(self.dict_to_str_repr(filtered_result), agent_id=agent_id)
        logger.info(f"Added agent memory: {added_agent_memory}")


        return result

    async def on_pre_evaluate(
        self,
        agent: FlockAgent,
        inputs: dict[str, Any],
        context: FlockContext | None = None,
    ) -> dict[str, Any]:
        if self.config.api_key:
            memory = MemoryClient(api_key=self.config.api_key)
        else:
            memory = Memory.from_config(config_dict=self.config.config)

        message = self.dict_to_str_repr(inputs)
        agent_id = self.config.agent_id if self.config.agent_id else agent.name

        relevant_agent_memories = memory.search(query=message, agent_id=agent_id, limit=self.config.top_k)
        logger.info(f"Relevant agent memories: {relevant_agent_memories}")

        relevant_user_memories = memory.search(query=message, user_id=self.config.user_id, limit=self.config.top_k)
        logger.info(f"Relevant user memories: {relevant_user_memories}")

        if relevant_agent_memories or relevant_user_memories:
            memories_str = ''
            if "results" in relevant_agent_memories:
                memories_str = "\n".join(f"- {entry['memory']}" for entry in relevant_agent_memories["results"])
            else:
                memories_str = "\n".join(f"- {entry}" for entry in relevant_agent_memories)

            if "results" in relevant_user_memories:
                memories_str = memories_str + "\n" + "\n".join(f"- {entry['memory']}" for entry in relevant_user_memories["results"])
            else:
                memories_str = memories_str + "\n" + "\n".join(f"- {entry}" for entry in relevant_user_memories)

            if memories_str:
                if self.config.memory_input_key:
                    inputs[self.config.memory_input_key] = memories_str
                else:
                    agent.description = agent.description + "\n\n Memories:" + memories_str


        return inputs
```

### src\flock\modules\mem0_async\__init__.py

- **Lines**: 1
- **Last modified**: 2025-05-21 19:51:15

```py
# Package for modules
```

### src\flock\modules\mem0_async\async_mem0_module.py

- **Lines**: 126
- **Last modified**: 2025-05-21 19:51:15

```py
from typing import Any

# from mem0.client.main import AsyncMemoryClient, MemoryClient
# from mem0.memory.main import AsyncMemory
from mem0 import AsyncMemory, AsyncMemoryClient
from pydantic import Field

from flock.core.context.context import FlockContext
from flock.core.flock_agent import FlockAgent
from flock.core.flock_module import FlockModule, FlockModuleConfig
from flock.core.flock_registry import flock_component
from flock.core.logging.logging import get_logger

logger = get_logger("module.mem0")


config = {
    "vector_store": {
        "provider": "chroma",
        "config": {
            "collection_name": "flock_memory",
            "path": ".flock/memory",
        }
    }
}


class AsyncMem0ModuleConfig(FlockModuleConfig):
    top_k: int = Field(default=10, description="Number of memories to retrieve")
    user_id: str = Field(default="flock", description="User ID the memories will be associated with")
    agent_id: str = Field(default="flock", description="Agent ID the memories will be associated with")
    memory_input_key: str | None = Field(default=None, description="Input key to use for memory, if none the description of the agent will be used")
    api_key: str | None = Field(default=None, description="API key for mem0 Platform")
    config: dict[str, Any] = Field(default=config, description="Configuration for mem0")


@flock_component(config_class=AsyncMem0ModuleConfig)
class AsyncMem0Module(FlockModule):

    name: str = "mem0"
    config: AsyncMem0ModuleConfig = AsyncMem0ModuleConfig()


    def __init__(self, name, config: AsyncMem0ModuleConfig) -> None:
        global memory
        """Initialize Mem0 module."""
        super().__init__(name=name, config=config)
        logger.debug("Initializing Mem0 module")




    def dict_to_str_repr(self,d: dict) -> str:
        return repr(d)


    async def on_post_evaluate(
        self,
        agent: FlockAgent,
        inputs: dict[str, Any],
        context: FlockContext | None = None,
        result: dict[str, Any] | None = None,
    ) -> dict[str, Any]:
        if self.config.api_key:
            memory = AsyncMemoryClient(api_key=self.config.api_key)
        else:
            memory = await AsyncMemory.from_config(config_dict=self.config.config)

        agent_id = self.config.agent_id if self.config.agent_id else agent.name

        # get the result without the inputs
        filtered_result = {k: v for k, v in result.items() if k not in inputs}
        # get the inputs without memory
        filtered_inputs = {k: v for k, v in inputs.items() if k not in [self.config.memory_input_key]}

        # add memories about the user inputs
        added_user_memory = await memory.add(self.dict_to_str_repr(filtered_inputs), user_id=self.config.user_id)
        logger.info(f"Added caller memory: {added_user_memory}")

        # add memories about the agent result
        added_agent_memory = await memory.add(self.dict_to_str_repr(filtered_result), agent_id=agent_id)
        logger.info(f"Added agent memory: {added_agent_memory}")


        return result

    async def on_pre_evaluate(
        self,
        agent: FlockAgent,
        inputs: dict[str, Any],
        context: FlockContext | None = None,
    ) -> dict[str, Any]:
        if self.config.api_key:
            memory = AsyncMemoryClient(api_key=self.config.api_key)
        else:
            memory = await AsyncMemory.from_config(config_dict=self.config.config)

        message = self.dict_to_str_repr(inputs)
        agent_id = self.config.agent_id if self.config.agent_id else agent.name

        relevant_agent_memories = await memory.search(query=message, agent_id=agent_id, limit=self.config.top_k)
        logger.info(f"Relevant agent memories: {relevant_agent_memories}")

        relevant_user_memories = await memory.search(query=message, user_id=self.config.user_id, limit=self.config.top_k)
        logger.info(f"Relevant user memories: {relevant_user_memories}")

        if relevant_agent_memories or relevant_user_memories:
            memories_str = ''
            if "results" in relevant_agent_memories:
                memories_str = "\n".join(f"- {entry['memory']}" for entry in relevant_agent_memories["results"])
            else:
                memories_str = "\n".join(f"- {entry}" for entry in relevant_agent_memories)

            if "results" in relevant_user_memories:
                memories_str = memories_str + "\n" + "\n".join(f"- {entry['memory']}" for entry in relevant_user_memories["results"])
            else:
                memories_str = memories_str + "\n" + "\n".join(f"- {entry}" for entry in relevant_user_memories)

            if memories_str:
                if self.config.memory_input_key:
                    inputs[self.config.memory_input_key] = memories_str
                else:
                    agent.description = agent.description + "\n\n Memories:" + memories_str


        return inputs
```

### src\flock\modules\memory\__init__.py

- **Lines**: 1
- **Last modified**: 2025-05-21 19:51:15

```py
# Package for modules
```

### src\flock\modules\memory\memory_module.py

- **Lines**: 429
- **Last modified**: 2025-05-21 19:51:15

```py
import json
import uuid
from datetime import datetime
from typing import Any, Literal

from pydantic import Field

from flock.core.context.context import FlockContext

# if TYPE_CHECKING:
#     from flock.core import FlockAgent
from flock.core.flock_agent import FlockAgent
from flock.core.flock_module import FlockModule, FlockModuleConfig
from flock.core.flock_registry import flock_component
from flock.core.logging.logging import get_logger
from flock.modules.memory.memory_parser import MemoryMappingParser
from flock.modules.memory.memory_storage import FlockMemoryStore, MemoryEntry

logger = get_logger("memory")


class MemoryModuleConfig(FlockModuleConfig):
    """Configuration for the MemoryModule.

    This class defines the configuration for the MemoryModule.
    """

    folder_path: str = Field(
        default=".flock/memory/",
        description="Directory where memory file and concept graph will be saved",
    )
    concept_graph_file: str = Field(
        default="concept_graph.png",
        description="Base filename for the concept graph image",
    )
    file_path: str | None = Field(
        default="agent_memory.json", description="Path to save memory file"
    )
    memory_mapping: str | None = Field(
        default=None, description="Memory mapping configuration"
    )
    similarity_threshold: float = Field(
        default=0.5, description="Threshold for semantic similarity"
    )
    max_length: int = Field(
        default=1000, description="Max length of memory entry before splitting"
    )
    save_after_update: bool = Field(
        default=True, description="Whether to save memory after each update"
    )
    splitting_mode: Literal["summary", "semantic", "characters", "none"] = (
        Field(default="characters", description="Mode to split memory content")
    )
    enable_read_only_mode: bool = Field(
        default=False, description="Whether to enable read only mode"
    )
    enable_write_only_mode: bool = Field(
        default=False, description="Whether to enable write only mode"
    )
    number_of_concepts_to_extract: int = Field(
        default=3, description="Number of concepts to extract from the memory"
    )
    memory_input_key: str | None = Field(default=None, description="Input key to use for memory, if none the description of the agent will be used")



@flock_component(config_class=MemoryModuleConfig)
class MemoryModule(FlockModule):
    """Module that adds memory capabilities to a Flock agent."""

    name: str = "memory"
    config: MemoryModuleConfig = Field(
        default_factory=MemoryModuleConfig,
        description="Memory module configuration",
    )
    memory_store: FlockMemoryStore | None = None
    memory_ops: list[Any] = []

    def __init__(self, name: str, config: MemoryModuleConfig):
        super().__init__(name=name, config=config)
        self.memory_store = FlockMemoryStore.load_from_file(
            self.get_memory_filename(name)
        )
        self.memory_ops = (
            MemoryMappingParser().parse(self.config.memory_mapping)
            if self.config.memory_mapping
            else [{"type": "semantic"}]
        )

    async def on_initialize(
        self,
        agent: FlockAgent,
        inputs: dict[str, Any],
        context: FlockContext | None = None,
    ) -> None:
        """Initialize memory store if needed."""
        if not self.memory_store:
            self.memory_store = FlockMemoryStore.load_from_file(
                self.get_memory_filename(self.name)
            )
        self.memory_ops = (
            MemoryMappingParser().parse(self.config.memory_mapping)
            if self.config.memory_mapping
            else [{"type": "semantic"}]
        )
        logger.debug(f"Initialized memory module for agent {agent.name}")

    async def on_pre_evaluate(
        self,
        agent: FlockAgent,
        inputs: dict[str, Any],
        context: FlockContext | None = None,
    ) -> dict[str, Any]:
        """Check memory before evaluation."""
        if not self.memory_store:
            return inputs

        if self.config.enable_write_only_mode:
            return inputs

        inputs = await self.search_memory(agent, inputs)

        if "context" in inputs:
            agent.input = (
                agent.input + ", context: list | context with more information"
            )

        return inputs

    def get_memory_filename(self, module_name: str) -> str:
        """Generate the full file path for the memory file."""
        folder = self.config.folder_path
        if not folder.endswith(("/", "\\")):
            folder += "/"
        import os

        if not os.path.exists(folder):
            os.makedirs(folder, exist_ok=True)
        # Determine base filename and extension from file_path config
        if self.config.file_path:
            file_name = self.config.file_path.rsplit("/", 1)[-1].rsplit(
                "\\", 1
            )[-1]
            if "." in file_name:
                base, ext = file_name.rsplit(".", 1)
                ext = f".{ext}"
            else:
                base, ext = file_name, ""
        else:
            base, ext = "agent_memory", ".json"
        return f"{folder}{module_name}_{base}{ext}"

    def get_concept_graph_filename(self, module_name: str) -> str:
        """Generate the full file path for the concept graph image."""
        folder = self.config.folder_path
        if not folder.endswith(("/", "\\")):
            folder += "/"
        import os

        if not os.path.exists(folder):
            os.makedirs(folder, exist_ok=True)
        # Use timestamp to create a unique filename
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]
        if self.config.concept_graph_file:
            file_name = self.config.concept_graph_file.rsplit("/", 1)[
                -1
            ].rsplit("\\", 1)[-1]
            if "." in file_name:
                base, ext = file_name.rsplit(".", 1)
                ext = f".{ext}"
            else:
                base, ext = file_name, ""
        else:
            base, ext = "concept_graph", ".png"
        return f"{folder}{module_name}_{base}_{timestamp}{ext}"

    async def search_memory(
        self, agent: FlockAgent, query: dict[str, Any]
    ) -> dict[str, Any]:
        """Search memory for the query."""
        if not self.memory_store:
            # No memory store loaded – just return the untouched input
            return query

        try:
            input_text = json.dumps(query)
            query_embedding = self.memory_store.compute_embedding(input_text)
            concepts = await self._extract_concepts(
                agent, input_text, self.config.number_of_concepts_to_extract
            )

            memory_results = []
            for op in self.memory_ops:
                if op["type"] == "semantic":
                    semantic_results = self.memory_store.retrieve(
                        query_embedding,
                        concepts,
                        similarity_threshold=self.config.similarity_threshold,
                    )
                    memory_results.extend(semantic_results)
                elif op["type"] == "exact":
                    exact_results = self.memory_store.exact_match(query)
                    memory_results.extend(exact_results)

            context: list[dict[str, Any]] = []
            if memory_results:
                for result in memory_results:
                    context.append(
                        {"content": result.content, "concepts": result.concepts}
                    )

                logger.debug(
                    f"Found {len(memory_results)} relevant memories",
                    agent=agent.name,
                )
                query["context"] = context

            return query

        except Exception as e:
            logger.warning(f"Memory retrieval failed: {e}", agent=agent.name)
            return query

    async def add_to_memory(
        self, agent: FlockAgent, data: dict[str, Any]
    ) -> None:
        """Add data to memory."""
        if not self.memory_store:
            return

        try:
            chunks = await self._get_chunks(agent, data, None)
            await self._store_chunks(agent, chunks)
        except Exception as e:
            logger.warning(f"Memory storage failed: {e}", agent=agent.name)

    async def on_post_evaluate(
        self,
        agent: FlockAgent,
        inputs: dict[str, Any],
        context: FlockContext | None = None,
        result: dict[str, Any] | None = None,
    ) -> dict[str, Any]:
        """Store results in memory after evaluation."""
        if not self.memory_store:
            return result

        if self.config.enable_read_only_mode:
            return result

        try:
            chunks = await self._get_chunks(agent, inputs, result)
            await self._store_chunks(agent, chunks)
        except Exception as e:
            logger.warning(f"Memory storage failed: {e}", agent=agent.name)

        return result

    async def on_terminate(
        self,
        agent: Any,
        inputs: dict[str, Any],
        result: dict[str, Any],
        context: FlockContext | None = None,
    ) -> None:
        """Save memory store if configured."""
        if self.config.save_after_update and self.memory_store:
            self.save_memory()

    async def _extract_concepts(
        self, agent: FlockAgent, text: str, number_of_concepts: int = 3
    ) -> set[str]:
        """Extract concepts using the agent's LLM capabilities."""
        existing_concepts = set()
        if self.memory_store and self.memory_store.concept_graph:
            existing_concepts = set(
                self.memory_store.concept_graph.graph.nodes()
            )

        input_signature = "text: str | Text to analyze"
        if existing_concepts:
            input_signature += ", existing_concepts: list[str] | Already known concepts that might apply"

        concept_signature = agent.create_dspy_signature_class(
            f"{agent.name}_concept_extractor",
            "Extract key concepts from text",
            f"{input_signature} -> concepts: list[str] | Max {number_of_concepts} key concepts all lower case",
        )

        agent._configure_language_model(agent.model, True, 0.0, 8192)
        predictor = agent._select_task(concept_signature, "Completion")
        result_obj = predictor(
            text=text,
            existing_concepts=list(existing_concepts)
            if existing_concepts
            else None,
        )
        concept_list = getattr(result_obj, "concepts", [])
        return set(concept_list)

    async def _summarize_mode(
        self,
        agent: FlockAgent,
        inputs: dict[str, Any],
        result: dict[str, Any],
    ) -> str:
        """Extract information chunks using summary mode."""
        split_signature = agent.create_dspy_signature_class(
            f"{agent.name}_splitter",
            "Extract a list of potentially needed data and information for future reference",
            """
            content: str | The content to split
            -> chunks: list[str] | List of data and information for future reference
            """,
        )
        agent._configure_language_model(agent.model, True, 0.0, 8192)
        splitter = agent._select_task(split_signature, "Completion")
        full_text = json.dumps(inputs) + json.dumps(result)
        split_result = splitter(content=full_text)
        return "\n".join(split_result.chunks)

    async def _semantic_splitter_mode(
        self,
        agent: FlockAgent,
        inputs: dict[str, Any],
        result: dict[str, Any],
    ) -> list[str]:
        """Extract information chunks using semantic mode."""
        split_signature = agent.create_dspy_signature_class(
            f"{self.name}_splitter",
            "Split content into meaningful, self-contained chunks",
            """
            content: str | The content to split
            -> chunks: list[dict[str,str]] | List of chunks as key-value pairs - keys are a short title and values are the chunk content
            """,
        )
        agent._configure_language_model(agent.model, True, 0.0, 8192)
        splitter = agent._select_task(split_signature, "Completion")
        full_text = json.dumps(inputs) + (json.dumps(result) if result else "")
        split_result = splitter(content=full_text)
        # Flatten list[dict] into list[str] of "title: content" strings to
        # keep downstream storage logic simple and type-safe.
        flattened: list[str] = []
        for chunk in split_result.chunks:
            if isinstance(chunk, dict):
                flattened.extend([f"{k}: {v}" for k, v in chunk.items()])
            else:
                flattened.append(str(chunk))
        return flattened

    async def _character_splitter_mode(
        self,
        agent: FlockAgent,
        inputs: dict[str, Any],
        result: dict[str, Any],
    ) -> list[str]:
        """Extract information chunks by splitting text into fixed character lengths."""
        full_text = json.dumps(inputs) + (json.dumps(result) if result else "")
        return [
            full_text[i : i + self.config.max_length]
            for i in range(0, len(full_text), self.config.max_length)
        ]

    async def _get_chunks(
        self,
        agent: FlockAgent,
        inputs: dict[str, Any],
        result: dict[str, Any] | None,
    ) -> str | list[str]:
        """Get memory chunks based on the configured splitting mode."""
        mode = self.config.splitting_mode
        if mode == "semantic":
            return await self._semantic_splitter_mode(agent, inputs, result)
        elif mode == "summary":
            return await self._summarize_mode(agent, inputs, result)
        elif mode == "characters":
            return await self._character_splitter_mode(agent, inputs, result)
        elif mode == "none":
            return (
                json.dumps(inputs) + json.dumps(result)
                if result
                else json.dumps(inputs)
            )
        else:
            raise ValueError(f"Unknown splitting mode: {mode}")

    async def _store_chunk(self, agent: FlockAgent, chunk: str) -> None:
        """Store a single chunk in memory."""
        chunk_concepts = await self._extract_concepts(
            agent, chunk, self.config.number_of_concepts_to_extract
        )
        entry = MemoryEntry(
            id=str(uuid.uuid4()),
            content=chunk,
            embedding=self.memory_store.compute_embedding(chunk).tolist(),
            concepts=chunk_concepts,
            timestamp=datetime.now(),
        )
        self.memory_store.add_entry(entry)
        if self.config.save_after_update:
            self.save_memory()
        logger.debug(
            "Stored interaction in memory",
            agent=agent.name,
            entry_id=entry.id,
            concepts=chunk_concepts,
        )

    async def _store_chunks(
        self, agent: FlockAgent, chunks: str | list[str]
    ) -> None:
        """Store chunks (single or multiple) in memory."""
        if isinstance(chunks, str):
            await self._store_chunk(agent, chunks)
        elif isinstance(chunks, list):
            # Avoid tqdm in async context – simple for-loop is safer.
            for chunk in chunks:
                await self._store_chunk(agent, chunk)

    def save_memory(self) -> None:
        """Save memory store to file."""
        if self.memory_store and self.config.file_path:
            json_str = self.memory_store.model_dump_json()
            filename = self.get_memory_filename(self.name)
            with open(filename, "w") as file:
                file.write(json_str)
            self.memory_store.concept_graph.save_as_image(
                self.get_concept_graph_filename(self.name)
            )
```

### src\flock\modules\memory\memory_parser.py

- **Lines**: 125
- **Last modified**: 2025-05-20 19:21:05

```py
"""Parser for memory mapping declarations into executable operations."""

import re
from typing import Any

from flock.modules.memory.memory_storage import (
    CombineOperation,
    EnrichOperation,
    ExactOperation,
    FilterOperation,
    MemoryOperation,
    MemoryScope,
    SemanticOperation,
    SortOperation,
)


class MemoryMappingParser:
    """Parses memory mapping declarations into executable operations."""

    def parse(self, mapping: str) -> list[MemoryOperation]:
        """Parse a memory mapping string into operations.

        Example mappings:
        "topic -> memory.semantic(threshold=0.9) | memory.exact -> output"
        "query -> memory.semantic(scope='global') | memory.filter(recency='7d') | memory.sort(by='relevance')"
        """
        operations = []
        stages = [s.strip() for s in mapping.split("|")]

        for stage in stages:
            if "->" not in stage:
                continue

            inputs, op_spec = stage.split("->")
            inputs = [i.strip() for i in inputs.split(",")]

            if "memory." in op_spec:
                # Extract operation name and parameters
                match = re.match(r"memory\.(\w+)(?:\((.*)\))?", op_spec.strip())
                if not match:
                    continue

                op_name, params_str = match.groups()
                params = self._parse_params(params_str or "")

                # Create appropriate operation object
                if op_name == "semantic":
                    operation = SemanticOperation(
                        threshold=params.get("threshold", 0.8),
                        scope=params.get("scope", MemoryScope.BOTH),
                        max_results=params.get("max_results", 10),
                    )
                elif op_name == "exact":
                    operation = ExactOperation(
                        keys=inputs, scope=params.get("scope", MemoryScope.BOTH)
                    )
                elif op_name == "enrich":
                    operation = EnrichOperation(
                        tools=params.get("tools", []),
                        strategy=params.get("strategy", "comprehensive"),
                        scope=params.get("scope", MemoryScope.BOTH),
                    )
                elif op_name == "filter":
                    operation = FilterOperation(
                        recency=params.get("recency"),
                        relevance=params.get("relevance"),
                        metadata=params.get("metadata", {}),
                    )
                elif op_name == "sort":
                    operation = SortOperation(
                        by=params.get("by", "relevance"),
                        ascending=params.get("ascending", False),
                    )
                elif op_name == "combine":
                    operation = CombineOperation(
                        weights=params.get(
                            "weights", {"semantic": 0.7, "exact": 0.3}
                        )
                    )

                operations.append(operation)

        return operations

    def _parse_params(self, params_str: str) -> dict[str, Any]:
        """Parse parameters string into a dictionary.

        Handles:
        - Quoted strings: threshold='high'
        - Numbers: threshold=0.9
        - Lists: tools=['web_search', 'extract_numbers']
        - Dictionaries: weights={'semantic': 0.7, 'exact': 0.3}
        """
        if not params_str:
            return {}

        params = {}
        # Split on commas not inside brackets or quotes
        param_pairs = re.findall(
            r"""
            (?:[^,"]|"[^"]*"|'[^']*')+  # Match everything except comma, or quoted strings
        """,
            params_str,
            re.VERBOSE,
        )

        for pair in param_pairs:
            if "=" not in pair:
                continue
            key, value = pair.split("=", 1)
            key = key.strip()
            value = value.strip()

            # Try to evaluate the value (for lists, dicts, numbers)
            try:
                # Safely evaluate the value
                value = eval(value, {"__builtins__": {}}, {})
            except:
                # If evaluation fails, treat as string
                value = value.strip("'\"")

            params[key] = value

        return params
```

### src\flock\modules\memory\memory_storage.py

- **Lines**: 736
- **Last modified**: 2025-05-20 19:21:13

```py
"""Flock memory storage with short-term and long-term memory, concept graph, and clustering.

Based on concept graph spreading activation and embedding-based semantic search.
"""

import json
from datetime import datetime
from enum import Enum
from typing import Any, Literal

import networkx as nx
import numpy as np
from networkx.readwrite import json_graph
from opentelemetry import trace
from pydantic import BaseModel, Field, PrivateAttr

# Import SentenceTransformer for production-grade embeddings.
from sentence_transformers import SentenceTransformer

# Import the Flock logger.
from flock.core.logging.logging import get_logger

tracer = trace.get_tracer(__name__)
logger = get_logger("memory")


class MemoryScope(Enum):
    LOCAL = "local"
    GLOBAL = "global"
    BOTH = "both"


class MemoryOperation(BaseModel):
    """Base class for memory operations."""

    type: str
    scope: MemoryScope = MemoryScope.BOTH


class CombineOperation(MemoryOperation):
    """Combine results from multiple operations using weighted scoring."""

    type: Literal["combine"] = "combine"
    weights: dict[str, float] = Field(
        default_factory=lambda: {"semantic": 0.7, "exact": 0.3}
    )


class SemanticOperation(MemoryOperation):
    """Semantic search operation."""

    type: Literal["semantic"] = "semantic"
    threshold: float = 0.5
    max_results: int = 10
    recency_filter: str | None = None  # e.g., "7d", "24h"


class ExactOperation(MemoryOperation):
    """Exact matching operation."""

    type: Literal["exact"] = "exact"
    keys: list[str] = Field(default_factory=list)


class ChunkOperation(MemoryOperation):
    """Operation for handling chunked entries."""

    type: Literal["chunk"] = "chunk"
    reconstruct: bool = True


class EnrichOperation(MemoryOperation):
    """Enrich memory with tool results."""

    type: Literal["enrich"] = "enrich"
    tools: list[str]
    strategy: Literal["comprehensive", "quick", "validated"] = "comprehensive"


class FilterOperation(MemoryOperation):
    """Filter memory results."""

    type: Literal["filter"] = "filter"
    recency: str | None = None
    relevance: float | None = None
    metadata: dict[str, Any] = Field(default_factory=dict)


class SortOperation(MemoryOperation):
    """Sort memory results."""

    type: Literal["sort"] = "sort"
    by: Literal["relevance", "recency", "access_count"] = "relevance"
    ascending: bool = False


class MemoryEntry(BaseModel):
    """A single memory entry."""

    id: str
    content: str
    embedding: list[float] | None = None
    timestamp: datetime = Field(default_factory=datetime.now)
    access_count: int = Field(default=0)
    concepts: set[str] = Field(default_factory=set)
    decay_factor: float = Field(default=1.0)


class MemoryGraph(BaseModel):
    """Graph representation of concept relationships.

    The graph is stored as a JSON string for serialization, while a private attribute holds the actual NetworkX graph.
    """

    # JSON representation using the node-link format with explicit edges="links" to avoid warnings.
    graph_json: str = Field(
        default_factory=lambda: json.dumps(
            json_graph.node_link_data(nx.Graph(), edges="links")
        )
    )
    # Private attribute for the actual NetworkX graph.
    _graph: nx.Graph = PrivateAttr()

    def __init__(self, **data):
        """Initialize the MemoryGraph with a NetworkX graph from JSON data."""
        super().__init__(**data)
        try:
            data_graph = json.loads(self.graph_json)
            self._graph = json_graph.node_link_graph(data_graph, edges="links")
            logger.debug(
                f"MemoryGraph initialized from JSON with {len(self._graph.nodes())} nodes."
            )
        except Exception as e:
            logger.error(f"Failed to load MemoryGraph from JSON: {e}")
            self._graph = nx.Graph()

    @property
    def graph(self) -> nx.Graph:
        """Provides access to the internal NetworkX graph."""
        return self._graph

    def update_graph_json(self) -> None:
        """Update the JSON representation based on the current state of the graph."""
        self.graph_json = json.dumps(
            json_graph.node_link_data(self._graph, edges="links")
        )
        logger.debug("MemoryGraph JSON updated.")

    def add_concepts(self, concepts: set[str]) -> None:
        """Add a set of concepts to the graph and update their associations."""
        concept_list = list(concepts)
        logger.debug(f"Adding concepts: {concept_list}")
        for concept in concepts:
            self._graph.add_node(concept)
        for c1 in concepts:
            for c2 in concepts:
                if c1 != c2:
                    if self._graph.has_edge(c1, c2):
                        self._graph[c1][c2]["weight"] += 1
                    else:
                        self._graph.add_edge(c1, c2, weight=1)
        self.update_graph_json()

    def spread_activation(
        self, initial_concepts: set[str], decay_factor: float = 0.5
    ) -> dict[str, float]:
        """Spread activation through the concept graph.

        Args:
            initial_concepts: The starting set of concepts.
            decay_factor: How much the activation decays at each step.

        Returns:
            A dictionary mapping each concept to its activation level.
        """
        logger.debug(f"Spreading activation from concepts: {initial_concepts}")
        activated = {concept: 1.0 for concept in initial_concepts}
        frontier = list(initial_concepts)

        while frontier:
            current = frontier.pop(0)
            current_activation = activated[current]
            for neighbor in self._graph.neighbors(current):
                weight = self._graph[current][neighbor]["weight"]
                new_activation = current_activation * decay_factor * weight
                if (
                    neighbor not in activated
                    or activated[neighbor] < new_activation
                ):
                    activated[neighbor] = new_activation
                    frontier.append(neighbor)

        logger.debug(f"Activation levels: {activated}")
        return activated

    def save_as_image(self, filename: str = "memory_graph.png") -> None:
        """Visualize the concept graph and save it as a PNG image with improved readability.

        This method uses matplotlib to create a clear and readable visualization by:
        - Using a larger figure size
        - Implementing better node spacing
        - Adding adjustable text labels
        - Using a more visually appealing color scheme
        - Adding edge weight visualization

        Args:
            filename: The path (including .png) where the image will be saved.
        """
        import matplotlib

        matplotlib.use("Agg")
        import matplotlib.pyplot as plt

        logger.info(f"Saving MemoryGraph visualization to '{filename}'")

        if self._graph.number_of_nodes() == 0:
            logger.warning("MemoryGraph is empty; skipping image creation.")
            return

        try:
            # Create a larger figure with higher DPI
            plt.figure(figsize=(16, 12), dpi=100)

            # Use Kamada-Kawai layout for better node distribution
            pos = nx.kamada_kawai_layout(self._graph)

            # Calculate node sizes based on degree
            node_degrees = dict(self._graph.degree())
            node_sizes = [
                2000 * (1 + node_degrees[node] * 0.2)
                for node in self._graph.nodes()
            ]

            # Calculate edge weights for width and transparency
            edge_weights = [
                d["weight"] for (u, v, d) in self._graph.edges(data=True)
            ]
            max_weight = max(edge_weights) if edge_weights else 1
            edge_widths = [1 + (w / max_weight) * 3 for w in edge_weights]
            edge_alphas = [0.2 + (w / max_weight) * 0.8 for w in edge_weights]

            # Draw the network with custom styling
            # Nodes
            nx.draw_networkx_nodes(
                self._graph,
                pos,
                node_size=node_sizes,
                node_color="#5fa4d4",  # Lighter blue
                alpha=0.7,
                edgecolors="white",
            )

            # Edges with varying width and transparency
            for (u, v, d), width, alpha in zip(
                self._graph.edges(data=True), edge_widths, edge_alphas
            ):
                nx.draw_networkx_edges(
                    self._graph,
                    pos,
                    edgelist=[(u, v)],
                    width=width,
                    alpha=alpha,
                    edge_color="#2c3e50",  # Darker blue-grey
                )

            # Add labels with better positioning and background
            labels = nx.get_node_attributes(self._graph, "name") or {
                node: node for node in self._graph.nodes()
            }
            label_pos = {
                node: (x, y + 0.02) for node, (x, y) in pos.items()
            }  # Slightly offset labels

            # Draw labels with white background for better readability
            for node, (x, y) in label_pos.items():
                plt.text(
                    x,
                    y,
                    labels[node],
                    horizontalalignment="center",
                    verticalalignment="center",
                    fontsize=8,
                    fontweight="bold",
                    bbox=dict(
                        facecolor="white", edgecolor="none", alpha=0.7, pad=2.0
                    ),
                )

            # Add edge weight labels for significant weights
            edge_labels = nx.get_edge_attributes(self._graph, "weight")
            significant_edges = {
                (u, v): w
                for (u, v), w in edge_labels.items()
                if w > max_weight * 0.3
            }
            if significant_edges:
                nx.draw_networkx_edge_labels(
                    self._graph,
                    pos,
                    edge_labels=significant_edges,
                    font_size=6,
                    bbox=dict(facecolor="white", edgecolor="none", alpha=0.7),
                )

            # Improve layout
            plt.title("Memory Concept Graph", fontsize=16, pad=20)
            plt.axis("off")

            # Add padding and save
            plt.tight_layout(pad=2.0)
            plt.savefig(filename, bbox_inches="tight", facecolor="white")
            plt.close()

            logger.info(f"MemoryGraph image saved successfully to '{filename}'")

        except Exception as e:
            logger.error(f"Failed to save MemoryGraph image: {e}")
            plt.close()


class FlockMemoryStore(BaseModel):
    """Enhanced Flock memory storage with short-term and long-term memory.

    including embedding-based semantic search, exact matching, and result combination.
    """

    short_term: list[MemoryEntry] = Field(default_factory=list)
    long_term: list[MemoryEntry] = Field(default_factory=list)
    concept_graph: MemoryGraph = Field(default_factory=MemoryGraph)
    clusters: dict[int, list[MemoryEntry]] = Field(default_factory=dict)
    # Instead of np.ndarray, store centroids as lists of floats.
    cluster_centroids: dict[int, list[float]] = Field(default_factory=dict)
    # The embedding model is stored as a private attribute, as it's not serializable.
    _embedding_model: SentenceTransformer | None = PrivateAttr(default=None)

    @classmethod
    def load_from_file(cls, file_path: str | None = None) -> "FlockMemoryStore":
        """Load a memory store from a JSON file.

        Args:
            file_path: Path to the JSON file containing the serialized memory store.
                      If None, returns an empty memory store.

        Returns:
            FlockMemoryStore: A new memory store instance with loaded data.

        Raises:
            FileNotFoundError: If the specified file doesn't exist
            JSONDecodeError: If the file contains invalid JSON
            ValueError: If the JSON structure is invalid
        """
        if file_path is None:
            logger.debug("No file path provided, creating new memory store")
            return cls()

        try:
            logger.info(f"Loading memory store from {file_path}")
            with open(file_path) as f:
                data = json.load(f)

            # Initialize a new store
            store = cls()

            # Load short-term memory entries
            store.short_term = [
                MemoryEntry(
                    id=entry["id"],
                    content=entry["content"],
                    embedding=entry.get("embedding"),
                    timestamp=datetime.fromisoformat(entry["timestamp"]),
                    access_count=entry.get("access_count", 0),
                    concepts=set(entry.get("concepts", [])),
                    decay_factor=entry.get("decay_factor", 1.0),
                )
                for entry in data.get("short_term", [])
            ]

            # Load long-term memory entries
            store.long_term = [
                MemoryEntry(
                    id=entry["id"],
                    content=entry["content"],
                    embedding=entry.get("embedding"),
                    timestamp=datetime.fromisoformat(entry["timestamp"]),
                    access_count=entry.get("access_count", 0),
                    concepts=set(entry.get("concepts", [])),
                    decay_factor=entry.get("decay_factor", 1.0),
                )
                for entry in data.get("long_term", [])
            ]

            # Load concept graph
            if "concept_graph" in data:
                graph_data = json.loads(data["concept_graph"]["graph_json"])
                store.concept_graph = MemoryGraph(
                    graph_json=json.dumps(graph_data)
                )

            # Load clusters
            if "clusters" in data:
                store.clusters = {
                    int(k): [
                        MemoryEntry(
                            id=entry["id"],
                            content=entry["content"],
                            embedding=entry.get("embedding"),
                            timestamp=datetime.fromisoformat(
                                entry["timestamp"]
                            ),
                            access_count=entry.get("access_count", 0),
                            concepts=set(entry.get("concepts", [])),
                            decay_factor=entry.get("decay_factor", 1.0),
                        )
                        for entry in v
                    ]
                    for k, v in data["clusters"].items()
                }

            # Load cluster centroids
            if "cluster_centroids" in data:
                store.cluster_centroids = {
                    int(k): v for k, v in data["cluster_centroids"].items()
                }

            # Initialize the embedding model
            store._embedding_model = None  # Will be lazy-loaded when needed

            logger.info(
                f"Successfully loaded memory store with "
                f"{len(store.short_term)} short-term and "
                f"{len(store.long_term)} long-term entries"
            )
            return store

        except FileNotFoundError:
            logger.warning(
                f"Memory file {file_path} not found, creating new store"
            )
            return cls()
        except json.JSONDecodeError as e:
            logger.error(f"Invalid JSON in memory file: {e}")
            raise
        except Exception as e:
            logger.error(f"Error loading memory store: {e}")
            raise ValueError(f"Failed to load memory store: {e}")

    @classmethod
    def merge_stores(
        cls, stores: list["FlockMemoryStore"]
    ) -> "FlockMemoryStore":
        """Merge multiple memory stores into a single store.

        Args:
            stores: List of FlockMemoryStore instances to merge

        Returns:
            FlockMemoryStore: A new memory store containing merged data
        """
        merged = cls()

        # Merge short-term and long-term memories
        for store in stores:
            merged.short_term.extend(store.short_term)
            merged.long_term.extend(store.long_term)

        # Merge concept graphs
        merged_graph = nx.Graph()
        for store in stores:
            if store.concept_graph and store.concept_graph.graph:
                merged_graph = nx.compose(
                    merged_graph, store.concept_graph.graph
                )

        merged.concept_graph = MemoryGraph(
            graph_json=json.dumps(
                nx.node_link_data(merged_graph, edges="links")
            )
        )

        # Recompute clusters for the merged data
        if merged.short_term:
            merged._update_clusters()

        return merged

    def get_embedding_model(self) -> SentenceTransformer:
        """Initialize and return the SentenceTransformer model.

        Uses "all-MiniLM-L6-v2" as the default model.
        """
        if self._embedding_model is None:
            try:
                logger.debug(
                    "Loading SentenceTransformer model 'all-MiniLM-L6-v2'."
                )
                self._embedding_model = SentenceTransformer("all-MiniLM-L6-v2")
            except Exception as e:
                logger.error(f"Failed to load embedding model: {e}")
                raise RuntimeError(f"Failed to load embedding model: {e}")
        return self._embedding_model

    def compute_embedding(self, text: str) -> np.ndarray:
        """Compute and return the embedding for the provided text as a NumPy array."""
        logger.debug(
            f"Computing embedding for text: {text[:100].replace('{', '{{').replace('}', '}}')}..."
        )  # Log first 30 chars for brevity.
        model = self.get_embedding_model()
        try:
            embedding = model.encode(text, convert_to_numpy=True)
            return embedding
        except Exception as e:
            logger.error(f"Error computing embedding: {e}")
            raise RuntimeError(f"Error computing embedding: {e}")

    def _calculate_similarity(
        self, query_embedding: np.ndarray, entry_embedding: np.ndarray
    ) -> float:
        """Compute the cosine similarity between two embeddings.

        Returns a float between 0 and 1.
        """
        try:
            norm_query = np.linalg.norm(query_embedding)
            norm_entry = np.linalg.norm(entry_embedding)
            if norm_query == 0 or norm_entry == 0:
                return 0.0
            similarity = float(
                np.dot(query_embedding, entry_embedding)
                / (norm_query * norm_entry)
            )
            return similarity
        except Exception as e:
            logger.error(f"Error computing similarity: {e}")
            raise RuntimeError(f"Error computing similarity: {e}")

    def exact_match(self, inputs: dict[str, Any]) -> list[MemoryEntry]:
        """Perform an exact key-based lookup in short-term memory.

        Returns entries where all provided key-value pairs exist in the entry's inputs.
        """
        logger.debug(f"Performing exact match lookup with inputs: {inputs}")
        matches = []
        for entry in self.short_term:
            if all(item in entry.inputs.items() for item in inputs.items()):
                matches.append(entry)
        logger.debug(f"Exact match found {len(matches)} entries.")
        return matches

    def combine_results(
        self, inputs: dict[str, Any], weights: dict[str, float]
    ) -> dict[str, Any]:
        """Combine semantic and exact match results using the provided weights.

        Args:
            inputs: Input dictionary to search memory.
            weights: Dictionary with keys "semantic" and "exact" for weighting.

        Returns:
            A dictionary with "combined_results" as a sorted list of memory entries.
        """
        logger.debug(
            f"Combining results for inputs: {inputs} with weights: {weights}"
        )
        query_text = " ".join(str(value) for value in inputs.values())
        query_embedding = self.compute_embedding(query_text)

        semantic_matches = self.retrieve(
            query_embedding, set(inputs.values()), similarity_threshold=0.8
        )
        exact_matches = self.exact_match(inputs)

        combined: dict[str, dict[str, Any]] = {}
        for entry in semantic_matches:
            if entry.embedding is None:
                continue
            semantic_score = self._calculate_similarity(
                query_embedding, np.array(entry.embedding)
            )
            combined[entry.id] = {
                "entry": entry,
                "semantic_score": semantic_score * weights.get("semantic", 0.7),
                "exact_score": 0.0,
            }
        for entry in exact_matches:
            if entry.id in combined:
                combined[entry.id]["exact_score"] = 1.0 * weights.get(
                    "exact", 0.3
                )
            else:
                combined[entry.id] = {
                    "entry": entry,
                    "semantic_score": 0.0,
                    "exact_score": 1.0 * weights.get("exact", 0.3),
                }
        results: list[tuple[float, MemoryEntry]] = []
        for data in combined.values():
            total_score = data["semantic_score"] + data["exact_score"]
            results.append((total_score, data["entry"]))
        results.sort(key=lambda x: x[0], reverse=True)
        logger.debug(f"Combined results count: {len(results)}")
        return {"combined_results": [entry for score, entry in results]}

    def add_entry(self, entry: MemoryEntry) -> None:
        """Add a new memory entry to short-term memory, update the concept graph and clusters.

        and check for promotion to long-term memory.
        """
        with tracer.start_as_current_span("memory.add_entry") as span:
            logger.info(f"Adding memory entry with id: {entry.id}")
            span.set_attribute("entry.id", entry.id)
            self.short_term.append(entry)
            self.concept_graph.add_concepts(entry.concepts)
            self._update_clusters()
            if entry.access_count > 10:
                self._promote_to_long_term(entry)

    def _promote_to_long_term(self, entry: MemoryEntry) -> None:
        """Promote an entry to long-term memory."""
        logger.info(f"Promoting entry {entry.id} to long-term memory.")
        if entry not in self.long_term:
            self.long_term.append(entry)

    def retrieve(
        self,
        query_embedding: np.ndarray,
        query_concepts: set[str],
        similarity_threshold: float = 0.8,
        exclude_last_n: int = 0,
    ) -> list[MemoryEntry]:
        """Retrieve memory entries using semantic similarity and concept-based activation."""
        with tracer.start_as_current_span("memory.retrieve") as span:
            logger.debug("Retrieving memory entries...")
            results = []
            current_time = datetime.now()
            decay_rate = 0.0001
            norm_query = query_embedding / (
                np.linalg.norm(query_embedding) + 1e-8
            )

            entries = (
                self.short_term[:-exclude_last_n]
                if exclude_last_n > 0
                else self.short_term
            )

            for entry in entries:
                if entry.embedding is None:
                    continue

                # Calculate base similarity
                entry_embedding = np.array(entry.embedding)
                norm_entry = entry_embedding / (
                    np.linalg.norm(entry_embedding) + 1e-8
                )
                similarity = float(np.dot(norm_query, norm_entry))

                # Calculate modifiers
                time_diff = (current_time - entry.timestamp).total_seconds()
                decay = np.exp(-decay_rate * time_diff)
                # Add 1 to base score so new entries aren't zeroed out
                reinforcement = 1.0 + np.log1p(entry.access_count)

                # Calculate final score
                final_score = (
                    similarity * decay * reinforcement * entry.decay_factor
                )

                span.add_event(
                    "memory score",
                    attributes={
                        "entry_id": entry.id,
                        "similarity": similarity,
                        "final_score": final_score,
                    },
                )

                # If base similarity passes threshold, include in results
                if similarity >= similarity_threshold:
                    results.append((final_score, entry))

            # Update access counts and decay for retrieved entries
            for _, entry in results:
                entry.access_count += 1
                self._update_decay_factors(entry)

            # Sort by final score
            results.sort(key=lambda x: x[0], reverse=True)
            logger.debug(f"Retrieved {len(results)} memory entries.")
            return [entry for score, entry in results]

    def _update_decay_factors(self, retrieved_entry: MemoryEntry) -> None:
        """Update decay factors: increase for the retrieved entry and decrease for others."""
        logger.debug(f"Updating decay factor for entry {retrieved_entry.id}")
        retrieved_entry.decay_factor *= 1.1
        for entry in self.short_term:
            if entry != retrieved_entry:
                entry.decay_factor *= 0.9

    def _update_clusters(self) -> None:
        """Update memory clusters using k-means clustering on entry embeddings."""
        logger.debug("Updating memory clusters...")
        if len(self.short_term) < 2:
            logger.debug("Not enough entries for clustering.")
            return

        valid_entries = [
            entry for entry in self.short_term if entry.embedding is not None
        ]
        if not valid_entries:
            logger.debug(
                "No valid entries with embeddings found for clustering."
            )
            return

        embeddings = [np.array(entry.embedding) for entry in valid_entries]
        embeddings_matrix = np.vstack(embeddings)

        from sklearn.cluster import KMeans

        n_clusters = min(10, len(embeddings))
        kmeans = KMeans(n_clusters=n_clusters, random_state=42)
        labels = kmeans.fit_predict(embeddings_matrix)

        self.clusters.clear()
        self.cluster_centroids.clear()

        for i in range(n_clusters):
            cluster_entries = [
                entry
                for entry, label in zip(valid_entries, labels)
                if label == i
            ]
            self.clusters[i] = cluster_entries
            # Convert the centroid (np.ndarray) to a list of floats.
            self.cluster_centroids[i] = kmeans.cluster_centers_[i].tolist()
        logger.debug(f"Clustering complete with {n_clusters} clusters.")
```

### src\flock\modules\output\__init__.py

- **Lines**: 1
- **Last modified**: 2025-05-21 19:51:15

```py
# Package for modules
```

### src\flock\modules\output\output_module.py

- **Lines**: 196
- **Last modified**: 2025-05-06 21:09:36

```py
"""Output formatting and display functionality for agents."""

from typing import TYPE_CHECKING, Any

from pydantic import Field

from flock.core.context.context_vars import FLOCK_BATCH_SILENT_MODE
from flock.core.flock_registry import flock_component

if TYPE_CHECKING:
    from flock.core import FlockAgent

from flock.core.context.context import FlockContext
from flock.core.flock_module import FlockModule, FlockModuleConfig
from flock.core.logging.formatters.themed_formatter import (
    ThemedAgentResultFormatter,
)
from flock.core.logging.formatters.themes import OutputTheme
from flock.core.logging.logging import get_logger

# from flock.core.logging.formatters.themes import OutputTheme
# from flock.core.logging.logging import get_logger
# from flock.core.serialization.json_encoder import FlockJSONEncoder

logger = get_logger("module.output")


class OutputModuleConfig(FlockModuleConfig):
    """Configuration for output formatting and display."""

    theme: OutputTheme = Field(
        default=OutputTheme.afterglow, description="Theme for output formatting"
    )
    render_table: bool = Field(
        default=False, description="Whether to render output as a table"
    )
    max_length: int = Field(
        default=1000, description="Maximum length for displayed output"
    )
    truncate_long_values: bool = Field(
        default=True, description="Whether to truncate long values in display"
    )
    show_metadata: bool = Field(
        default=True, description="Whether to show metadata like timestamps"
    )
    format_code_blocks: bool = Field(
        default=True,
        description="Whether to apply syntax highlighting to code blocks",
    )
    custom_formatters: dict[str, str] = Field(
        default_factory=dict,
        description="Custom formatters for specific output types",
    )
    no_output: bool = Field(
        default=False,
        description="Whether to suppress output",
    )
    print_context: bool = Field(
        default=False,
        description="Whether to print the context",
    )


@flock_component(config_class=OutputModuleConfig)
class OutputModule(FlockModule):
    """Module that handles output formatting and display."""

    name: str = "output"
    config: OutputModuleConfig = Field(
        default_factory=OutputModuleConfig, description="Output configuration"
    )

    def __init__(self, name: str, config: OutputModuleConfig):
        super().__init__(name=name, config=config)
        self._formatter = ThemedAgentResultFormatter(
            theme=self.config.theme,
            max_length=self.config.max_length,
            render_table=self.config.render_table,
        )

    def _format_value(self, value: Any, key: str) -> str:
        """Format a single value based on its type and configuration."""
        # Check for custom formatter
        if key in self.config.custom_formatters:
            formatter_name = self.config.custom_formatters[key]
            if hasattr(self, f"_format_{formatter_name}"):
                return getattr(self, f"_format_{formatter_name}")(value)

        # Default formatting based on type
        if isinstance(value, dict):
            return self._format_dict(value)
        elif isinstance(value, list):
            return self._format_list(value)
        elif isinstance(value, str) and self.config.format_code_blocks:
            return self._format_potential_code(value)
        else:
            return str(value)

    def _format_dict(self, d: dict[str, Any], indent: int = 0) -> str:
        """Format a dictionary with proper indentation."""
        lines = []
        for k, v in d.items():
            formatted_value = self._format_value(v, k)
            if (
                self.config.truncate_long_values
                and len(formatted_value) > self.config.max_length
            ):
                formatted_value = (
                    formatted_value[: self.config.max_length] + "..."
                )
            lines.append(f"{'  ' * indent}{k}: {formatted_value}")
        return "\n".join(lines)

    def _format_list(self, lst: list[Any]) -> str:
        """Format a list with proper indentation."""
        return "\n".join(f"- {self._format_value(item, '')}" for item in lst)

    def _format_potential_code(self, text: str) -> str:
        """Format text that might contain code blocks."""
        import re

        def replace_code_block(match):
            code = match.group(2)
            lang = match.group(1) if match.group(1) else ""
            # Here you could add syntax highlighting
            return f"```{lang}\n{code}\n```"

        # Replace code blocks with formatted versions
        text = re.sub(
            r"```(\w+)?\n(.*?)\n```", replace_code_block, text, flags=re.DOTALL
        )
        return text

    async def on_post_evaluate(
        self,
        agent: "FlockAgent",
        inputs: dict[str, Any],
        context: FlockContext | None = None,
        result: dict[str, Any] | None = None,
    ) -> dict[str, Any]:
        """Format and display the output."""
        logger.debug("Formatting and displaying output")

        # Determine if output should be suppressed
        is_silent = self.config.no_output or (
            context and context.get_variable(FLOCK_BATCH_SILENT_MODE, False)
        )

        if is_silent:
            logger.debug("Output suppressed (config or batch silent mode).")
            return result  # Skip console output

        logger.debug("Formatting and displaying output to console.")

        if self.config.print_context and context:
            # Add context snapshot if requested (be careful with large contexts)
            try:
                # Create a copy or select relevant parts to avoid modifying original result dict directly
                display_result = result.copy()
                display_result["context_snapshot"] = (
                    context.to_dict()
                )  # Potential performance hit
            except Exception:
                display_result = result.copy()
                display_result["context_snapshot"] = (
                    "[Error serializing context]"
                )
            result_to_display = display_result
        else:
            result_to_display = result

        if not hasattr(self, "_formatter") or self._formatter is None:
            self._formatter = ThemedAgentResultFormatter(
                theme=self.config.theme,
                max_length=self.config.max_length,
                render_table=self.config.render_table,
                wait_for_input=self.config.wait_for_input,
            )
        self._formatter.display_result(result_to_display, agent.name)

        return result  # Return the original, unmodified result

    def update_theme(self, new_theme: OutputTheme) -> None:
        """Update the output theme."""
        self.config.theme = new_theme
        self._formatter = ThemedAgentResultFormatter(
            theme=self.config.theme,
            max_length=self.config.max_length,
            render_table=self.config.render_table,
            wait_for_input=self.config.wait_for_input,
            write_to_file=self.config.write_to_file,
        )

    def add_custom_formatter(self, key: str, formatter_name: str) -> None:
        """Add a custom formatter for a specific output key."""
        self.config.custom_formatters[key] = formatter_name
```

### src\flock\modules\performance\__init__.py

- **Lines**: 1
- **Last modified**: 2025-05-21 19:51:15

```py
# Package for modules
```

### src\flock\modules\performance\metrics_module.py

- **Lines**: 678
- **Last modified**: 2025-05-22 21:27:37

```py
"""Performance and metrics tracking for Flock agents."""

import json
import os
import time
from collections import defaultdict
from datetime import datetime
from typing import Any, Literal

import numpy as np
import psutil
from pydantic import BaseModel, Field, validator

from flock.core.context.context import FlockContext
from flock.core.flock_agent import FlockAgent
from flock.core.flock_module import FlockModule, FlockModuleConfig
from flock.core.flock_registry import flock_component
from flock.core.mcp.flock_mcp_server import FlockMCPServerBase


class MetricPoint(BaseModel):
    """Single metric measurement."""

    timestamp: datetime
    value: int | float | str
    tags: dict[str, str] = {}

    class Config:
        arbitrary_types_allowed = True


class MetricsModuleConfig(FlockModuleConfig):
    """Configuration for performance metrics collection."""

    # Collection settings
    collect_timing: bool = Field(
        default=True, description="Collect timing metrics"
    )
    collect_memory: bool = Field(
        default=True, description="Collect memory usage"
    )
    collect_token_usage: bool = Field(
        default=True, description="Collect token usage stats"
    )
    collect_cpu: bool = Field(default=True, description="Collect CPU usage")

    # Storage settings
    storage_type: Literal["json", "prometheus", "memory"] = Field(
        default="json", description="Where to store metrics"
    )
    metrics_dir: str = Field(
        default=".flock/metrics/", description="Directory for metrics storage"
    )

    # Aggregation settings
    aggregation_interval: str = Field(
        default="1h", description="Interval for metric aggregation"
    )
    retention_days: int = Field(default=30, description="Days to keep metrics")

    # Alerting settings
    alert_on_high_latency: bool = Field(
        default=True, description="Alert on high latency"
    )
    latency_threshold_ms: int = Field(
        default=1000, description="Threshold for latency alerts"
    )

    @validator("aggregation_interval")
    def validate_interval(cls, v):
        """Validate time interval format."""
        if v[-1] not in ["s", "m", "h", "d"]:
            raise ValueError("Interval must end with s, m, h, or d")
        return v


@flock_component(config_class=MetricsModuleConfig)
class MetricsModule(FlockModule):
    """Module for collecting and analyzing agent performance metrics."""

    # --- Singleton holder for convenient static access ---
    _INSTANCE: "MetricsModule | None" = None

    name: str = "performance_metrics"
    config: MetricsModuleConfig = Field(
        default_factory=MetricsModuleConfig,
        description="Performance metrics configuration",
    )

    def __init__(self, name, config):
        super().__init__(name=name, config=config)
        # Register singleton for static helpers
        MetricsModule._INSTANCE = self
        self._metrics = defaultdict(list)
        self._start_time: float | None = None
        self._server_start_time: float | None = None
        self._start_memory: int | None = None
        self._server_start_memory: int | None = None
        self._client_refreshs: int = 0

        # Set up storage
        if self.config.storage_type == "json":
            os.makedirs(self.config.metrics_dir, exist_ok=True)

        # Set up prometheus if needed
        if self.config.storage_type == "prometheus":
            try:
                from prometheus_client import Counter, Gauge, Histogram

                self._prom_latency = Histogram(
                    "flock_agent_latency_seconds",
                    "Time taken for agent evaluation",
                    ["agent_name"],
                )
                self._prom_memory = Gauge(
                    "flock_agent_memory_bytes",
                    "Memory usage by agent",
                    ["agent_name"],
                )
                self._prom_tokens = Counter(
                    "flock_agent_tokens_total",
                    "Token usage by agent",
                    ["agent_name", "type"],
                )
                self._prom_errors = Counter(
                    "flock_agent_errors_total",
                    "Error count by agent",
                    ["agent_name", "error_type"],
                )
            except ImportError:
                self.config.storage_type = "json"

    """Fixes for metrics summary calculation."""

    def _load_metrics_from_files(
        self, metric_name: str = None
    ) -> dict[str, list[MetricPoint]]:
        """Load metrics from JSON files."""
        metrics = defaultdict(list)

        try:
            # Get all metric files
            files = [
                f
                for f in os.listdir(self.config.metrics_dir)
                if f.endswith(".json") and not f.startswith("summary_")
            ]

            # Filter by metric name if specified
            if metric_name:
                files = [f for f in files if f.startswith(f"{metric_name}_")]

            for filename in files:
                filepath = os.path.join(self.config.metrics_dir, filename)
                with open(filepath) as f:
                    for line in f:
                        try:
                            data = json.loads(line)
                            point = MetricPoint(
                                timestamp=datetime.fromisoformat(
                                    data["timestamp"]
                                ),
                                value=data["value"],
                                tags=data["tags"],
                            )
                            name = filename.split("_")[
                                0
                            ]  # Get metric name from filename
                            metrics[name].append(point)
                        except json.JSONDecodeError:
                            continue

            return dict(metrics)
        except Exception as e:
            print(f"Error loading metrics from files: {e}")
            return {}

    def get_metrics(
        self,
        metric_name: str | None = None,
        start_time: datetime | None = None,
        end_time: datetime | None = None,
    ) -> dict[str, list[MetricPoint]]:
        """Get recorded metrics with optional filtering."""
        # Get metrics from appropriate source
        if self.config.storage_type == "json":
            metrics = self._load_metrics_from_files(metric_name)
        else:
            metrics = self._metrics
            if metric_name:
                metrics = {metric_name: metrics[metric_name]}

        # Apply time filtering if needed
        if start_time or end_time:
            filtered_metrics = defaultdict(list)
            for name, points in metrics.items():
                filtered_points = [
                    p
                    for p in points
                    if (not start_time or p.timestamp >= start_time)
                    and (not end_time or p.timestamp <= end_time)
                ]
                filtered_metrics[name] = filtered_points
            metrics = filtered_metrics

        return dict(metrics)

    def get_statistics(
        self, metric_name: str, percentiles: list[float] = [50, 90, 95, 99]
    ) -> dict[str, float]:
        """Calculate statistics for a metric."""
        # Get all points for this metric
        metrics = self.get_metrics(metric_name=metric_name)
        points = metrics.get(metric_name, [])

        if not points:
            return {}

        values = [p.value for p in points if isinstance(p.value, (int, float))]
        if not values:
            return {}

        stats = {
            "min": min(values),
            "max": max(values),
            "mean": float(
                np.mean(values)
            ),  # Convert to float for JSON serialization
            "std": float(np.std(values)),
            "count": len(values),
            "last_value": values[-1],
        }

        for p in percentiles:
            stats[f"p{p}"] = float(np.percentile(values, p))

        return stats

    async def on_terminate(
        self,
        agent: FlockAgent,
        inputs: dict[str, Any],
        context: FlockContext | None = None,
        result: dict[str, Any] | None = None,
    ) -> None:
        """Clean up and final metric recording."""
        if self.config.storage_type == "json":
            # Save aggregated metrics
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            summary_file = os.path.join(
                self.config.metrics_dir,
                f"summary_{agent.name}_{timestamp}.json",
            )

            # Calculate summary for all metrics
            summary = {
                "agent": agent.name,
                "timestamp": timestamp,
                "metrics": {},
            }

            # Get all unique metric names from files
            all_metrics = self._load_metrics_from_files()

            for metric_name in all_metrics:
                stats = self.get_statistics(metric_name)
                if stats:  # Only include metrics that have data
                    summary["metrics"][metric_name] = stats

            with open(summary_file, "w") as f:
                json.dump(summary, f, indent=2)

    def _record_metric(
        self, name: str, value: int | float | str, tags: dict[str, str] = None
    ) -> None:
        """Record a single metric point."""
        point = MetricPoint(
            timestamp=datetime.now(), value=value, tags=tags or {}
        )

        # Store metric
        if self.config.storage_type == "memory":
            self._metrics[name].append(point)

        elif self.config.storage_type == "prometheus":
            if name == "latency":
                self._prom_latency.labels(**tags).observe(value)
            elif name == "memory":
                self._prom_memory.labels(**tags).set(value)
            elif name == "tokens":
                self._prom_tokens.labels(**tags).inc(value)

        elif self.config.storage_type == "json":
            self._save_metric_to_file(name, point)

    def _save_metric_to_file(self, name: str, point: MetricPoint) -> None:
        """Save metric to JSON file."""
        filename = f"{name}_{point.timestamp.strftime('%Y%m')}.json"
        filepath = os.path.join(self.config.metrics_dir, filename)

        data = {
            "timestamp": point.timestamp.isoformat(),
            "value": point.value,
            "tags": point.tags,
        }

        # Append to file
        with open(filepath, "a") as f:
            f.write(json.dumps(data) + "\n")

    def _get_tokenizer(self, model: str):
        """Get the appropriate tokenizer for the model."""
        try:
            import tiktoken

            # Handle different model naming conventions
            if model.startswith("openai/"):
                model = model[7:]  # Strip 'openai/' prefix

            try:
                return tiktoken.encoding_for_model(model)
            except KeyError:
                # Fallback to cl100k_base for unknown models
                return tiktoken.get_encoding("cl100k_base")

        except ImportError:
            return None

    def _calculate_token_usage(self, text: str, model: str = "gpt-4") -> int:
        """Calculate token count using tiktoken when available."""
        tokenizer = self._get_tokenizer(model)

        if tokenizer:
            # Use tiktoken for accurate count
            return len(tokenizer.encode(text))
        else:
            # Fallback to estimation if tiktoken not available
            # Simple estimation - words / 0.75 for average tokens per word
            token_estimate = int(len(text.split()) / 0.75)

            # Log warning about estimation
            print(
                f"Warning: Using estimated token count. Install tiktoken for accurate counting."
            )

    def _should_alert(self, metric: str, value: float) -> bool:
        """Check if metric should trigger alert."""
        if metric == "latency" and self.config.alert_on_high_latency:
            return value * 1000 > self.config.latency_threshold_ms
        return False

    async def on_initialize(
        self,
        agent: FlockAgent,
        inputs: dict[str, Any],
        context: FlockContext | None = None,
    ) -> None:
        """Initialize metrics collection."""
        self._start_time = time.time()

        if self.config.collect_memory:
            self._start_memory = psutil.Process().memory_info().rss
            self._record_metric(
                "memory",
                self._start_memory,
                {"agent": agent.name, "phase": "start"},
            )

    def _calculate_cost(
        self, text: str, model: str, is_completion: bool = False
    ) -> tuple[int, float]:
        """Calculate both token count and cost."""
        # Get token count
        try:
            from litellm import cost_per_token

            token_count = self._calculate_token_usage(text, model)
            # Calculate total cost
            if is_completion:
                total_cost = token_count * cost_per_token(
                    model, completion_tokens=token_count
                )
            else:
                total_cost = token_count * cost_per_token(
                    model, prompt_tokens=token_count
                )

            return token_count, total_cost
        except Exception:
            token_count = 0
            total_cost = 0.0
            return token_count, total_cost

    async def on_pre_evaluate(
        self,
        agent: FlockAgent,
        inputs: dict[str, Any],
        context: FlockContext | None = None,
    ) -> dict[str, Any]:
        """Record pre-evaluation metrics."""
        if self.config.collect_token_usage:
            # Calculate input tokens and cost
            total_input_tokens = 0
            total_input_cost = 0.0

            for v in inputs.values():
                tokens, cost = self._calculate_cost(
                    str(v), agent.model, is_completion=False
                )
                total_input_tokens += tokens
                if isinstance(cost, float):
                    total_input_cost += cost
                else:
                    total_input_cost += cost[1]

            self._record_metric(
                "tokens",
                total_input_tokens,
                {"agent": agent.name, "type": "input"},
            )
            self._record_metric(
                "cost", total_input_cost, {"agent": agent.name, "type": "input"}
            )

        if self.config.collect_cpu:
            cpu_percent = psutil.Process().cpu_percent()
            self._record_metric(
                "cpu",
                cpu_percent,
                {"agent": agent.name, "phase": "pre_evaluate"},
            )

        return inputs

    async def on_post_evaluate(
        self,
        agent: FlockAgent,
        inputs: dict[str, Any],
        context: FlockContext | None = None,
        result: dict[str, Any] | None = None,
    ) -> dict[str, Any]:
        """Record post-evaluation metrics."""
        if self.config.collect_timing and self._start_time:
            latency = time.time() - self._start_time
            self._record_metric("latency", latency, {"agent": agent.name})

            # Check for alerts
            if self._should_alert("latency", latency):
                # In practice, you'd want to integrate with a proper alerting system
                print(f"ALERT: High latency detected: {latency * 1000:.2f}ms")

        if self.config.collect_token_usage:
            # Calculate output tokens and cost
            total_output_tokens = 0
            total_output_cost = 0.0

            for v in result.values():
                tokens, cost = self._calculate_cost(
                    str(v), agent.model, is_completion=True
                )
                total_output_tokens += tokens
                if isinstance(cost, float):
                    total_output_cost += cost
                else:
                    total_output_cost += cost[1]

            self._record_metric(
                "tokens",
                total_output_tokens,
                {"agent": agent.name, "type": "output"},
            )
            self._record_metric(
                "cost",
                total_output_cost,
                {"agent": agent.name, "type": "output"},
            )

            # Record total cost for this operation
            self._record_metric(
                "total_cost",
                total_output_cost + total_output_cost,
                {"agent": agent.name},
            )

        if self.config.collect_memory and self._start_memory:
            current_memory = psutil.Process().memory_info().rss
            memory_diff = current_memory - self._start_memory
            self._record_metric(
                "memory", memory_diff, {"agent": agent.name, "phase": "end"}
            )

        return result

    async def on_error(
        self,
        agent: FlockAgent,
        error: Exception,
        inputs: dict[str, Any],
        context: FlockContext | None = None,
    ) -> None:
        """Record error metrics."""
        self._record_metric(
            "errors",
            1,
            {"agent": agent.name, "error_type": type(error).__name__},
        )

    # --------------------------------------------------
    # Public helper for external modules
    # --------------------------------------------------
    @classmethod
    def record(cls, name: str, value: int | float | str, tags: dict[str, str] | None = None):
        """Record a metric from anywhere in the codebase.

        Example:
            MetricsModule.record("custom_latency", 123, {"stage": "inference"})
        The call will forward to the *first* instantiated MetricsModule.  If no
        instance exists in the current run the call is a no-op so that importing
        this helper never crashes test-code.
        """
        instance = cls._INSTANCE
        if instance is None:
            return  # silently ignore if module isn't active
        instance._record_metric(name, value, tags or {})

    # --- MCP Server Lifecycle Hooks ---
    async def on_server_error(
        self, server: FlockMCPServerBase, error: Exception
    ) -> None:
        """Record server error metrics."""
        self._record_metric(
            "errors",
            1,
            {
                "server": server.config.name,
                "error_type": type(error).__name__,
            },
        )

    async def on_pre_server_init(self, server: FlockMCPServerBase):
        """Initialize metrics collection for server."""
        self._server_start_time = time.time()

        if self.config.collect_memory:
            self._server_start_memory = psutil.Process().memory_info().rss
            self._record_metric(
                "server_memory",
                self._server_start_memory,
                {"server": server.config.name, "phase": "pre_init"},
            )

    async def on_post_server_init(self, server: FlockMCPServerBase):
        """Collect metrics after server starts."""
        if self.config.collect_memory:
            checkpoint_memory = psutil.Process().memory_info().rss
            self._record_metric(
                "server_memory",
                checkpoint_memory,
                {"server": server.config.name, "phase": "post_init"},
            )

    async def on_pre_server_terminate(self, server: FlockMCPServerBase):
        """Collect metrics before server terminates."""
        if self.config.collect_memory:
            checkpoint_memory = psutil.Process().memory_info().rss
            self._record_metric(
                "server_memory",
                checkpoint_memory,
                {"server": server.config.name, "phase": "pre_terminate"},
            )

    async def on_post_server_terminate(self, server: FlockMCPServerBase):
        """Collect metrics after server terminates.

        Clean up and final metric recording.
        """
        if self.config.storage_type == "json":
            # Save aggregated metrics
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            summary_file = os.path.join(
                self.config.metrics_dir,
                f"summary_{server.config.name}_{timestamp}.json",
            )

            # Calculate summary for all metrics
            summary = {
                "server": server.config.name,
                "timestamp": timestamp,
                "metrics": {},
            }

            # Get all unique metric names from files
            all_metrics = self._load_metrics_from_files()

            for metric_name in all_metrics:
                stats = self.get_statistics(metric_name)
                if stats:  # Only include metrics that have data
                    summary["metrics"][metric_name] = stats
            with open(summary_file, "w") as f:
                json.dump(summary, f, indent=2)

    async def on_pre_mcp_call(
        self, server: FlockMCPServerBase, arguments: Any | None = None
    ):
        """Record pre-call metrics."""
        if self.config.collect_cpu:
            cpu_percent = psutil.Process().cpu_percent()
            self._record_metric(
                "cpu",
                cpu_percent,
                {"server": server.config.name, "phase": "pre_mcp_call"},
            )
        if self.config.collect_memory:
            current_memory = psutil.Process().memory_info().rss
            memory_diff = current_memory - self._server_start_memory
            self._record_metric(
                "memory",
                memory_diff,
                {"server": server.config.name, "phase": "pre_mcp_call"},
            )

        if isinstance(arguments, dict):
            self._record_metric(
                "arguments",
                len(arguments),
                {
                    "server": server.config.name,
                    "phase": "pre_mcp_call",
                }.update(arguments),
            )

    async def on_post_mcp_call(
        self, server: FlockMCPServerBase, result: Any | None = None
    ):
        """Record post-call metrics."""
        if self.config.collect_timing and self._server_start_time:
            latency = time.time() - self._server_start_time
            self._record_metric(
                "latency", latency, {"server": server.config.name}
            )

            # Check for alerts
            if self._should_alert("latency", latency):
                # In practice, you'd want to integrate with a proper alerting system
                print(f"ALERT: High latency detected: {latency * 1000:.2f}ms")

        if self.config.collect_cpu:
            cpu_percent = psutil.Process().cpu_percent()
            self._record_metric(
                "cpu",
                cpu_percent,
                {"server": server.config.name, "phase": "post_mcp_call"},
            )
        if self.config.collect_memory:
            current_memory = psutil.Process().memory_info().rss
            memory_diff = current_memory - self._server_start_memory
            self._record_metric(
                "memory",
                memory_diff,
                {"server": server.config.name, "phase": "post_mcp_call"},
            )

    async def on_connect(
        self, server: FlockMCPServerBase, additional_params: dict[str, Any]
    ) -> dict[str, Any]:
        """Collect metrics during connect."""
        # We should track the refresh rate for clients
        if "refresh_client" in additional_params and additional_params.get(
            "refresh_client", False
        ):
            self._client_refreshs += 1
            self._record_metric(
                "client_refreshs",
                self._client_refreshs,
                {"server": server.config.name, "phase": "connect"},
            )

        return additional_params
```

### src\flock\modules\zep\__init__.py

- **Lines**: 1
- **Last modified**: 2025-05-21 19:51:15

```py
# Package for modules
```

### src\flock\modules\zep\zep_module.py

- **Lines**: 192
- **Last modified**: 2025-05-21 19:51:15

```py
import uuid
from typing import Any

from pydantic import Field
from zep_python.client import Zep
from zep_python.types import Message as ZepMessage, SessionSearchResult

from flock.core.context.context import FlockContext
from flock.core.flock_agent import FlockAgent
from flock.core.flock_module import FlockModule, FlockModuleConfig
from flock.core.flock_registry import flock_component
from flock.core.logging.logging import get_logger

logger = get_logger("module.zep")


class ZepModuleConfig(FlockModuleConfig):
    """Configuration for the Zep module."""

    zep_url: str = "http://localhost:8000"
    zep_api_key: str = "apikey"
    min_fact_rating: float = Field(
        default=0.7, description="Minimum rating for facts to be considered"
    )
    enable_read: bool = True
    enable_write: bool = False
    top_k: int = Field(default=10, description="Number of memories to retrieve")
    user_id: str = Field(default="flock", description="User ID the memories will be associated with")
    agent_id: str = Field(default="flock", description="Agent ID the memories will be associated with")
    memory_input_key: str | None = Field(default=None, description="Input key to use for memory, if none the description of the agent will be used")



@flock_component(config_class=ZepModuleConfig)
class ZepModule(FlockModule):
    """Module that adds Zep capabilities to a Flock agent."""

    name: str = "zep"
    config: ZepModuleConfig = ZepModuleConfig()
    session_id: str | None = None
    user_id: str | None = None

    def __init__(self, name, config: ZepModuleConfig) -> None:
        """Initialize Zep module."""
        super().__init__(name=name, config=config)
        logger.debug("Initializing Zep module")
        zep_client = Zep(
            base_url=self.config.zep_url, api_key=self.config.zep_api_key
        )
        self.user_id = self.name
        self._setup_user(zep_client)
        self.session_id = str(uuid.uuid4())
        self._setup_session(zep_client)

    def _setup_user(self, zep_client: Zep) -> None:
        """Set up user in Zep."""
        if not zep_client or not self.user_id:
            raise ValueError("Zep service or user_id not initialized")

        try:
            user = zep_client.user.get(user_id=self.user_id)
            if not user:
                zep_client.user.add(user_id=self.user_id)
        except Exception:
            zep_client.user.add(user_id=self.user_id)

    def _setup_session(self, zep_client: Zep) -> None:
        """Set up new session."""
        if not zep_client or not self.user_id or not self.session_id:
            raise ValueError(
                "Zep service, user_id, or session_id not initialized"
            )

        zep_client.memory.add_session(
            user_id=self.user_id,
            session_id=self.session_id,
        )

    def get_client(self) -> Zep:
        """Get Zep client."""
        return Zep(
            base_url=self.config.zep_url, api_key=self.config.zep_api_key
        )

    def get_memory(self, zep_client: Zep) -> str | None:
        """Get memory for the current session."""
        if not zep_client or not self.session_id:
            logger.error("Zep service or session_id not initialized")
            return None

        try:
            memory = zep_client.memory.get(
                self.session_id, min_rating=self.config.min_fact_rating
            )
            if memory:
                return f"{memory.relevant_facts}"
        except Exception as e:
            logger.error(f"Error fetching memory: {e}")
            return None

        return None

    def split_text(
        self, text: str | None, max_length: int = 1000
    ) -> list[ZepMessage]:
        """Split text into smaller chunks."""
        result: list[ZepMessage] = []
        if not text:
            return result
        if len(text) <= max_length:
            return [ZepMessage(role="user", content=text, role_type="user")]
        for i in range(0, len(text), max_length):
            result.append(
                ZepMessage(
                    role="user",
                    content=text[i : i + max_length],
                    role_type="user",
                )
            )
        return result

    def add_to_memory(self, text: str, zep_client: Zep) -> None:
        """Add text to memory."""
        if not zep_client or not self.session_id:
            logger.error("Zep service or session_id not initialized")
            return

        messages = self.split_text(text)
        zep_client.memory.add(session_id=self.session_id, messages=messages)

    def search_memory(
        self, query: str, zep_client: Zep
    ) -> list[SessionSearchResult]:
        """Search memory for a query."""
        if not zep_client or not self.user_id:
            logger.error("Zep service or user_id not initialized")
            return []

        response = zep_client.memory.search_sessions(
            text=query,
            user_id=self.user_id,
            search_scope="facts",
            min_fact_rating=self.config.min_fact_rating,
        )
        if not response.results:
            return []
        return response.results

    async def on_post_evaluate(
        self,
        agent: FlockAgent,
        inputs: dict[str, Any],
        context: FlockContext | None = None,
        result: dict[str, Any] | None = None,
    ) -> dict[str, Any]:
        """Format and display the output."""
        if not self.config.enable_write:
            return result
        logger.debug("Saving data to memory")
        zep_client = Zep(
            base_url=self.config.zep_url, api_key=self.config.zep_api_key
        )
        self.add_to_memory(str(result), zep_client)
        return result

    async def on_pre_evaluate(
        self,
        agent: FlockAgent,
        inputs: dict[str, Any],
        context: FlockContext | None = None,
    ) -> dict[str, Any]:
        """Format and display the output."""
        if not self.config.enable_read:
            return inputs

        zep_client = Zep(
            base_url=self.config.zep_url, api_key=self.config.zep_api_key
        )

        logger.debug("Searching memory")
        facts = self.search_memory(str(inputs), zep_client)

        # Add memory to inputs
        facts_str = ""
        if facts:
            for fact in facts:
                facts_str += fact.fact.fact + "\n"
            logger.debug("Found facts in memory: {}", facts_str)
            agent.input = agent.input + ", memory"
            inputs["memory"] = facts_str

        return inputs
```

### src\flock\platform\docker_tools.py

- **Lines**: 49
- **Last modified**: 2025-04-16 00:11:15

```py
import subprocess
import time


def _check_docker_running():
    """Check if Docker is running by calling 'docker info'."""
    try:
        result = subprocess.run(
            ["docker", "info"],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
        )
        return result.returncode == 0
    except Exception:
        return False


def _start_docker():
    """Attempt to start Docker.
    This example first tries 'systemctl start docker' and then 'service docker start'.
    Adjust as needed for your environment.
    """
    try:
        print("Attempting to start Docker...")
        result = subprocess.run(
            ["sudo", "systemctl", "start", "docker"],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
        )
        if result.returncode != 0:
            result = subprocess.run(
                ["sudo", "service", "docker", "start"],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
            )
        # Give Docker a moment to start.
        time.sleep(3)
        if _check_docker_running():
            print("Docker is now running.")
            return True
        else:
            print("Docker did not start successfully.")
            return False
    except Exception as e:
        print(f"Exception when trying to start Docker: {e}")
        return False
```

### src\flock\platform\jaeger_install.py

- **Lines**: 86
- **Last modified**: 2025-04-16 00:11:15

```py
import socket
import subprocess
from urllib.parse import urlparse


class JaegerInstaller:
    jaeger_endpoint: str = None
    jaeger_transport: str = "grpc"

    def _check_jaeger_running(self):
        """Check if Jaeger is reachable by attempting a socket connection.
        For HTTP transport, we parse the URL; for gRPC, we expect "host:port".
        """
        try:
            if self.jaeger_transport == "grpc":
                host, port = self.jaeger_endpoint.split(":")
                port = int(port)
            elif self.jaeger_transport == "http":
                parsed = urlparse(self.jaeger_endpoint)
                host = parsed.hostname
                port = parsed.port if parsed.port else 80
            else:
                return False

            # Try connecting to the host and port.
            with socket.create_connection((host, port), timeout=3):
                return True
        except Exception:
            return False

    def _is_jaeger_container_running(self):
        """Check if a Jaeger container (using the official all-in-one image) is running.
        This uses 'docker ps' to filter for containers running the Jaeger image.
        """
        try:
            result = subprocess.run(
                [
                    "docker",
                    "ps",
                    "--filter",
                    "ancestor=jaegertracing/all-in-one:latest",
                    "--format",
                    "{{.ID}}",
                ],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
            )
            return bool(result.stdout.strip())
        except Exception:
            return False

    def _provision_jaeger_container(self):
        """Provision a Jaeger container using Docker."""
        try:
            print("Provisioning Jaeger container using Docker...")
            result = subprocess.run(
                [
                    "docker",
                    "run",
                    "-d",
                    "--name",
                    "jaeger",
                    "-p",
                    "16686:16686",
                    "-p",
                    "14250:14250",
                    "-p",
                    "14268:14268",
                    "jaegertracing/all-in-one:latest",
                ],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
            )
            if result.returncode == 0:
                print("Jaeger container started successfully.")
                return True
            else:
                print(
                    f"Failed to start Jaeger container. Error: {result.stderr}"
                )
                return False
        except Exception as e:
            print(f"Exception when provisioning Jaeger container: {e}")
            return False
```

### src\flock\routers\__init__.py

- **Lines**: 1
- **Last modified**: 2025-04-16 00:11:15

```py
"""Routers for the Flock framework."""
```

### src\flock\routers\agent\__init__.py

- **Lines**: 1
- **Last modified**: 2025-04-16 00:11:15

```py
"""Agent-based router implementation for the Flock framework."""
```

### src\flock\routers\agent\agent_router.py

- **Lines**: 236
- **Last modified**: 2025-04-19 00:19:04

```py
"""Agent-based router implementation for the Flock framework."""

from typing import Any

from flock.core.context.context import FlockContext
from flock.core.flock_agent import FlockAgent
from flock.core.flock_registry import flock_component
from flock.core.flock_router import (
    FlockRouter,
    FlockRouterConfig,
    HandOffRequest,
)
from flock.core.logging.formatters.themes import OutputTheme
from flock.core.logging.logging import get_logger
from flock.evaluators.declarative.declarative_evaluator import (
    DeclarativeEvaluator,
    DeclarativeEvaluatorConfig,
)
from flock.modules.output.output_module import OutputModule, OutputModuleConfig
from flock.routers.agent.handoff_agent import (
    AgentInfo,
    HandoffAgent,
)

logger = get_logger("agent_router")


class AgentRouterConfig(FlockRouterConfig):
    """Configuration for the agent router.

    This class extends FlockRouterConfig with parameters specific to the agent router.
    """

    with_output: bool = False
    confidence_threshold: float = 0.5  # No additional parameters needed for now


@flock_component(config_class=AgentRouterConfig)
class AgentRouter(FlockRouter):
    """Router that uses a FlockAgent to determine the next agent in a workflow.

    This class is responsible for:
    1. Creating and managing a HandoffAgent
    2. Analyzing available agents in the registry
    3. Using the HandoffAgent to determine the best next agent
    4. Creating a HandOff object with the selected agent
    """

    def __init__(
        self,
        name: str = "agent_router",
        config: AgentRouterConfig | None = None,
    ):
        """Initialize the AgentRouter.

        Args:
            registry: The agent registry containing all available agents
            name: The name of the router
            config: The router configuration
        """
        super().__init__(
            name=name, config=config or AgentRouterConfig(name=name)
        )

    async def route(
        self,
        current_agent: FlockAgent,
        result: dict[str, Any],
        context: FlockContext,
    ) -> HandOffRequest:
        """Determine the next agent to hand off to based on the current agent's output.

        Args:
            current_agent: The agent that just completed execution
            result: The output from the current agent
            context: The global execution context

        Returns:
            A HandOff object containing the next agent and input data
        """
        # Get all available agents from context.agent_definitions
        agent_definitions = context.agent_definitions
        handoff_agent = HandoffAgent(model=current_agent.model)
        handoff_agent.evaluator = DeclarativeEvaluator(
            name="evaluator",
            config=DeclarativeEvaluatorConfig(
                model=current_agent.model,
                use_cache=True,
                max_tokens=1000,
                temperature=0.0,
            ),
        )
        if self.config.with_output:
            handoff_agent.add_module(
                OutputModule(
                    name="output",
                    config=OutputModuleConfig(
                        theme=OutputTheme.abernathy,
                    ),
                )
            )
        available_agents = self._get_available_agents(
            agent_definitions, current_agent.name
        )

        if not available_agents:
            logger.warning("No available agents for agent-based routing")
            return HandOffRequest(
                next_agent="",
                output_to_input_merge_strategy="add",
                override_next_agent=None,
                override_context=None,
            )

        # Prepare input for the handoff agent
        handoff_input = {
            "current_agent_name": current_agent.name,
            "current_agent_description": current_agent.description,
            "current_agent_input": current_agent.input,
            "current_agent_output": current_agent.output,
            "current_result": result,
            "available_agents": available_agents,
        }

        try:
            # Run the handoff agent to determine the next agent
            handoff_result = await handoff_agent.run_async(handoff_input)

            # Extract the decision
            next_agent_name = handoff_result.get("agent_name")
            confidence = handoff_result.get("confidence")
            reasoning = handoff_result.get("reasoning")
            logger.info(
                f"Agent router selected agent '{next_agent_name}' with confidence {confidence} and reasoning: {reasoning}"
            )

            if confidence < self.config.confidence_threshold:
                logger.info(
                    f"No suitable next agent found (best score: {confidence})"
                )
                return HandOffRequest(
                    next_agent="",
                    output_to_input_merge_strategy="add",
                    override_next_agent=None,
                    override_context=None,
                )

            next_agent = agent_definitions.get(next_agent_name)
            if not next_agent:
                logger.error(
                    f"Selected agent '{next_agent_name}' not found in agent definitions"
                )
                return HandOffRequest(
                    next_agent="",
                    output_to_input_merge_strategy="add",
                    override_next_agent=None,
                    override_context=None,
                )

            logger.info(
                f"Agent router selected agent '{next_agent_name}' with confidence {confidence}"
            )
            return HandOffRequest(
                next_agent=next_agent_name,
                output_to_input_merge_strategy="add",
                override_next_agent=None,
                override_context=None,
            )

        except Exception as e:
            logger.error(f"Error in agent-based routing: {e}")
            return HandOffRequest(
                next_agent="",
                output_to_input_merge_strategy="add",
                override_next_agent=None,
                override_context=None,
            )

    def _get_available_agents(
        self, agent_definitions: dict[str, Any], current_agent_name: str
    ) -> list[AgentInfo]:
        """Get all available agents except the current one and the handoff agent.

        Args:
            agent_definitions: Dictionary of available agents
            current_agent_name: Name of the current agent to exclude

        Returns:
            List of available agents as AgentInfo objects
        """
        agents = []
        for agent_name in agent_definitions:
            if agent_name != current_agent_name:
                agent = agent_definitions[agent_name]
                agent_info = AgentInfo(
                    name=agent_name,
                    description=agent.agent_data["description"]
                    if agent.agent_data["description"]
                    else "",
                    input_schema=agent.agent_data["input"],
                    output_schema=agent.agent_data["output"],
                )
                agents.append(agent_info)
        return agents

    def _get_schema_from_agent(
        self, agent: Any, schema_type: str
    ) -> dict[str, Any]:
        """Extract input or output schema from an agent.

        Args:
            agent: The agent to extract schema from
            schema_type: Either "input" or "output"

        Returns:
            Dictionary representation of the schema
        """
        schema = {}
        schema_str = agent.agent_data.get(schema_type, "")

        # Parse the schema string to extract field names, types, and descriptions
        if schema_str:
            fields = schema_str.split(",")
            for field in fields:
                field = field.strip()
                if ":" in field:
                    name, rest = field.split(":", 1)
                    name = name.strip()
                    schema[name] = rest.strip()
                else:
                    schema[field] = "Any"

        return schema

    # The _create_next_input method is no longer needed since we're using hand_off_mode="add"
    # instead of manually preparing inputs for the next agent
```

### src\flock\routers\agent\handoff_agent.py

- **Lines**: 58
- **Last modified**: 2025-04-16 00:11:15

```py
"""Handoff agent for the agent-based router."""

from pydantic import BaseModel

from flock.core.flock_agent import FlockAgent


class AgentInfo(BaseModel):
    """Information about an agent for handoff decisions."""

    name: str
    description: str = ""
    input_schema: str = ""
    output_schema: str = ""


class HandoffDecision(BaseModel):
    """Decision about which agent to hand off to."""

    agent_name: str
    confidence: float
    reasoning: str


class HandoffAgent(FlockAgent):
    """Agent that decides which agent to hand off to next.

    This agent analyzes the current agent's output and available agents
    to determine the best next agent in the workflow.
    """

    def __init__(
        self,
        name: str = "handoff_agent",
        model: str | None = None,
        description: str = "Decides which agent to hand off to next",
    ):
        """Initialize the HandoffAgent.

        Args:
            name: The name of the agent
            model: The model to use (e.g., 'openai/gpt-4o')
            description: A human-readable description of the agent
        """
        super().__init__(
            name=name,
            model=model,
            description=description,
            input=(
                "current_agent_name: str | Name of the current agent, "
                "current_agent_description: str | Description of the current agent, "
                "current_agent_input: str | Input schema of the current agent, "
                "current_agent_output: str | Output schema of the current agent, "
                "current_result: dict | Output from the current agent, "
                "available_agents: list[AgentInfo] | List of available agents"
            ),
            output="agent_name: str | Name of the agent to hand off to, confidence: float | Confidence in the decision, reasoning: str | Reasoning for the decision",
        )
```

### src\flock\routers\conditional\conditional_router.py

- **Lines**: 482
- **Last modified**: 2025-04-19 00:19:04

```py
# src/flock/routers/conditional/conditional_router.py

import re
from collections.abc import Callable
from typing import Any, Literal

from pydantic import Field, model_validator

from flock.core.context.context import FlockContext
from flock.core.flock_agent import FlockAgent
from flock.core.flock_registry import flock_component, get_registry
from flock.core.flock_router import (
    FlockRouter,
    FlockRouterConfig,
    HandOffRequest,
)
from flock.core.logging.logging import get_logger

logger = get_logger("router.conditional")


class ConditionalRouterConfig(FlockRouterConfig):
    """Configuration for the ConditionalRouter."""

    condition_context_key: str = Field(
        default="flock.condition",
        description="Context key containing the value to evaluate the condition against.",
    )

    # --- Define ONE type of condition check ---
    condition_callable: (
        str | Callable[[Any], tuple[bool, str | None]] | None
    ) = Field(
        default=None,
        description="A callable (or registered name) that takes the context value and returns a tuple containing: (bool: True if condition passed, False otherwise, Optional[str]: Feedback message if condition failed).",
    )
    # String Checks
    expected_string: str | None = Field(
        default=None, description="String value to compare against."
    )
    string_mode: Literal[
        "equals",
        "contains",
        "regex",
        "startswith",
        "endswith",
        "not_equals",
        "not_contains",
    ] = Field(default="equals", description="How to compare strings.")
    ignore_case: bool = Field(
        default=True, description="Ignore case during string comparison."
    )
    # Length Checks (String or List)
    min_length: int | None = Field(
        default=None,
        description="Minimum length for strings or items for lists.",
    )
    max_length: int | None = Field(
        default=None,
        description="Maximum length for strings or items for lists.",
    )
    # Number Checks
    expected_number: int | float | None = Field(
        default=None, description="Number to compare against."
    )
    number_mode: Literal["<", "<=", "==", "!=", ">=", ">"] = Field(
        default="==", description="How to compare numbers."
    )
    # List Checks
    min_items: int | None = Field(
        default=None, description="Minimum number of items in a list."
    )
    max_items: int | None = Field(
        default=None, description="Maximum number of items in a list."
    )
    # Type Check
    expected_type_name: str | None = Field(
        default=None,
        description="Registered name of the expected Python type (e.g., 'str', 'list', 'MyCustomType').",
    )
    # Boolean Check
    expected_bool: bool | None = Field(
        default=None, description="Expected boolean value (True or False)."
    )
    # Existence Check
    check_exists: bool | None = Field(
        default=None,
        description="If True, succeeds if key exists; if False, succeeds if key *doesn't* exist. Ignores value.",
    )

    # --- Routing Targets ---
    success_agent: str | None = Field(
        default=None,
        description="Agent name to route to if the condition evaluates to True.",
    )
    failure_agent: str | None = Field(
        default=None,
        description="Agent name to route to if the condition evaluates to False (after retries, if enabled).",
    )
    retry_agent: str | None = Field(
        default=None,
        description="Agent name to route to if the condition evaluates to False (during retries, if enabled).",
    )

    # --- Optional Retry Logic (for Failure Path) ---
    retry_on_failure: bool = Field(
        default=False,
        description="If True, route back to the retry_agent on failure before going to failure_agent.",
    )
    max_retries: int = Field(
        default=1,
        description="Maximum number of times to retry the current agent on failure.",
    )
    feedback_context_key: str | None = Field(
        default="flock.assertion_feedback",  # Useful if paired with AssertionCheckerModule
        description="Optional context key containing feedback message to potentially include when retrying.",
    )
    retry_count_context_key_prefix: str = Field(
        default="flock.conditional_retry_count_",
        description="Internal prefix for context key storing retry attempts per agent.",
    )

    # --- Validator to ensure only one condition type is set ---
    @model_validator(mode="after")
    def check_exclusive_condition(self) -> "ConditionalRouterConfig":
        conditions_set = [
            self.condition_callable is not None,
            self.expected_string is not None
            or self.min_length is not None
            or self.max_length is not None,  # String/Length group
            self.expected_number is not None,  # Number group
            self.min_items is not None
            or self.max_items is not None,  # List size group
            self.expected_type_name is not None,  # Type group
            self.expected_bool is not None,  # Bool group
            self.check_exists is not None,  # Existence group
        ]
        if sum(conditions_set) > 1:
            raise ValueError(
                "Only one type of condition (callable, string/length, number, list size, type, boolean, exists) can be configured per ConditionalRouter."
            )
        if sum(conditions_set) == 0:
            raise ValueError(
                "At least one condition type must be configured for ConditionalRouter."
            )
        return self


@flock_component(config_class=ConditionalRouterConfig)
class ConditionalRouter(FlockRouter):
    """Routes workflow based on evaluating a condition against a value in the FlockContext.
    Supports various built-in checks (string, number, list, type, bool, existence)
    or a custom callable. Can optionally retry the current agent on failure.
    """

    name: str = "conditional_router"
    config: ConditionalRouterConfig = Field(
        default_factory=ConditionalRouterConfig
    )

    def _evaluate_condition(self, value: Any) -> tuple[bool, str | None]:
        """Evaluates the condition based on the router's configuration.

        Returns:
            Tuple[bool, Optional[str]]: A tuple containing:
                - bool: True if the condition passed, False otherwise.
                - Optional[str]: A feedback message if the condition failed, otherwise None.
        """
        cfg = self.config
        condition_passed = False
        feedback = cfg.feedback_on_failure  # Default feedback
        condition_type = "unknown"

        try:
            # 0. Check Existence first (simplest)
            if cfg.check_exists is not None:
                condition_type = "existence"
                value_exists = value is not None
                condition_passed = (
                    value_exists if cfg.check_exists else not value_exists
                )
                if not condition_passed:
                    feedback = f"Existence check failed: Expected key '{cfg.condition_context_key}' to {'exist' if cfg.check_exists else 'not exist or be None'}, but it was {'found' if value_exists else 'missing/None'}."

            # 1. Custom Callable
            elif cfg.condition_callable:
                condition_type = "callable"
                callable_func = cfg.condition_callable
                if isinstance(callable_func, str):  # Lookup registered callable
                    registry = get_registry()
                    try:
                        callable_func = registry.get_callable(callable_func)
                    except KeyError:
                        feedback = f"Condition callable '{cfg.condition_callable}' not found in registry."
                        logger.error(feedback)
                        return False, feedback  # Treat as failure

                if callable(callable_func):
                    eval_result = callable_func(value)
                    if (
                        isinstance(eval_result, tuple)
                        and len(eval_result) == 2
                        and isinstance(eval_result[0], bool)
                    ):
                        condition_passed, custom_feedback = eval_result
                        if not condition_passed and isinstance(
                            custom_feedback, str
                        ):
                            feedback = custom_feedback
                    elif isinstance(eval_result, bool):
                        condition_passed = eval_result
                        if not condition_passed:
                            feedback = f"Callable condition '{getattr(callable_func, '__name__', 'anonymous')}' returned False."
                    else:
                        feedback = f"Condition callable '{getattr(callable_func, '__name__', 'anonymous')}' returned unexpected type: {type(eval_result)}."
                        logger.warning(feedback)
                        return False, feedback  # Treat as failure
                else:
                    feedback = f"Configured condition_callable '{cfg.condition_callable}' is not callable."
                    logger.error(feedback)
                    return False, feedback

            # 2. String / Length Checks
            elif (
                cfg.expected_string is not None
                or cfg.min_length is not None
                or cfg.max_length is not None
            ):
                condition_type = "string/length"
                if not isinstance(value, str):
                    feedback = f"Cannot perform string/length check on non-string value: {type(value)}."
                    logger.warning(feedback)
                    return False, feedback
                s_value = value
                val_len = len(s_value)
                length_passed = True
                length_feedback = []
                if cfg.min_length is not None and val_len < cfg.min_length:
                    length_passed = False
                    length_feedback.append(
                        f"length {val_len} is less than minimum {cfg.min_length}"
                    )
                if cfg.max_length is not None and val_len > cfg.max_length:
                    length_passed = False
                    length_feedback.append(
                        f"length {val_len} is greater than maximum {cfg.max_length}"
                    )

                content_passed = True
                content_feedback = ""
                if cfg.expected_string is not None:
                    expected = cfg.expected_string
                    s1 = s_value if not cfg.ignore_case else s_value.lower()
                    s2 = expected if not cfg.ignore_case else expected.lower()
                    mode = cfg.string_mode
                    if mode == "equals":
                        content_passed = s1 == s2
                    elif mode == "contains":
                        content_passed = s2 in s1
                    elif mode == "startswith":
                        content_passed = s1.startswith(s2)
                    elif mode == "endswith":
                        content_passed = s1.endswith(s2)
                    elif mode == "not_equals":
                        content_passed = s1 != s2
                    elif mode == "not_contains":
                        content_passed = s2 not in s1
                    elif mode == "regex":
                        content_passed = bool(re.search(expected, value))
                    else:
                        content_passed = False
                    if not content_passed:
                        content_feedback = f"String content check '{mode}' failed against expected '{expected}' (ignore_case={cfg.ignore_case})."

                condition_passed = length_passed and content_passed
                if not condition_passed:
                    feedback_parts = length_feedback + (
                        [content_feedback] if content_feedback else []
                    )
                    feedback = (
                        "; ".join(feedback_parts)
                        if feedback_parts
                        else "String/length condition failed."
                    )

            # 3. Number Check
            elif cfg.expected_number is not None:
                condition_type = "number"
                if not isinstance(value, (int, float)):
                    feedback = f"Cannot perform number check on non-numeric value: {type(value)}."
                    logger.warning(feedback)
                    return False, feedback
                num_value = value
                expected = cfg.expected_number
                mode = cfg.number_mode
                op_map = {
                    "<": lambda a, b: a < b,
                    "<=": lambda a, b: a <= b,
                    "==": lambda a, b: a == b,
                    "!=": lambda a, b: a != b,
                    ">=": lambda a, b: a >= b,
                    ">": lambda a, b: a > b,
                }
                if mode in op_map:
                    condition_passed = op_map[mode](num_value, expected)
                    if not condition_passed:
                        feedback = f"Number check failed: {num_value} {mode} {expected} is false."
                else:
                    condition_passed = False
                    feedback = f"Invalid number comparison mode: {mode}"

            # 4. List Size Check
            elif cfg.min_items is not None or cfg.max_items is not None:
                condition_type = "list size"
                if not isinstance(value, list):
                    feedback = f"Cannot perform list size check on non-list value: {type(value)}."
                    logger.warning(feedback)
                    return False, feedback
                list_len = len(value)
                size_passed = True
                size_feedback = []
                if cfg.min_items is not None and list_len < cfg.min_items:
                    size_passed = False
                    size_feedback.append(
                        f"list size {list_len} is less than minimum {cfg.min_items}"
                    )
                if cfg.max_items is not None and list_len > cfg.max_items:
                    size_passed = False
                    size_feedback.append(
                        f"list size {list_len} is greater than maximum {cfg.max_items}"
                    )
                condition_passed = size_passed
                if not condition_passed:
                    feedback = "; ".join(size_feedback)

            # 5. Type Check
            elif cfg.expected_type_name is not None:
                condition_type = "type"
                registry = get_registry()
                try:
                    expected_type = registry.get_type(cfg.expected_type_name)
                    condition_passed = isinstance(value, expected_type)
                    if not condition_passed:
                        feedback = f"Type check failed: Value type '{type(value).__name__}' is not instance of expected '{cfg.expected_type_name}'."
                except KeyError:
                    feedback = f"Expected type '{cfg.expected_type_name}' not found in registry."
                    logger.error(feedback)
                    return False, feedback

            # 6. Boolean Check
            elif cfg.expected_bool is not None:
                condition_type = "boolean"
                if not isinstance(value, bool):
                    feedback = f"Cannot perform boolean check on non-bool value: {type(value)}."
                    logger.warning(feedback)
                    return False, feedback
                condition_passed = value == cfg.expected_bool
                if not condition_passed:
                    feedback = f"Boolean check failed: Value '{value}' is not expected '{cfg.expected_bool}'."

            logger.debug(
                f"Condition check '{condition_type}' result: {condition_passed}"
            )
            return condition_passed, feedback if not condition_passed else None

        except Exception as e:
            feedback = (
                f"Error evaluating condition type '{condition_type}': {e}"
            )
            logger.error(feedback, exc_info=True)
            return (
                False,
                feedback,
            )  # Treat evaluation errors as condition failure

    async def route(
        self,
        current_agent: FlockAgent,
        result: dict[str, Any],
        context: FlockContext,
    ) -> HandOffRequest:
        cfg = self.config
        condition_value = context.get_variable(cfg.condition_context_key, None)
        feedback_value = context.get_variable(cfg.feedback_context_key, None)

        logger.debug(
            f"Routing based on condition key '{cfg.condition_context_key}', value: {str(condition_value)[:100]}..."
        )

        # Evaluate the condition and get feedback on failure
        condition_passed, feedback_msg = self._evaluate_condition(
            condition_value
        )

        if condition_passed:
            # --- Success Path ---
            logger.info(
                f"Condition PASSED for agent '{current_agent.name}'. Routing to success path."
            )
            # Reset retry count if applicable
            if cfg.retry_on_failure:
                retry_key = (
                    f"{cfg.retry_count_context_key_prefix}{current_agent.name}"
                )
                if retry_key in context.state:
                    del context.state[retry_key]
                    logger.debug(
                        f"Reset retry count for agent '{current_agent.name}'."
                    )

            # Clear feedback from context on success
            if (
                cfg.feedback_context_key
                and cfg.feedback_context_key in context.state
            ):
                del context.state[cfg.feedback_context_key]
                logger.debug(
                    f"Cleared feedback key '{cfg.feedback_context_key}' on success."
                )

            next_agent = cfg.success_agent or ""  # Stop chain if None
            logger.debug(f"Success route target: '{next_agent}'")
            return HandOffRequest(next_agent=next_agent)

        else:
            # --- Failure Path ---
            logger.warning(
                f"Condition FAILED for agent '{current_agent.name}'. Reason: {feedback_msg}"
            )

            if cfg.retry_on_failure:
                # --- Retry Logic ---
                retry_key = (
                    f"{cfg.retry_count_context_key_prefix}{current_agent.name}"
                )
                retry_count = context.get_variable(retry_key, 0)

                if retry_count < cfg.max_retries:
                    next_retry_count = retry_count + 1
                    context.set_variable(retry_key, next_retry_count)
                    logger.info(
                        f"Routing back to agent '{current_agent.name}' for retry #{next_retry_count}/{cfg.max_retries}."
                    )

                    # Add specific feedback to context if retry is enabled
                    if cfg.feedback_context_key:
                        context.set_variable(
                            cfg.feedback_context_key,
                            feedback_msg or cfg.feedback_on_failure,
                        )
                        logger.debug(
                            f"Set feedback key '{cfg.feedback_context_key}': {feedback_msg or cfg.feedback_on_failure}"
                        )

                    return HandOffRequest(
                        next_agent=current_agent.name,  # Route back to self
                        output_to_input_merge_strategy="add",  # Make feedback available
                    )
                else:
                    # --- Max Retries Exceeded ---
                    logger.error(
                        f"Max retries ({cfg.max_retries}) exceeded for agent '{current_agent.name}'."
                    )
                    if retry_key in context.state:
                        del context.state[retry_key]  # Reset count
                    # Clear feedback before final failure route? Optional.
                    # if cfg.feedback_context_key in context.state: del context.state[cfg.feedback_context_key]
                    next_agent = cfg.failure_agent or ""
                    logger.debug(
                        f"Failure route target (after retries): '{next_agent}'"
                    )
                    return HandOffRequest(next_agent=next_agent)
            else:
                # --- No Retry Logic ---
                next_agent = (
                    cfg.failure_agent or ""
                )  # Use failure agent or stop
                logger.debug(f"Failure route target (no retry): '{next_agent}'")
                # Optionally add feedback even if not retrying?
                # if cfg.feedback_context_key:
                #     context.set_variable(cfg.feedback_context_key, feedback_msg or cfg.feedback_on_failure)
                return HandOffRequest(next_agent=next_agent)
```

### src\flock\routers\default\__init__.py

- **Lines**: 1
- **Last modified**: 2025-04-16 00:11:15

```py
"""Default router implementation for the Flock framework."""
```

### src\flock\routers\default\default_router.py

- **Lines**: 80
- **Last modified**: 2025-04-19 00:19:04

```py
"""Default router implementation for the Flock framework."""

from collections.abc import Callable
from typing import Any

from pydantic import Field

from flock.core.context.context import FlockContext
from flock.core.flock_agent import FlockAgent
from flock.core.flock_registry import flock_component
from flock.core.flock_router import (
    FlockRouter,
    FlockRouterConfig,
    HandOffRequest,
)
from flock.core.logging.logging import get_logger

logger = get_logger("default_router")


class DefaultRouterConfig(FlockRouterConfig):
    """Configuration for the default router."""

    hand_off: str | HandOffRequest | Callable[..., HandOffRequest] = Field(
        default="", description="Next agent to hand off to"
    )


@flock_component(config_class=DefaultRouterConfig)
class DefaultRouter(FlockRouter):
    """Default router implementation.

    This router simply uses the agent's hand_off property to determine the next agent.
    It does not perform any dynamic routing.
    """

    name: str = "default_router"
    config: DefaultRouterConfig = Field(
        default_factory=DefaultRouterConfig, description="Output configuration"
    )

    def __init__(
        self,
        name: str = "default_router",
        config: DefaultRouterConfig | None = None,
    ):
        """Initialize the DefaultRouter.

        Args:
            name: The name of the router
            config: The router configuration
        """
        super().__init__(
            name=name, config=config or DefaultRouterConfig(name=name)
        )

    async def route(
        self,
        current_agent: FlockAgent,
        result: dict[str, Any],
        context: FlockContext,
    ) -> HandOffRequest:
        """Determine the next agent to hand off to based on the current agent's output.

        Args:
            current_agent: The agent that just completed execution
            result: The output from the current agent
            context: The global execution context

        Returns:
            A HandOff object containing the next agent and input data
        """
        handoff = self.config.hand_off
        if callable(handoff):
            handoff = handoff(context, result)
        if isinstance(handoff, str):
            handoff = HandOffRequest(
                next_agent=handoff, output_to_input_merge_strategy="match"
            )
        return handoff
```

### src\flock\routers\feedback\feedback_router.py

- **Lines**: 114
- **Last modified**: 2025-04-19 00:19:04

```py
# src/flock/routers/correction/correction_router.py (New File)

from typing import Any

from pydantic import Field

from flock.core.context.context import FlockContext
from flock.core.flock_agent import FlockAgent
from flock.core.flock_registry import flock_component
from flock.core.flock_router import (
    FlockRouter,
    FlockRouterConfig,
    HandOffRequest,
)
from flock.core.logging.logging import get_logger

logger = get_logger("router.correction")


class FeedbackRetryRouterConfig(FlockRouterConfig):
    max_retries: int = Field(
        default=1,
        description="Maximum number of times to retry the same agent on failure.",
    )
    feedback_context_key: str = Field(
        default="flock.assertion_feedback",
        description="Context key containing feedback from AssertionCheckerModule.",
    )
    retry_count_context_key_prefix: str = Field(
        default="flock.retry_count_",
        description="Prefix for context key storing retry attempts per agent.",
    )
    fallback_agent: str | None = Field(
        None, description="Agent to route to if max_retries is exceeded."
    )


@flock_component(config_class=FeedbackRetryRouterConfig)
class FeedbackRetryRouter(FlockRouter):
    """Routes based on assertion feedback in the context.

    If feedback exists for the current agent and retries are not exhausted,
    it routes back to the same agent, adding the feedback to its input.
    Otherwise, it can route to a fallback agent or stop the chain.
    """

    name: str = "feedback_retry_router"
    config: FeedbackRetryRouterConfig = Field(
        default_factory=FeedbackRetryRouterConfig
    )

    async def route(
        self,
        current_agent: FlockAgent,
        result: dict[str, Any],
        context: FlockContext,
    ) -> HandOffRequest:
        feedback = context.get_variable(self.config.feedback_context_key)

        if feedback:
            logger.warning(
                f"Assertion feedback detected for agent '{current_agent.name}'. Attempting retry."
            )

            retry_key = f"{self.config.retry_count_context_key_prefix}{current_agent.name}"
            retry_count = context.get_variable(retry_key, 0)
            logger.warning(f"Feedback: {feedback} - Retry Count {retry_count}")

            if retry_count < self.config.max_retries:
                logger.info(
                    f"Routing back to agent '{current_agent.name}' for retry #{retry_count + 1}"
                )
                context.set_variable(retry_key, retry_count + 1)
                context.set_variable(
                    f"{current_agent.name}_prev_result", result
                )
                # Add feedback to the *next* agent's input (which is the same agent)
                # Requires the agent's signature to potentially accept a 'feedback' input field.
                return HandOffRequest(
                    next_agent=current_agent.name,
                    output_to_input_merge_strategy="match",  # Add feedback to existing context/previous results
                    add_input_fields=[
                        f"{self.config.feedback_context_key} | Feedback for prev result",
                        f"{current_agent.name}_prev_result | Previous Result",
                    ],
                    add_description=f"Try to fix the previous result based on the feedback.",
                    override_context=None,  # Context already updated with feedback and retry count
                )
            else:
                logger.error(
                    f"Max retries ({self.config.max_retries}) exceeded for agent '{current_agent.name}'."
                )
                # Max retries exceeded, route to fallback or stop
                if self.config.fallback_agent:
                    logger.info(
                        f"Routing to fallback agent '{self.config.fallback_agent}'"
                    )
                    # Clear feedback before going to fallback? Optional.
                    if self.config.feedback_context_key in context.state:
                        del context.state[self.config.feedback_context_key]
                    return HandOffRequest(next_agent=self.config.fallback_agent)
                else:
                    logger.info("No fallback agent defined. Stopping workflow.")
                    return HandOffRequest(next_agent="")  # Stop the chain

        else:
            # No feedback, assertions passed or module not configured for feedback
            logger.debug(
                f"No assertion feedback for agent '{current_agent.name}'. Proceeding normally."
            )
            # Default behavior: Stop the chain if no other routing is defined
            # In a real system, you might chain this with another router (e.g., LLMRouter)
            # to decide the *next different* agent if assertions passed.
            return HandOffRequest(next_agent="")  # Stop or pass to next router
```

### src\flock\routers\list_generator\list_generator_router.py

- **Lines**: 166
- **Last modified**: 2025-04-19 00:19:04

```py
# src/flock/routers/list_generator/iterative_list_router.py (New File)

from typing import Any

from pydantic import Field

from flock.core.context.context import FlockContext
from flock.core.flock_agent import FlockAgent
from flock.core.flock_registry import flock_component
from flock.core.flock_router import (
    FlockRouter,
    FlockRouterConfig,
    HandOffRequest,
)
from flock.core.logging.logging import get_logger

# Need signature utils

logger = get_logger("router.list_generator")


class IterativeListGeneratorRouterConfig(FlockRouterConfig):
    target_list_field: str = Field(
        ...,
        description="Name of the final list output field (e.g., 'chapters').",
    )
    item_output_field: str = Field(
        ...,
        description="Name of the single item output field for each iteration (e.g., 'chapter').",
    )
    context_input_field: str = Field(
        default="previous_items",
        description="Input field name for passing back generated items (e.g., 'existing_chapters').",
    )
    max_iterations: int = Field(
        default=10, description="Maximum number of items to generate."
    )
    # More advanced: termination_condition: Optional[Callable] = None
    # Store iteration state in context under this prefix
    context_state_prefix: str = Field(
        default="flock.iterator_state_",
        description="Prefix for context keys storing iteration state.",
    )

    # Field to extract item type from target_list_field signature
    # This might require parsing the original agent's output signature
    # item_type_str: Optional[str] = None # e.g., 'dict[str, str]' or 'MyChapterType'


@flock_component(config_class=IterativeListGeneratorRouterConfig)
class IterativeListGeneratorRouter(FlockRouter):
    name: str = "iterative_list_generator"
    config: IterativeListGeneratorRouterConfig = Field(
        default_factory=IterativeListGeneratorRouterConfig
    )

    # Helper to get state keys
    def _get_state_keys(self, agent_name: str) -> tuple[str, str]:
        prefix = self.config.context_state_prefix
        list_key = f"{prefix}{agent_name}_{self.config.target_list_field}"
        count_key = f"{prefix}{agent_name}_iteration_count"
        return list_key, count_key

    async def route(
        self,
        current_agent: FlockAgent,
        result: dict[str, Any],
        context: FlockContext,
    ) -> HandOffRequest:
        list_key, count_key = self._get_state_keys(current_agent.name)

        # --- State Initialization (First Run) ---
        if count_key not in context.state:
            logger.debug(
                f"Initializing iterative list generation for '{self.config.target_list_field}' in agent '{current_agent.name}'."
            )
            context.set_variable(count_key, 0)
            context.set_variable(list_key, [])
            # Modify agent signature for the *first* iteration (remove context_input_field, use item_output_field)
            # This requires modifying the agent's internal state or creating a temporary one.
            # Let's try modifying the context passed to the *next* run instead.
            context.set_variable(
                f"{current_agent.name}.next_run_output_field",
                self.config.item_output_field,
            )
            context.set_variable(
                f"{current_agent.name}.next_run_input_fields_to_exclude",
                {self.config.context_input_field},
            )

        # --- Process Result of Previous Iteration ---
        iteration_count = context.get_variable(count_key, 0)
        generated_items = context.get_variable(list_key, [])

        # Get the single item generated in the *last* run
        # The result dict should contain the 'item_output_field' if it wasn't the very first run
        new_item = result.get(self.config.item_output_field)

        if (
            new_item is not None and iteration_count > 0
        ):  # Add item from previous run (not the init run)
            generated_items.append(new_item)
            context.set_variable(list_key, generated_items)  # Update context
            logger.info(
                f"Added item #{iteration_count} to list '{self.config.target_list_field}' for agent '{current_agent.name}'."
            )
        elif iteration_count > 0:
            logger.warning(
                f"Iteration {iteration_count} for agent '{current_agent.name}' did not produce expected output field '{self.config.item_output_field}'."
            )
            # Decide how to handle: stop, retry, continue? Let's continue for now.

        # Increment iteration count *after* processing the result of the previous one
        current_iteration = iteration_count + 1
        context.set_variable(count_key, current_iteration)

        # --- Termination Check ---
        if current_iteration > self.config.max_iterations:
            logger.info(
                f"Max iterations ({self.config.max_iterations}) reached for '{self.config.target_list_field}' in agent '{current_agent.name}'. Finalizing."
            )
            # Clean up state
            del context.state[count_key]
            # Final result should be the list itself under the target_list_field key
            final_result = {self.config.target_list_field: generated_items}
            # Handoff with empty next_agent to stop, but potentially override the *result*
            # This is tricky. Routers usually decide the *next agent*, not the *final output*.
            # Maybe the router should just signal termination, and the Flock run loop handles assembling the final output?
            # Let's assume the router signals termination by returning next_agent=""
            # The final list is already in the context under list_key.
            # A final "AssemblerAgent" could read this context variable.
            # OR we modify the HandOffRequest:
            return HandOffRequest(
                next_agent="", final_output_override=final_result
            )  # Needs HandOffRequest modification

        # --- Prepare for Next Iteration ---
        logger.info(
            f"Routing back to agent '{current_agent.name}' for item #{current_iteration} of '{self.config.target_list_field}'."
        )

        # The agent needs the context (previously generated items) and the original inputs again.
        # We will pass the generated items via the context_input_field.
        # The original inputs (like story_outline) should still be in the context.
        next_input_override = {
            self.config.context_input_field: generated_items  # Pass the list back
        }

        # Modify agent signature for the *next* iteration (add context_input_field, use item_output_field)
        # This is the trickiest part - how to modify the agent's perceived signature for the next run?
        # Option 1: Pass overrides via HandOffRequest (cleanest)
        next_signature_input = f"{current_agent.input}, {self.config.context_input_field}: list | Previously generated items"  # Needs smarter joining
        next_signature_output = (
            self.config.item_output_field
        )  # Only ask for one item

        # This requires HandOffRequest and Flock execution loop to support signature overrides
        return HandOffRequest(
            next_agent=current_agent.name,
            output_to_input_merge_strategy="add",  # Add the context_input_field to existing context
            input_override=next_input_override,  # Provide the actual list data
            # --- Hypothetical Overrides ---
            next_run_input_signature_override=next_signature_input,
            next_run_output_signature_override=next_signature_output,
            # -----------------------------
        )
```

### src\flock\routers\llm\__init__.py

- **Lines**: 1
- **Last modified**: 2025-04-16 00:11:15

```py
"""LLM-based router implementation for the Flock framework."""
```

### src\flock\routers\llm\llm_router.py

- **Lines**: 365
- **Last modified**: 2025-04-19 00:19:04

```py
"""LLM-based router implementation for the Flock framework."""

import json
from typing import Any

import litellm

from flock.core.context.context import FlockContext
from flock.core.flock_agent import FlockAgent
from flock.core.flock_registry import flock_component
from flock.core.flock_router import (
    FlockRouter,
    FlockRouterConfig,
    HandOffRequest,
)
from flock.core.logging.logging import get_logger

logger = get_logger("llm_router")


class LLMRouterConfig(FlockRouterConfig):
    """Configuration for the LLM router.

    This class extends FlockRouterConfig with parameters specific to the LLM router.
    """

    temperature: float = 0.2
    max_tokens: int = 500
    confidence_threshold: float = 0.5
    prompt: str = ""


@flock_component(config_class=LLMRouterConfig)
class LLMRouter(FlockRouter):
    """Router that uses an LLM to determine the next agent in a workflow.

    This class is responsible for:
    1. Analyzing available agents in the registry
    2. Using an LLM to score each agent's suitability as the next step
    3. Selecting the highest-scoring agent
    4. Creating a HandOff object with the selected agent
    """

    def __init__(
        self,
        name: str = "llm_router",
        config: LLMRouterConfig | None = None,
    ):
        """Initialize the LLMRouter.

        Args:
            registry: The agent registry containing all available agents
            name: The name of the router
            config: The router configuration
        """
        logger.info(f"Initializing LLM Router '{name}'")
        super().__init__(name=name, config=config or LLMRouterConfig(name=name))
        logger.debug(
            "LLM Router configuration",
            temperature=self.config.temperature,
            max_tokens=self.config.max_tokens,
        )

    async def route(
        self,
        current_agent: FlockAgent,
        result: dict[str, Any],
        context: FlockContext,
    ) -> HandOffRequest:
        """Determine the next agent to hand off to based on the current agent's output.

        Args:
            current_agent: The agent that just completed execution
            result: The output from the current agent
            context: The global execution context

        Returns:
            A HandOff object containing the next agent and input data
        """
        logger.info(
            f"Routing from agent '{current_agent.name}'",
            current_agent=current_agent.name,
        )
        logger.debug("Current agent result", result=result)

        agent_definitions = context.agent_definitions
        # Get all available agents from the registry
        available_agents = self._get_available_agents(
            agent_definitions, current_agent.name
        )
        logger.debug(
            "Available agents for routing",
            count=len(available_agents),
            agents=[a.agent_data["name"] for a in available_agents],
        )

        if not available_agents:
            logger.warning(
                "No available agents for routing",
                current_agent=current_agent.name,
            )
            return HandOffRequest(
                next_agent="", override_next_agent={}, override_context=None
            )

        # Use LLM to determine the best next agent
        next_agent_name, score = await self._select_next_agent(
            current_agent, result, available_agents
        )
        logger.info(
            "Agent selection result",
            next_agent=next_agent_name,
            score=score,
        )

        if not next_agent_name or score < self.config.confidence_threshold:
            logger.warning(
                "No suitable next agent found",
                best_score=score,
            )
            return HandOffRequest(
                next_agent="", override_next_agent={}, override_context=None
            )

        # Get the next agent from the registry
        next_agent = agent_definitions.get(next_agent_name)
        if not next_agent:
            logger.error(
                "Selected agent not found in registry",
                agent_name=next_agent_name,
            )
            return HandOffRequest(
                next_agent="", override_next_agent={}, override_context=None
            )

        # Create input for the next agent

        logger.success(
            f"Successfully routed to agent '{next_agent_name}'",
            score=score,
            from_agent=current_agent.name,
        )
        return HandOffRequest(
            next_agent=next_agent_name,
            output_to_input_merge_strategy="add",
            override_next_agent=None,
            override_context=None,
        )

    def _get_available_agents(
        self, agent_definitions: dict[str, Any], current_agent_name: str
    ) -> list[FlockAgent]:
        """Get all available agents except the current one.

        Args:
            current_agent_name: Name of the current agent to exclude

        Returns:
            List of available agents
        """
        logger.debug(
            "Getting available agents",
            total_agents=len(agent_definitions),
            current_agent=current_agent_name,
        )
        agents = []
        for agent in agent_definitions:
            if agent != current_agent_name:
                agents.append(agent_definitions.get(agent))
        return agents

    async def _select_next_agent(
        self,
        current_agent: FlockAgent,
        result: dict[str, Any],
        available_agents: list[FlockAgent],
    ) -> tuple[str, float]:
        """Use an LLM to select the best next agent.

        Args:
            current_agent: The agent that just completed execution
            result: The output from the current agent
            available_agents: List of available agents to choose from

        Returns:
            Tuple of (selected_agent_name, confidence_score)
        """
        logger.debug(
            "Selecting next agent",
            current_agent=current_agent.name,
            available_count=len(available_agents),
        )

        # Prepare the prompt for the LLM
        prompt = self._create_selection_prompt(
            current_agent, result, available_agents
        )
        logger.debug("Generated selection prompt", prompt_length=len(prompt))

        try:
            logger.info(
                "Calling LLM for agent selection",
                model=current_agent.model,
                temperature=self.config.temperature,
            )
            # Call the LLM to get the next agent
            response = await litellm.acompletion(
                model=current_agent.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=self.config.temperature
                if isinstance(self.config, LLMRouterConfig)
                else 0.2,
                max_tokens=self.config.max_tokens
                if isinstance(self.config, LLMRouterConfig)
                else 500,
            )

            content = response.choices[0].message.content
            # Parse the response to get the agent name and score
            try:
                # extract the json object from the response
                content = content.split("```json")[1].split("```")[0]
                data = json.loads(content)
                next_agent = data.get("next_agent", "")
                score = float(data.get("score", 0))
                reasoning = data.get("reasoning", "")
                logger.info(
                    "Successfully parsed LLM response",
                    next_agent=next_agent,
                    score=score,
                    reasoning=reasoning,
                )
                return next_agent, score
            except (json.JSONDecodeError, ValueError) as e:
                logger.error(
                    "Failed to parse LLM response",
                    error=str(e),
                    raw_response=content,
                )
                logger.debug("Attempting fallback parsing")

                # Fallback: try to extract the agent name from the text
                for agent in available_agents:
                    if agent.agent_data["name"] in content:
                        logger.info(
                            "Found agent name in response using fallback",
                            agent=agent.agent_data["name"],
                        )
                        return agent.agent_data[
                            "name"
                        ], 0.6  # Default score for fallback

                return "", 0.0

        except Exception as e:
            logger.error(
                "Error calling LLM for agent selection",
                error=str(e),
                current_agent=current_agent.name,
            )
            return "", 0.0

    def _create_selection_prompt(
        self,
        current_agent: FlockAgent,
        result: dict[str, Any],
        available_agents: list[FlockAgent],
    ) -> str:
        """Create a prompt for the LLM to select the next agent.

        Args:
            current_agent: The agent that just completed execution
            result: The output from the current agent
            available_agents: List of available agents to choose from

        Returns:
            Prompt string for the LLM
        """
        # Format the current agent's output
        result_str = json.dumps(result, indent=2)

        # Format the available agents' information
        agents_info = []
        for agent in available_agents:
            agent_info = {
                "name": agent.agent_data["name"],
                "description": agent.agent_data["description"]
                if agent.agent_data["description"]
                else "",
                "input": agent.agent_data["input"],
                "output": agent.agent_data["output"],
            }
            agents_info.append(agent_info)

        agents_str = json.dumps(agents_info, indent=2)

        # Create the prompt
        if self.config.prompt:
            prompt = self.config.prompt
        else:
            prompt = f"""
You are a workflow router that determines the next agent to execute in a multi-agent system.

CURRENT AGENT:
Name: {current_agent.name}
Description: {current_agent.description}
Input: {current_agent.input}
Output: {current_agent.output}

CURRENT AGENT'S OUTPUT:
{result_str}

AVAILABLE AGENTS:
{agents_str}

Based on the current agent's output and the available agents, determine which agent should be executed next.
Consider the following:
1. Which agent's input requirements best match the current agent's output?
2. Which agent's purpose and description make it the most logical next step?
3. Which agent would provide the most value in continuing the workflow?

Respond with a JSON object containing:
1. "next_agent": The name of the selected agent
2. "score": A confidence score between 0 and 1 indicating how suitable this agent is
3. "reasoning": A brief explanation of why this agent was selected

If no agent is suitable, set "next_agent" to an empty string and "score" to 0.

JSON Response:
"""
        return prompt

    def _create_next_input(
        self,
        current_agent: FlockAgent,
        result: dict[str, Any],
        next_agent: FlockAgent,
    ) -> dict[str, Any]:
        """Create the input for the next agent, including the previous agent's output.

        Args:
            current_agent: The agent that just completed execution
            result: The output from the current agent
            next_agent: The next agent to execute

        Returns:
            Input dictionary for the next agent
        """
        # Start with an empty input
        next_input = {}

        # Add a special field for the previous agent's output
        next_input["previous_agent_output"] = {
            "agent_name": current_agent.name,
            "result": result,
        }

        # Try to map the current agent's output to the next agent's input
        # This is a simple implementation that could be enhanced with more sophisticated mapping
        for key in result:
            # If the next agent expects this key, add it directly
            if key in next_agent.input:
                next_input[key] = result[key]

        return next_input
```

### src\flock\tools\__init__.py

- **Lines**: 0
- **Last modified**: 2025-05-24 17:10:34

```py

```

### src\flock\tools\azure_tools.py

- **Lines**: 781
- **Last modified**: 2025-05-21 19:51:15

```py
import os
from typing import Any

from azure.core.credentials import AzureKeyCredential
from azure.search.documents import SearchClient
from azure.search.documents.indexes import SearchIndexClient
from azure.search.documents.indexes.models import (
    ExhaustiveKnnAlgorithmConfiguration,
    HnswAlgorithmConfiguration,
    SearchableField,
    SearchField,
    SearchFieldDataType,
    SearchIndex,
    SimpleField,
    VectorSearch,
    VectorSearchProfile,
)
from azure.search.documents.models import VectorizedQuery
from azure.storage.blob import (
    BlobServiceClient,
    ContentSettings,
)

from flock.core.logging.trace_and_logged import traced_and_logged


def _get_default_endpoint() -> str:
    """Get the default Azure Search endpoint from environment variables."""
    endpoint = os.environ.get("AZURE_SEARCH_ENDPOINT")
    if not endpoint:
        raise ValueError(
            "AZURE_SEARCH_ENDPOINT environment variable is not set"
        )
    return endpoint


def _get_default_api_key() -> str:
    """Get the default Azure Search API key from environment variables."""
    api_key = os.environ.get("AZURE_SEARCH_API_KEY")
    if not api_key:
        raise ValueError("AZURE_SEARCH_API_KEY environment variable is not set")
    return api_key


def _get_default_index_name() -> str:
    """Get the default Azure Search index name from environment variables."""
    index_name = os.environ.get("AZURE_SEARCH_INDEX_NAME")
    if not index_name:
        raise ValueError(
            "AZURE_SEARCH_INDEX_NAME environment variable is not set"
        )
    return index_name


@traced_and_logged
def azure_search_initialize_clients(
    endpoint: str | None = None,
    api_key: str | None = None,
    index_name: str | None = None,
) -> dict[str, Any]:
    """Initialize Azure AI Search clients.

    Args:
        endpoint: The Azure AI Search service endpoint URL (defaults to AZURE_SEARCH_ENDPOINT env var)
        api_key: The Azure AI Search API key (defaults to AZURE_SEARCH_API_KEY env var)
        index_name: Optional index name for SearchClient initialization (defaults to AZURE_SEARCH_INDEX_NAME env var if not None)

    Returns:
        Dictionary containing the initialized clients
    """
    # Use environment variables as defaults if not provided
    endpoint = endpoint or _get_default_endpoint()
    api_key = api_key or _get_default_api_key()

    credential = AzureKeyCredential(api_key)

    # Create the search index client
    search_index_client = SearchIndexClient(
        endpoint=endpoint, credential=credential
    )

    # Create clients dictionary
    clients = {
        "index_client": search_index_client,
    }

    # Add search client if index_name was provided or available in env
    if index_name is None and os.environ.get("AZURE_SEARCH_INDEX_NAME"):
        index_name = _get_default_index_name()

    if index_name:
        search_client = SearchClient(
            endpoint=endpoint, index_name=index_name, credential=credential
        )
        clients["search_client"] = search_client

    return clients


@traced_and_logged
def azure_search_create_index(
    index_name: str | None = None,
    fields: list[SearchField] = None,
    vector_search: VectorSearch | None = None,
    endpoint: str | None = None,
    api_key: str | None = None,
) -> dict[str, Any]:
    """Create a new search index in Azure AI Search.

    Args:
        index_name: Name of the search index to create (defaults to AZURE_SEARCH_INDEX_NAME env var)
        fields: List of field definitions for the index
        vector_search: Optional vector search configuration
        endpoint: The Azure AI Search service endpoint URL (defaults to AZURE_SEARCH_ENDPOINT env var)
        api_key: The Azure AI Search API key (defaults to AZURE_SEARCH_API_KEY env var)

    Returns:
        Dictionary containing information about the created index
    """
    # Use environment variables as defaults if not provided
    endpoint = endpoint or _get_default_endpoint()
    api_key = api_key or _get_default_api_key()
    index_name = index_name or _get_default_index_name()

    if fields is None:
        raise ValueError("Fields must be provided for index creation")

    clients = azure_search_initialize_clients(endpoint, api_key)
    index_client = clients["index_client"]

    # Create the index
    index = SearchIndex(
        name=index_name, fields=fields, vector_search=vector_search
    )

    result = index_client.create_or_update_index(index)

    return {
        "index_name": result.name,
        "fields": [field.name for field in result.fields],
        "created": True,
    }


@traced_and_logged
def azure_search_upload_documents(
    documents: list[dict[str, Any]],
    index_name: str | None = None,
    endpoint: str | None = None,
    api_key: str | None = None,
) -> dict[str, Any]:
    """Upload documents to an Azure AI Search index.

    Args:
        documents: List of documents to upload (as dictionaries)
        index_name: Name of the search index (defaults to AZURE_SEARCH_INDEX_NAME env var)
        endpoint: The Azure AI Search service endpoint URL (defaults to AZURE_SEARCH_ENDPOINT env var)
        api_key: The Azure AI Search API key (defaults to AZURE_SEARCH_API_KEY env var)

    Returns:
        Dictionary containing the upload results
    """
    # Use environment variables as defaults if not provided
    endpoint = endpoint or _get_default_endpoint()
    api_key = api_key or _get_default_api_key()
    index_name = index_name or _get_default_index_name()

    clients = azure_search_initialize_clients(endpoint, api_key, index_name)
    search_client = clients["search_client"]

    result = search_client.upload_documents(documents=documents)

    # Process results
    succeeded = sum(1 for r in result if r.succeeded)

    return {
        "succeeded": succeeded,
        "failed": len(result) - succeeded,
        "total": len(result),
    }


@traced_and_logged
def azure_search_query(
    search_text: str | None = None,
    filter: str | None = None,
    select: list[str] | None = None,
    top: int | None = 50,
    vector: list[float] | None = None,
    vector_field: str | None = None,
    vector_k: int | None = 10,
    index_name: str | None = None,
    endpoint: str | None = None,
    api_key: str | None = None,
) -> list[dict[str, Any]]:
    """Search documents in an Azure AI Search index.

    Args:
        search_text: Optional text to search for (keyword search)
        filter: Optional OData filter expression
        select: Optional list of fields to return
        top: Maximum number of results to return
        vector: Optional vector for vector search
        vector_field: Name of the field containing vectors for vector search
        vector_k: Number of nearest neighbors to retrieve in vector search
        index_name: Name of the search index (defaults to AZURE_SEARCH_INDEX_NAME env var)
        endpoint: The Azure AI Search service endpoint URL (defaults to AZURE_SEARCH_ENDPOINT env var)
        api_key: The Azure AI Search API key (defaults to AZURE_SEARCH_API_KEY env var)

    Returns:
        List of search results as dictionaries
    """
    # Use environment variables as defaults if not provided
    endpoint = endpoint or _get_default_endpoint()
    api_key = api_key or _get_default_api_key()
    index_name = index_name or _get_default_index_name()

    clients = azure_search_initialize_clients(endpoint, api_key, index_name)
    search_client = clients["search_client"]

    # Set up vector query if vector is provided
    vectorized_query = None
    if vector and vector_field:
        vectorized_query = VectorizedQuery(
            vector=vector, k=vector_k, fields=[vector_field]
        )

    # Execute the search
    results = search_client.search(
        search_text=search_text,
        filter=filter,
        select=select,
        top=top,
        vector_queries=[vectorized_query] if vectorized_query else None,
    )

    # Convert results to list of dictionaries
    # filter out the text_vector field
    result_list = [{**dict(result), "text_vector": ""} for result in results]

    return result_list


@traced_and_logged
def azure_search_get_document(
    key: str,
    select: list[str] | None = None,
    index_name: str | None = None,
    endpoint: str | None = None,
    api_key: str | None = None,
) -> dict[str, Any]:
    """Retrieve a specific document from an Azure AI Search index by key.

    Args:
        key: The unique key of the document to retrieve
        select: Optional list of fields to return
        index_name: Name of the search index (defaults to AZURE_SEARCH_INDEX_NAME env var)
        endpoint: The Azure AI Search service endpoint URL (defaults to AZURE_SEARCH_ENDPOINT env var)
        api_key: The Azure AI Search API key (defaults to AZURE_SEARCH_API_KEY env var)

    Returns:
        The retrieved document as a dictionary
    """
    # Use environment variables as defaults if not provided
    endpoint = endpoint or _get_default_endpoint()
    api_key = api_key or _get_default_api_key()
    index_name = index_name or _get_default_index_name()

    clients = azure_search_initialize_clients(endpoint, api_key, index_name)
    search_client = clients["search_client"]

    result = search_client.get_document(key=key, selected_fields=select)

    return dict(result)


@traced_and_logged
def azure_search_delete_documents(
    keys: list[str],
    key_field_name: str = "id",
    index_name: str | None = None,
    endpoint: str | None = None,
    api_key: str | None = None,
) -> dict[str, Any]:
    """Delete documents from an Azure AI Search index.

    Args:
        keys: List of document keys to delete
        key_field_name: Name of the key field (defaults to "id")
        index_name: Name of the search index (defaults to AZURE_SEARCH_INDEX_NAME env var)
        endpoint: The Azure AI Search service endpoint URL (defaults to AZURE_SEARCH_ENDPOINT env var)
        api_key: The Azure AI Search API key (defaults to AZURE_SEARCH_API_KEY env var)

    Returns:
        Dictionary containing the deletion results
    """
    # Use environment variables as defaults if not provided
    endpoint = endpoint or _get_default_endpoint()
    api_key = api_key or _get_default_api_key()
    index_name = index_name or _get_default_index_name()

    clients = azure_search_initialize_clients(endpoint, api_key, index_name)
    search_client = clients["search_client"]

    # Format documents for deletion (only need the key field)
    documents_to_delete = [{key_field_name: key} for key in keys]

    result = search_client.delete_documents(documents=documents_to_delete)

    # Process results
    succeeded = sum(1 for r in result if r.succeeded)

    return {
        "succeeded": succeeded,
        "failed": len(result) - succeeded,
        "total": len(result),
    }


@traced_and_logged
def azure_search_list_indexes(
    endpoint: str | None = None, api_key: str | None = None
) -> list[dict[str, Any]]:
    """List all indexes in the Azure AI Search service.

    Args:
        endpoint: The Azure AI Search service endpoint URL (defaults to AZURE_SEARCH_ENDPOINT env var)
        api_key: The Azure AI Search API key (defaults to AZURE_SEARCH_API_KEY env var)

    Returns:
        List of indexes as dictionaries
    """
    # Use environment variables as defaults if not provided
    endpoint = endpoint or _get_default_endpoint()
    api_key = api_key or _get_default_api_key()

    clients = azure_search_initialize_clients(endpoint, api_key)
    index_client = clients["index_client"]

    result = index_client.list_indexes()

    # Convert index objects to dictionaries with basic information
    indexes = [
        {
            "name": index.name,
            "fields": [field.name for field in index.fields],
            "field_count": len(index.fields),
        }
        for index in result
    ]

    return indexes


@traced_and_logged
def azure_search_get_index_statistics(
    index_name: str | None = None,
    endpoint: str | None = None,
    api_key: str | None = None,
) -> dict[str, Any]:
    """Get statistics for a specific Azure AI Search index.

    Args:
        index_name: Name of the search index (defaults to AZURE_SEARCH_INDEX_NAME env var)
        endpoint: The Azure AI Search service endpoint URL (defaults to AZURE_SEARCH_ENDPOINT env var)
        api_key: The Azure AI Search API key (defaults to AZURE_SEARCH_API_KEY env var)

    Returns:
        Dictionary containing index statistics
    """
    # Use environment variables as defaults if not provided
    endpoint = endpoint or _get_default_endpoint()
    api_key = api_key or _get_default_api_key()
    index_name = index_name or _get_default_index_name()

    clients = azure_search_initialize_clients(endpoint, api_key, index_name)
    search_client = clients["search_client"]

    stats = search_client.get_document_count()

    return {"document_count": stats}


@traced_and_logged
def azure_search_create_vector_index(
    fields: list[dict[str, Any]],
    vector_dimensions: int,
    index_name: str | None = None,
    algorithm_kind: str = "hnsw",
    endpoint: str | None = None,
    api_key: str | None = None,
) -> dict[str, Any]:
    """Create a vector search index in Azure AI Search.

    Args:
        fields: List of field configurations (dicts with name, type, etc.)
        vector_dimensions: Dimensions of the vector field
        index_name: Name of the search index (defaults to AZURE_SEARCH_INDEX_NAME env var)
        algorithm_kind: Vector search algorithm ("hnsw" or "exhaustive")
        endpoint: The Azure AI Search service endpoint URL (defaults to AZURE_SEARCH_ENDPOINT env var)
        api_key: The Azure AI Search API key (defaults to AZURE_SEARCH_API_KEY env var)

    Returns:
        Dictionary with index creation result
    """
    # Use environment variables as defaults if not provided
    endpoint = endpoint or _get_default_endpoint()
    api_key = api_key or _get_default_api_key()
    index_name = index_name or _get_default_index_name()

    clients = azure_search_initialize_clients(endpoint, api_key)
    index_client = clients["index_client"]

    # Convert field configurations to SearchField objects
    index_fields = []
    vector_fields = []

    for field_config in fields:
        field_name = field_config["name"]
        field_type = field_config["type"]
        field_searchable = field_config.get("searchable", False)
        field_filterable = field_config.get("filterable", False)
        field_sortable = field_config.get("sortable", False)
        field_key = field_config.get("key", False)
        field_vector = field_config.get("vector", False)

        if field_searchable and field_type == "string":
            field = SearchableField(
                name=field_name,
                type=SearchFieldDataType.String,
                key=field_key,
                filterable=field_filterable,
                sortable=field_sortable,
            )
        else:
            data_type = None
            if field_type == "string":
                data_type = SearchFieldDataType.String
            elif field_type == "int":
                data_type = SearchFieldDataType.Int32
            elif field_type == "double":
                data_type = SearchFieldDataType.Double
            elif field_type == "boolean":
                data_type = SearchFieldDataType.Boolean
            elif field_type == "collection":
                data_type = SearchFieldDataType.Collection(
                    SearchFieldDataType.String
                )

            field = SimpleField(
                name=field_name,
                type=data_type,
                key=field_key,
                filterable=field_filterable,
                sortable=field_sortable,
            )

        index_fields.append(field)

        if field_vector:
            vector_fields.append(field_name)

    # Set up vector search configuration
    algorithm_config = None
    if algorithm_kind.lower() == "hnsw":
        algorithm_config = HnswAlgorithmConfiguration(
            name="hnsw-config",
            parameters={"m": 4, "efConstruction": 400, "efSearch": 500},
        )
    else:
        algorithm_config = ExhaustiveKnnAlgorithmConfiguration(
            name="exhaustive-config"
        )

    # Create vector search configuration
    vector_search = VectorSearch(
        algorithms=[algorithm_config],
        profiles=[
            VectorSearchProfile(
                name="vector-profile",
                algorithm_configuration_name=algorithm_config.name,
            )
        ],
    )

    # Create the search index
    index = SearchIndex(
        name=index_name, fields=index_fields, vector_search=vector_search
    )

    try:
        result = index_client.create_or_update_index(index)
        return {
            "index_name": result.name,
            "vector_fields": vector_fields,
            "vector_dimensions": vector_dimensions,
            "algorithm": algorithm_kind,
            "created": True,
        }
    except Exception as e:
        return {"error": str(e), "created": False}


# --- Azure Blob Storage Tools ---

def _get_blob_service_client(conn_string_env_var: str) -> BlobServiceClient:
    """Helper function to get BlobServiceClient using a connection string from an environment variable."""
    actual_connection_string = os.environ.get(conn_string_env_var)
    if not actual_connection_string:
        raise ValueError(f"Environment variable '{conn_string_env_var}' for Azure Storage connection string is not set or is empty.")
    return BlobServiceClient.from_connection_string(actual_connection_string)


@traced_and_logged
def azure_storage_list_containers(conn_string_env_var: str) -> list[str]:
    """Lists all containers in the Azure Storage account.

    Args:
        conn_string_env_var: The name of the environment variable holding the Azure Storage connection string.

    Returns:
        A list of container names.
    """
    blob_service_client = _get_blob_service_client(conn_string_env_var)
    containers = blob_service_client.list_containers()
    return [container.name for container in containers]


@traced_and_logged
def azure_storage_create_container(container_name: str, conn_string_env_var: str) -> dict[str, Any]:
    """Creates a new container in the Azure Storage account.

    Args:
        container_name: The name of the container to create.
        conn_string_env_var: The name of the environment variable holding the Azure Storage connection string.

    Returns:
        A dictionary with creation status.
    """
    blob_service_client = _get_blob_service_client(conn_string_env_var)
    try:
        blob_service_client.create_container(container_name)
        return {"container_name": container_name, "created": True, "message": f"Container '{container_name}' created successfully."}
    except Exception as e:
        return {"container_name": container_name, "created": False, "error": str(e)}


@traced_and_logged
def azure_storage_delete_container(container_name: str, conn_string_env_var: str) -> dict[str, Any]:
    """Deletes an existing container from the Azure Storage account.

    Args:
        container_name: The name of the container to delete.
        conn_string_env_var: The name of the environment variable holding the Azure Storage connection string.

    Returns:
        A dictionary with deletion status.
    """
    blob_service_client = _get_blob_service_client(conn_string_env_var)
    try:
        blob_service_client.delete_container(container_name)
        return {"container_name": container_name, "deleted": True, "message": f"Container '{container_name}' deleted successfully."}
    except Exception as e:
        return {"container_name": container_name, "deleted": False, "error": str(e)}


@traced_and_logged
def azure_storage_list_blobs(container_name: str, conn_string_env_var: str) -> list[str]:
    """Lists all blobs in a specified container.

    Args:
        container_name: The name of the container.
        conn_string_env_var: The name of the environment variable holding the Azure Storage connection string.

    Returns:
        A list of blob names.
    """
    blob_service_client = _get_blob_service_client(conn_string_env_var)
    container_client = blob_service_client.get_container_client(container_name)
    blob_list = container_client.list_blobs()
    return [blob.name for blob in blob_list]


@traced_and_logged
def azure_storage_upload_blob_text(container_name: str, blob_name: str, text_content: str, conn_string_env_var: str, overwrite: bool = True) -> dict[str, Any]:
    """Uploads text content as a blob to the specified container.

    Args:
        container_name: The name of the container.
        blob_name: The name of the blob to create.
        text_content: The string content to upload.
        conn_string_env_var: The name of the environment variable holding the Azure Storage connection string.
        overwrite: Whether to overwrite the blob if it already exists. Defaults to True.

    Returns:
        A dictionary with upload status.
    """
    blob_service_client = _get_blob_service_client(conn_string_env_var)
    blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)
    try:
        content_settings = ContentSettings(content_type='text/plain')
        blob_client.upload_blob(text_content.encode('utf-8'), overwrite=overwrite, content_settings=content_settings)
        return {"container_name": container_name, "blob_name": blob_name, "uploaded": True, "message": "Text content uploaded successfully."}
    except Exception as e:
        return {"container_name": container_name, "blob_name": blob_name, "uploaded": False, "error": str(e)}


@traced_and_logged
def azure_storage_upload_blob_bytes(container_name: str, blob_name: str, bytes_content: bytes, conn_string_env_var: str, overwrite: bool = True) -> dict[str, Any]:
    """Uploads bytes content as a blob to the specified container.

    Args:
        container_name: The name of the container.
        blob_name: The name of the blob to create.
        bytes_content: The bytes content to upload.
        conn_string_env_var: The name of the environment variable holding the Azure Storage connection string.
        overwrite: Whether to overwrite the blob if it already exists. Defaults to True.

    Returns:
        A dictionary with upload status.
    """
    blob_service_client = _get_blob_service_client(conn_string_env_var)
    blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)
    try:
        content_settings = ContentSettings(content_type='application/octet-stream')
        blob_client.upload_blob(bytes_content, overwrite=overwrite, content_settings=content_settings)
        return {"container_name": container_name, "blob_name": blob_name, "uploaded": True, "message": "Bytes content uploaded successfully."}
    except Exception as e:
        return {"container_name": container_name, "blob_name": blob_name, "uploaded": False, "error": str(e)}


@traced_and_logged
def azure_storage_upload_blob_from_file(container_name: str, blob_name: str, file_path: str, conn_string_env_var: str, overwrite: bool = True) -> dict[str, Any]:
    """Uploads a local file to a blob in the specified container.

    Args:
        container_name: The name of the container.
        blob_name: The name of the blob to create.
        file_path: The local path to the file to upload.
        conn_string_env_var: The name of the environment variable holding the Azure Storage connection string.
        overwrite: Whether to overwrite the blob if it already exists. Defaults to True.

    Returns:
        A dictionary with upload status.
    """
    blob_service_client = _get_blob_service_client(conn_string_env_var)
    blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)
    try:
        with open(file_path, "rb") as data:
            blob_client.upload_blob(data, overwrite=overwrite)
        return {"container_name": container_name, "blob_name": blob_name, "file_path": file_path, "uploaded": True, "message": "File uploaded successfully."}
    except FileNotFoundError:
        return {"container_name": container_name, "blob_name": blob_name, "file_path": file_path, "uploaded": False, "error": "File not found."}
    except Exception as e:
        return {"container_name": container_name, "blob_name": blob_name, "file_path": file_path, "uploaded": False, "error": str(e)}


@traced_and_logged
def azure_storage_download_blob_to_text(container_name: str, blob_name: str, conn_string_env_var: str) -> str:
    """Downloads a blob's content as text.

    Args:
        container_name: The name of the container.
        blob_name: The name of the blob to download.
        conn_string_env_var: The name of the environment variable holding the Azure Storage connection string.

    Returns:
        The blob content as a string.

    Raises:
        Exception: If download fails or blob is not text.
    """
    blob_service_client = _get_blob_service_client(conn_string_env_var)
    blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)
    try:
        download_stream = blob_client.download_blob()
        return download_stream.readall().decode('utf-8')
    except Exception as e:
        raise Exception(f"Failed to download or decode blob '{blob_name}' from container '{container_name}': {e!s}")


@traced_and_logged
def azure_storage_download_blob_to_bytes(container_name: str, blob_name: str, conn_string_env_var: str) -> bytes:
    """Downloads a blob's content as bytes.

    Args:
        container_name: The name of the container.
        blob_name: The name of the blob to download.
        conn_string_env_var: The name of the environment variable holding the Azure Storage connection string.

    Returns:
        The blob content as bytes.
    """
    blob_service_client = _get_blob_service_client(conn_string_env_var)
    blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)
    download_stream = blob_client.download_blob()
    return download_stream.readall()


@traced_and_logged
def azure_storage_download_blob_to_file(container_name: str, blob_name: str, file_path: str, conn_string_env_var: str, overwrite: bool = True) -> dict[str, Any]:
    """Downloads a blob to a local file.

    Args:
        container_name: The name of the container.
        blob_name: The name of the blob to download.
        file_path: The local path to save the downloaded file.
        conn_string_env_var: The name of the environment variable holding the Azure Storage connection string.
        overwrite: Whether to overwrite the local file if it exists. Defaults to True.

    Returns:
        A dictionary with download status.
    """
    if not overwrite and os.path.exists(file_path):
        return {"container_name": container_name, "blob_name": blob_name, "file_path": file_path, "downloaded": False, "error": "File exists and overwrite is False."}

    blob_service_client = _get_blob_service_client(conn_string_env_var)
    blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)
    try:
        with open(file_path, "wb") as download_file:
            download_stream = blob_client.download_blob()
            download_file.write(download_stream.readall())
        return {"container_name": container_name, "blob_name": blob_name, "file_path": file_path, "downloaded": True, "message": "File downloaded successfully."}
    except Exception as e:
        return {"container_name": container_name, "blob_name": blob_name, "file_path": file_path, "downloaded": False, "error": str(e)}


@traced_and_logged
def azure_storage_delete_blob(container_name: str, blob_name: str, conn_string_env_var: str) -> dict[str, Any]:
    """Deletes a specified blob from a container.

    Args:
        container_name: The name of the container.
        blob_name: The name of the blob to delete.
        conn_string_env_var: The name of the environment variable holding the Azure Storage connection string.

    Returns:
        A dictionary with deletion status.
    """
    blob_service_client = _get_blob_service_client(conn_string_env_var)
    blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)
    try:
        blob_client.delete_blob()
        return {"container_name": container_name, "blob_name": blob_name, "deleted": True, "message": "Blob deleted successfully."}
    except Exception as e:
        return {"container_name": container_name, "blob_name": blob_name, "deleted": False, "error": str(e)}


@traced_and_logged
def azure_storage_get_blob_properties(container_name: str, blob_name: str, conn_string_env_var: str) -> dict[str, Any]:
    """Retrieves properties of a specified blob.

    Args:
        container_name: The name of the container.
        blob_name: The name of the blob.
        conn_string_env_var: The name of the environment variable holding the Azure Storage connection string.

    Returns:
        A dictionary containing blob properties.
    """
    blob_service_client = _get_blob_service_client(conn_string_env_var)
    blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)
    try:
        properties = blob_client.get_blob_properties()
        return {
            "name": properties.name,
            "container": properties.container,
            "size": properties.size,
            "content_type": properties.content_settings.content_type,
            "last_modified": properties.last_modified.isoformat() if properties.last_modified else None,
            "etag": properties.etag,
            # Add more properties as needed
        }
    except Exception as e:
        return {"container_name": container_name, "blob_name": blob_name, "error": str(e)}

# Potential future tools:
# - azure_storage_set_blob_metadata
# - azure_storage_get_blob_metadata
# - azure_storage_generate_sas_token_blob
# - azure_storage_copy_blob
```

### src\flock\tools\code_tools.py

- **Lines**: 56
- **Last modified**: 2025-05-25 23:17:06

```py
from flock.core.interpreter.python_interpreter import PythonInterpreter
from flock.core.logging.trace_and_logged import traced_and_logged


@traced_and_logged
def code_evaluate_math(expression: str) -> float:
    try:
        result = PythonInterpreter(
            {},
            [
                "os",
                "math",
                "random",
                "datetime",
                "time",
                "string",
                "collections",
                "itertools",
                "functools",
                "typing",
                "enum",
                "json",
                "ast",
            ],
            verbose=True,
        ).execute(expression)
        return result
    except Exception:
        raise


@traced_and_logged
def code_code_eval(python_code: str) -> str:
    try:
        result = PythonInterpreter(
            {},
            [
                "os",
                "math",
                "random",
                "datetime",
                "time",
                "string",
                "collections",
                "itertools",
                "functools",
                "typing",
                "enum",
                "json",
                "ast",
            ],
            verbose=True,
        ).execute(python_code)
        return result
    except Exception:
        raise
```

### src\flock\tools\file_tools.py

- **Lines**: 140
- **Last modified**: 2025-05-21 19:51:15

```py
import importlib
import json
from typing import Any

from flock.core.logging.trace_and_logged import traced_and_logged


@traced_and_logged
def file_get_anything_as_markdown(url_or_file_path: str):
    if importlib.util.find_spec("docling") is not None:
        from docling.document_converter import DocumentConverter

        try:
            converter = DocumentConverter()
            result = converter.convert(url_or_file_path)
            markdown = result.document.export_to_markdown()
            return markdown
        except Exception:
            raise
    else:
        raise ImportError(
            "Optional tool dependencies not installed. Install with 'pip install flock-core[file-tools]'."
        )


@traced_and_logged
def file_save_to_file(content: str, filename: str):
    try:
        with open(filename, "w") as f:
            f.write(content)
    except Exception:
        raise


@traced_and_logged
def file_read_from_file(filename: str) -> str:
    with open(filename, encoding="utf-8") as file:
        return file.read()


@traced_and_logged
def file_json_parse_safe(text: str) -> dict:
    try:
        result = json.loads(text)
        return result
    except Exception:
        return {}


@traced_and_logged
def file_json_search(
    json_file_path: str, search_query: str, case_sensitive: bool = False
) -> list:
    """Search a JSON file for objects containing the specified search query.

    Args:
        json_file_path (str): Path to the JSON file to search
        search_query (str): Text to search for within the JSON objects
        case_sensitive (bool, optional): Whether to perform a case-sensitive search. Defaults to False.

    Returns:
        list: List of JSON objects (as dicts) that contain the search query

    Example:
        >>> matching_tickets = file_json_search("tickets.json", "error 404")
        >>> print(
        ...     f"Found {len(matching_tickets)} tickets mentioning '404 error'"
        ... )
    """
    try:
        # Read the JSON file
        file_content = file_read_from_file(json_file_path)

        # Parse the JSON content
        json_data = file_json_parse_safe(file_content)

        # Convert search query to lowercase if case-insensitive search
        if not case_sensitive:
            search_query = search_query.lower()

        results = []

        # Determine if the JSON root is an object or array
        if isinstance(json_data, dict):
            # Handle case where root is a dictionary object
            for key, value in json_data.items():
                if isinstance(value, list):
                    # If this key contains a list of objects, search within them
                    matching_items = _search_in_list(
                        value, search_query, case_sensitive
                    )
                    results.extend(matching_items)
                elif _contains_text(value, search_query, case_sensitive):
                    # The entire object matches
                    results.append(json_data)
                    break
        elif isinstance(json_data, list):
            # Handle case where root is an array
            matching_items = _search_in_list(
                json_data, search_query, case_sensitive
            )
            results.extend(matching_items)

        return results

    except Exception as e:
        return [{"error": f"Error searching JSON file: {e!s}"}]


def _search_in_list(
    items: list, search_query: str, case_sensitive: bool
) -> list:
    """Helper function to search for text in a list of items."""
    matching_items = []
    for item in items:
        if _contains_text(item, search_query, case_sensitive):
            matching_items.append(item)
    return matching_items


def _contains_text(obj: Any, search_query: str, case_sensitive: bool) -> bool:
    """Recursively check if an object contains the search query in any of its string values."""
    if isinstance(obj, str):
        # For string values, check if they contain the search query
        if case_sensitive:
            return search_query in obj
        else:
            return search_query in obj.lower()
    elif isinstance(obj, dict):
        # For dictionaries, check each value
        for value in obj.values():
            if _contains_text(value, search_query, case_sensitive):
                return True
    elif isinstance(obj, list):
        # For lists, check each item
        for item in obj:
            if _contains_text(item, search_query, case_sensitive):
                return True
    # For other types (numbers, booleans, None), return False
    return False
```

### src\flock\tools\github_tools.py

- **Lines**: 157
- **Last modified**: 2025-05-21 19:51:15

```py
"""This module provides tools for interacting with GitHub repositories."""

import base64
import os

import httpx

from flock.core.logging.trace_and_logged import traced_and_logged


@traced_and_logged
def github_create_user_stories_as_github_issue(title: str, body: str) -> str:
    github_pat = os.getenv("GITHUB_PAT")
    github_repo = os.getenv("GITHUB_REPO")

    url = f"https://api.github.com/repos/{github_repo}/issues"
    headers = {
        "Authorization": f"Bearer {github_pat}",
        "Accept": "application/vnd.github+json",
    }
    issue_title = title
    issue_body = body

    payload = {"title": issue_title, "body": issue_body}
    response = httpx.post(url, json=payload, headers=headers)

    if response.status_code == 201:
        return "Issue created successfully."
    else:
        return "Failed to create issue. Please try again later."


@traced_and_logged
def github_upload_readme(content: str):
    GITHUB_USERNAME = os.getenv("GITHUB_USERNAME")
    REPO_NAME = os.getenv("GITHUB_REPO")
    GITHUB_TOKEN = os.getenv("GITHUB_PAT")

    if not GITHUB_USERNAME or not REPO_NAME or not GITHUB_TOKEN:
        raise ValueError(
            "Missing environment variables: GITHUB_USERNAME, GITHUB_REPO, or GITHUB_PAT"
        )

    GITHUB_API_URL = f"https://api.github.com/repos/{GITHUB_USERNAME}/{REPO_NAME}/contents/README.md"

    encoded_content = base64.b64encode(content.encode()).decode()

    with httpx.Client() as client:
        response = client.get(
            GITHUB_API_URL,
            headers={
                "Authorization": f"Bearer {GITHUB_TOKEN}",
                "Accept": "application/vnd.github.v3+json",
            },
        )

        data = response.json()
        sha = data.get("sha", None)

        payload = {
            "message": "Updating README.md",
            "content": encoded_content,
            "branch": "main",
        }

        if sha:
            payload["sha"] = sha

        response = client.put(
            GITHUB_API_URL,
            json=payload,
            headers={
                "Authorization": f"Bearer {GITHUB_TOKEN}",
                "Accept": "application/vnd.github.v3+json",
            },
        )

        if response.status_code in [200, 201]:
            print("README.md successfully uploaded/updated!")
        else:
            print("Failed to upload README.md:", response.json())


@traced_and_logged
def github_create_files(file_paths) -> str:
    """Create multiple files in a GitHub repository with a predefined content.

    This function iterates over a list of file paths (relative to the repository root) and creates
    each file in the specified GitHub repository with the content "#created by flock". For each file,
    it checks whether the file already exists; if it does, that file is skipped. The function
    uses the following environment variables for authentication and repository information:

      - GITHUB_USERNAME: Your GitHub username.
      - GITHUB_REPO: The name of the repository.
      - GITHUB_PAT: Your GitHub Personal Access Token for authentication.

    Parameters:
        file_paths (list of str): A list of file paths (relative to the repository root) to be created.

    Returns:
        str: A message indicating whether the files were created successfully or if there was a failure.
    """
    try:
        GITHUB_USERNAME = os.getenv("GITHUB_USERNAME")
        REPO_NAME = os.getenv("GITHUB_REPO")
        GITHUB_TOKEN = os.getenv("GITHUB_PAT")

        if not GITHUB_USERNAME or not REPO_NAME or not GITHUB_TOKEN:
            raise ValueError(
                "Missing environment variables: GITHUB_USERNAME, GITHUB_REPO, or GITHUB_PAT"
            )

        encoded_content = base64.b64encode(b"#created by flock").decode()

        with httpx.Client() as client:
            for file_path in file_paths:
                GITHUB_API_URL = f"https://api.github.com/repos/{GITHUB_USERNAME}/{REPO_NAME}/contents/{file_path}"

                response = client.get(
                    GITHUB_API_URL,
                    headers={
                        "Authorization": f"token {GITHUB_TOKEN}",
                        "Accept": "application/vnd.github.v3+json",
                    },
                )

                data = response.json()
                sha = data.get("sha", None)

                payload = {
                    "message": f"Creating {file_path}",
                    "content": encoded_content,
                    "branch": "main",
                }

                if sha:
                    print(f"Skipping {file_path}, file already exists.")
                    continue

                response = client.put(
                    GITHUB_API_URL,
                    json=payload,
                    headers={
                        "Authorization": f"token {GITHUB_TOKEN}",
                        "Accept": "application/vnd.github.v3+json",
                    },
                )

                if response.status_code in [200, 201]:
                    print(f"{file_path} successfully created!")
                else:
                    print(f"Failed to create {file_path}:", response.json())

        return "Files created successfully."

    except Exception:
        return "Failed to create file. Please try again later."
```

### src\flock\tools\markdown_tools.py

- **Lines**: 205
- **Last modified**: 2025-05-21 19:51:15

```py
import re
from typing import Any

from flock.core.logging.trace_and_logged import traced_and_logged


@traced_and_logged
def markdown_split_by_headers(
    markdown_text: str, min_header_level: int = 1, max_header_level: int = 2
) -> list[dict[str, Any]]:
    if not markdown_text:
        return []

    # Pattern to match headers from level min_header_level to max_header_level
    header_pattern = re.compile(
        f"^({'#' * min_header_level}){{'1,{max_header_level - min_header_level + 1}'}}\\s+(.+)$",
        re.MULTILINE,
    )

    # Find all headers
    headers = list(header_pattern.finditer(markdown_text))

    if not headers:
        return [{"title": "Text", "content": markdown_text, "level": 0}]

    chunks = []

    # Process each section
    for i in range(len(headers)):
        current_header = headers[i]
        header_text = current_header.group(2).strip()
        header_level = len(current_header.group(1))

        # Determine section content
        if i < len(headers) - 1:
            next_header_start = headers[i + 1].start()
            content = markdown_text[current_header.end() : next_header_start]
        else:
            content = markdown_text[current_header.end() :]

        chunks.append(
            {
                "title": header_text,
                "content": content.strip(),
                "level": header_level,
            }
        )

    # Check if there's content before the first header
    if headers[0].start() > 0:
        preamble = markdown_text[: headers[0].start()].strip()
        if preamble:
            chunks.insert(
                0, {"title": "Preamble", "content": preamble, "level": 0}
            )

    return chunks


@traced_and_logged
def markdown_extract_code_blocks(
    markdown_text: str, language: str = None
) -> list[dict[str, str]]:
    if not markdown_text:
        return []

    # Pattern to match markdown code blocks
    if language:
        # Match only code blocks with the specified language
        pattern = rf"```{language}\s*([\s\S]*?)\s*```"
    else:
        # Match all code blocks, capturing the language specifier if present
        pattern = r"```(\w*)\s*([\s\S]*?)\s*```"

    blocks = []

    if language:
        # If language is specified, we only capture the code content
        matches = re.finditer(pattern, markdown_text)
        for match in matches:
            blocks.append(
                {"language": language, "code": match.group(1).strip()}
            )
    else:
        # If no language is specified, we capture both language and code content
        matches = re.finditer(pattern, markdown_text)
        for match in matches:
            lang = match.group(1).strip() if match.group(1) else "text"
            blocks.append({"language": lang, "code": match.group(2).strip()})

    return blocks


@traced_and_logged
def markdown_extract_links(markdown_text: str) -> list[dict[str, str]]:
    if not markdown_text:
        return []

    # Pattern to match markdown links [text](url)
    link_pattern = re.compile(r"\[([^\]]+)\]\(([^)]+)\)")
    matches = link_pattern.findall(markdown_text)

    return [{"text": text, "url": url} for text, url in matches]


@traced_and_logged
def markdown_extract_tables(markdown_text: str) -> list[dict[str, Any]]:
    if not markdown_text:
        return []

    # Split the text by lines
    lines = markdown_text.split("\n")

    tables = []
    current_table = None
    header_row = None

    for line in lines:
        line = line.strip()

        # Table rows are indicated by starting with |
        if line.startswith("|") and line.endswith("|"):
            if current_table is None:
                current_table = []
                # This is the header row
                header_row = [
                    cell.strip() for cell in line.strip("|").split("|")
                ]
            elif "|--" in line or "|:-" in line:
                # This is the separator row, ignore it
                pass
            else:
                # This is a data row
                row_data = [cell.strip() for cell in line.strip("|").split("|")]

                # Create a dictionary mapping headers to values
                row_dict = {}
                for i, header in enumerate(header_row):
                    if i < len(row_data):
                        row_dict[header] = row_data[i]
                    else:
                        row_dict[header] = ""

                current_table.append(row_dict)
        else:
            # End of table
            if current_table is not None:
                tables.append({"headers": header_row, "rows": current_table})
                current_table = None
                header_row = None

    # Don't forget to add the last table if we're at the end of the document
    if current_table is not None:
        tables.append({"headers": header_row, "rows": current_table})

    return tables


@traced_and_logged
def markdown_to_plain_text(markdown_text: str) -> str:
    if not markdown_text:
        return ""

    # Replace headers
    text = re.sub(r"^#{1,6}\s+(.+)$", r"\1", markdown_text, flags=re.MULTILINE)

    # Replace bold and italic
    text = re.sub(r"\*\*(.*?)\*\*", r"\1", text)
    text = re.sub(r"__(.*?)__", r"\1", text)
    text = re.sub(r"\*(.*?)\*", r"\1", text)
    text = re.sub(r"_(.*?)_", r"\1", text)

    # Replace links
    text = re.sub(r"\[(.*?)\]\((.*?)\)", r"\1 (\2)", text)

    # Replace code blocks
    text = re.sub(r"```(?:\w+)?\s*([\s\S]*?)\s*```", r"\1", text)
    text = re.sub(r"`([^`]*?)`", r"\1", text)

    # Replace bullet points
    text = re.sub(r"^[\*\-\+]\s+(.+)$", r"• \1", text, flags=re.MULTILINE)

    # Replace numbered lists (keeping the numbers)
    text = re.sub(r"^\d+\.\s+(.+)$", r"\1", text, flags=re.MULTILINE)

    # Replace blockquotes
    text = re.sub(r"^>\s+(.+)$", r"\1", text, flags=re.MULTILINE)

    # Remove HTML tags
    text = re.sub(r"<.*?>", "", text)

    # Normalize whitespace
    text = re.sub(r"\n{3,}", "\n\n", text)

    return text.strip()



@traced_and_logged
def extract_links_from_markdown(markdown: str, url: str) -> list:
    # Regular expression to find all markdown links
    link_pattern = re.compile(r"\[([^\]]+)\]\(([^)]+)\)")
    links = link_pattern.findall(markdown)
    return [url + link[1] for link in links]

```

### src\flock\tools\system_tools.py

- **Lines**: 9
- **Last modified**: 2025-05-21 19:51:15

```py
from flock.core.logging.trace_and_logged import traced_and_logged


@traced_and_logged
def get_current_time() -> str:
    import datetime

    time = datetime.datetime.now().isoformat()
    return time
```

### src\flock\tools\text_tools.py

- **Lines**: 810
- **Last modified**: 2025-05-21 20:02:34

```py
import hashlib
import json
import re
from collections.abc import Callable
from typing import Any

import nltk

from flock.core.logging.trace_and_logged import traced_and_logged

# Ensure NLTK data is downloaded
try:
    nltk.data.find("tokenizers/punkt")
except LookupError:
    nltk.download("punkt")

try:
    nltk.data.find("corpora/stopwords")
except LookupError:
    nltk.download("stopwords")


@traced_and_logged
def text_split_by_sentences(text: str) -> list[str]:
    return nltk.sent_tokenize(text)


@traced_and_logged
def text_split_by_characters(
    text: str, chunk_size: int = 4000, overlap: int = 200
) -> list[str]:
    if chunk_size <= 0:
        raise ValueError("chunk_size must be positive")

    if overlap >= chunk_size:
        raise ValueError("overlap must be smaller than chunk_size")

    if not text:
        return []

    chunks = []
    start = 0
    text_length = len(text)

    while start < text_length:
        end = min(start + chunk_size, text_length)

        # If we're not at the end and the next character isn't a space, try to find a suitable break point
        if end < text_length and text[end] not in [
            " ",
            "\n",
            ".",
            ",",
            "!",
            "?",
            ";",
            ":",
            "-",
        ]:
            # Look for the last occurrence of a good break character
            break_chars = [" ", "\n", ".", ",", "!", "?", ";", ":", "-"]
            for i in range(end, max(start, end - 100), -1):
                if text[i] in break_chars:
                    end = i + 1  # Include the break character
                    break

        chunks.append(text[start:end])
        start = end - overlap if end < text_length else text_length

    return chunks


@traced_and_logged
def text_split_by_tokens(
    text: str,
    tokenizer: Callable[[str], list[str]],
    max_tokens: int = 1024,
    overlap_tokens: int = 100,
) -> list[str]:
    tokens = tokenizer(text)
    chunks = []

    i = 0
    while i < len(tokens):
        chunk = tokens[i : i + max_tokens]
        chunks.append("".join(chunk))
        i += max_tokens - overlap_tokens

    return chunks


@traced_and_logged
def text_split_by_separator(text: str, separator: str = "\n\n") -> list[str]:
    if not text:
        return []

    chunks = text.split(separator)
    return [chunk for chunk in chunks if chunk.strip()]


@traced_and_logged
def text_recursive_splitter(
    text: str,
    chunk_size: int = 4000,
    separators: list[str] = ["\n\n", "\n", ". ", ", ", " ", ""],
    keep_separator: bool = True,
) -> list[str]:
    if not text:
        return []

    if len(text) <= chunk_size:
        return [text]

    if not separators:
        return [
            text[:chunk_size],
            *text_recursive_splitter(text[chunk_size:], chunk_size, separators),
        ]

    separator = separators[0]
    new_separators = separators[1:]

    if separator == "":
        # If we're at the character level, just split by characters
        return text_split_by_characters(text, chunk_size=chunk_size, overlap=0)

    splits = text.split(separator)
    separator_len = len(separator) if keep_separator else 0

    # Add separator back to the chunks if needed
    if keep_separator and separator:
        splits = [f"{split}{separator}" for split in splits[:-1]] + [splits[-1]]

    # Process each split
    result = []
    current_chunk = []
    current_length = 0

    for split in splits:
        split_len = len(split)

        if split_len > chunk_size:
            # If current split is too large, handle current chunk and recursively split this large piece
            if current_chunk:
                result.append("".join(current_chunk))
                current_chunk = []
                current_length = 0

            # Recursively split this large piece
            smaller_chunks = text_recursive_splitter(
                split, chunk_size, new_separators, keep_separator
            )
            result.extend(smaller_chunks)
        elif current_length + split_len <= chunk_size:
            # If we can fit this split in the current chunk, add it
            current_chunk.append(split)
            current_length += split_len
        else:
            # If we can't fit this split, complete the current chunk and start a new one
            result.append("".join(current_chunk))
            current_chunk = [split]
            current_length = split_len

    # Don't forget the last chunk
    if current_chunk:
        result.append("".join(current_chunk))

    return result


@traced_and_logged
def text_chunking_for_embedding(
    text: str, file_name: str, chunk_size: int = 1000, overlap: int = 100
) -> list[dict[str, Any]]:
    chunks = text_split_by_characters(text, chunk_size=chunk_size, overlap=overlap)

    # Create metadata for each chunk
    result = []
    for i, chunk in enumerate(chunks):
        result.append(
            {
                "chunk_id": file_name + "_" + str(i),
                "text": chunk,
                "file": file_name,
                "total_chunks": len(chunks),
            }
        )

    return result


@traced_and_logged
def text_split_code_by_functions(code: str) -> list[dict[str, Any]]:
    if not code:
        return []

    # Basic pattern for Python functions
    function_pattern = re.compile(
        r"(^|\n)def\s+([a-zA-Z_][a-zA-Z0-9_]*)\s*\((.*?)\)(?:\s*->.*?)?:"
    )
    matches = list(function_pattern.finditer(code))

    if not matches:
        return [{"name": "Main", "content": code, "type": "code"}]

    functions = []

    # Process each function
    for i in range(len(matches)):
        current_match = matches[i]
        function_name = current_match.group(2)

        # Determine function content
        if i < len(matches) - 1:
            next_function_start = matches[i + 1].start()
            content = code[current_match.start() : next_function_start]
        else:
            content = code[current_match.start() :]

        functions.append(
            {
                "name": function_name,
                "content": content.strip(),
                "type": "function",
            }
        )

    # Check if there's content before the first function
    if matches[0].start() > 0:
        preamble = code[: matches[0].start()].strip()
        if preamble:
            functions.insert(
                0,
                {"name": "Imports/Setup", "content": preamble, "type": "code"},
            )

    return functions


@traced_and_logged
def text_count_tokens(text: str, model: str = "gpt-3.5-turbo") -> int:
    """Count tokens using tiktoken."""
    if not text:
        return 0

    try:
        import tiktoken

        # Map model names to encoding types
        if model.startswith(("gpt-4", "gpt-3.5")):
            encoding_name = "cl100k_base"  # For newer OpenAI models
        elif model.startswith("text-davinci"):
            encoding_name = "p50k_base"  # For older OpenAI models
        elif "llama" in model.lower() or "mistral" in model.lower():
            encoding_name = (
                "cl100k_base"  # Best approximation for LLaMA/Mistral
            )
        else:
            # Default to cl100k_base as fallback
            encoding_name = "cl100k_base"

        # Try to get the specific encoder for the model if available
        try:
            encoding = tiktoken.encoding_for_model(model)
        except KeyError:
            # Fall back to the encoding name
            encoding = tiktoken.get_encoding(encoding_name)

        # Count tokens
        token_integers = encoding.encode(text)
        return len(token_integers)

    except ImportError:
        # Fallback to character-based estimation if tiktoken is not installed
        return text_count_tokens_estimate(text, model)


@traced_and_logged
def text_count_tokens_estimate(text: str, model: str = "gpt-3.5-turbo") -> int:
    """Estimate token count for different models."""
    if not text:
        return 0

    # Rough token estimations for different models
    if model.startswith(("gpt-3", "gpt-4")):
        # OpenAI models: ~4 chars per token
        return len(text) // 4 + 1
    elif model.startswith("claude"):
        # Anthropic models: ~3.5 chars per token
        return len(text) // 3.5 + 1
    elif "llama" in model.lower():
        # LLaMA-based models: ~3.7 chars per token
        return len(text) // 3.7 + 1
    else:
        # Default estimation
        return len(text) // 4 + 1


@traced_and_logged
def text_truncate_to_token_limit(
    text: str, max_tokens: int = 4000, model: str = "gpt-3.5-turbo"
) -> str:
    if not text:
        return ""

    # Try to use tiktoken for accurate truncation
    try:
        import tiktoken

        # Get appropriate encoding
        try:
            encoding = tiktoken.encoding_for_model(model)
        except KeyError:
            # Fall back to cl100k_base (used by most newer models)
            encoding = tiktoken.get_encoding("cl100k_base")

        # Encode the text to tokens
        tokens = encoding.encode(text)

        # If we're already under the limit, return the original text
        if len(tokens) <= max_tokens:
            return text

        # Truncate tokens and decode back to text
        truncated_tokens = tokens[:max_tokens]
        return encoding.decode(truncated_tokens)

    except ImportError:
        # Fallback to the character-based method if tiktoken is not available
        estimated_tokens = text_count_tokens_estimate(text, model)

        if estimated_tokens <= max_tokens:
            return text

        # Calculate approximate character limit
        char_per_token = 4  # Default for most models
        if model.startswith("claude"):
            char_per_token = 3.5
        elif "llama" in model.lower():
            char_per_token = 3.7

        char_limit = int(max_tokens * char_per_token)

        # Try to find a good breaking point
        if char_limit < len(text):
            # Look for sentence or paragraph break near the limit
            for i in range(char_limit - 1, max(0, char_limit - 100), -1):
                if i < len(text) and text[i] in [".", "!", "?", "\n"]:
                    return text[: i + 1]

        # Fallback to hard truncation
        return text[:char_limit]


@traced_and_logged
def text_extract_keywords(text: str, top_n: int = 10) -> list[str]:
    if not text:
        return []

    # Get stopwords
    try:
        from nltk.corpus import stopwords

        stop_words = set(stopwords.words("english"))
    except:
        # Fallback basic stopwords if NLTK data isn't available
        stop_words = {
            "i",
            "me",
            "my",
            "myself",
            "we",
            "our",
            "ours",
            "ourselves",
            "you",
            "you're",
            "you've",
            "you'll",
            "you'd",
            "your",
            "yours",
            "yourself",
            "yourselves",
            "he",
            "him",
            "his",
            "himself",
            "she",
            "she's",
            "her",
            "hers",
            "herself",
            "it",
            "it's",
            "its",
            "itself",
            "they",
            "them",
            "their",
            "theirs",
            "themselves",
            "what",
            "which",
            "who",
            "whom",
            "this",
            "that",
            "that'll",
            "these",
            "those",
            "am",
            "is",
            "are",
            "was",
            "were",
            "be",
            "been",
            "being",
            "have",
            "has",
            "had",
            "having",
            "do",
            "does",
            "did",
            "doing",
            "a",
            "an",
            "the",
            "and",
            "but",
            "if",
            "or",
            "because",
            "as",
            "until",
            "while",
            "of",
            "at",
            "by",
            "for",
            "with",
            "about",
            "against",
            "between",
            "into",
            "through",
            "during",
            "before",
            "after",
            "above",
            "below",
            "to",
            "from",
            "up",
            "down",
            "in",
            "out",
            "on",
            "off",
            "over",
            "under",
            "again",
            "further",
            "then",
            "once",
        }

    # Tokenize and remove punctuation
    words = re.findall(r"\b[a-zA-Z]{3,}\b", text.lower())

    # Remove stopwords
    words = [word for word in words if word not in stop_words]

    # Count word frequencies
    word_freq = {}
    for word in words:
        if word in word_freq:
            word_freq[word] += 1
        else:
            word_freq[word] = 1

    # Sort by frequency
    sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)

    # Return top N keywords
    return [word for word, freq in sorted_words[:top_n]]


@traced_and_logged
def text_clean_text(
    text: str,
    remove_urls: bool = True,
    remove_html: bool = True,
    normalize_whitespace: bool = True,
) -> str:
    if not text:
        return ""

    result = text

    # Remove URLs
    if remove_urls:
        result = re.sub(r"https?://\S+|www\.\S+", "", result)

    # Remove HTML tags
    if remove_html:
        result = re.sub(r"<.*?>", "", result)

    # Normalize whitespace
    if normalize_whitespace:
        # Replace multiple spaces, tabs, newlines with a single space
        result = re.sub(r"\s+", " ", result)
        result = result.strip()

    return result


@traced_and_logged
def text_format_chat_history(
    messages: list[dict[str, str]],
    format_type: str = "text",
    system_prefix: str = "System: ",
    user_prefix: str = "User: ",
    assistant_prefix: str = "Assistant: ",
) -> str:
    if not messages:
        return ""

    result = []

    if format_type == "text":
        for msg in messages:
            role = msg.get("role", "").lower()
            content = msg.get("content", "")

            if role == "system":
                result.append(f"{system_prefix}{content}")
            elif role == "user":
                result.append(f"{user_prefix}{content}")
            elif role == "assistant":
                result.append(f"{assistant_prefix}{content}")
            else:
                result.append(f"{role.capitalize()}: {content}")

        return "\n\n".join(result)

    elif format_type == "markdown":
        for msg in messages:
            role = msg.get("role", "").lower()
            content = msg.get("content", "")

            if role == "system":
                result.append(f"**{system_prefix.strip()}** {content}")
            elif role == "user":
                result.append(f"**{user_prefix.strip()}** {content}")
            elif role == "assistant":
                result.append(f"**{assistant_prefix.strip()}** {content}")
            else:
                result.append(f"**{role.capitalize()}:** {content}")

        return "\n\n".join(result)

    else:
        raise ValueError(f"Unsupported format type: {format_type}")


@traced_and_logged
def text_extract_json_from_text(text: str) -> dict[str, Any] | None:
    if not text:
        return None

    # Find JSON-like patterns between curly braces
    json_pattern = re.compile(r"({[\s\S]*?})")
    json_matches = json_pattern.findall(text)

    # Try to parse each match
    for json_str in json_matches:
        try:
            return json.loads(json_str)
        except json.JSONDecodeError:
            continue

    # Try to find JSON with markdown code blocks
    code_block_pattern = re.compile(r"```(?:json)?\s*([\s\S]*?)\s*```")
    code_blocks = code_block_pattern.findall(text)

    for block in code_blocks:
        # Clean up any trailing ``` that might have been captured
        block = block.replace("```", "")
        try:
            return json.loads(block)
        except json.JSONDecodeError:
            continue

    # No valid JSON found
    return None


@traced_and_logged
def text_calculate_hash(text: str, algorithm: str = "sha256") -> str:
    if not text:
        return ""

    if algorithm == "md5":
        return hashlib.md5(text.encode()).hexdigest()
    elif algorithm == "sha1":
        return hashlib.sha1(text.encode()).hexdigest()
    elif algorithm == "sha256":
        return hashlib.sha256(text.encode()).hexdigest()
    else:
        raise ValueError(f"Unsupported hash algorithm: {algorithm}")


@traced_and_logged
def text_format_table_from_dicts(data: list[dict[str, Any]]) -> str:
    if not data:
        return ""

    # Extract all possible keys
    keys = set()
    for item in data:
        keys.update(item.keys())

    # Convert to list and sort for consistent output
    keys = sorted(list(keys))

    # Calculate column widths
    widths = {key: len(key) for key in keys}
    for item in data:
        for key in keys:
            if key in item:
                value_str = str(item[key])
                widths[key] = max(widths[key], len(value_str))

    # Create header
    header = " | ".join(f"{key:{widths[key]}}" for key in keys)
    separator = "-+-".join("-" * widths[key] for key in keys)

    # Create rows
    rows = []
    for item in data:
        row = " | ".join(f"{item.get(key, '')!s:{widths[key]}}" for key in keys)
        rows.append(row)

    # Combine everything
    return f"{header}\n{separator}\n" + "\n".join(rows)


@traced_and_logged
def text_detect_language(text: str) -> str:
    """Simple language detection"""
    if not text or len(text.strip()) < 10:
        return "unknown"

    try:
        # Try to use langdetect if available
        from langdetect import detect

        return detect(text)
    except ImportError:
        # Fallback to simple detection based on character frequency
        # This is very simplistic and only works for a few common languages
        text = text.lower()

        # Count character frequencies that may indicate certain languages
        special_chars = {
            "á": 0,
            "é": 0,
            "í": 0,
            "ó": 0,
            "ú": 0,
            "ü": 0,
            "ñ": 0,  # Spanish
            "ä": 0,
            "ö": 0,
            "ß": 0,  # German
            "ç": 0,
            "à": 0,
            "è": 0,
            "ù": 0,  # French
            "å": 0,
            "ø": 0,  # Nordic
            "й": 0,
            "ы": 0,
            "ъ": 0,
            "э": 0,  # Russian/Cyrillic
            "的": 0,
            "是": 0,
            "在": 0,  # Chinese
            "の": 0,
            "は": 0,
            "で": 0,  # Japanese
            "한": 0,
            "국": 0,
            "어": 0,  # Korean
        }

        for char in text:
            if char in special_chars:
                special_chars[char] += 1

        # Detect based on character frequencies
        spanish = sum(
            special_chars[c] for c in ["á", "é", "í", "ó", "ú", "ü", "ñ"]
        )
        german = sum(special_chars[c] for c in ["ä", "ö", "ß"])
        french = sum(special_chars[c] for c in ["ç", "à", "è", "ù"])
        nordic = sum(special_chars[c] for c in ["å", "ø"])
        russian = sum(special_chars[c] for c in ["й", "ы", "ъ", "э"])
        chinese = sum(special_chars[c] for c in ["的", "是", "在"])
        japanese = sum(special_chars[c] for c in ["の", "は", "で"])
        korean = sum(special_chars[c] for c in ["한", "국", "어"])

        scores = {
            "es": spanish,
            "de": german,
            "fr": french,
            "no": nordic,
            "ru": russian,
            "zh": chinese,
            "ja": japanese,
            "ko": korean,
        }

        # If we have a clear signal from special characters
        max_score = max(scores.values())
        if max_score > 0:
            return max(scores, key=scores.get)

        # Otherwise assume English (very simplistic)
        return "en"


@traced_and_logged
def text_tiktoken_split(
    text: str,
    model: str = "gpt-3.5-turbo",
    chunk_size: int = 1000,
    overlap: int = 50,
) -> list[str]:
    """Split text based on tiktoken tokens with proper overlap handling."""
    if not text:
        return []

    try:
        import tiktoken

        try:
            encoding = tiktoken.encoding_for_model(model)
        except KeyError:
            encoding = tiktoken.get_encoding("cl100k_base")

        # Encode the text to tokens
        tokens = encoding.encode(text)
        total_tokens = len(tokens)

        # Check if we need to split at all
        if total_tokens <= chunk_size:
            return [text]

        # Create chunks with overlap
        chunks = []
        start_idx = 0

        while start_idx < total_tokens:
            # Define the end of this chunk
            end_idx = min(start_idx + chunk_size, total_tokens)

            # Decode this chunk of tokens back to text
            chunk_tokens = tokens[start_idx:end_idx]
            chunk_text = encoding.decode(chunk_tokens)
            chunks.append(chunk_text)

            # Move to the next chunk, accounting for overlap
            start_idx += chunk_size - overlap

            # Avoid tiny final chunks
            if start_idx < total_tokens and start_idx + overlap >= total_tokens:
                break

        return chunks
    except ImportError:
        # Fallback to character-based chunking if tiktoken is not available
        return text_split_by_characters(
            text, chunk_size=chunk_size * 4, overlap=overlap * 4
        )


@traced_and_logged
def text_count_words(text: str) -> int:
    if not text:
        return 0
    return len(text.split())


@traced_and_logged
def text_extract_urls(text: str) -> list[str]:
    if not text:
        return []
    # A more robust regex might be needed for complex cases
    return re.findall(r"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+", text)


@traced_and_logged
def text_extract_numbers(text: str) -> list[float]:
    if not text:
        return []
    return [float(num) for num in re.findall(r"[-+]?\d*\.?\d+", text)]
```

### src\flock\tools\web_tools.py

- **Lines**: 90
- **Last modified**: 2025-05-21 19:51:15

```py

import importlib
import os
from typing import Literal

from flock.core.logging.trace_and_logged import traced_and_logged


@traced_and_logged
def web_search_tavily(query: str):
    if importlib.util.find_spec("tavily") is not None:
        from tavily import TavilyClient

        client = TavilyClient(api_key=os.getenv("TAVILY_API_KEY"))
        try:
            response = client.search(query, include_answer=True)  # type: ignore
            return response
        except Exception:
            raise
    else:
        raise ImportError(
            "Optional tool dependencies not installed. Install with 'pip install flock-core[tools]'."
        )


@traced_and_logged
def web_search_duckduckgo(
    keywords: str, search_type: Literal["news", "web"] = "web"
):
    try:
        if importlib.util.find_spec("duckduckgo_search") is not None:
            from duckduckgo_search import DDGS

            if search_type == "news":
                response = DDGS().news(keywords)
            else:
                response = DDGS().text(keywords)

                return response
        else:
            raise ImportError(
                "Optional tool dependencies not installed. Install with 'pip install flock-core[tools]'."
            )
    except Exception:
        raise


@traced_and_logged
def web_search_bing(keywords: str):
    try:
        import httpx

        subscription_key = os.environ["BING_SEARCH_V7_SUBSCRIPTION_KEY"]
        endpoint = "https://api.bing.microsoft.com/v7.0/search"

        # Query term(s) to search for.
        query = keywords

        # Construct a request
        mkt = "en-US"
        params = {"q": query, "mkt": mkt}
        headers = {"Ocp-Apim-Subscription-Key": subscription_key}

        response = httpx.get(endpoint, headers=headers, params=params)
        response.raise_for_status()
        search_results = response.json()
        return search_results["webPages"]
    except Exception:
        raise

@traced_and_logged
def web_content_as_markdown(url: str) -> str:
    if (
        importlib.util.find_spec("httpx") is not None
        and importlib.util.find_spec("markdownify") is not None
    ):
        import httpx
        from markdownify import markdownify as md

        try:
            response = httpx.get(url)
            response.raise_for_status()
            markdown = md(response.text)
            return markdown
        except Exception:
            raise
    else:
        raise ImportError(
            "Optional tool dependencies not installed. Install with 'pip install flock-core[tools]'."
        )
```

### src\flock\tools\zendesk_tools.py

- **Lines**: 147
- **Last modified**: 2025-05-24 17:10:34

```py
"""Tools for interacting with Zendesk."""

import os

import httpx
from mcp.server.fastmcp import FastMCP

mcp = FastMCP("ZendeskTools")


ZENDESK_BEARER_TOKEN = os.getenv("ZENDESK_BEARER_TOKEN")

HEADERS = {
    "Authorization": f"Bearer {ZENDESK_BEARER_TOKEN}",
    "Accept": "application/json",
}


@mcp.tool()
def zendesk_get_tickets(number_of_tickets: int = 10) -> list[dict]:
    """Get all tickets."""
    ZENDESK_SUBDOMAIN = os.getenv("ZENDESK_SUBDOMAIN_TICKET")
    BASE_URL = f"https://{ZENDESK_SUBDOMAIN}.zendesk.com"
    url = f"{BASE_URL}/api/v2/tickets.json"
    all_tickets = []
    with httpx.Client(headers=HEADERS, timeout=30.0) as client:
        while url and len(all_tickets) < number_of_tickets:
            response = client.get(url)
            response.raise_for_status()

            data = response.json()
            tickets = data.get("tickets", [])
            all_tickets.extend(tickets)

            url = data.get("next_page")
    return all_tickets

@mcp.tool()
def zendesk_get_ticket_by_id(ticket_id: str) -> dict:
    """Get a ticket by ID."""
    ZENDESK_SUBDOMAIN = os.getenv("ZENDESK_SUBDOMAIN_TICKET")
    BASE_URL = f"https://{ZENDESK_SUBDOMAIN}.zendesk.com"
    url = f"{BASE_URL}/api/v2/tickets/{ticket_id}"
    with httpx.Client(headers=HEADERS, timeout=30.0) as client:
        response = client.get(url)
        response.raise_for_status()
        return response.json()["ticket"]

@mcp.tool()
def zendesk_get_comments_by_ticket_id(ticket_id: str) -> list[dict]:
    """Get all comments for a ticket."""
    ZENDESK_SUBDOMAIN = os.getenv("ZENDESK_SUBDOMAIN_TICKET")
    BASE_URL = f"https://{ZENDESK_SUBDOMAIN}.zendesk.com"
    url = f"{BASE_URL}/api/v2/tickets/{ticket_id}/comments"
    with httpx.Client(headers=HEADERS, timeout=30.0) as client:
        response = client.get(url)
        response.raise_for_status()
        return response.json()["comments"]

@mcp.tool()
def zendesk_get_article_by_id(article_id: str) -> dict:
    """Get an article by ID."""
    ZENDESK_LOCALE = os.getenv("ZENDESK_ARTICLE_LOCALE")
    ZENDESK_SUBDOMAIN = os.getenv("ZENDESK_SUBDOMAIN_ARTICLE")
    BASE_URL = f"https://{ZENDESK_SUBDOMAIN}.zendesk.com"
    url = (
        f"{BASE_URL}/api/v2/help_center/{ZENDESK_LOCALE}/articles/{article_id}"
    )
    with httpx.Client(headers=HEADERS, timeout=30.0) as client:
        response = client.get(url)
        response.raise_for_status()
        return response.json()["article"]

@mcp.tool()
def zendesk_get_articles() -> list[dict]:
    """Get all articles."""
    ZENDESK_LOCALE = os.getenv("ZENDESK_ARTICLE_LOCALE")
    ZENDESK_SUBDOMAIN = os.getenv("ZENDESK_SUBDOMAIN_ARTICLE")
    BASE_URL = f"https://{ZENDESK_SUBDOMAIN}.zendesk.com"
    url = f"{BASE_URL}/api/v2/help_center/{ZENDESK_LOCALE}/articles.json"
    with httpx.Client(headers=HEADERS, timeout=30.0) as client:
        response = client.get(url)
        response.raise_for_status()
        return response.json()["articles"]

@mcp.tool()   
def zendesk_get_articles_count() -> int:
    """
    Count every Help-Center article in the configured locale.

    Uses cursor pagination (page[size]=100) because it’s faster and
    has no 10 000-record ceiling. Falls back to offset pagination
    if the account hasn’t been migrated yet.
    """
    ZENDESK_LOCALE     = os.getenv("ZENDESK_ARTICLE_LOCALE")  # e.g. "en-us"
    ZENDESK_SUBDOMAIN  = os.getenv("ZENDESK_SUBDOMAIN_ARTICLE")
    BASE_URL           = f"https://{ZENDESK_SUBDOMAIN}.zendesk.com"
    url                = (
        f"{BASE_URL}/api/v2/help_center/{ZENDESK_LOCALE}/articles.json"
        "?page[size]=100"            # max page size for HC APIs
    )

    total = 0
    with httpx.Client(headers=HEADERS, timeout=30.0) as client:
        while url:
            resp = client.get(url)
            resp.raise_for_status()
            data = resp.json()

            total += len(data.get("articles", []))
            print(f"Locale: {ZENDESK_LOCALE}")
            print(f"Number of articles: {total}")

            # Cursor pagination (preferred)
            if data.get("meta", {}).get("has_more"):
                url = data.get("links", {}).get("next")
                continue

            # Offset pagination fallback
            url = data.get("next_page")

    return total

@mcp.tool()
def zendesk_search_articles(query: str) -> list[dict]:
    """Search Zendesk Help Center articles using a query string."""
    ZENDESK_LOCALE = os.getenv("ZENDESK_ARTICLE_LOCALE")  # e.g., "en-us"
    ZENDESK_SUBDOMAIN = os.getenv("ZENDESK_SUBDOMAIN_ARTICLE")
    BASE_URL = f"https://{ZENDESK_SUBDOMAIN}.zendesk.com"
    url = f"{BASE_URL}/api/v2/help_center/articles/search.json"

    params = {
        "query": query,
        "locale": ZENDESK_LOCALE,
        "sort_by": "updated_at",
        "sort_order": "desc",
    }

    with httpx.Client(headers=HEADERS, timeout=30.0) as client:
        response = client.get(url, params=params)
        response.raise_for_status()
        return response.json().get("results", [])


if __name__ == "__main__":
    transport = os.getenv("ZENDESK_MCP_TRANSPORT", "stdio")
    mcp.run(transport=transport)
```

### src\flock\webapp\__init__.py

- **Lines**: 1
- **Last modified**: 2025-05-09 01:11:35

```py
"""Flock Web Application Package."""
```

### src\flock\webapp\app\__init__.py

- **Lines**: 0
- **Last modified**: 2025-05-09 01:11:35

```py

```

### src\flock\webapp\app\api\__init__.py

- **Lines**: 0
- **Last modified**: 2025-05-09 01:11:35

```py

```

### src\flock\webapp\app\api\agent_management.py

- **Lines**: 241
- **Last modified**: 2025-05-21 19:51:15

```py
# src/flock/webapp/app/api/agent_management.py
import json
from pathlib import Path
from typing import TYPE_CHECKING

from fastapi import (  # Added Depends and HTTPException
    APIRouter,
    Depends,
    Form,
    Request,
)
from fastapi.responses import HTMLResponse
from fastapi.templating import Jinja2Templates

if TYPE_CHECKING:
    from flock.core.flock import Flock

# Import the dependency to get the current Flock instance
from flock.webapp.app.dependencies import (
    get_flock_instance,
    get_optional_flock_instance,
)

# Import service functions that now take app_state (or directly the Flock instance)
from flock.webapp.app.services.flock_service import (
    add_agent_to_current_flock_service,
    get_registered_items_service,  # This is fine as it doesn't depend on current_flock
    remove_agent_from_current_flock_service,
    update_agent_in_current_flock_service,
)

router = APIRouter()
BASE_DIR = Path(__file__).resolve().parent.parent.parent # Points to flock-ui/
templates = Jinja2Templates(directory=str(BASE_DIR / "templates"))


@router.get("/htmx/agent-list", response_class=HTMLResponse)
async def htmx_get_agent_list(
    request: Request,
    message: str = None,
    success: bool = None,
    # Use Depends to get the current flock instance.
    # Use get_optional_flock_instance if the route should work even without a flock loaded.
    current_flock: "Flock | None" = Depends(get_optional_flock_instance)
):
    if not current_flock:
        # If used in a context where flock might not be loaded (e.g. main agent manager view before load)
        # it should be handled by the page template or a higher-level redirect.
        # For a partial, returning an error or empty state is reasonable.
        return HTMLResponse("<div id='agent-list-container'><p class='error'>No Flock loaded to display agents.</p></div>", headers={"HX-Retarget": "#agent-list-container", "HX-Reswap": "innerHTML"})

    return templates.TemplateResponse(
        "partials/_agent_list.html",
        {
            "request": request,
            "flock": current_flock, # Pass the injected flock instance
            "message": message,
            "success": success,
        },
    )


@router.get("/htmx/agents/{agent_name}/details-form", response_class=HTMLResponse)
async def htmx_get_agent_details_form(
    request: Request,
    agent_name: str,
    current_flock: "Flock" = Depends(get_flock_instance) # Expect flock to be loaded
):
    # flock instance is now injected by FastAPI
    agent = current_flock.agents.get(agent_name)
    if not agent:
        return HTMLResponse(
            f"<p class='error'>Agent '{agent_name}' not found in the current Flock.</p>"
        )

    registered_tools = get_registered_items_service("tool")
    current_agent_tools = (
        [tool.__name__ for tool in agent.tools] if agent.tools else []
    )

    return templates.TemplateResponse(
        "partials/_agent_detail_form.html",
        {
            "request": request,
            "agent": agent,
            "is_new": False,
            "registered_tools": registered_tools,
            "current_tools": current_agent_tools,
        },
    )


@router.get("/htmx/agents/new-agent-form", response_class=HTMLResponse)
async def htmx_get_new_agent_form(
    request: Request,
    current_flock: "Flock" = Depends(get_flock_instance) # Expect flock for context, even for new agent
):
    # current_flock is injected, primarily to ensure context if needed by template/tools list
    registered_tools = get_registered_items_service("tool")
    return templates.TemplateResponse(
        "partials/_agent_detail_form.html",
        {
            "request": request,
            "agent": None,
            "is_new": True,
            "registered_tools": registered_tools,
            "current_tools": [],
        },
    )


@router.post("/htmx/agents", response_class=HTMLResponse)
async def htmx_create_agent(
    request: Request,
    agent_name: str = Form(...),
    agent_description: str = Form(""),
    agent_model: str = Form(None),
    input_signature: str = Form(...),
    output_signature: str = Form(...),
    tools: list[str] = Form([]),
    # current_flock: Flock = Depends(get_flock_instance) # Service will use app_state
):
    # The service function add_agent_to_current_flock_service now takes app_state
    if (not agent_name.strip() or not input_signature.strip() or not output_signature.strip()):
        registered_tools = get_registered_items_service("tool")
        return templates.TemplateResponse(
            "partials/_agent_detail_form.html",
            {
                "request": request, "agent": None, "is_new": True,
                "error_message": "Name, Input Signature, and Output Signature are required.",
                "registered_tools": registered_tools, "current_tools": tools,
            })

    agent_config = {
        "name": agent_name, "description": agent_description,
        "model": agent_model if agent_model and agent_model.strip() else None,
        "input": input_signature, "output": output_signature, "tools_names": tools,
    }
    # Pass request.app.state to the service function
    success = add_agent_to_current_flock_service(agent_config, request.app.state)

    response_headers = {}
    if success:
        response_headers["HX-Trigger"] = json.dumps({"agentListChanged": None, "notify": {"type":"success", "message": f"Agent '{agent_name}' created."}})


    # Re-render the form or an empty state for the detail panel
    # The agent list itself will be refreshed by the agentListChanged trigger
    new_form_context = {
        "request": request, "agent": None, "is_new": True,
        "registered_tools": get_registered_items_service("tool"),
        "current_tools": [], # Reset tools for new form
        "form_message": "Agent created successfully!" if success else "Failed to create agent. Check logs.",
        "success": success,
    }
    return templates.TemplateResponse("partials/_agent_detail_form.html", new_form_context, headers=response_headers)


@router.put("/htmx/agents/{original_agent_name}", response_class=HTMLResponse)
async def htmx_update_agent(
    request: Request,
    original_agent_name: str,
    agent_name: str = Form(...),
    agent_description: str = Form(""),
    agent_model: str = Form(None),
    input_signature: str = Form(...),
    output_signature: str = Form(...),
    tools: list[str] = Form([]),
    # current_flock: Flock = Depends(get_flock_instance) # Service will use app_state
):
    agent_config = {
        "name": agent_name, "description": agent_description,
        "model": agent_model if agent_model and agent_model.strip() else None,
        "input": input_signature, "output": output_signature, "tools_names": tools,
    }
    # Pass request.app.state
    success = update_agent_in_current_flock_service(original_agent_name, agent_config, request.app.state)

    response_headers = {}
    if success:
        response_headers["HX-Trigger"] = json.dumps({"agentListChanged": None, "notify": {"type":"success", "message": f"Agent '{agent_name}' updated."}})


    # After update, get the (potentially renamed) agent from app.state's flock
    updated_agent_instance: Flock | None = getattr(request.app.state, 'flock_instance', None)
    updated_agent = updated_agent_instance.agents.get(agent_name) if updated_agent_instance else None

    registered_tools = get_registered_items_service("tool")
    current_agent_tools = []
    if updated_agent and updated_agent.tools:
        current_agent_tools = [tool.__name__ for tool in updated_agent.tools]

    updated_form_context = {
        "request": request, "agent": updated_agent, "is_new": False,
        "form_message": "Agent updated successfully!" if success else "Failed to update agent. Check logs.",
        "success": success,
        "registered_tools": registered_tools, "current_tools": current_agent_tools,
    }
    return templates.TemplateResponse("partials/_agent_detail_form.html", updated_form_context, headers=response_headers)


@router.delete("/htmx/agents/{agent_name}", response_class=HTMLResponse)
async def htmx_delete_agent(
    request: Request,
    agent_name: str,
    # current_flock: Flock = Depends(get_flock_instance) # Service will use app_state
):
    # Pass request.app.state
    success = remove_agent_from_current_flock_service(agent_name, request.app.state)
    response_headers = {}

    if success:
        response_headers["HX-Trigger"] = json.dumps({"agentListChanged": None, "notify": {"type":"info", "message": f"Agent '{agent_name}' removed."}})
        # Return an empty agent detail form to clear that panel
        empty_form_context = {
            "request": request, "agent": None, "is_new": True,
            "registered_tools": get_registered_items_service("tool"),
            "current_tools": [],
            # "form_message": f"Agent '{agent_name}' removed.", # Message handled by notify
            # "success": True, # Not strictly needed if form is cleared
        }
        return templates.TemplateResponse("partials/_agent_detail_form.html", empty_form_context, headers=response_headers)
    else:
        # Deletion failed, re-render the form for the agent that failed to delete (if it still exists)
        flock_instance_from_state: Flock | None = getattr(request.app.state, 'flock_instance', None)
        agent_still_exists = flock_instance_from_state.agents.get(agent_name) if flock_instance_from_state else None

        registered_tools = get_registered_items_service("tool")
        current_tools = []
        if agent_still_exists and agent_still_exists.tools:
            current_tools = [tool.__name__ for tool in agent_still_exists.tools]

        error_form_context = {
            "request": request, "agent": agent_still_exists, "is_new": False,
            "form_message": f"Failed to remove agent '{agent_name}'. It might have already been removed or an error occurred.",
            "success": False,
            "registered_tools": registered_tools, "current_tools": current_tools,
        }
        # Trigger a notification for the error as well
        response_headers["HX-Trigger"] = json.dumps({"notify": {"type":"error", "message": f"Failed to remove agent '{agent_name}'."}})
        return templates.TemplateResponse("partials/_agent_detail_form.html", error_form_context, headers=response_headers)
```

### src\flock\webapp\app\api\execution.py

- **Lines**: 299
- **Last modified**: 2025-05-25 23:17:06

```py
# src/flock/webapp/app/api/execution.py
import json
from pathlib import Path
from typing import TYPE_CHECKING, Any

import markdown2  # Import markdown2
from fastapi import (  # Ensure Form and HTTPException are imported
    APIRouter,
    Depends,
    Form,
    Request,
)
from fastapi.responses import HTMLResponse
from fastapi.templating import Jinja2Templates

if TYPE_CHECKING:
    from flock.core.flock import Flock


from flock.core.logging.logging import (
    get_logger as get_flock_logger,  # For logging within the new endpoint
)
from flock.core.util.spliter import parse_schema

# Import the dependency to get the current Flock instance
from flock.webapp.app.dependencies import (
    get_flock_instance,
    get_optional_flock_instance,
    get_shared_link_store,
)

# Service function now takes app_state
from flock.webapp.app.services.flock_service import (
    run_current_flock_service,
    # get_current_flock_instance IS NO LONGER IMPORTED
)
from flock.webapp.app.services.sharing_store import SharedLinkStoreInterface

router = APIRouter()
BASE_DIR = Path(__file__).resolve().parent.parent.parent
templates = Jinja2Templates(directory=str(BASE_DIR / "templates"))

# Add markdown2 filter to Jinja2 environment for this router
def markdown_filter(text):
    return markdown2.markdown(text, extras=["tables", "fenced-code-blocks"])

templates.env.filters['markdown'] = markdown_filter


@router.get("/htmx/execution-form-content", response_class=HTMLResponse)
async def htmx_get_execution_form_content(
    request: Request,
    current_flock: "Flock | None" = Depends(get_optional_flock_instance) # Use optional if form can show 'no flock'
):
    # flock instance is injected
    return templates.TemplateResponse(
        "partials/_execution_form.html",
        {
            "request": request,
            "flock": current_flock, # Pass the injected flock instance
            "input_fields": [],
            "selected_agent_name": None, # Form starts with no agent selected
        },
    )


@router.get("/htmx/agents/{agent_name}/input-form", response_class=HTMLResponse)
async def htmx_get_agent_input_form(
    request: Request,
    agent_name: str,
    current_flock: "Flock" = Depends(get_flock_instance) # Expect flock to be loaded
):
    # flock instance is injected
    agent = current_flock.agents.get(agent_name)
    if not agent:
        return HTMLResponse(
            f"<p class='error'>Agent '{agent_name}' not found in the current Flock.</p>"
        )

    input_fields = []
    if agent.input and isinstance(agent.input, str):
        try:
            parsed_spec = parse_schema(agent.input)
            for name, type_str, description in parsed_spec:
                field_info = {
                    "name": name,
                    "type": type_str.lower(),
                    "description": description or "",
                }
                if "bool" in field_info["type"]: field_info["html_type"] = "checkbox"
                elif "int" in field_info["type"] or "float" in field_info["type"]: field_info["html_type"] = "number"
                elif "list" in field_info["type"] or "dict" in field_info["type"]:
                    field_info["html_type"] = "textarea"
                    field_info["placeholder"] = f"Enter JSON for {field_info['type']}"
                else: field_info["html_type"] = "text"
                input_fields.append(field_info)
        except Exception as e:
            return HTMLResponse(
                f"<p class='error'>Error parsing input signature for {agent_name}: {e}</p>"
            )
    return templates.TemplateResponse(
        "partials/_dynamic_input_form_content.html",
        {"request": request, "input_fields": input_fields},
    )


@router.post("/htmx/run", response_class=HTMLResponse)
async def htmx_run_flock(
    request: Request,
):
    current_flock_from_state: Flock | None = getattr(request.app.state, 'flock_instance', None)
    logger = get_flock_logger("webapp.execution.regular_run")

    if not current_flock_from_state:
        logger.error("HTMX Run (Regular): No Flock loaded in app_state.")
        return HTMLResponse("<p class='error'>No Flock loaded to run.</p>")

    form_data = await request.form()
    start_agent_name = form_data.get("start_agent_name")

    if not start_agent_name:
        logger.warning("HTMX Run (Regular): Starting agent not selected.")
        return HTMLResponse("<p class='error'>Starting agent not selected.</p>")

    agent = current_flock_from_state.agents.get(start_agent_name)
    if not agent:
        logger.error(f"HTMX Run (Regular): Agent '{start_agent_name}' not found in Flock '{current_flock_from_state.name}'.")
        return HTMLResponse(
            f"<p class='error'>Agent '{start_agent_name}' not found in the current Flock.</p>"
        )

    inputs = {}
    if agent.input and isinstance(agent.input, str):
        try:
            parsed_spec = parse_schema(agent.input)
            for name, type_str, _ in parsed_spec:
                form_field_name = f"agent_input_{name}"
                raw_value = form_data.get(form_field_name)
                if raw_value is None and "bool" in type_str.lower(): inputs[name] = False; continue
                if raw_value is None: inputs[name] = None; continue
                if "int" in type_str.lower(): inputs[name] = int(raw_value)
                elif "float" in type_str.lower(): inputs[name] = float(raw_value)
                elif "bool" in type_str.lower(): inputs[name] = raw_value.lower() in ["true", "on", "1", "yes"]
                elif "list" in type_str.lower() or "dict" in type_str.lower(): inputs[name] = json.loads(raw_value)
                else: inputs[name] = raw_value
        except ValueError as ve:
            logger.error(f"HTMX Run (Regular): Input parsing error for agent '{start_agent_name}': {ve}", exc_info=True)
            return HTMLResponse(f"<p class='error'>Invalid input format: {ve!s}</p>")
        except Exception as e_parse:
            logger.error(f"HTMX Run (Regular): Error processing inputs for '{start_agent_name}': {e_parse}", exc_info=True)
            return HTMLResponse(f"<p class='error'>Error processing inputs for {start_agent_name}: {e_parse}</p>")

    result_data = await run_current_flock_service(start_agent_name, inputs, request.app.state)
    raw_json_for_template = json.dumps(result_data, indent=2)
    # Unescape newlines for proper display in HTML <pre> tag
    result_data_raw_json_str = raw_json_for_template.replace('\\n', '\n')

    return templates.TemplateResponse(
        "partials/_results_display.html",
        {
            "request": request,
            "result": result_data,
            "result_raw_json": result_data_raw_json_str,
            "feedback_endpoint": "/ui/api/flock/htmx/feedback",
            "share_id": None,
            "flock_name": current_flock_from_state.name,
            "agent_name": start_agent_name,
            "flock_definition": current_flock_from_state.to_yaml(),
        }
    )


# --- NEW ENDPOINT FOR SHARED RUNS ---
@router.post("/htmx/run-shared", response_class=HTMLResponse)
async def htmx_run_shared_flock(
    request: Request,
    share_id: str = Form(...),
):
    shared_logger = get_flock_logger("webapp.execution.shared_run_stateful")
    form_data = await request.form()
    start_agent_name = form_data.get("start_agent_name")

    if not start_agent_name:
        shared_logger.warning("HTMX Run Shared: Starting agent not selected.")
        return HTMLResponse("<p class='error'>Starting agent not selected for shared run.</p>")

    inputs: dict[str, Any] = {}
    try:
        shared_flocks_store = getattr(request.app.state, 'shared_flocks', {})
        temp_flock = shared_flocks_store.get(share_id)

        if not temp_flock:
            shared_logger.error(f"HTMX Run Shared: Flock instance for share_id '{share_id}' not found in app.state.")
            return HTMLResponse(f"<p class='error'>Shared session not found or expired. Please try accessing the shared link again.</p>")

        shared_logger.info(f"HTMX Run Shared: Successfully retrieved pre-loaded Flock '{temp_flock.name}' for agent '{start_agent_name}' (share_id: {share_id}).")

        agent = temp_flock.agents.get(start_agent_name)
        if not agent:
            shared_logger.error(f"HTMX Run Shared: Agent '{start_agent_name}' not found in shared Flock '{temp_flock.name}'.")
            return HTMLResponse(f"<p class='error'>Agent '{start_agent_name}' not found in the provided shared Flock definition.</p>")

        if agent.input and isinstance(agent.input, str):
            parsed_spec = parse_schema(agent.input)
            for name, type_str, _ in parsed_spec:
                form_field_name = f"agent_input_{name}"
                raw_value = form_data.get(form_field_name)
                if raw_value is None and "bool" in type_str.lower(): inputs[name] = False; continue
                if raw_value is None: inputs[name] = None; continue
                if "int" in type_str.lower(): inputs[name] = int(raw_value)
                elif "float" in type_str.lower(): inputs[name] = float(raw_value)
                elif "bool" in type_str.lower(): inputs[name] = raw_value.lower() in ["true", "on", "1", "yes"]
                elif "list" in type_str.lower() or "dict" in type_str.lower(): inputs[name] = json.loads(raw_value)
                else: inputs[name] = raw_value

        shared_logger.info(f"HTMX Run Shared: Executing agent '{start_agent_name}' in pre-loaded Flock '{temp_flock.name}'. Inputs: {list(inputs.keys())}")
        result_data = await temp_flock.run_async(start_agent=start_agent_name, input=inputs, box_result=False)
        raw_json_for_template = json.dumps(result_data, indent=2)
        # Unescape newlines for proper display in HTML <pre> tag
        result_data_raw_json_str = raw_json_for_template.replace('\\n', '\n')
        shared_logger.info(f"HTMX Run Shared: Agent '{start_agent_name}' executed. Result keys: {list(result_data.keys()) if isinstance(result_data, dict) else 'N/A'}")

    except ValueError as ve:
        shared_logger.error(f"HTMX Run Shared: Input parsing error for '{start_agent_name}' (share_id: {share_id}): {ve}", exc_info=True)
        return HTMLResponse(f"<p class='error'>Invalid input format: {ve!s}</p>")
    except Exception as e:
        shared_logger.error(f"HTMX Run Shared: Error during execution for '{start_agent_name}' (share_id: {share_id}): {e}", exc_info=True)
        return HTMLResponse(f"<p class='error'>An unexpected error occurred: {e!s}</p>")

    return templates.TemplateResponse(
        "partials/_results_display.html",
        {
            "request": request,
            "result": result_data,
            "result_raw_json": result_data_raw_json_str,
            "feedback_endpoint": "/ui/api/flock/htmx/feedback-shared",
            "share_id": share_id,
            "flock_name": temp_flock.name,
            "agent_name": start_agent_name,
            "flock_definition": temp_flock.to_yaml(),
        }
    )

# --- Feedback endpoints ---
@router.post("/htmx/feedback", response_class=HTMLResponse)
async def htmx_submit_feedback(
    request: Request,
    reason: str = Form(...),
    expected_response: str | None = Form(None),
    actual_response: str | None = Form(None),
    flock_name: str | None = Form(None),
    agent_name: str | None = Form(None),
    flock_definition: str | None = Form(None),
    store: SharedLinkStoreInterface = Depends(get_shared_link_store),
):
    from uuid import uuid4

    from flock.webapp.app.services.sharing_models import FeedbackRecord

    record = FeedbackRecord(
        feedback_id=uuid4().hex,
        share_id=None,
        context_type="agent_run",
        reason=reason,
        expected_response=expected_response,
        actual_response=actual_response,
        flock_name=flock_name,
        agent_name=agent_name,
        flock_definition=flock_definition,
    )
    await store.save_feedback(record)
    return HTMLResponse("<p>🙏 Feedback received – thank you!</p>")


@router.post("/htmx/feedback-shared", response_class=HTMLResponse)
async def htmx_submit_feedback_shared(
    request: Request,
    share_id: str = Form(...),
    reason: str = Form(...),
    expected_response: str | None = Form(None),
    actual_response: str | None = Form(None),
    flock_definition: str | None = Form(None),
    store: SharedLinkStoreInterface = Depends(get_shared_link_store),
):
    from uuid import uuid4

    from flock.webapp.app.services.sharing_models import FeedbackRecord

    record = FeedbackRecord(
        feedback_id=uuid4().hex,
        share_id=share_id,
        context_type="agent_run",
        reason=reason,
        expected_response=expected_response,
        actual_response=actual_response,
        flock_definition=flock_definition,
    )
    await store.save_feedback(record)
    return HTMLResponse("<p>🙏 Feedback received for shared run – thank you!</p>")
```

### src\flock\webapp\app\api\flock_management.py

- **Lines**: 129
- **Last modified**: 2025-05-21 19:51:15

```py
# src/flock/webapp/app/api/flock_management.py
from pathlib import Path
from typing import TYPE_CHECKING

from fastapi import APIRouter, Depends, Form, Request  # Added Depends
from fastapi.responses import HTMLResponse
from fastapi.templating import Jinja2Templates

if TYPE_CHECKING:
    from flock.core.flock import Flock

# Import the dependency to get the current Flock instance
from flock.webapp.app.dependencies import (
    get_flock_instance,
)

# Service functions now take app_state
from flock.webapp.app.services.flock_service import (
    save_current_flock_to_file_service,
    update_flock_properties_service,
    # get_current_flock_filename IS NO LONGER IMPORTED
    # get_current_flock_instance IS NO LONGER IMPORTED
)

router = APIRouter()
BASE_DIR = Path(__file__).resolve().parent.parent.parent
templates = Jinja2Templates(directory=str(BASE_DIR / "templates"))


@router.get("/htmx/flock-properties-form", response_class=HTMLResponse)
async def htmx_get_flock_properties_form(
    request: Request,
    update_message: str = None,
    success: bool = None,
    current_flock: "Flock" = Depends(get_flock_instance) # Expect flock to be loaded for this form
):
    # current_flock is now injected by FastAPI
    # Get the filename from app.state, as it's managed there
    current_filename: str | None = getattr(request.app.state, 'flock_filename', None)

    if not current_flock: # Should be caught by Depends if get_flock_instance raises error
        return HTMLResponse(
            "<div class='error'>Error: No flock loaded. Please load or create one first.</div>"
        )
    return templates.TemplateResponse(
        "partials/_flock_properties_form.html",
        {
            "request": request,
            "flock": current_flock,
            "current_filename": current_filename,
            "update_message": update_message,
            "success": success,
        },
    )


@router.post("/htmx/flock-properties", response_class=HTMLResponse)
async def htmx_update_flock_properties(
    request: Request,
    flock_name: str = Form(...),
    default_model: str = Form(...),
    description: str = Form(""),
    # current_flock: Flock = Depends(get_flock_instance) # Service will use app_state
):
    # Pass request.app.state to the service function
    success_update = update_flock_properties_service(
        flock_name, default_model, description, request.app.state
    )

    # Retrieve updated flock and filename from app.state for rendering the form
    updated_flock: Flock | None = getattr(request.app.state, 'flock_instance', None)
    updated_filename: str | None = getattr(request.app.state, 'flock_filename', None)

    return templates.TemplateResponse(
        "partials/_flock_properties_form.html",
        {
            "request": request,
            "flock": updated_flock,
            "current_filename": updated_filename,
            "update_message": "Flock properties updated!"
            if success_update
            else "Failed to update properties. Check logs.",
            "success": success_update,
        },
    )


@router.post("/htmx/save-flock", response_class=HTMLResponse)
async def htmx_save_flock(
    request: Request,
    save_filename: str = Form(...),
    # current_flock: Flock = Depends(get_flock_instance) # Service will use app_state
):
    current_flock_from_state: Flock | None = getattr(request.app.state, 'flock_instance', None)
    current_filename_from_state: str | None = getattr(request.app.state, 'flock_filename', None)

    if not save_filename.strip():
        return templates.TemplateResponse(
            "partials/_flock_properties_form.html",
            {
                "request": request,
                "flock": current_flock_from_state,
                "current_filename": current_filename_from_state,
                "save_message": "Filename cannot be empty.",
                "success": False,
            },
        )

    if not (save_filename.endswith(".yaml") or save_filename.endswith(".yml") or save_filename.endswith(".flock")):
        save_filename += ".flock.yaml"

    # Pass request.app.state to the service function
    success, message = save_current_flock_to_file_service(save_filename, request.app.state)

    # Retrieve potentially updated flock and filename from app.state
    saved_flock: Flock | None = getattr(request.app.state, 'flock_instance', None)
    saved_filename: str | None = getattr(request.app.state, 'flock_filename', None)


    return templates.TemplateResponse(
        "partials/_flock_properties_form.html",
        {
            "request": request,
            "flock": saved_flock, # Use the instance from app_state
            "current_filename": saved_filename, # Use the filename from app_state (updated on successful save)
            "save_message": message,
            "success": success,
        },
    )
```

### src\flock\webapp\app\api\registry_viewer.py

- **Lines**: 30
- **Last modified**: 2025-05-09 01:11:35

```py
from pathlib import Path

from fastapi import APIRouter, Request
from fastapi.responses import HTMLResponse
from fastapi.templating import Jinja2Templates

from flock.webapp.app.services.flock_service import get_registered_items_service

router = APIRouter()
BASE_DIR = Path(__file__).resolve().parent.parent.parent
templates = Jinja2Templates(directory=str(BASE_DIR / "templates"))


@router.get("/htmx/{item_type}/table", response_class=HTMLResponse)
async def htmx_get_registry_table(request: Request, item_type: str):
    valid_item_types = ["type", "tool", "component"]
    if item_type not in valid_item_types:
        return HTMLResponse(
            "<p class='error'>Invalid item type requested.</p>", status_code=400
        )

    items = get_registered_items_service(item_type)
    return templates.TemplateResponse(
        "partials/_registry_table.html",
        {
            "request": request,
            "item_type_display": item_type.capitalize() + "s",
            "items": items,
        },
    )
```

### src\flock\webapp\app\chat.py

- **Lines**: 604
- **Last modified**: 2025-05-21 19:51:15

```py
from __future__ import annotations

import ast  # Add import ast
import json
from datetime import datetime
from uuid import uuid4

import markdown2  # Added for Markdown to HTML conversion
from fastapi import APIRouter, Depends, Form, Request, Response
from fastapi.responses import HTMLResponse
from pydantic import BaseModel

from flock.core.flock import Flock
from flock.core.logging.logging import get_logger
from flock.webapp.app.dependencies import get_shared_link_store
from flock.webapp.app.main import get_base_context_web, templates
from flock.webapp.app.services.sharing_models import (
    FeedbackRecord,
    SharedLinkConfig,
)
from flock.webapp.app.services.sharing_store import SharedLinkStoreInterface

router = APIRouter()
logger = get_logger("webapp.chat")

# ---------------------------------------------------------------------------
# In-memory session store (cookie-based). Not suitable for production scale.
# ---------------------------------------------------------------------------
_chat_sessions: dict[str, list[dict[str, str]]] = {}

COOKIE_NAME = "chat_sid"


def _ensure_session(request: Request) -> tuple[str, list[dict[str, str]]]:
    """Returns (sid, history_list) tuple and guarantees cookie presence."""
    sid: str | None = request.cookies.get(COOKIE_NAME)
    if not sid:
        sid = uuid4().hex
    if sid not in _chat_sessions:
        _chat_sessions[sid] = []
    return sid, _chat_sessions[sid]


def _get_history_for_shared_chat(request: Request, share_id: str) -> list[dict[str, str]]:
    """Manages history for a shared chat session, namespaced by share_id and user's session_id."""
    user_sid: str | None = request.cookies.get(COOKIE_NAME)
    if not user_sid: # Should have been set by _ensure_session on page load
        user_sid = uuid4().hex
        # Note: This history will be ephemeral if the cookie isn't set back to the client,
        # but _ensure_session on the shared chat page load should handle cookie setting.

    # Composite key for shared chat history
    shared_session_key = f"shared_{share_id}_{user_sid}"
    if shared_session_key not in _chat_sessions:
        _chat_sessions[shared_session_key] = []
    return _chat_sessions[shared_session_key]


# ---------------------------------------------------------------------------
# Chat configuration (per app instance for non-shared, or from SharedLinkConfig for shared)
# ---------------------------------------------------------------------------


class ChatConfig(BaseModel):
    agent_name: str | None = None  # Name of the Flock agent to chat with
    message_key: str = "message"
    history_key: str = "history"
    response_key: str = "response"


# Store a single global chat config on the FastAPI app state for non-shared chat
def get_chat_config(request: Request) -> ChatConfig:
    if not hasattr(request.app.state, "chat_config"):
        request.app.state.chat_config = ChatConfig()
    return request.app.state.chat_config


# ---------------------------------------------------------------------------
# Helper for Shared Chat Context
# ---------------------------------------------------------------------------
async def _get_shared_chat_context(
    request: Request,
    share_id: str,
    store: SharedLinkStoreInterface = Depends(get_shared_link_store)
) -> tuple[ChatConfig | None, Flock | None, SharedLinkConfig | None]:
    shared_config_db = await store.get_config(share_id)

    if not shared_config_db or shared_config_db.share_type != "chat":
        logger.warning(f"Shared chat link {share_id} not found or not a chat share type.")
        return None, None, None

    # Retrieve the pre-loaded Flock instance for this share_id
    # This is loaded by the /chat/shared/{share_id} endpoint in main.py (or will be)
    # For chat.py, we will create a specific /chat/shared/{share_id} endpoint

    loaded_flock: Flock | None = None
    if hasattr(request.app.state, 'shared_flocks') and share_id in request.app.state.shared_flocks:
        loaded_flock = request.app.state.shared_flocks[share_id]
    else:
        # Attempt to load on-the-fly if not found (e.g., direct API call without page load)
        # This is a fallback and might be slower if the Flock definition is large.
        # The main /chat/shared/{share_id} page route should pre-load this.
        try:
            from flock.core.flock import Flock as ConcreteFlock  # Local import
            loaded_flock = ConcreteFlock.from_yaml(shared_config_db.flock_definition)
            if not hasattr(request.app.state, 'shared_flocks'):
                request.app.state.shared_flocks = {}
            request.app.state.shared_flocks[share_id] = loaded_flock # Cache it
            logger.info(f"On-the-fly load of Flock for shared chat {share_id}.")
        except Exception as e_load:
            logger.error(f"Failed to load Flock from definition for shared chat {share_id}: {e_load}", exc_info=True)
            return None, None, shared_config_db


    frozen_chat_cfg = ChatConfig(
        agent_name=shared_config_db.agent_name, # agent_name from SharedLinkConfig is the chat agent
        message_key=shared_config_db.chat_message_key or "message",
        history_key=shared_config_db.chat_history_key or "history",
        response_key=shared_config_db.chat_response_key or "response",
    )
    return frozen_chat_cfg, loaded_flock, shared_config_db


# ---------------------------------------------------------------------------
# Routes
# ---------------------------------------------------------------------------


@router.get("/chat", response_class=HTMLResponse, tags=["Chat"])
async def chat_page(request: Request):
    """Full-page chat UI (works even when the main UI is disabled)."""
    sid, history = _ensure_session(request)
    cfg = get_chat_config(request)
    context = get_base_context_web(request, ui_mode="standalone")
    context.update({"history": history, "chat_cfg": cfg, "chat_subtitle": f"Agent: {cfg.agent_name}" if cfg.agent_name else "Echo demo", "is_shared_chat": False, "share_id": None})
    response = templates.TemplateResponse("chat.html", context)
    # Set cookie if not already present
    if COOKIE_NAME not in request.cookies:
        response.set_cookie(COOKIE_NAME, sid, max_age=60 * 60 * 24 * 7)
    return response


@router.get("/chat/messages", response_class=HTMLResponse, tags=["Chat"], include_in_schema=False)
async def chat_history_partial(request: Request):
    """HTMX endpoint that returns the rendered message list."""
    _, history = _ensure_session(request)
    return templates.TemplateResponse(
        "partials/_chat_messages.html",
        {"request": request, "history": history, "now": datetime.now}
    )


@router.post("/chat/send", response_class=HTMLResponse, tags=["Chat"])
async def chat_send(request: Request, message: str = Form(...)):
    """Echo-back mock implementation. Adds user msg + bot reply to history."""
    _, history = _ensure_session(request)
    current_time = datetime.now().strftime('%H:%M')
    cfg = get_chat_config(request)
    history.append({"role": "user", "text": message, "timestamp": current_time})
    start_time = datetime.now()
    is_error = False # Initialize is_error

    flock_inst = getattr(request.app.state, "flock_instance", None)
    bot_agent = cfg.agent_name if cfg.agent_name else None
    bot_text: str

    if bot_agent and flock_inst and bot_agent in getattr(flock_inst, "agents", {}):
        run_input: dict = {}
        if cfg.message_key: run_input[cfg.message_key] = message
        if cfg.history_key: run_input[cfg.history_key] = [h["text"] for h in history if h.get("role") == "user" or h.get("role") == "bot"] # Simple text history

        try:
            result_dict = await flock_inst.run_async(start_agent=bot_agent, input=run_input, box_result=False)
            # Assuming result_dict might be the actual dict, or its string representation is what we need.
            # For now, we work with bot_text derived from it.
            if cfg.response_key:
                bot_text = str(result_dict.get(cfg.response_key, result_dict))
            else:
                bot_text = str(result_dict)

        except Exception as e:
            bot_text = f"Error: {e}"
            is_error = True

        if not is_error:
            original_bot_text = bot_text # Keep a copy
            formatted_as_json = False

            stripped_text = bot_text.strip()
            if (stripped_text.startswith('{') and stripped_text.endswith('}')) or \
               (stripped_text.startswith('[') and stripped_text.endswith(']')):
                try:
                    parsed_obj = json.loads(bot_text)
                    if isinstance(parsed_obj, (dict, list)):
                        pretty_json = json.dumps(parsed_obj, indent=2).replace('\\n', '\n')
                        bot_text = f'''<pre><code class="language-json">{pretty_json}</code></pre>'''
                        formatted_as_json = True
                except json.JSONDecodeError:
                    try:
                        evaluated_obj = ast.literal_eval(bot_text)
                        if isinstance(evaluated_obj, (dict, list)):
                            pretty_json = json.dumps(evaluated_obj, indent=2).replace('\\n', '\n')
                            bot_text = f'''<pre><code class="language-json">{pretty_json}</code></pre>'''
                            formatted_as_json = True
                    except (ValueError, SyntaxError, TypeError):
                        pass # Fall through
                except Exception as e_json_fmt:
                    logger.error(f"Error formatting likely JSON: {e_json_fmt}. Original: {original_bot_text[:200]}", exc_info=True)
                    bot_text = original_bot_text

            if not formatted_as_json:
                try:
                    bot_text = markdown2.markdown(original_bot_text, extras=["fenced-code-blocks", "tables", "break-on-newline"])
                except Exception:
                    logger.error(f"Error during Markdown conversion for bot_text. Original: {original_bot_text[:200]}", exc_info=True)
                    bot_text = original_bot_text # Fallback to original text, will be HTML escaped by Jinja if not |safe

    else:
        # Fallback echo behavior or agent not found messages
        is_error = True # Treat these as plain text, no special formatting
        if bot_agent and not flock_inst:
            bot_text = f"Agent '{bot_agent}' configured, but no Flock loaded."
        elif bot_agent and flock_inst and bot_agent not in getattr(flock_inst, "agents", {}):
             bot_text = f"Agent '{bot_agent}' configured, but not found in the loaded Flock."
        else: # No agent configured
            bot_text = f"Echo: {message}"
            # If even echo should be markdown, remove is_error=True here and let it pass through. For now, plain.

    duration_ms = int((datetime.now() - start_time).total_seconds() * 1000)
    history.append({
        "role": "bot",
        "text": bot_text,
        "timestamp": current_time,
        "agent": bot_agent or "echo",
        "duration_ms": duration_ms,
        "raw_json": original_bot_text if 'original_bot_text' in locals() else bot_text,
        "flock_yaml": getattr(flock_inst, 'to_yaml', lambda: "")()
    })
    # Return updated history partial
    return templates.TemplateResponse(
        "partials/_chat_messages.html",
        {"request": request, "history": history, "now": datetime.now}
    )


@router.get("/ui/htmx/chat-view", response_class=HTMLResponse, tags=["Chat"], include_in_schema=False)
async def chat_container_partial(request: Request):
    _ensure_session(request)
    return templates.TemplateResponse("partials/_chat_container.html", {"request": request})


# ---------------------------------------------------------------------------
# Chat settings management
# ---------------------------------------------------------------------------


@router.get("/ui/htmx/chat-settings-form", response_class=HTMLResponse, include_in_schema=False)
async def chat_settings_form(request: Request):
    """Returns the form for configuring chat behaviour (HTMX partial)."""
    cfg = get_chat_config(request)
    flock_inst = getattr(request.app.state, "flock_instance", None)
    input_fields, output_fields = [], []
    if cfg.agent_name and flock_inst and cfg.agent_name in flock_inst.agents:
        agent_obj = flock_inst.agents[cfg.agent_name]
        # Expect signatures like "field: type | desc, ..." or "field: type" etc.
        def _extract(sig: str):
            fields = []
            for seg in sig.split(','):
                parts = seg.strip().split(':')
                if parts:
                    fields.append(parts[0].strip())
            return [f for f in fields if f]
        input_fields = _extract(agent_obj.input) if getattr(agent_obj, 'input', '') else []
        output_fields = _extract(agent_obj.output) if getattr(agent_obj, 'output', '') else []

    context = get_base_context_web(request)
    context.update({
        "chat_cfg": cfg,
        "current_flock": flock_inst,
        "input_fields": input_fields,
        "output_fields": output_fields,
    })
    return templates.TemplateResponse("partials/_chat_settings_form.html", context)


@router.post("/chat/settings", include_in_schema=False)
async def chat_settings_submit(
    request: Request,
    agent_name: str | None = Form(default=None),
    message_key: str = Form("message"),
    history_key: str = Form("history"),
    response_key: str = Form("response"),
):
    """Handles chat settings submission and triggers a toast notification."""
    cfg = get_chat_config(request)
    cfg.agent_name = agent_name
    cfg.message_key = message_key
    cfg.history_key = history_key
    cfg.response_key = response_key

    logger.info(f"Chat settings updated: Agent: {cfg.agent_name}, MsgKey: {cfg.message_key}, HistKey: {cfg.history_key}, RespKey: {cfg.response_key}")

    toast_event = {
        "showGlobalToast": {
            "message": "Chat settings saved successfully!",
            "type": "success"
        }
    }
    headers = {"HX-Trigger": json.dumps(toast_event)}
    return Response(status_code=204, headers=headers)


# --- Stand-alone Chat HTML page access to settings --------------------------


@router.get("/chat/settings-standalone", response_class=HTMLResponse, tags=["Chat"], include_in_schema=False)
async def chat_settings_standalone(request: Request):
    """Standalone page to render chat settings (used by full-page chat HTML)."""
    cfg = get_chat_config(request)
    context = get_base_context_web(request, ui_mode="standalone")
    context.update({
        "chat_cfg": cfg,
        "current_flock": getattr(request.app.state, "flock_instance", None),
    })
    return templates.TemplateResponse("chat_settings.html", context)


# ---------------------------------------------------------------------------
# Stand-alone HTMX partials (chat view & settings) for in-page swapping
# ---------------------------------------------------------------------------


@router.get("/chat/htmx/chat-view", response_class=HTMLResponse, include_in_schema=False)
async def htmx_chat_view(request: Request):
    """Return chat container partial for standalone page reload via HTMX."""
    _ensure_session(request)
    return templates.TemplateResponse("partials/_chat_container.html", {"request": request})


@router.get("/chat/htmx/settings-form", response_class=HTMLResponse, include_in_schema=False)
async def htmx_chat_settings_partial(request: Request):
    cfg = get_chat_config(request)
    # Allow temporarily selecting agent via query param without saving
    agent_override = request.query_params.get("agent_name")
    if agent_override is not None:
        cfg = cfg.copy()
        cfg.agent_name = agent_override or None

    flock_inst = getattr(request.app.state, "flock_instance", None)
    input_fields, output_fields = [], []
    if cfg.agent_name and flock_inst and cfg.agent_name in flock_inst.agents:
        agent_obj = flock_inst.agents[cfg.agent_name]
        def _extract(sig: str):
            return [seg.strip().split(':')[0].strip() for seg in sig.split(',') if seg.strip()]
        input_fields = _extract(agent_obj.input) if getattr(agent_obj, 'input', '') else []
        output_fields = _extract(agent_obj.output) if getattr(agent_obj, 'output', '') else []

    context = {"request": request, "chat_cfg": cfg, "current_flock": flock_inst, "input_fields": input_fields, "output_fields": output_fields}
    return templates.TemplateResponse("partials/_chat_settings_form.html", context)


# ---------------------------------------------------------------------------
# Shared Chat Routes
# ---------------------------------------------------------------------------

@router.get("/chat/shared/{share_id}", response_class=HTMLResponse, tags=["Chat Sharing"])
async def page_shared_chat(
    request: Request,
    share_id: str,
    store: SharedLinkStoreInterface = Depends(get_shared_link_store)
):
    """Serves the chat page for a shared chat session."""
    logger.info(f"Accessing shared chat page for share_id: {share_id}")

    sid, _ = _ensure_session(request) # Ensures user has a session for history tracking

    shared_config_db = await store.get_config(share_id)

    if not shared_config_db or shared_config_db.share_type != "chat":
        logger.warning(f"Shared chat link {share_id} not found or not a chat share type.")
        # Consider rendering an error template or redirecting
        error_context = get_base_context_web(request, ui_mode="standalone", error="Shared chat link is invalid or has expired.")
        return templates.TemplateResponse("error_page.html", {**error_context, "error_title": "Invalid Link"}, status_code=404)

    # Load Flock from definition and cache in app.state.shared_flocks
    loaded_flock: Flock | None = None
    if hasattr(request.app.state, 'shared_flocks') and share_id in request.app.state.shared_flocks:
        loaded_flock = request.app.state.shared_flocks[share_id]
    else:
        try:
            from flock.core.flock import Flock as ConcreteFlock
            loaded_flock = ConcreteFlock.from_yaml(shared_config_db.flock_definition)
            if not hasattr(request.app.state, 'shared_flocks'):
                request.app.state.shared_flocks = {}
            request.app.state.shared_flocks[share_id] = loaded_flock
            logger.info(f"Loaded and cached Flock for shared chat {share_id} in app.state.shared_flocks.")
        except Exception as e_load:
            logger.error(f"Fatal: Could not load Flock from definition for shared chat {share_id}: {e_load}", exc_info=True)
            error_context = get_base_context_web(request, ui_mode="standalone", error=f"Could not load the shared Flock configuration: {e_load!s}")
            return templates.TemplateResponse("error_page.html", {**error_context, "error_title": "Configuration Error"}, status_code=500)

    frozen_chat_cfg = ChatConfig(
        agent_name=shared_config_db.agent_name,
        message_key=shared_config_db.chat_message_key or "message",
        history_key=shared_config_db.chat_history_key or "history",
        response_key=shared_config_db.chat_response_key or "response",
    )

    # Get history specific to this user and this shared chat
    history = _get_history_for_shared_chat(request, share_id)

    context = get_base_context_web(request, ui_mode="standalone")
    context.update({
        "history": history, # User-specific history for this shared chat
        "chat_cfg": frozen_chat_cfg, # The "frozen" config from the share link
        "chat_subtitle": f"Shared Chat - Agent: {frozen_chat_cfg.agent_name}" if frozen_chat_cfg.agent_name else "Shared Echo Chat",
        "is_shared_chat": True,
        "share_id": share_id,
        "flock": loaded_flock # Pass flock for potential display, though backend uses cached one
    })

    response = templates.TemplateResponse("chat.html", context)
    if COOKIE_NAME not in request.cookies: # Ensure cookie is set if _ensure_session created a new one
        response.set_cookie(COOKIE_NAME, sid, max_age=60 * 60 * 24 * 7)
    return response

@router.get("/chat/messages-shared/{share_id}", response_class=HTMLResponse, tags=["Chat Sharing"], include_in_schema=False)
async def chat_history_partial_shared(request: Request, share_id: str):
    """HTMX endpoint that returns the rendered message list for a shared chat."""
    # _ensure_session called on page load, so cookie should exist for history keying
    history = _get_history_for_shared_chat(request, share_id)
    return templates.TemplateResponse(
        "partials/_chat_messages.html",
        {"request": request, "history": history, "now": datetime.now}
    )

@router.post("/chat/send-shared", response_class=HTMLResponse, tags=["Chat Sharing"])
async def chat_send_shared(
    request: Request,
    share_id: str = Form(...),
    message: str = Form(...),
    # Note: Dependencies need to be declared at the route level for FastAPI to inject them.
    # So, we re-declare get_shared_link_store here or pass it to the helper if FastAPI handles sub-dependencies.
    # For simplicity with current structure, let _get_shared_chat_context handle its own dependency.
    # We can also make _get_shared_chat_context a Depends() if preferred.
    store: SharedLinkStoreInterface = Depends(get_shared_link_store)
):
    """Handles message sending for a shared chat session."""
    frozen_chat_cfg, flock_inst, _ = await _get_shared_chat_context(request, share_id, store)
    is_error = False # Initialize is_error

    if not frozen_chat_cfg or not flock_inst:
        # Error response if config or flock couldn't be loaded
        # This history is ephemeral as it won't be saved if the config is bad
        error_history = [{"role": "bot", "text": "Error: Shared chat configuration is invalid or Flock not found.", "timestamp": datetime.now().strftime('%H:%M')}]
        return templates.TemplateResponse(
            "partials/_chat_messages.html",
            {"request": request, "history": error_history, "now": datetime.now},
            status_code=404
        )

    history = _get_history_for_shared_chat(request, share_id)
    current_time = datetime.now().strftime('%H:%M')
    history.append({"role": "user", "text": message, "timestamp": current_time})
    start_time = datetime.now()

    bot_agent = frozen_chat_cfg.agent_name
    bot_text: str

    if bot_agent and bot_agent in getattr(flock_inst, "agents", {}):
        run_input: dict = {}
        if frozen_chat_cfg.message_key: run_input[frozen_chat_cfg.message_key] = message
        if frozen_chat_cfg.history_key: run_input[frozen_chat_cfg.history_key] = [h["text"] for h in history if h.get("role") == "user" or h.get("role") == "bot"]

        try:
            result_dict = await flock_inst.run_async(start_agent=bot_agent, input=run_input, box_result=False)
            if frozen_chat_cfg.response_key:
                bot_text = str(result_dict.get(frozen_chat_cfg.response_key, result_dict))
            else:
                bot_text = str(result_dict)

        except Exception as e:
            bot_text = f"Error running agent {bot_agent} in shared chat: {e}"
            is_error = True
            logger.error(f"Error in /chat/send-shared (agent: {bot_agent}, share: {share_id}): {e}", exc_info=True)

        if not is_error:
            original_bot_text = bot_text # Keep a copy
            formatted_as_json = False

            stripped_text = bot_text.strip()
            if (stripped_text.startswith('{') and stripped_text.endswith('}')) or \
               (stripped_text.startswith('[') and stripped_text.endswith(']')):
                try:
                    parsed_obj = json.loads(bot_text)
                    if isinstance(parsed_obj, (dict, list)):
                        pretty_json = json.dumps(parsed_obj, indent=2).replace('\\n', '\n')
                        bot_text = f'''<pre><code class="language-json">{pretty_json}</code></pre>'''
                        formatted_as_json = True
                except json.JSONDecodeError:
                    try:
                        evaluated_obj = ast.literal_eval(bot_text)
                        if isinstance(evaluated_obj, (dict, list)):
                            pretty_json = json.dumps(evaluated_obj, indent=2).replace('\\n', '\n')
                            bot_text = f'''<pre><code class="language-json">{pretty_json}</code></pre>'''
                            formatted_as_json = True
                    except (ValueError, SyntaxError, TypeError):
                        pass # Fall through
                except Exception as e_json_fmt:
                    logger.error(f"Error formatting likely JSON (shared chat): {e_json_fmt}. Original: {original_bot_text[:200]}", exc_info=True)
                    bot_text = original_bot_text

            if not formatted_as_json:
                try:
                    bot_text = markdown2.markdown(original_bot_text, extras=["fenced-code-blocks", "tables", "break-on-newline"])
                except Exception:
                    logger.error(f"Error during Markdown conversion for shared chat bot_text. Original: {original_bot_text[:200]}", exc_info=True)
                    bot_text = original_bot_text
    else:
        # Fallback if agent misconfigured or not found in the specific shared flock
        is_error = True # Treat these as plain text
        if bot_agent and bot_agent not in getattr(flock_inst, "agents", {}):
             bot_text = f"Agent '{bot_agent}' (shared) not found in its Flock."
        elif not bot_agent:
             bot_text = f"No agent configured for this shared chat. Echoing: {message}"
        else: # Should not happen if frozen_chat_cfg and flock_inst were valid earlier
             bot_text = f"Shared Echo: {message}"

    duration_ms = int((datetime.now() - start_time).total_seconds() * 1000)
    history.append({
        "role": "bot",
        "text": bot_text,
        "timestamp": current_time,
        "agent": bot_agent or "shared-echo",
        "duration_ms": duration_ms,
        "raw_json": original_bot_text if 'original_bot_text' in locals() else bot_text,
        "flock_yaml": getattr(flock_inst, 'to_yaml', lambda: "")()
    })

    return templates.TemplateResponse(
        "partials/_chat_messages.html",
        {"request": request, "history": history, "now": datetime.now}
    )

# ---------------- Feedback endpoints ----------------
@router.post("/chat/htmx/feedback", response_class=HTMLResponse, include_in_schema=False)
async def chat_feedback(request: Request,
    reason: str = Form(...),
    expected_response: str | None = Form(None),
    actual_response: str | None = Form(None),
    flock_definition: str | None = Form(None),
    agent_name: str | None = Form(None),
    store: SharedLinkStoreInterface = Depends(get_shared_link_store)):
    from uuid import uuid4
    rec = FeedbackRecord(
        feedback_id=uuid4().hex,
        share_id=None,
        context_type="chat",
        reason=reason,
        expected_response=expected_response,
        actual_response=actual_response,
        flock_definition=flock_definition,
        agent_name=agent_name,
    )
    await store.save_feedback(rec)
    toast_event = {
        "showGlobalToast": {
            "message": "Feedback received! Thanks",
            "type": "success"
        }
    }
    headers = {"HX-Trigger": json.dumps(toast_event)}
    return Response(status_code=204, headers=headers)


@router.post("/chat/htmx/feedback-shared", response_class=HTMLResponse, include_in_schema=False)
async def chat_feedback_shared(request: Request,
    share_id: str = Form(...),
    reason: str = Form(...),
    expected_response: str | None = Form(None),
    actual_response: str | None = Form(None),
    flock_definition: str | None = Form(None),
    agent_name: str | None = Form(None),
    store: SharedLinkStoreInterface = Depends(get_shared_link_store)):
    from uuid import uuid4
    rec = FeedbackRecord(
        feedback_id=uuid4().hex,
        share_id=share_id,
        context_type="chat",
        reason=reason,
        expected_response=expected_response,
        actual_response=actual_response,
        flock_definition=flock_definition,
        agent_name=agent_name,
    )
    await store.save_feedback(rec)
    toast_event = {
        "showGlobalToast": {
            "message": "Feedback received! Thanks",
            "type": "success"
        }
    }
    headers = {"HX-Trigger": json.dumps(toast_event)}
    return Response(status_code=204, headers=headers)
```

### src\flock\webapp\app\config.py

- **Lines**: 104
- **Last modified**: 2025-05-21 19:51:15

```py
import os
import random  # Added for random theme selection
from pathlib import Path
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from flock.core.flock import Flock

from flock.core.logging.formatters.themes import OutputTheme

FLOCK_FILES_DIR = Path(os.getenv("FLOCK_FILES_DIR", "./.flock_ui_projects"))
FLOCK_FILES_DIR.mkdir(parents=True, exist_ok=True)

# --- Shared Links Database Configuration ---
# Default path is relative to the .flock/ directory in the workspace root if FLOCK_ROOT is not set
# or if .flock is not a sibling of FLOCK_BASE_DIR.
# More robustly, place it inside a user-specific or project-specific data directory.
_default_shared_links_db_parent = Path(os.getenv("FLOCK_ROOT", ".")) / ".flock"
SHARED_LINKS_DB_PATH = Path(
    os.getenv(
        "SHARED_LINKS_DB_PATH",
        str(_default_shared_links_db_parent / "shared_links.db")
    )
)
# Ensure the directory for the DB exists, though the store will also do this.
SHARED_LINKS_DB_PATH.parent.mkdir(parents=True, exist_ok=True)

# --- Theme Configuration ---
# Calculate themes directory relative to this config file's location, assuming structure:
# src/flock/webapp/app/config.py
# src/flock/themes/
CONFIG_FILE_PATH = Path(__file__).resolve() # src/flock/webapp/app/config.py
FLOCK_WEBAPP_DIR = CONFIG_FILE_PATH.parent.parent # src/flock/webapp/
FLOCK_BASE_DIR = FLOCK_WEBAPP_DIR.parent # src/flock/
THEMES_DIR = FLOCK_BASE_DIR / "themes"

# Global state for MVP - NOT SUITABLE FOR PRODUCTION/MULTI-USER
CURRENT_FLOCK_INSTANCE: "Flock | None" = None
CURRENT_FLOCK_FILENAME: str | None = None

DEFAULT_THEME_NAME = OutputTheme.ciapre.value # Default if random fails or invalid theme specified

def list_available_themes() -> list[str]:
    """Scans the THEMES_DIR for .toml files and returns their names (without .toml)."""
    if not THEMES_DIR.exists() or not THEMES_DIR.is_dir():
        return []
    return sorted([p.stem for p in THEMES_DIR.glob("*.toml") if p.is_file()])

# Initialize CURRENT_THEME_NAME
_initial_theme_from_env = os.environ.get("FLOCK_WEB_THEME")
_resolved_initial_theme = DEFAULT_THEME_NAME

if _initial_theme_from_env:
    if _initial_theme_from_env.lower() == "random":
        available_themes = list_available_themes()
        if available_themes:
            _resolved_initial_theme = random.choice(available_themes)
            print(f"Config: Initial theme from FLOCK_WEB_THEME='random' resolved to: {_resolved_initial_theme}")
        else:
            print(f"Warning: FLOCK_WEB_THEME='random' specified, but no themes found in {THEMES_DIR}. Using default: {DEFAULT_THEME_NAME}")
            _resolved_initial_theme = DEFAULT_THEME_NAME
    elif _initial_theme_from_env in [t.value for t in OutputTheme] or _initial_theme_from_env in list_available_themes():
        _resolved_initial_theme = _initial_theme_from_env
        print(f"Config: Initial theme set from FLOCK_WEB_THEME env var: {_resolved_initial_theme}")
    else:
        print(f"Warning: Invalid theme name '{_initial_theme_from_env}' in FLOCK_WEB_THEME. Using default: {DEFAULT_THEME_NAME}")
        _resolved_initial_theme = DEFAULT_THEME_NAME

CURRENT_THEME_NAME: str = _resolved_initial_theme

def set_current_theme_name(theme_name: str | None):
    """Sets the globally accessible current theme name.
    If 'random' is passed, a random theme is chosen.
    """
    global CURRENT_THEME_NAME
    resolved_theme = DEFAULT_THEME_NAME # Default to start

    if theme_name:
        if theme_name.lower() == "random":
            available = list_available_themes()
            if available:
                resolved_theme = random.choice(available)
                print(f"Config: Theme 'random' resolved to: {resolved_theme}")
            else:
                print(f"Warning: Theme 'random' specified, but no themes found in {THEMES_DIR}. Using default: {DEFAULT_THEME_NAME}")
                # resolved_theme remains DEFAULT_THEME_NAME
        elif theme_name in list_available_themes():
            resolved_theme = theme_name
        else:
            print(f"Warning: Invalid theme name provided ('{theme_name}'). Using default: {DEFAULT_THEME_NAME}")
            # resolved_theme remains DEFAULT_THEME_NAME
    else: # theme_name is None
        # resolved_theme remains DEFAULT_THEME_NAME (set at the beginning of function)
        pass

    CURRENT_THEME_NAME = resolved_theme
    # Ensure the theme name is set in the environment if we want other processes to see it,
    # though this might be better handled by the calling process (e.g. CLI)
    # os.environ["FLOCK_WEB_THEME"] = CURRENT_THEME_NAME
    print(f"Config: Current theme explicitly set to: {CURRENT_THEME_NAME}")

def get_current_theme_name() -> str:
    """Gets the globally accessible current theme name."""
    return CURRENT_THEME_NAME
```

### src\flock\webapp\app\dependencies.py

- **Lines**: 117
- **Last modified**: 2025-05-21 19:51:15

```py
# src/flock/webapp/app/dependencies.py
from collections.abc import Sequence
from typing import TYPE_CHECKING, Optional

# Import FlockEndpoint for type hinting
from flock.core.api.custom_endpoint import FlockEndpoint

if TYPE_CHECKING:
    from flock.core.api.run_store import RunStore
    from flock.core.flock import Flock
    from flock.webapp.app.services.sharing_store import SharedLinkStoreInterface


# These will be set once when the FastAPI app starts, via set_global_flock_services
_flock_instance: Optional["Flock"] = None
_run_store_instance: Optional["RunStore"] = None
_shared_link_store_instance: Optional["SharedLinkStoreInterface"] = None

# Global-like variable (scoped to this module) to temporarily store custom endpoints
# before the app is fully configured and the lifespan event runs.
_pending_custom_endpoints: list[FlockEndpoint] = []


def add_pending_custom_endpoints(endpoints: Sequence[FlockEndpoint] | None):
    """Temporarily stores custom endpoints. Called by `start_unified_server`
    before the FastAPI app's lifespan event can process them.
    """
    global _pending_custom_endpoints
    if endpoints:
        # Ensure we are adding FlockEndpoint instances
        for ep in endpoints:
            if not isinstance(ep, FlockEndpoint):
                # If it's a dict, try to convert it, assuming it was the old format
                if isinstance(ep, dict) and "path" in ep and "callback" in ep:
                    try:
                        # Attempt to create FlockEndpoint from dict - assumes correct keys
                        # This is a basic conversion; more robust parsing might be needed
                        # if the dict structure is very different.
                        _pending_custom_endpoints.append(FlockEndpoint(**ep)) # type: ignore
                    except Exception as e:
                        print(f"Warning: Could not convert custom endpoint dict to FlockEndpoint: {ep}. Error: {e}")
                else:
                    print(f"Warning: Custom endpoint is not a FlockEndpoint instance and cannot be automatically converted: {type(ep)}")
            else:
                _pending_custom_endpoints.append(ep)
        print(f"Dependencies: Added {len(endpoints)} pending custom endpoints.")


def get_pending_custom_endpoints_and_clear() -> list[FlockEndpoint]:
    """Retrieves and clears pending custom endpoints.
    Called by the FastAPI app's lifespan event manager in `webapp/app/main.py`.
    """
    global _pending_custom_endpoints
    endpoints_to_add = _pending_custom_endpoints[:] # Create a copy
    _pending_custom_endpoints = [] # Clear the pending list
    if endpoints_to_add:
        print(f"Dependencies: Retrieved {len(endpoints_to_add)} pending custom endpoints for app registration.")
    return endpoints_to_add


def set_global_flock_services(flock: Optional["Flock"], run_store: "RunStore"):
    """Called once at application startup to set the global Flock and RunStore.
    This is typically invoked by the server startup script (e.g., in webapp/run.py).
    """
    global _flock_instance, _run_store_instance
    from flock.core.logging.logging import (
        get_logger,  # Local import to avoid circularity at module level
    )
    logger = get_logger("dependencies")

    if _flock_instance is not None or _run_store_instance is not None:
        logger.warning("Global Flock services are being re-initialized in dependencies.py.")

    _flock_instance = flock
    _run_store_instance = run_store
    logger.info(f"Global services set in dependencies: Flock='{flock.name if flock else 'None'}', RunStore type='{type(run_store)}'")


def set_global_shared_link_store(store: "SharedLinkStoreInterface"):
    """Called once at application startup to set the global SharedLinkStore."""
    global _shared_link_store_instance
    from flock.core.logging.logging import get_logger
    logger = get_logger("dependencies")

    if _shared_link_store_instance is not None:
        logger.warning("Global SharedLinkStore is being re-initialized in dependencies.py.")

    _shared_link_store_instance = store
    logger.info(f"Global SharedLinkStore set in dependencies: Store type='{type(store)}'")


def get_flock_instance() -> "Flock":
    """FastAPI dependency to get the globally available Flock instance."""
    if _flock_instance is None:
        # This can happen if accessed before `set_global_flock_services` or if it was cleared.
        # The application should handle this gracefully, e.g. by redirecting to a setup page.
        raise RuntimeError("Flock instance has not been initialized or is not currently available for DI.")
    return _flock_instance

def get_optional_flock_instance() -> Optional["Flock"]:
    """FastAPI dependency to optionally get the Flock instance. Returns None if not set."""
    return _flock_instance


def get_run_store() -> "RunStore":
    """FastAPI dependency to get the globally available RunStore instance."""
    if _run_store_instance is None:
        # Similar to Flock instance, should be initialized at app startup.
        raise RuntimeError("RunStore instance has not been initialized in the application.")
    return _run_store_instance


def get_shared_link_store() -> "SharedLinkStoreInterface":
    """FastAPI dependency to get the globally available SharedLinkStore instance."""
    if _shared_link_store_instance is None:
        raise RuntimeError("SharedLinkStore instance has not been initialized in the application.")
    return _shared_link_store_instance
```

### src\flock\webapp\app\models_ui.py

- **Lines**: 7
- **Last modified**: 2025-05-09 01:11:35

```py
# Pydantic models specific to UI interactions, if needed.
# For MVP, we might not need many here, as we'll primarily pass basic dicts to flock_service.
# Example:
# from pydantic import BaseModel
# class SaveFlockRequest(BaseModel):
#     current_flock_json: str # Or a more structured model if preferred
#     new_filename: str
```

### src\flock\webapp\app\services\__init__.py

- **Lines**: 0
- **Last modified**: 2025-05-09 01:11:35

```py

```

### src\flock\webapp\app\services\flock_service.py

- **Lines**: 337
- **Last modified**: 2025-05-22 21:27:37

```py
# src/flock/webapp/app/services/flock_service.py
from typing import TYPE_CHECKING, Any, Optional

import yaml

# Conditional import for Flock and FlockFactory for type hinting
if TYPE_CHECKING:
    from flock.core.flock import Flock
    from flock.core.flock_factory import FlockFactory
else:
    Flock = "flock.core.flock.Flock"
    FlockFactory = "flock.core.flock_factory.FlockFactory"

from flock.core.api.run_store import RunStore
from flock.core.flock_registry import get_registry
from flock.core.logging.logging import get_logger
from flock.webapp.app.config import FLOCK_FILES_DIR
from flock.webapp.app.dependencies import set_global_flock_services

logger = get_logger("webapp.service")


def get_available_flock_files() -> list[str]:
    """Returns a sorted list of available .yaml, .yml, or .flock files."""
    if not FLOCK_FILES_DIR.exists():
        return []
    return sorted(
        [
            f.name
            for f in FLOCK_FILES_DIR.iterdir()
            if f.is_file() and (f.suffix.lower() in [".yaml", ".yml", ".flock"])
        ]
    )

def _ensure_run_store_in_app_state(app_state: object) -> RunStore: # app_state is Starlette's State
    """Ensures a RunStore instance exists in app_state, creating if necessary."""
    run_store = getattr(app_state, 'run_store', None)
    if not isinstance(run_store, RunStore):
        logger.info("RunStore not found or invalid in app_state, creating a new one for this session.")
        run_store = RunStore()
        setattr(app_state, 'run_store', run_store)
    return run_store


def load_flock_from_file_service(filename: str, app_state: object) -> Optional["Flock"]:
    """Loads a Flock instance from a file.
    Updates app_state with the loaded flock/filename and updates DI services.
    """
    from flock.core.flock import Flock as ConcreteFlock

    file_path = FLOCK_FILES_DIR / filename
    if not file_path.exists():
        logger.error(f"Flock file not found: {file_path}")
        clear_current_flock_service(app_state)
        return None
    try:
        logger.info(f"Loading flock from: {file_path}")
        loaded_flock = ConcreteFlock.load_from_file(str(file_path))

        setattr(app_state, 'flock_instance', loaded_flock)
        setattr(app_state, 'flock_filename', filename)
        run_store = _ensure_run_store_in_app_state(app_state)
        set_global_flock_services(loaded_flock, run_store)

        logger.info(f"Service: Successfully loaded flock '{loaded_flock.name}' from '{filename}'. DI updated.")
        return loaded_flock
    except Exception as e:
        logger.error(f"Service: Error loading flock from {file_path}: {e}", exc_info=True)
        clear_current_flock_service(app_state)
        return None


def create_new_flock_service(
    name: str, model: str | None, description: str | None, app_state: object
) -> "Flock":
    """Creates a new Flock instance.
    Updates app_state and DI services.
    """
    from flock.core.flock import Flock as ConcreteFlock

    effective_model = model.strip() if model and model.strip() else None
    new_flock = ConcreteFlock(
        name=name,
        model=effective_model,
        description=description,
        show_flock_banner=False,
    )
    default_filename = f"{name.replace(' ', '_').lower()}.flock.yaml"

    setattr(app_state, 'flock_instance', new_flock)
    setattr(app_state, 'flock_filename', default_filename)
    run_store = _ensure_run_store_in_app_state(app_state)
    set_global_flock_services(new_flock, run_store)

    logger.info(f"Service: Created new flock '{name}'. DI updated. Default filename: '{default_filename}'.")
    return new_flock


def clear_current_flock_service(app_state: object):
    """Clears the current Flock from app_state and updates DI services.
    """
    if hasattr(app_state, 'flock_instance'):
        delattr(app_state, 'flock_instance')
    if hasattr(app_state, 'flock_filename'):
        delattr(app_state, 'flock_filename')

    run_store = _ensure_run_store_in_app_state(app_state)
    set_global_flock_services(None, run_store)
    logger.info("Service: Current flock cleared from app_state. DI updated (Flock is None).")


def save_current_flock_to_file_service(new_filename: str, app_state: object) -> tuple[bool, str]:
    """Saves the Flock from app_state to a file. Updates app_state's current filename on success.
    """
    current_flock: Flock | None = getattr(app_state, 'flock_instance', None)
    if not current_flock:
        return False, "No flock loaded to save."
    if not new_filename.strip():
        return False, "Filename cannot be empty."

    save_path = FLOCK_FILES_DIR / new_filename
    try:
        current_flock.to_yaml_file(str(save_path))
        setattr(app_state, 'flock_filename', new_filename) # Update filename in app_state
        logger.info(f"Service: Flock '{current_flock.name}' saved to '{new_filename}'.")
        return True, f"Flock saved to '{new_filename}'."
    except Exception as e:
        logger.error(f"Service: Error saving flock '{current_flock.name}' to {save_path}: {e}", exc_info=True)
        return False, f"Error saving flock: {e}"


def update_flock_properties_service(
    name: str, model: str | None, description: str | None, app_state: object
) -> bool:
    """Updates properties of the Flock in app_state. Updates app_state's filename if name changes.
    """
    current_flock: Flock | None = getattr(app_state, 'flock_instance', None)
    current_filename: str | None = getattr(app_state, 'flock_filename', None)

    if not current_flock:
        logger.warning("Service: Attempted to update properties, but no flock loaded.")
        return False

    old_name = current_flock.name
    old_name_default_filename = f"{old_name.replace(' ', '_').lower()}.flock.yaml"

    current_flock.name = name
    current_flock.model = model.strip() if model and model.strip() else None
    current_flock.description = description

    if current_filename == old_name_default_filename and old_name != name:
        new_default_filename = f"{name.replace(' ', '_').lower()}.flock.yaml"
        setattr(app_state, 'flock_filename', new_default_filename)
        logger.info(f"Service: Default filename updated to '{new_default_filename}' due to flock name change.")

    logger.info(f"Service: Flock properties updated for '{name}'.")
    return True


def add_agent_to_current_flock_service(agent_config: dict, app_state: object) -> bool:
    """Adds an agent to the Flock in app_state."""
    from flock.core.flock_factory import FlockFactory as ConcreteFlockFactory

    current_flock: Flock | None = getattr(app_state, 'flock_instance', None)
    if not current_flock:
        logger.warning("Service: Cannot add agent, no flock loaded.")
        return False

    registry = get_registry()
    tools_instances = []
    if agent_config.get("tools_names"):
        for tool_name in agent_config["tools_names"]:
            try: tools_instances.append(registry.get_callable(tool_name))
            except KeyError: logger.warning(f"Service: Tool '{tool_name}' not found for agent '{agent_config['name']}'.")
    try:
        agent = ConcreteFlockFactory.create_default_agent(
            name=agent_config["name"],
            description=agent_config.get("description"),
            model=agent_config.get("model"),
            input=agent_config["input"],
            output=agent_config["output"],
            tools=tools_instances or None,
        )
        current_flock.add_agent(agent)
        logger.info(f"Service: Agent '{agent.name}' added to flock '{current_flock.name}'.")
        return True
    except Exception as e:
        logger.error(f"Service: Error adding agent to flock '{current_flock.name}': {e}", exc_info=True)
        return False


def update_agent_in_current_flock_service(
    original_agent_name: str, agent_config: dict, app_state: object
) -> bool:
    """Updates an agent in the Flock in app_state."""
    current_flock: Flock | None = getattr(app_state, 'flock_instance', None)
    if not current_flock:
        logger.warning("Service: Cannot update agent, no flock loaded.")
        return False

    agent_to_update = current_flock.agents.get(original_agent_name)
    if not agent_to_update:
        logger.warning(f"Service: Agent '{original_agent_name}' not found in flock '{current_flock.name}' for update.")
        return False

    registry = get_registry()
    tools_instances = []
    if agent_config.get("tools_names"):
        for tool_name in agent_config["tools_names"]:
            try: tools_instances.append(registry.get_callable(tool_name))
            except KeyError: logger.warning(f"Service: Tool '{tool_name}' not found during agent update.")
    try:
        new_name = agent_config["name"]
        agent_to_update.description = agent_config.get("description")
        current_agent_model = agent_config.get("model")
        agent_to_update.model = current_agent_model if current_agent_model and current_agent_model.strip() else None
        agent_to_update.input = agent_config["input"]
        agent_to_update.output = agent_config["output"]
        agent_to_update.tools = tools_instances or []

        if original_agent_name != new_name:
            current_flock._agents.pop(original_agent_name)
            agent_to_update.name = new_name
            current_flock.add_agent(agent_to_update)
            logger.info(f"Service: Agent '{original_agent_name}' renamed to '{new_name}' and updated in flock '{current_flock.name}'.")
        else:
            logger.info(f"Service: Agent '{original_agent_name}' updated in flock '{current_flock.name}'.")
        return True
    except Exception as e:
        logger.error(f"Service: Error updating agent '{original_agent_name}' in flock '{current_flock.name}': {e}", exc_info=True)
        return False


def remove_agent_from_current_flock_service(agent_name: str, app_state: object) -> bool:
    """Removes an agent from the Flock in app_state."""
    current_flock: Flock | None = getattr(app_state, 'flock_instance', None)
    if not current_flock or agent_name not in current_flock.agents:
        logger.warning(f"Service: Cannot remove agent '{agent_name}', no flock loaded or agent not found.")
        return False
    try:
        del current_flock._agents[agent_name]
        logger.info(f"Service: Agent '{agent_name}' removed from flock '{current_flock.name}'.")
        return True
    except Exception as e:
        logger.error(f"Service: Error removing agent '{agent_name}' from flock '{current_flock.name}': {e}", exc_info=True)
        return False


async def run_current_flock_service(
    start_agent_name: str,
    inputs: dict[str, Any],
    app_state: Any,
) -> dict[str, Any]:
    """Runs the specified agent from the current flock instance in app_state."""
    logger.info(f"Attempting to run agent: {start_agent_name} using flock from app_state.")

    current_flock: Flock | None = getattr(app_state, "flock_instance", None)
    run_store: RunStore | None = getattr(app_state, "run_store", None)

    if not current_flock:
        logger.error("Run service: No Flock instance available in app_state.")
        return {"error": "No Flock loaded in the application."}
    if not run_store:
        logger.error("Run service: No RunStore instance available in app_state.")
        # Attempt to initialize a default run_store if missing and this service is critical
        # This might indicate an issue in the application lifecycle setup
        logger.warning("Run service: Initializing a default RunStore as none was found in app_state.")
        run_store = RunStore()
        setattr(app_state, "run_store", run_store)
        # Also update global DI if this is how it's managed elsewhere for consistency,
        # though ideally DI setup handles this more centrally.
        # from flock.webapp.app.dependencies import set_global_flock_services
        # set_global_flock_services(current_flock, run_store)


    if start_agent_name not in current_flock.agents:
        logger.error(f"Run service: Agent '{start_agent_name}' not found in current flock '{current_flock.name}'.")
        return {"error": f"Agent '{start_agent_name}' not found."}

    try:
        logger.info(f"Executing agent '{start_agent_name}' from flock '{current_flock.name}' using app_state.")
        # Direct execution using the flock from app_state
        result = await current_flock.run_async(
            start_agent=start_agent_name, input=inputs, box_result=False
        )
        # Store run details using the run_store from app_state
        if hasattr(run_store, "add_run_details"): # Check if RunStore has this method
            run_id = result.get("run_id", "unknown_run_id") # Assuming run_async result might contain run_id
            run_store.add_run_details(run_id=run_id, agent_name=start_agent_name, inputs=inputs, outputs=result)
        return result
    except Exception as e:
        logger.error(f"Run service: Error during agent execution: {e}", exc_info=True)
        return {"error": f"An error occurred: {e}"}


def get_registered_items_service(item_type: str) -> list[dict]:
    """Retrieves items of a specific type from the global FlockRegistry."""
    registry = get_registry()
    items_dict: dict | None = None
    if item_type == "type":      items_dict = registry._types
    elif item_type == "tool":    items_dict = registry._callables
    elif item_type == "component": items_dict = registry._components
    else: return []

    if items_dict is None: return []

    items = []
    for name, item_obj in items_dict.items():
        module_path = "N/A"
        try: module_path = item_obj.__module__
        except AttributeError: pass
        items.append({"name": name, "module": module_path})
    return sorted(items, key=lambda x: x["name"])


def get_flock_preview_service(filename: str) -> dict | None:
    """Loads basic properties of a flock file for preview."""
    file_path = FLOCK_FILES_DIR / filename
    if not file_path.exists():
        logger.warning(f"Service: Preview failed, file not found {file_path}")
        return None
    try:
        with file_path.open("r", encoding="utf-8") as f:
            data = yaml.safe_load(f)
            if isinstance(data, dict):
                return {
                    "name": data.get("name", filename),
                    "model": data.get("model"),
                    "description": data.get("description"),
                    "agents_count": len(data.get("agents", {})),
                    "enable_temporal": data.get("enable_temporal", False)
                }
        logger.warning(f"Service: Preview failed, '{filename}' is not a valid Flock YAML (not a dict).")
        return {"name": filename, "error": "Not a valid Flock YAML structure"}
    except Exception as e:
        logger.error(f"Service: Error getting flock preview for {filename}: {e}", exc_info=True)
        return {"name": filename, "error": str(e)}
```

### src\flock\webapp\app\services\sharing_models.py

- **Lines**: 81
- **Last modified**: 2025-05-21 19:51:15

```py
from datetime import datetime

from pydantic import BaseModel, Field


class SharedLinkConfig(BaseModel):
    """Configuration for a shared Flock agent execution link or chat session."""

    share_id: str = Field(..., description="Unique identifier for the shared link.")
    agent_name: str = Field(..., description="The name of the agent being shared (for run) or the chat agent (for chat).")
    flock_definition: str = Field(..., description="The YAML/JSON string definition of the Flock the agent belongs to.")
    created_at: datetime = Field(
        default_factory=datetime.utcnow, description="Timestamp of when the link was created."
    )
    share_type: str = Field(default="agent_run", description="Type of share: 'agent_run' or 'chat'")

    # Chat-specific settings (only relevant if share_type is 'chat')
    chat_message_key: str | None = Field(None, description="Message key for chat input mapping.")
    chat_history_key: str | None = Field(None, description="History key for chat input mapping.")
    chat_response_key: str | None = Field(None, description="Response key for chat output mapping.")

    # Placeholder for future enhancement: pre-filled input values
    # input_values: Optional[Dict[str, Any]] = Field(
    #     None, description="Optional pre-filled input values for the agent."
    # )

    model_config = {
        "from_attributes": True,
        "json_schema_extra": {
            "examples": [
                {
                    "share_id": "abcdef123456",
                    "agent_name": "MyChatAgent",
                    "flock_definition": "name: MySharedFlock\nagents:\n  MyChatAgent:\n    input: 'message: str'\n    output: 'response: str'\n    # ... rest of flock YAML ...",
                    "created_at": "2023-10-26T10:00:00Z",
                    "share_type": "chat",
                    "chat_message_key": "user_input",
                    "chat_history_key": "conversation_history",
                    "chat_response_key": "agent_output"
                }
            ]
        }
    }

# -----------------------------------------------------------
# Feedback model (user ratings / corrections)
# -----------------------------------------------------------


class FeedbackRecord(BaseModel):
    """A user-submitted piece of feedback for an agent run or chat turn."""

    feedback_id: str = Field(..., description="Unique identifier for this feedback entry.")
    share_id: str | None = Field(None, description="If the feedback refers to a shared link, its ID; otherwise None.")
    context_type: str = Field(
        default="agent_run",
        description="Where the feedback originates (agent_run | chat | other)",
    )
    reason: str = Field(..., description="User-supplied reason / comment.")
    expected_response: str | None = Field(
        None,
        description="Desired or corrected response in JSON or plain text.",
    )
    actual_response: str | None = Field(
        None,
        description="Original response shown to the user (optional, for context).",
    )
    created_at: datetime = Field(default_factory=datetime.utcnow)
    # When share_id is None store explicit target identifiers
    flock_name: str | None = Field(
        None,
        description="Name of the flock that produced the result when no share_id is used.",
    )
    agent_name: str | None = Field(
        None,
        description="Name of the agent or chat involved in the feedback.",
    )
    flock_definition: str | None = Field(
        None,
        description="Full YAML definition of the flock when feedback was submitted.",
    )
```

### src\flock\webapp\app\services\sharing_store.py

- **Lines**: 388
- **Last modified**: 2025-05-26 17:42:12

```py
"""Shared link and feedback storage implementations supporting SQLite and Azure Table Storage."""

import logging
import sqlite3
from abc import ABC, abstractmethod
from pathlib import Path

import aiosqlite

from flock.webapp.app.services.sharing_models import (
    FeedbackRecord,
    SharedLinkConfig,
)

# Azure Table Storage imports - will be conditionally imported
try:
    from azure.core.exceptions import ResourceExistsError, ResourceNotFoundError
    from azure.data.tables.aio import TableServiceClient
    AZURE_AVAILABLE = True
except ImportError:
    AZURE_AVAILABLE = False
    TableServiceClient = None
    ResourceNotFoundError = None
    ResourceExistsError = None

# Get a logger instance
logger = logging.getLogger(__name__)

class SharedLinkStoreInterface(ABC):
    """Interface for storing and retrieving shared link configurations."""

    @abstractmethod
    async def initialize(self) -> None:
        """Initialize the store (e.g., create tables)."""
        pass

    @abstractmethod
    async def save_config(self, config: SharedLinkConfig) -> SharedLinkConfig:
        """Saves a shared link configuration."""
        pass

    @abstractmethod
    async def get_config(self, share_id: str) -> SharedLinkConfig | None:
        """Retrieves a shared link configuration by its ID."""
        pass

    @abstractmethod
    async def delete_config(self, share_id: str) -> bool:
        """Deletes a shared link configuration by its ID. Returns True if deleted, False otherwise."""
        pass

    # Feedback
    @abstractmethod
    async def save_feedback(self, record: FeedbackRecord):
        """Persist a feedback record."""
        pass

class SQLiteSharedLinkStore(SharedLinkStoreInterface):
    """SQLite implementation for storing and retrieving shared link configurations."""

    def __init__(self, db_path: str):
        """Initialize SQLite store with database path."""
        self.db_path = Path(db_path)
        self.db_path.parent.mkdir(parents=True, exist_ok=True) # Ensure directory exists
        logger.info(f"SQLiteSharedLinkStore initialized with db_path: {self.db_path}")

    async def initialize(self) -> None:
        """Initializes the database and creates/updates the table if it doesn't exist."""
        try:
            async with aiosqlite.connect(self.db_path) as db:
                # Ensure the table exists with the base schema first
                await db.execute(
                    """
                    CREATE TABLE IF NOT EXISTS shared_links (
                        share_id TEXT PRIMARY KEY,
                        agent_name TEXT NOT NULL,
                        flock_definition TEXT NOT NULL,
                        created_at TEXT NOT NULL
                        /* New columns will be added below if they don't exist */
                    )
                    """
                )

                # Add new columns individually, ignoring errors if they already exist
                new_columns = [
                    ("share_type", "TEXT DEFAULT 'agent_run' NOT NULL"),
                    ("chat_message_key", "TEXT"),
                    ("chat_history_key", "TEXT"),
                    ("chat_response_key", "TEXT")
                ]

                for column_name, column_type in new_columns:
                    try:
                        await db.execute(f"ALTER TABLE shared_links ADD COLUMN {column_name} {column_type}")
                        logger.info(f"Added column '{column_name}' to shared_links table.")
                    except sqlite3.OperationalError as e:
                        if "duplicate column name" in str(e).lower():
                            logger.debug(f"Column '{column_name}' already exists in shared_links table.")
                        else:
                            raise # Re-raise if it's a different operational error

                # Feedback table
                await db.execute(
                    """
                    CREATE TABLE IF NOT EXISTS feedback (
                        feedback_id TEXT PRIMARY KEY,
                        share_id TEXT,
                        context_type TEXT NOT NULL,
                        reason TEXT NOT NULL,
                        expected_response TEXT,
                        actual_response TEXT,
                        flock_name TEXT,
                        agent_name TEXT,
                        flock_definition TEXT,
                        created_at TEXT NOT NULL,
                        FOREIGN KEY(share_id) REFERENCES shared_links(share_id)
                    )
                    """
                )

                await db.commit()
            logger.info(f"Database initialized and shared_links table schema ensured at {self.db_path}")
        except sqlite3.Error as e:
            logger.error(f"SQLite error during initialization: {e}", exc_info=True)
            raise

    async def save_config(self, config: SharedLinkConfig) -> SharedLinkConfig:
        """Saves a shared link configuration to the SQLite database."""
        try:
            async with aiosqlite.connect(self.db_path) as db:
                await db.execute(
                    """INSERT INTO shared_links (
                        share_id, agent_name, created_at, flock_definition, 
                        share_type, chat_message_key, chat_history_key, chat_response_key
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?)""",
                    (
                        config.share_id,
                        config.agent_name,
                        config.created_at.isoformat(),
                        config.flock_definition,
                        config.share_type,
                        config.chat_message_key,
                        config.chat_history_key,
                        config.chat_response_key,
                    ),
                )
                await db.commit()
            logger.info(f"Saved shared link config for ID: {config.share_id} with type: {config.share_type}")
            return config
        except sqlite3.Error as e:
            logger.error(f"SQLite error saving config for ID {config.share_id}: {e}", exc_info=True)
            raise

    async def get_config(self, share_id: str) -> SharedLinkConfig | None:
        """Retrieves a shared link configuration from SQLite by its ID."""
        try:
            async with aiosqlite.connect(self.db_path) as db:
                async with db.execute(
                    """SELECT 
                        share_id, agent_name, created_at, flock_definition, 
                        share_type, chat_message_key, chat_history_key, chat_response_key 
                    FROM shared_links WHERE share_id = ?""",
                    (share_id,)
                ) as cursor:
                    row = await cursor.fetchone()
            if row:
                logger.debug(f"Retrieved shared link config for ID: {share_id}")
                return SharedLinkConfig(
                    share_id=row[0],
                    agent_name=row[1],
                    created_at=row[2], # SQLite stores as TEXT, Pydantic will parse from ISO format
                    flock_definition=row[3],
                    share_type=row[4],
                    chat_message_key=row[5],
                    chat_history_key=row[6],
                    chat_response_key=row[7],
                )
            logger.debug(f"No shared link config found for ID: {share_id}")
            return None
        except sqlite3.Error as e:
            logger.error(f"SQLite error retrieving config for ID {share_id}: {e}", exc_info=True)
            return None # Or raise, depending on desired error handling

    async def delete_config(self, share_id: str) -> bool:
        """Deletes a shared link configuration from SQLite by its ID."""
        try:
            async with aiosqlite.connect(self.db_path) as db:
                result = await db.execute("DELETE FROM shared_links WHERE share_id = ?", (share_id,))
                await db.commit()
                deleted_count = result.rowcount
            if deleted_count > 0:
                logger.info(f"Deleted shared link config for ID: {share_id}")
                return True
            logger.info(f"Attempted to delete non-existent shared link config for ID: {share_id}")
            return False
        except sqlite3.Error as e:
            logger.error(f"SQLite error deleting config for ID {share_id}: {e}", exc_info=True)
            return False # Or raise

    # ----------------------- Feedback methods -----------------------

    async def save_feedback(self, record: FeedbackRecord) -> FeedbackRecord:
        """Persist a feedback record to SQLite."""
        try:
            async with aiosqlite.connect(self.db_path) as db:
                await db.execute(
                    """INSERT INTO feedback (
                        feedback_id, share_id, context_type, reason,
                        expected_response, actual_response, flock_name, agent_name, flock_definition, created_at
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)""",
                    (
                        record.feedback_id,
                        record.share_id,
                        record.context_type,
                        record.reason,
                        record.expected_response,
                        record.actual_response,
                        record.flock_name,
                        record.agent_name,
                        record.flock_definition,
                        record.created_at.isoformat(),
                    ),
                )
                await db.commit()
            logger.info(f"Saved feedback {record.feedback_id} (share={record.share_id})")
            return record
        except sqlite3.Error as e:
            logger.error(f"SQLite error saving feedback {record.feedback_id}: {e}", exc_info=True)
            raise

class AzureTableSharedLinkStore(SharedLinkStoreInterface):
    """Azure Table Storage implementation for storing and retrieving shared link configurations."""

    def __init__(self, connection_string: str):
        """Initialize Azure Table Storage store with connection string."""
        if not AZURE_AVAILABLE:
            raise ImportError("Azure Table Storage dependencies not available. Install with: pip install azure-data-tables")

        self.connection_string = connection_string
        self.table_service_client = TableServiceClient.from_connection_string(connection_string)
        self.shared_links_table_name = "flocksharedlinks"
        self.feedback_table_name = "flockfeedback"
        logger.info("AzureTableSharedLinkStore initialized")

    async def initialize(self) -> None:
        """Initializes the Azure Tables (creates them if they don't exist)."""
        try:
            # Create shared_links table
            try:
                await self.table_service_client.create_table(self.shared_links_table_name)
                logger.info(f"Created Azure Table: {self.shared_links_table_name}")
            except ResourceExistsError:
                logger.debug(f"Azure Table already exists: {self.shared_links_table_name}")

            # Create feedback table
            try:
                await self.table_service_client.create_table(self.feedback_table_name)
                logger.info(f"Created Azure Table: {self.feedback_table_name}")
            except ResourceExistsError:
                logger.debug(f"Azure Table already exists: {self.feedback_table_name}")

            logger.info("Azure Table Storage initialized successfully")
        except Exception as e:
            logger.error(f"Error initializing Azure Table Storage: {e}", exc_info=True)
            raise

    async def save_config(self, config: SharedLinkConfig) -> SharedLinkConfig:
        """Saves a shared link configuration to Azure Table Storage."""
        try:
            table_client = self.table_service_client.get_table_client(self.shared_links_table_name)

            entity = {
                "PartitionKey": "shared_links",  # Use a fixed partition key for simplicity
                "RowKey": config.share_id,
                "share_id": config.share_id,
                "agent_name": config.agent_name,
                "flock_definition": config.flock_definition,
                "created_at": config.created_at.isoformat(),
                "share_type": config.share_type,
                "chat_message_key": config.chat_message_key,
                "chat_history_key": config.chat_history_key,
                "chat_response_key": config.chat_response_key,
            }

            await table_client.upsert_entity(entity)
            logger.info(f"Saved shared link config to Azure Table Storage for ID: {config.share_id} with type: {config.share_type}")
            return config
        except Exception as e:
            logger.error(f"Error saving config to Azure Table Storage for ID {config.share_id}: {e}", exc_info=True)
            raise

    async def get_config(self, share_id: str) -> SharedLinkConfig | None:
        """Retrieves a shared link configuration from Azure Table Storage by its ID."""
        try:
            table_client = self.table_service_client.get_table_client(self.shared_links_table_name)

            entity = await table_client.get_entity(partition_key="shared_links", row_key=share_id)

            logger.debug(f"Retrieved shared link config from Azure Table Storage for ID: {share_id}")
            return SharedLinkConfig(
                share_id=entity["share_id"],
                agent_name=entity["agent_name"],
                created_at=entity["created_at"],  # Pydantic will parse from ISO format
                flock_definition=entity["flock_definition"],
                share_type=entity.get("share_type", "agent_run"),
                chat_message_key=entity.get("chat_message_key"),
                chat_history_key=entity.get("chat_history_key"),
                chat_response_key=entity.get("chat_response_key"),
            )
        except ResourceNotFoundError:
            logger.debug(f"No shared link config found in Azure Table Storage for ID: {share_id}")
            return None
        except Exception as e:
            logger.error(f"Error retrieving config from Azure Table Storage for ID {share_id}: {e}", exc_info=True)
            return None

    async def delete_config(self, share_id: str) -> bool:
        """Deletes a shared link configuration from Azure Table Storage by its ID."""
        try:
            table_client = self.table_service_client.get_table_client(self.shared_links_table_name)

            await table_client.delete_entity(partition_key="shared_links", row_key=share_id)
            logger.info(f"Deleted shared link config from Azure Table Storage for ID: {share_id}")
            return True
        except ResourceNotFoundError:
            logger.info(f"Attempted to delete non-existent shared link config from Azure Table Storage for ID: {share_id}")
            return False
        except Exception as e:
            logger.error(f"Error deleting config from Azure Table Storage for ID {share_id}: {e}", exc_info=True)
            return False

    # ----------------------- Feedback methods -----------------------

    async def save_feedback(self, record: FeedbackRecord) -> FeedbackRecord:
        """Persist a feedback record to Azure Table Storage."""
        try:
            table_client = self.table_service_client.get_table_client(self.feedback_table_name)

            entity = {
                "PartitionKey": "feedback",  # Use a fixed partition key for simplicity
                "RowKey": record.feedback_id,
                "feedback_id": record.feedback_id,
                "share_id": record.share_id,
                "context_type": record.context_type,
                "reason": record.reason,
                "expected_response": record.expected_response,
                "actual_response": record.actual_response,
                "flock_name": record.flock_name,
                "agent_name": record.agent_name,
                "flock_definition": record.flock_definition,
                "created_at": record.created_at.isoformat(),
            }

            await table_client.upsert_entity(entity)
            logger.info(f"Saved feedback to Azure Table Storage: {record.feedback_id} (share={record.share_id})")
            return record
        except Exception as e:
            logger.error(f"Error saving feedback to Azure Table Storage {record.feedback_id}: {e}", exc_info=True)
            raise


# ----------------------- Factory Function -----------------------

def create_shared_link_store(store_type: str | None = None, connection_string: str | None = None) -> SharedLinkStoreInterface:
    """Factory function to create the appropriate shared link store based on configuration.
    
    Args:
        store_type: Type of store to create ("local" for SQLite, "azure-storage" for Azure Table Storage)
        connection_string: Connection string for the store (file path for SQLite, connection string for Azure)
    
    Returns:
        Configured SharedLinkStoreInterface implementation
    """
    import os

    # Get values from environment if not provided
    if store_type is None:
        store_type = os.getenv("FLOCK_WEBAPP_STORE", "local").lower()

    if connection_string is None:
        connection_string = os.getenv("FLOCK_WEBAPP_STORE_CONNECTION", ".flock/shared_links.db")

    if store_type == "local":
        return SQLiteSharedLinkStore(connection_string)
    elif store_type == "azure-storage":
        return AzureTableSharedLinkStore(connection_string)
    else:
        raise ValueError(f"Unsupported store type: {store_type}. Supported types: 'local', 'azure-storage'")
```

### src\flock\webapp\app\utils.py

- **Lines**: 85
- **Last modified**: 2025-05-09 01:11:35

```py
import inspect
import json
from typing import Any

def is_pydantic_model(obj: Any) -> bool:
    """
    Check if an object is a Pydantic model instance.
    More robust detection for both v1 and v2 Pydantic models.
    """
    # Check for Pydantic v2 model
    if hasattr(obj, "__class__") and hasattr(obj.__class__, "model_dump"):
        return True
    
    # Check for Pydantic v1 model
    if hasattr(obj, "__class__") and hasattr(obj.__class__, "schema") and hasattr(obj, "dict"):
        return True
    
    # Check if it has a __pydantic_core__ attribute (v2 models)
    if hasattr(obj, "__pydantic_core__"):
        return True
        
    # Final check: class name check and module check
    if hasattr(obj, "__class__"):
        cls = obj.__class__
        cls_name = cls.__name__
        module_name = getattr(cls, "__module__", "")
        if "pydantic" in module_name.lower() or "basemodel" in cls_name.lower():
            return True
    
    return False

def is_json_serializable(obj: Any) -> bool:
    """Check if an object can be serialized to JSON."""
    try:
        json.dumps(obj)
        return True
    except (TypeError, ValueError):
        return False

def pydantic_to_dict(obj: Any) -> Any:
    """
    Recursively convert Pydantic models to dictionaries.
    Works with nested models, lists, and dictionaries containing models.
    Falls back to string representation if object can't be serialized.
    """
    if obj is None:
        return None
        
    if is_pydantic_model(obj):
        # Handle Pydantic v2 models
        if hasattr(obj, "model_dump"):
            return obj.model_dump()
        # Handle Pydantic v1 models
        elif hasattr(obj, "dict"):
            return obj.dict()
        # Last resort - try __dict__ if it exists
        elif hasattr(obj, "__dict__"):
            return {k: pydantic_to_dict(v) for k, v in obj.__dict__.items() 
                   if not k.startswith("_")}
    
    elif isinstance(obj, dict):
        # Handle dictionaries that might contain Pydantic models
        return {k: pydantic_to_dict(v) for k, v in obj.items()}
    
    elif isinstance(obj, list):
        # Handle lists that might contain Pydantic models
        return [pydantic_to_dict(item) for item in obj]
    
    elif isinstance(obj, tuple) and hasattr(obj, '_fields'):
        # Handle namedtuples
        return dict(zip(obj._fields, (pydantic_to_dict(item) for item in obj)))
    
    elif isinstance(obj, (list, tuple)):
        # Handle regular lists and tuples
        return [pydantic_to_dict(item) for item in obj]
    
    # Final check - if object is not JSON serializable, convert to string
    if not is_json_serializable(obj):
        try:
            return str(obj)
        except Exception:
            return f"<Unserializable object of type {type(obj).__name__}>"
    
    # Return other types unchanged - these should be JSON serializable by default
    return obj 
```

### src\flock\webapp\run.py

- **Lines**: 215
- **Last modified**: 2025-05-26 14:21:32

```py
# src/flock/webapp/run.py
import sys
from collections.abc import Callable, Sequence
from pathlib import Path
from typing import TYPE_CHECKING, Any

import uvicorn

# Import core Flock components
if TYPE_CHECKING:
    from flock.core.api.custom_endpoint import FlockEndpoint
    from flock.core.flock import Flock

# --- Ensure src is in path for imports ---
current_file_path = Path(__file__).resolve()
flock_webapp_dir = current_file_path.parent
flock_dir = flock_webapp_dir.parent
src_dir = flock_dir.parent # Assuming `flock` is a package within `src`

if str(src_dir) not in sys.path:
    sys.path.insert(0, str(src_dir))

# --- Main Server Startup Function ---
def start_unified_server(
    flock_instance: "Flock",
    host: str,
    port: int,
    server_title: str,
    enable_ui_routes: bool,
    enable_chat_routes: bool = False,
    ui_theme: str | None = None,
    custom_endpoints: Sequence["FlockEndpoint"] | dict[tuple[str, list[str] | None], Callable[..., Any]] | None = None,
):
    """Starts the unified FastAPI server for Flock.
    - Initializes the web application (imported from webapp.app.main).
    - Sets the provided Flock instance and a RunStore for dependency injection
      and makes them available via app.state.
    - Configures the UI theme.
    - Stores custom API endpoints for registration during app lifespan startup.
    - Optionally registers chat routes.
    - Runs Uvicorn.
    """
    print(f"Attempting to start unified server for Flock '{flock_instance.name}' on http://{host}:{port}")
    print(f"UI Routes Enabled: {enable_ui_routes}, Theme: {ui_theme or 'Default'}")

    try:
        # Import necessary webapp components HERE, after path setup.
        from flock.core.api.run_store import RunStore
        from flock.core.logging.logging import get_logger  # For logging
        from flock.webapp.app.config import (  # For logging resolved theme
            get_current_theme_name,
            set_current_theme_name,
        )
        from flock.webapp.app.dependencies import (
            add_pending_custom_endpoints,
            set_global_flock_services,
        )
        from flock.webapp.app.main import (
            app as fastapi_app,  # The single FastAPI app instance
        )

        logger = get_logger("webapp.run") # Use a logger

        # 1. Set UI Theme globally for the webapp
        set_current_theme_name(ui_theme)
        logger.info(f"Unified server configured to use theme: {get_current_theme_name()}")

        # 2. Create RunStore & Set Global Services for Dependency Injection
        run_store_instance = RunStore()
        set_global_flock_services(flock_instance, run_store_instance)
        logger.info("Global Flock instance and RunStore set for dependency injection.")

        # 3. Make Flock instance and filename available on app.state
        fastapi_app.state.flock_instance = flock_instance
        source_file_attr = "_source_file_path" # Attribute where Flock might store its load path
        fastapi_app.state.flock_filename = getattr(flock_instance, source_file_attr, None) or \
                                           f"{flock_instance.name.replace(' ', '_').lower()}.flock.yaml"
        fastapi_app.state.run_store = run_store_instance
        fastapi_app.state.chat_enabled = enable_chat_routes

        logger.info(f"Flock '{flock_instance.name}' (from '{fastapi_app.state.flock_filename}') made available via app.state.")

        # 4. Store Custom Endpoints for registration by the lifespan manager in app.main
        processed_custom_endpoints = []
        if custom_endpoints:
            from flock.core.api.custom_endpoint import (
                FlockEndpoint,  # Ensure it's imported
            )
            if isinstance(custom_endpoints, dict):
                for (path_val, methods_val), cb_val in custom_endpoints.items():
                    processed_custom_endpoints.append(
                        FlockEndpoint(path=path_val, methods=list(methods_val) if methods_val else ["GET"], callback=cb_val)
                    )
            else: # Assumed Sequence[FlockEndpoint]
                processed_custom_endpoints.extend(list(custom_endpoints))

        if processed_custom_endpoints:
            add_pending_custom_endpoints(processed_custom_endpoints)
            logger.info(f"{len(processed_custom_endpoints)} custom endpoints stored for registration by app lifespan.")

        # 5. Update FastAPI app title (FastAPI app instance is now imported from main)
        fastapi_app.title = server_title

        # 5a. Optionally strip UI routes if UI is disabled
        if not enable_ui_routes:
            from fastapi.routing import APIRoute

            allowed_tags = {"Flock API Core", "Flock API Custom Endpoints", "Chat"}

            def _route_is_allowed(route: APIRoute) -> bool:  # type: ignore
                # Keep documentation and non-API utility routes (no tags)
                if not hasattr(route, "tags") or not route.tags:
                    return True
                # Keep if any tag is in the allowed list
                return any(tag in allowed_tags for tag in route.tags)  # type: ignore

            original_count = len(fastapi_app.router.routes)
            fastapi_app.router.routes = [r for r in fastapi_app.router.routes if _route_is_allowed(r)]

            # Clear cached OpenAPI schema so FastAPI regenerates it with the reduced route set
            if hasattr(fastapi_app, "openapi_schema"):
                fastapi_app.openapi_schema = None  # type: ignore

            logger.info(
                f"UI disabled: removed {original_count - len(fastapi_app.router.routes)} UI routes. Remaining routes: {len(fastapi_app.router.routes)}"
            )

        # 5b. Include Chat routes if requested
        if enable_chat_routes:
            try:
                from flock.webapp.app.chat import (
                    router as chat_router,  # type: ignore
                )
                fastapi_app.include_router(chat_router, tags=["Chat"])
                logger.info("Chat routes enabled and registered.")
            except Exception as e:
                logger.error(f"Failed to include chat routes: {e}")

        # 6. Run Uvicorn
        logger.info(f"Running Uvicorn with application: flock.webapp.app.main:app")
        uvicorn.run(
            "flock.webapp.app.main:app",
            host=host,
            port=port,
            reload=False, # Critical for programmatically set state like flock_instance
       #     root_path=os.getenv("FLOCK_ROOT_PATH", "")
        )

    except ImportError as e:
        # More specific error logging
        print(f"CRITICAL: Error importing components for unified server: {e}", file=sys.stderr)
        print(f"Module not found: {e.name}", file=sys.stderr)
        print("This usually means a problem with sys.path or missing dependencies.", file=sys.stderr)
        print(f"Current sys.path: {sys.path}", file=sys.stderr)
        sys.exit(1)
    except Exception as e:
        print(f"CRITICAL: Error starting unified server: {e}", file=sys.stderr)
        # Consider logging the full traceback for easier debugging
        import traceback
        traceback.print_exc(file=sys.stderr)
        sys.exit(1)


# --- Standalone Webapp Runner (for `flock --web` or direct execution `python -m flock.webapp.run`) ---
def main():
    """Runs the Flock web application in standalone mode.
    In this mode, no specific Flock is pre-loaded by the startup script;
    the user will load or create one via the UI.
    The FastAPI app (`webapp.app.main:app`) will initialize with DI services
    set to None for Flock, and a new RunStore.
    """
    print("Starting Flock web application in standalone mode...")

    from flock.core.api.run_store import RunStore
    from flock.webapp.app.config import (
        get_current_theme_name,  # To log the theme being used
    )
    from flock.webapp.app.dependencies import set_global_flock_services

    # No pre-loaded Flock instance; create a RunStore so API calls can still function
    standalone_run_store = RunStore()
    set_global_flock_services(None, standalone_run_store)

    print(
        f"Standalone mode: Initialized global services. Flock: None, RunStore: {type(standalone_run_store)}"
    )
    print(f"Standalone webapp using theme: {get_current_theme_name()}")

    host = "127.0.0.1"
    port = 8344
    try:
        import os

        host = os.environ.get("FLOCK_WEB_HOST", host)
        port = int(os.environ.get("FLOCK_WEB_PORT", port))
        webapp_reload = os.environ.get("FLOCK_WEB_RELOAD", "true").lower() == "true"
    except Exception:
        webapp_reload = True

    app_import_string = "flock.webapp.app.main:app"
    print(
        f"Running Uvicorn: app='{app_import_string}', host='{host}', port={port}, reload={webapp_reload}"
    )

    uvicorn.run(
        app_import_string,
        host=host,
        port=port,
        reload=webapp_reload,
       # root_path=os.getenv("FLOCK_ROOT_PATH", "")
    )


if __name__ == "__main__":
    main()
```

### src\flock\workflow\__init__.py

- **Lines**: 0
- **Last modified**: 2025-04-16 00:11:15

```py

```

### src\flock\workflow\activities.py

- **Lines**: 221
- **Last modified**: 2025-05-21 19:51:15

```py
"""Defines Temporal activities for running a chain of agents with logging and tracing."""

from datetime import datetime

from opentelemetry import trace
from temporalio import activity

from flock.core.context.context import FlockContext
from flock.core.context.context_vars import FLOCK_CURRENT_AGENT, FLOCK_MODEL
from flock.core.flock_agent import FlockAgent
from flock.core.flock_registry import get_registry
from flock.core.flock_router import HandOffRequest
from flock.core.logging.logging import get_logger
from flock.core.util.input_resolver import resolve_inputs

logger = get_logger("activities")
tracer = trace.get_tracer(__name__)


@activity.defn
async def run_agent(context: FlockContext) -> dict:
    """Runs a chain of agents using the provided context.

    The context contains state, history, and agent definitions.
    After each agent run, its output is merged into the context.
    """
    # Start a top-level span for the entire run_agent activity.
    with tracer.start_as_current_span("run_agent") as span:
        registry = get_registry()

        previous_agent_name = ""
        if isinstance(context, dict):
            context = FlockContext.from_dict(context)
        current_agent_name = context.get_variable(FLOCK_CURRENT_AGENT)
        span.set_attribute("initial.agent", current_agent_name)
        logger.info("Starting agent chain", initial_agent=current_agent_name)

        agent = registry.get_agent(current_agent_name)
        if agent.model is None or agent.evaluator.config.model is None:
            agent.set_model(context.get_variable(FLOCK_MODEL))
        agent.resolve_callables(context=context)
        if not agent:
            logger.error("Agent not found", agent=current_agent_name)
            span.record_exception(
                Exception(f"Agent '{current_agent_name}' not found")
            )
            return {"error": f"Agent '{current_agent_name}' not found."}

        # Loop over agents in the chain.
        while agent:
            # Create a nested span for this iteration.
            with tracer.start_as_current_span("agent_iteration") as iter_span:
                iter_span.set_attribute("agent.name", agent.name)
                agent.context = context
                # Resolve inputs for the agent.
                agent_inputs = resolve_inputs(
                    agent.input, context, previous_agent_name
                )
                iter_span.add_event(
                    "resolved inputs", attributes={"inputs": str(agent_inputs)}
                )

                # Execute the agent with its own span.
                with tracer.start_as_current_span("execute_agent") as exec_span:
                    logger.info("Executing agent", agent=agent.name)
                    try:
                        result = await agent.run_async(agent_inputs)
                        exec_span.set_attribute("result", str(result))
                        logger.debug(
                            "Agent execution completed", agent=agent.name
                        )
                    except Exception as e:
                        logger.error(
                            "Agent execution failed",
                            agent=agent.name,
                            error=str(e),
                        )
                        exec_span.record_exception(e)
                        raise

                # Determine the next agent using the handoff router if available
                handoff_data = HandOffRequest()

                if agent.handoff_router:
                    logger.info(
                        f"Using handoff router: {agent.handoff_router.__class__.__name__}",
                        agent=agent.name,
                    )
                    try:
                        # Route to the next agent
                        handoff_data = await agent.handoff_router.route(
                            agent, result, context
                        )

                        if callable(handoff_data):
                            logger.debug(
                                "Executing handoff function", agent=agent.name
                            )
                            try:
                                handoff_data = handoff_data(context, result)
                                if isinstance(
                                    handoff_data.next_agent, FlockAgent
                                ):
                                    handoff_data.next_agent = (
                                        handoff_data.next_agent.name
                                    )
                            except Exception as e:
                                logger.error(
                                    "Handoff function error {} {}",
                                    agent=agent.name,
                                    error=str(e),
                                )
                                iter_span.record_exception(e)
                                return {"error": f"Handoff function error: {e}"}
                        elif isinstance(handoff_data.next_agent, FlockAgent):
                            handoff_data.next_agent = (
                                handoff_data.next_agent.name
                            )

                        if not handoff_data.next_agent:
                            logger.info(
                                "Router found no suitable next agent",
                                agent=agent.name,
                            )
                            context.record(
                                agent.name,
                                result,
                                timestamp=datetime.now().isoformat(),
                                hand_off=None,
                                called_from=previous_agent_name,
                            )
                            logger.info("Completing chain", agent=agent.name)
                            iter_span.add_event("chain completed")
                            return result
                    except Exception as e:
                        logger.error(
                            "Router error {} {}",
                            agent.name,
                            str(e),
                        )
                        iter_span.record_exception(e)
                        return {"error": f"Router error: {e}"}
                else:
                    # No router, so no handoff
                    logger.info(
                        "No handoff router defined, completing chain",
                        agent=agent.name,
                    )
                    context.record(
                        agent.name,
                        result,
                        timestamp=datetime.now().isoformat(),
                        hand_off=None,
                        called_from=previous_agent_name,
                    )
                    iter_span.add_event("chain completed")
                    return result

                # Record the agent run in the context.
                context.record(
                    agent.name,
                    result,
                    timestamp=datetime.now().isoformat(),
                    hand_off=handoff_data.model_dump(),
                    called_from=previous_agent_name,
                )
                previous_agent_name = agent.name
                previous_agent_output = agent.output
                if handoff_data.override_context:
                    context.update(handoff_data.override_context)

                # Prepare the next agent.
                try:
                    agent = registry.get_agent(handoff_data.next_agent)
                    if handoff_data.output_to_input_merge_strategy == "add":
                        agent.input = previous_agent_output + ", " + agent.input

                    if handoff_data.add_input_fields:
                        for field in handoff_data.add_input_fields:
                            agent.input = field + ", " + agent.input

                    if handoff_data.add_output_fields:
                        for field in handoff_data.add_output_fields:
                            agent.output = field + ", " + agent.output

                    if handoff_data.add_description:
                        if agent.description:
                            agent.description = (
                                agent.description
                                + "\n"
                                + handoff_data.add_description
                            )
                        else:
                            agent.description = handoff_data.add_description

                    agent.resolve_callables(context=context)
                    if not agent:
                        logger.error(
                            "Next agent not found",
                            agent=handoff_data.next_agent,
                        )
                        iter_span.record_exception(
                            Exception(
                                f"Next agent '{handoff_data.next_agent}' not found"
                            )
                        )
                        return {
                            "error": f"Next agent '{handoff_data.next_agent}' not found."
                        }

                    context.set_variable(FLOCK_CURRENT_AGENT, agent.name)

                    logger.info("Handing off to next agent", next=agent.name)
                    iter_span.set_attribute("next.agent", agent.name)
                except Exception as e:
                    logger.error("Error during handoff", error=str(e))
                    iter_span.record_exception(e)
                    return {"error": f"Error during handoff: {e}"}

        # If the loop exits unexpectedly, return the initial input.
        return context.get_variable("init_input")
```

### src\flock\workflow\agent_activities.py

- **Lines**: 24
- **Last modified**: 2025-04-16 00:11:15

```py
from temporalio import activity

from flock.core.context.context import FlockContext
from flock.core.flock_agent import FlockAgent


@activity.defn
async def run_declarative_agent_activity(params: dict) -> dict:
    """Temporal activity to run a declarative (or batch) agent.

    Expects a dictionary with:
      - "agent_data": a dict representation of the agent (as produced by .dict()),
      - "context_data": a dict containing the FlockContext state and optionally other fields.

    The activity reconstructs the agent and a FlockContext, then calls the agent’s _evaluate() method.
    """
    agent_data = params.get("agent_data")
    context_data = params.get("context_data", {})
    # Reconstruct the agent from its serialized representation.
    agent = FlockAgent.from_dict(agent_data)
    # Reconstruct the FlockContext from the state.
    context = FlockContext.from_dict(context_data)
    result = await agent.evaluate(context)
    return result
```

### src\flock\workflow\agent_execution_activity.py

- **Lines**: 228
- **Last modified**: 2025-04-19 03:51:50

```py
"""Defines granular Temporal activities for executing a single agent
and determining the next agent in a Flock workflow.
"""

from collections.abc import Callable

from opentelemetry import trace
from temporalio import activity

# Third-party imports only within activity functions if needed, or pass context
# For core flock types, import directly
from flock.core.context.context import FlockContext
from flock.core.context.context_vars import FLOCK_MODEL
from flock.core.flock_agent import FlockAgent  # Import concrete class if needed
from flock.core.flock_registry import get_registry
from flock.core.flock_router import HandOffRequest
from flock.core.logging.logging import get_logger
from flock.core.util.input_resolver import resolve_inputs

logger = get_logger("agent_activity")  # Using a distinct logger category
tracer = trace.get_tracer(__name__)
registry = get_registry()  # Get registry instance once


@activity.defn
async def execute_single_agent(agent_name: str, context: FlockContext) -> dict:
    """Executes a single specified agent and returns its result.

    Args:
        agent_name: The name of the agent to execute.
        context: The current FlockContext (passed from the workflow).

    Returns:
        The raw result dictionary from the agent's execution.

    Raises:
        ValueError: If the agent is not found in the registry.
        Exception: Propagates exceptions from agent execution for Temporal retries.
    """
    with tracer.start_as_current_span("execute_single_agent") as span:
        span.set_attribute("agent.name", agent_name)
        logger.info("Executing single agent", agent=agent_name)

        agent = registry.get_agent(agent_name)
        if not agent:
            logger.error("Agent not found in registry", agent=agent_name)
            # Raise error for Temporal to potentially retry/fail the activity
            raise ValueError(f"Agent '{agent_name}' not found in registry.")

        # Set agent's context reference (transient, for this execution)
        agent.context = context

        # Ensure model is set (using context value if needed)
        # Consider if this should be done once when agent is added or workflow starts
        if agent.model is None:
            agent_model = context.get_variable(FLOCK_MODEL)
            if agent_model:
                agent.set_model(agent_model)
                logger.debug(
                    f"Set model for agent '{agent_name}' from context: {agent_model}"
                )

        # Resolve agent-specific callables if necessary
        # This might be better handled in the workflow before the loop starts
        # or when agents are initially loaded. Assuming it's handled elsewhere for now.
        # agent.resolve_callables(context=context)

        # Resolve inputs for this specific agent run
        previous_agent_name = (
            context.get_last_agent_name()
        )  # Relies on context method
        logger.debug(
            f"Resolving inputs for {agent_name} with previous agent {previous_agent_name}"
        )
        agent_inputs = resolve_inputs(agent.input, context, previous_agent_name)
        span.add_event(
            "resolved inputs", attributes={"inputs": str(agent_inputs)}
        )

        try:
            # Execute just this agent
            result = await agent.run_async(agent_inputs)
            # Avoid logging potentially large results directly to span attributes
            result_str = str(result)
            span.set_attribute("result.type", type(result).__name__)
            span.set_attribute(
                "result.preview",
                result_str[:500] + ("..." if len(result_str) > 500 else ""),
            )
            logger.info("Single agent execution completed", agent=agent_name)
            return result
        except Exception as e:
            logger.error(
                "Single agent execution failed",
                agent=agent_name,
                error=str(e),
                exc_info=True,
            )
            span.record_exception(e)
            # Re-raise the exception for Temporal to handle based on retry policy
            raise


@activity.defn
async def determine_next_agent(
    current_agent_name: str, result: dict, context: FlockContext
) -> dict | None:
    """Determines the next agent using the current agent's handoff router.

    Args:
        current_agent_name: The name of the agent that just ran.
        result: The result produced by the current agent.
        context: The current FlockContext.

    Returns:
        A dictionary representing the HandOffRequest (serialized via model_dump),
        or None if no handoff occurs or router doesn't specify a next agent.

    Raises:
        ValueError: If the current agent cannot be found.
        Exception: Propagates exceptions from router execution for Temporal retries.
    """
    with tracer.start_as_current_span("determine_next_agent") as span:
        span.set_attribute("agent.name", current_agent_name)
        logger.info("Determining next agent after", agent=current_agent_name)

        agent = registry.get_agent(current_agent_name)
        if not agent:
            logger.error(
                "Agent not found for routing", agent=current_agent_name
            )
            raise ValueError(
                f"Agent '{current_agent_name}' not found for routing."
            )

        if not agent.handoff_router:
            logger.info(
                "No handoff router defined for agent", agent=current_agent_name
            )
            span.add_event("no_router")
            return None  # Indicate no handoff

        logger.debug(
            f"Using router {agent.handoff_router.__class__.__name__}",
            agent=agent.name,
        )
        try:
            # Execute the routing logic
            handoff_data: (
                HandOffRequest | Callable
            ) = await agent.handoff_router.route(agent, result, context)

            # Handle callable handoff functions - This is complex in distributed systems.
            # Consider if this pattern should be supported or if routing should always
            # return serializable data directly. Executing arbitrary code from context
            # within an activity can have side effects and security implications.
            # Assuming for now it MUST return HandOffRequest or structure convertible to it.
            if callable(handoff_data):
                logger.warning(
                    "Callable handoff detected - executing function.",
                    agent=agent.name,
                )
                # Ensure context is available if the callable needs it
                try:
                    handoff_data = handoff_data(
                        context, result
                    )  # Potential side effects
                    if not isinstance(handoff_data, HandOffRequest):
                        logger.error(
                            "Handoff function did not return a HandOffRequest object.",
                            agent=agent.name,
                        )
                        raise TypeError(
                            "Handoff function must return a HandOffRequest object."
                        )
                except Exception as e:
                    logger.error(
                        "Handoff function execution failed",
                        agent=agent.name,
                        error=str(e),
                        exc_info=True,
                    )
                    span.record_exception(e)
                    raise  # Propagate error

            # Ensure we have a HandOffRequest object after potentially calling function
            if not isinstance(handoff_data, HandOffRequest):
                logger.error(
                    "Router returned unexpected type",
                    type=type(handoff_data).__name__,
                    agent=agent.name,
                )
                raise TypeError(
                    f"Router for agent '{agent.name}' did not return a HandOffRequest object."
                )

            # Ensure agent instance is converted to name for serialization across boundaries
            if isinstance(handoff_data.next_agent, FlockAgent):
                handoff_data.next_agent = handoff_data.next_agent.name

            # If router logic determines no further agent, return None
            if not handoff_data.next_agent:
                logger.info("Router determined no next agent", agent=agent.name)
                span.add_event("no_next_agent_from_router")
                return None

            logger.info(
                "Handoff determined",
                next_agent=handoff_data.next_agent,
                agent=agent.name,
            )
            span.set_attribute("next_agent", handoff_data.next_agent)
            # Return the serializable HandOffRequest data using Pydantic's export method
            return handoff_data.model_dump(
                mode="json"
            )  # Ensure JSON-serializable

        except Exception as e:
            # Catch potential errors during routing execution
            logger.error(
                "Router execution failed",
                agent=agent.name,
                error=str(e),
                exc_info=True,
            )
            span.record_exception(e)
            # Let Temporal handle the activity failure based on retry policy
            raise
```

### src\flock\workflow\flock_workflow.py

- **Lines**: 225
- **Last modified**: 2025-04-19 03:53:31

```py
from datetime import timedelta
from typing import Any

from temporalio import workflow

# Import activities from the new file
with workflow.unsafe.imports_passed_through():
    from flock.core.context.context import AgentDefinition, FlockContext
    from flock.core.context.context_vars import FLOCK_CURRENT_AGENT
    from flock.core.flock_router import HandOffRequest
    from flock.core.logging.logging import get_logger
    from flock.workflow.agent_execution_activity import (
        determine_next_agent,
        execute_single_agent,
    )
    from flock.workflow.temporal_config import (
        TemporalActivityConfig,
        TemporalRetryPolicyConfig,
    )


logger = get_logger("workflow")


@workflow.defn
class FlockWorkflow:
    # No need for __init__ storing context anymore if passed to run

    @workflow.run
    async def run(self, workflow_args: dict[str, Any]) -> dict:
        # --- Workflow Initialization ---
        # Arguments are packed into a single dictionary
        context_dict = workflow_args["context_dict"]
        default_retry_config_dict = workflow_args["default_retry_config_dict"]

        # Deserialize context and default retry config
        context = FlockContext.from_dict(context_dict)
        default_retry_config = TemporalRetryPolicyConfig.model_validate(
            default_retry_config_dict
        )

        context.workflow_id = workflow.info().workflow_id
        context.workflow_timestamp = workflow.info().start_time.strftime(
            "%Y-%m-%d %H:%M:%S"
        )

        current_agent_name = context.get_variable(FLOCK_CURRENT_AGENT)
        final_result = None
        previous_agent_name = (
            None  # Keep track of the agent that called the current one
        )

        logger.info(
            "Starting workflow execution",
            workflow_id=context.workflow_id,
            start_time=context.workflow_timestamp,
            initial_agent=current_agent_name,
        )

        try:
            while current_agent_name:
                logger.info(
                    "Executing agent activity", agent=current_agent_name
                )

                # --- Determine Activity Settings ---
                agent_def: AgentDefinition | None = (
                    context.get_agent_definition(current_agent_name)
                )
                agent_activity_config: TemporalActivityConfig | None = None
                final_retry_config = (
                    default_retry_config  # Start with the workflow default
                )

                if agent_def and agent_def.agent_data.get(
                    "temporal_activity_config"
                ):
                    try:
                        agent_activity_config = (
                            TemporalActivityConfig.model_validate(
                                agent_def.agent_data["temporal_activity_config"]
                            )
                        )
                        logger.debug(
                            f"Loaded agent-specific temporal config for {current_agent_name}"
                        )
                    except Exception as e:
                        logger.warn(
                            f"Failed to validate agent temporal config for {current_agent_name}: {e}. Using defaults."
                        )

                # Layering logic: Agent config overrides workflow default config
                activity_task_queue = (
                    workflow.info().task_queue
                )  # Default to workflow task queue
                activity_timeout = timedelta(
                    minutes=5
                )  # Fallback default timeout

                if agent_activity_config:
                    activity_task_queue = (
                        agent_activity_config.task_queue or activity_task_queue
                    )
                    activity_timeout = (
                        agent_activity_config.start_to_close_timeout
                        or activity_timeout
                    )
                    if agent_activity_config.retry_policy:
                        final_retry_config = agent_activity_config.retry_policy

                # Convert config to actual Temporal object
                final_retry_policy = final_retry_config.to_temporalio_policy()

                logger.debug(
                    f"Final activity settings for {current_agent_name}: "
                    f"queue='{activity_task_queue}', timeout={activity_timeout}, "
                    f"retries={final_retry_policy.maximum_attempts}"
                )

                # --- Execute the current agent activity ---
                agent_result = await workflow.execute_activity(
                    execute_single_agent,
                    args=[current_agent_name, context],
                    task_queue=activity_task_queue,  # Use determined task queue
                    start_to_close_timeout=activity_timeout,  # Use determined timeout
                    retry_policy=final_retry_policy,  # Use determined retry policy
                )

                # Record the execution in the context history
                # Note: The 'called_from' is the agent *before* this one
                context.record(
                    agent_name=current_agent_name,
                    data=agent_result,
                    timestamp=workflow.now().isoformat(),  # Use deterministic workflow time
                    hand_off=None,  # Will be updated if handoff occurs
                    called_from=previous_agent_name,  # Pass the correct previous agent
                )

                final_result = agent_result  # Store the result of the last successful agent

                logger.info(
                    "Determining next agent activity",
                    current_agent=current_agent_name,
                )
                # --- Determine the next agent activity (using workflow defaults for now) ---
                # We could apply similar config logic to determine_next_agent if needed
                handoff_data_dict = await workflow.execute_activity(
                    determine_next_agent,
                    args=[current_agent_name, agent_result, context],
                    # Using sensible defaults, but could be configured via workflow_config?
                    start_to_close_timeout=timedelta(minutes=1),
                    retry_policy=default_retry_config.to_temporalio_policy(),  # Use default retry
                )

                # Update previous agent name for the next loop iteration
                previous_agent_name = current_agent_name

                if handoff_data_dict:
                    logger.debug(
                        "Handoff data received", data=handoff_data_dict
                    )
                    # Deserialize handoff data back into Pydantic model for easier access
                    handoff_request = HandOffRequest.model_validate(
                        handoff_data_dict
                    )

                    # Update context based on handoff overrides
                    if handoff_request.override_context:
                        context.state.update(handoff_request.override_context)
                        logger.info("Context updated based on handoff override")

                    # Update the last record's handoff information
                    if context.history:
                        context.history[-1].hand_off = handoff_data_dict

                    # Set the next agent
                    current_agent_name = handoff_request.next_agent
                    if current_agent_name:
                        context.set_variable(
                            FLOCK_CURRENT_AGENT, current_agent_name
                        )
                        logger.info("Next agent set", agent=current_agent_name)
                    else:
                        logger.info(
                            "Handoff requested termination (no next agent)"
                        )
                        break  # Exit loop if router explicitly returned no next agent

                else:
                    # No handoff data returned (no router or router returned None)
                    logger.info("No handoff occurred, workflow terminating.")
                    current_agent_name = None  # End the loop

            # --- Workflow Completion ---
            logger.success(
                "Workflow completed successfully",
                final_agent=previous_agent_name,
            )
            context.set_variable(
                "flock.result",
                {
                    "result": final_result,  # Return the last agent's result
                    "success": True,
                },
            )
            return final_result  # Return the actual result of the last agent

        except Exception as e:
            # Catch exceptions from activities (e.g., after retries fail)
            # or workflow logic errors
            logger.exception("Workflow execution failed", error=str(e))
            context.set_variable(
                "flock.result",
                {
                    "result": f"Workflow failed: {e}",
                    "success": False,
                },
            )
            # It's often better to let Temporal record the failure status
            # by re-raising the exception rather than returning a custom error dict.
            # However, returning the context might be useful for debugging.
            # Consider re-raising: raise
            return context.model_dump(
                mode="json"
            )  # Return context state on failure
```

### src\flock\workflow\temporal_config.py

- **Lines**: 96
- **Last modified**: 2025-04-19 03:53:31

```py
# src/flock/config/temporal_config.py

"""Pydantic models for configuring Temporal execution settings."""

from __future__ import annotations

from datetime import timedelta
from typing import TYPE_CHECKING

# Conditionally import for type hinting only
if TYPE_CHECKING:
    from temporalio.common import RetryPolicy

# Note: Importing temporalio types directly into config models can complicate serialization
# if these models are meant to be purely data containers (e.g., for YAML/JSON).
# We define the structure and provide a helper method to convert to the actual Temporal object.
# Be careful if using workflow/activity decorators directly on methods within these config models.
from pydantic import BaseModel, Field


class TemporalRetryPolicyConfig(BaseModel):
    """Configuration parameters for Temporal Retry Policies."""

    initial_interval: timedelta = Field(
        default=timedelta(seconds=1),
        description="Initial delay before the first retry.",
    )
    backoff_coefficient: float = Field(
        default=2.0, description="Multiplier for the delay between retries."
    )
    maximum_interval: timedelta | None = Field(
        default=timedelta(seconds=100),
        description="Maximum delay between retries.",
    )
    maximum_attempts: int = Field(
        default=3,
        description="Maximum number of retry attempts (0 means no retries after first failure).",
    )
    non_retryable_error_types: list[str] = Field(
        default_factory=list,
        description="List of error type names (strings) that should not be retried.",
    )

    # Helper to convert to actual Temporalio object when needed (e.g., in workflow/executor)
    def to_temporalio_policy(self) -> RetryPolicy:
        # Import locally to avoid making temporalio a hard dependency of the config module itself
        # The type hint RetryPolicy is now available due to TYPE_CHECKING block
        from temporalio.common import RetryPolicy

        return RetryPolicy(
            initial_interval=self.initial_interval,
            backoff_coefficient=self.backoff_coefficient,
            maximum_interval=self.maximum_interval,
            maximum_attempts=self.maximum_attempts,
            non_retryable_error_types=self.non_retryable_error_types,
        )


class TemporalWorkflowConfig(BaseModel):
    """Configuration specific to Temporal Workflow Execution for a Flock."""

    task_queue: str = Field(
        default="flock-queue",
        description="Default task queue for the workflow execution.",
    )
    workflow_execution_timeout: timedelta | None = Field(
        default=None,  # Default to no timeout (Temporal server default)
        description="Total time limit for the workflow execution.",
    )
    workflow_run_timeout: timedelta | None = Field(
        default=None,  # Default to no timeout (Temporal server default)
        description="Time limit for a single workflow run attempt.",
    )
    # Default retry policy for activities if not specified per-agent
    default_activity_retry_policy: TemporalRetryPolicyConfig = Field(
        default_factory=TemporalRetryPolicyConfig,
        description="Default retry policy applied to activities if not overridden by the agent.",
    )


class TemporalActivityConfig(BaseModel):
    """Configuration specific to Temporal Activity Execution (per Agent)."""

    task_queue: str | None = Field(
        default=None,
        description="Specific task queue for this agent's activity execution (overrides workflow default).",
    )
    start_to_close_timeout: timedelta | None = Field(
        default=timedelta(minutes=5),  # Default to 5 minutes
        description="Time limit for a single activity attempt.",
    )
    retry_policy: TemporalRetryPolicyConfig | None = Field(
        default=None,
        description="Specific retry policy for this activity (overrides workflow default).",
    )
    # Other timeouts like schedule_to_start, heartbeat_timeout could be added here if needed
```

### src\flock\workflow\temporal_setup.py

- **Lines**: 60
- **Last modified**: 2025-04-19 03:53:31

```py
import uuid

from temporalio.client import Client
from temporalio.worker import Worker


async def create_temporal_client() -> Client:
    # Consider making the address configurable
    client = await Client.connect("localhost:7233")
    return client


async def setup_worker(
    client: Client, task_queue: str, workflow: type, activities: list
) -> Worker:
    """Creates and configures a worker instance, but does not run it.

    Args:
        client: The Temporal client to associate with the worker.
        task_queue: The task queue the worker should listen on.
        workflow: The workflow class definition.
        activities: A list of activity functions.

    Returns:
        A configured Worker instance.
    """
    # Creates and configures the worker instance
    worker = Worker(
        client,
        task_queue=task_queue,
        workflows=[workflow],
        activities=activities,
    )
    return worker  # Return the configured worker instance


async def run_worker(client: Client, task_queue: str, workflows, activities):
    worker = Worker(
        client,
        task_queue=task_queue,
        workflows=workflows,
        activities=activities,
    )
    await worker.run()


async def run_activity(client: Client, name: str, func, param):
    run_id = f"{name}_{uuid.uuid4().hex[:4]}"

    try:
        result = await client.execute_activity(
            func,
            param,
            id=run_id,
            task_queue="flock-queue",
            start_to_close_timeout=300,  # e.g., 5 minutes
        )
        return result
    except Exception:
        raise
```

### test_store_integration.py

- **Lines**: 107
- **Last modified**: 2025-05-26 17:36:40

```py
#!/usr/bin/env python3
"""Test script to verify the Azure Table Storage integration with environment variables."""

import asyncio
import os


# Set environment variables to test the factory function
def test_sqlite_store():
    """Test SQLite store (default)."""
    os.environ["FLOCK_WEBAPP_STORE"] = "local"
    os.environ["FLOCK_WEBAPP_STORE_CONNECTION"] = "test_shared_links.db"

    from flock.webapp.app.services.sharing_models import SharedLinkConfig
    from flock.webapp.app.services.sharing_store import create_shared_link_store

    async def run_test():
        print("Testing SQLite store...")
        store = create_shared_link_store()
        print(f"Created store: {type(store).__name__}")

        # Initialize the store
        await store.initialize()
        print("Store initialized successfully")

        # Create a test config
        config = SharedLinkConfig(
            share_id="test123",
            agent_name="TestAgent",
            flock_definition="name: TestFlock\nagents:\n  TestAgent:\n    input: 'message: str'",
            share_type="agent_run"
        )

        # Test save
        saved_config = await store.save_config(config)
        print(f"Saved config: {saved_config.share_id}")

        # Test retrieve
        retrieved_config = await store.get_config("test123")
        if retrieved_config:
            print(f"Retrieved config: {retrieved_config.share_id}")
        else:
            print("Failed to retrieve config")

        # Test delete
        deleted = await store.delete_config("test123")
        print(f"Deleted config: {deleted}")

    asyncio.run(run_test())

def test_azure_store():
    """Test Azure Table Storage store."""
    os.environ["FLOCK_WEBAPP_STORE"] = "azure-storage"
    os.environ["FLOCK_WEBAPP_STORE_CONNECTION"] = "DefaultEndpointsProtocol=https;AccountName=test;AccountKey=test;EndpointSuffix=core.windows.net"

    from flock.webapp.app.services.sharing_store import (
        AZURE_AVAILABLE,
        create_shared_link_store,
    )

    if not AZURE_AVAILABLE:
        print("Azure dependencies not available - this is expected if azure-data-tables is not installed")
        return

    try:
        print("Testing Azure Table Storage store...")
        store = create_shared_link_store()
        print(f"Created store: {type(store).__name__}")
        print("Azure Table Storage store created successfully (connection will fail with fake credentials)")
    except Exception as e:
        print(f"Expected error with fake credentials: {e}")

def test_invalid_store_type():
    """Test invalid store type."""
    os.environ["FLOCK_WEBAPP_STORE"] = "invalid"

    from flock.webapp.app.services.sharing_store import create_shared_link_store

    try:
        store = create_shared_link_store()
        print("ERROR: Should have failed with invalid store type")
    except ValueError as e:
        print(f"Correctly caught error: {e}")

if __name__ == "__main__":
    print("=== Testing Store Integration ===\n")

    # Test SQLite store
    test_sqlite_store()
    print()

    # Test Azure store creation
    test_azure_store()
    print()

    # Test invalid store type
    test_invalid_store_type()
    print()

    print("=== Tests completed ===")

    # Clean up test database
    try:
        os.remove("test_shared_links.db")
        print("Cleaned up test database")
    except FileNotFoundError:
        pass
```

### tests\__init__.py

- **Lines**: 1
- **Last modified**: 2025-05-06 12:21:20

```py
"""Test package for Flock."""
```

### tests\api\test_custom_endpoints.py

- **Lines**: 50
- **Last modified**: 2025-05-21 19:51:15

```py
import pytest
from fastapi.testclient import TestClient
from pydantic import BaseModel

from flock.core import Flock
from flock.core.api.custom_endpoint import FlockEndpoint
from flock.core.api.main import FlockAPI


class EchoRequest(BaseModel):
    text: str


async def echo_endpoint(body: EchoRequest):  # type: ignore[valid-type]
    """Simple echo used for tests."""
    return {"echo": body.text}


@pytest.fixture()
def api_client() -> TestClient:
    """Returns a TestClient with a single custom endpoint mounted."""
    flock = Flock(show_flock_banner=False)

    ep = FlockEndpoint(
        path="/api/echo",
        methods=["POST"],
        callback=echo_endpoint,
        request_model=EchoRequest,
        summary="Echo endpoint (test-only)",
    )

    api = FlockAPI(flock, custom_endpoints=[ep])
    return TestClient(api.app)


def test_custom_endpoint_in_openapi(api_client: TestClient):
    openapi = api_client.get("/openapi.json").json()
    assert "/api/echo" in openapi["paths"], "Custom endpoint missing from OpenAPI schema"


def test_custom_endpoint_success(api_client: TestClient):
    resp = api_client.post("/api/echo", json={"text": "hi"})
    assert resp.status_code == 200
    assert resp.json() == {"echo": "hi"}


def test_custom_endpoint_validation_error(api_client: TestClient):
    # Missing required field 'text'
    resp = api_client.post("/api/echo", json={})
    assert resp.status_code == 422 
```

### tests\core\test_flock_batch.py

- **Lines**: 75
- **Last modified**: 2025-05-22 21:27:37

```py


import os
import pytest
import pandas as pd
from flock.core import Flock, FlockAgent, FlockFactory
from flock.core.flock_registry import get_registry
from flock.evaluators.test.test_case_evaluator import TestCaseEvaluator, TestCaseEvaluatorConfig



@pytest.fixture
def basic_flock() -> Flock:
    """Fixture for a basic Flock instance."""
    return Flock(name="test_basic_flock", model="test-model", show_flock_banner=False)

@pytest.fixture
def simple_agent() -> FlockAgent:
    """Fixture for a simple agent instance."""
    agent = FlockFactory.create_default_agent(name="agent1", input="query", output="col1,col2,col3,col4")
    agent.evaluator = TestCaseEvaluator(name="test_case_evaluator", config=TestCaseEvaluatorConfig())
    return agent


@pytest.fixture(autouse=True)
def clear_registry():
    """Fixture to ensure a clean registry for each test."""
    registry = get_registry()
    registry._initialize() # Reset internal dictionaries
    yield # Run the test
    registry._initialize() # Clean up after test  


@pytest.mark.asyncio
async def test_batch_execution_with_dataframe_input(basic_flock: Flock, simple_agent: FlockAgent):
    """Test batch execution with CSV input."""
    batch_inputs = pd.DataFrame({
        "query": ["test1", "test2", "test3"],
    })
    results = await basic_flock.run_batch_async(
        start_agent=simple_agent,
        batch_inputs=batch_inputs,
        input_mapping={"query": "query"},
        parallel=True,
    )
    assert len(results) == 3
    assert results[0]["col1"] == "Test Result"
    assert results[1]["col2"] == "Test Result"
    assert results[2]["col3"] == "Test Result"
    assert results[2]["col4"] == "Test Result"
    
    
@pytest.mark.asyncio
async def test_batch_execution_with_dataframe_input_and_csv_output(basic_flock: Flock, simple_agent: FlockAgent):
    """Test batch execution with CSV input."""
    batch_inputs = pd.DataFrame({
        "query": ["test1", "test2", "test3"],
    })
    results = await basic_flock.run_batch_async(
        start_agent=simple_agent,
        batch_inputs=batch_inputs,
        input_mapping={"query": "query"},
        parallel=True,
        write_to_csv="test_output.csv",
        hide_columns=["col1", "col2"],
        delimiter="-",
    )
    assert os.path.exists("test_output.csv")
    df = pd.read_csv("test_output.csv", delimiter="-")
    assert len(df) == 3
    assert df.columns.tolist() == ["col3", "col4"]
    assert df["col3"][0] == "Test Result"
    assert df["col4"][1] == "Test Result"
    os.remove("test_output.csv")

```

### tests\integration\__init__.py

- **Lines**: 1
- **Last modified**: 2025-05-26 17:33:51

```py
# Integration tests for Flock
```

### tests\integration\test_shared_link_store.py

- **Lines**: 257
- **Last modified**: 2025-05-26 17:36:40

```py
"""
Integration tests for the shared link store implementations.

Tests both SQLite and Azure Table Storage backends.
"""

import asyncio
import os
import tempfile
from datetime import datetime
from pathlib import Path

import pytest

from flock.webapp.app.services.sharing_models import FeedbackRecord, SharedLinkConfig
from flock.webapp.app.services.sharing_store import create_shared_link_store


class TestSharedLinkStoreIntegration:
    """Integration tests for shared link store implementations."""

    def test_sqlite_store_integration(self):
        """Test SQLite store with environment variables."""
        asyncio.run(self._test_sqlite_store())

    async def _test_sqlite_store(self):
        """Test SQLite store functionality."""
        with tempfile.TemporaryDirectory() as temp_dir:
            db_path = Path(temp_dir) / "test_shared_links.db"
            
            # Set environment variables for SQLite
            original_store = os.environ.get("FLOCK_WEBAPP_STORE")
            original_conn = os.environ.get("FLOCK_WEBAPP_STORE_CONNECTION")
            
            try:
                os.environ["FLOCK_WEBAPP_STORE"] = "local"
                os.environ["FLOCK_WEBAPP_STORE_CONNECTION"] = str(db_path)
                
                # Create store using factory
                store = create_shared_link_store()
                await store.initialize()
                
                # Test shared link config
                config = SharedLinkConfig(
                    share_id="test-123",
                    agent_name="TestAgent",
                    flock_definition="name: TestFlock\nagents:\n  TestAgent:\n    input: 'message: str'",
                    share_type="agent_run"
                )
                
                # Test save and retrieve
                saved_config = await store.save_config(config)
                assert saved_config.share_id == "test-123"
                
                retrieved_config = await store.get_config("test-123")
                assert retrieved_config is not None
                assert retrieved_config.share_id == "test-123"
                assert retrieved_config.agent_name == "TestAgent"
                assert retrieved_config.share_type == "agent_run"
                
                # Test feedback
                feedback = FeedbackRecord(
                    feedback_id="feedback-123",
                    share_id="test-123",
                    context_type="agent_run",
                    reason="Test feedback",
                    expected_response="Expected output",
                    actual_response="Actual output",
                    flock_name="TestFlock",
                    agent_name="TestAgent",
                    flock_definition=config.flock_definition
                )
                
                saved_feedback = await store.save_feedback(feedback)
                assert saved_feedback.feedback_id == "feedback-123"
                
                # Test delete
                deleted = await store.delete_config("test-123")
                assert deleted is True
                
                # Verify deletion
                retrieved_after_delete = await store.get_config("test-123")
                assert retrieved_after_delete is None
                
                print("✅ SQLite store integration test passed!")
                
            finally:
                # Restore environment variables
                if original_store is not None:
                    os.environ["FLOCK_WEBAPP_STORE"] = original_store
                elif "FLOCK_WEBAPP_STORE" in os.environ:
                    del os.environ["FLOCK_WEBAPP_STORE"]
                    
                if original_conn is not None:
                    os.environ["FLOCK_WEBAPP_STORE_CONNECTION"] = original_conn
                elif "FLOCK_WEBAPP_STORE_CONNECTION" in os.environ:
                    del os.environ["FLOCK_WEBAPP_STORE_CONNECTION"]

    @pytest.mark.skipif(
        not os.environ.get("AZURE_STORAGE_CONNECTION_STRING"),
        reason="Azure Storage connection string not provided"
    )
    def test_azure_table_store_integration(self):
        """Test Azure Table Storage store with environment variables."""
        asyncio.run(self._test_azure_table_store())

    async def _test_azure_table_store(self):
        """Test Azure Table Storage functionality."""
        azure_conn_string = os.environ.get("AZURE_STORAGE_CONNECTION_STRING")
        if not azure_conn_string:
            print("⚠️ Skipping Azure Table Storage test - no connection string provided")
            return
            
        # Set environment variables for Azure Table Storage
        original_store = os.environ.get("FLOCK_WEBAPP_STORE")
        original_conn = os.environ.get("FLOCK_WEBAPP_STORE_CONNECTION")
        
        try:
            os.environ["FLOCK_WEBAPP_STORE"] = "azure-storage"
            os.environ["FLOCK_WEBAPP_STORE_CONNECTION"] = azure_conn_string
            
            # Create store using factory
            store = create_shared_link_store()
            await store.initialize()
            
            # Test shared link config
            test_id = f"test-{datetime.utcnow().strftime('%Y%m%d-%H%M%S')}"
            config = SharedLinkConfig(
                share_id=test_id,
                agent_name="TestAgent",
                flock_definition="name: TestFlock\nagents:\n  TestAgent:\n    input: 'message: str'",
                share_type="chat",
                chat_message_key="user_input",
                chat_history_key="history",
                chat_response_key="response"
            )
            
            # Test save and retrieve
            saved_config = await store.save_config(config)
            assert saved_config.share_id == test_id
            
            retrieved_config = await store.get_config(test_id)
            assert retrieved_config is not None
            assert retrieved_config.share_id == test_id
            assert retrieved_config.agent_name == "TestAgent"
            assert retrieved_config.share_type == "chat"
            assert retrieved_config.chat_message_key == "user_input"
            
            # Test feedback
            feedback_id = f"feedback-{datetime.utcnow().strftime('%Y%m%d-%H%M%S')}"
            feedback = FeedbackRecord(
                feedback_id=feedback_id,
                share_id=test_id,
                context_type="chat",
                reason="Test feedback from Azure",
                expected_response="Expected chat response",
                actual_response="Actual chat response",
                flock_name="TestFlock",
                agent_name="TestAgent",
                flock_definition=config.flock_definition
            )
            
            saved_feedback = await store.save_feedback(feedback)
            assert saved_feedback.feedback_id == feedback_id
            
            # Test delete
            deleted = await store.delete_config(test_id)
            assert deleted is True
            
            # Verify deletion
            retrieved_after_delete = await store.get_config(test_id)
            assert retrieved_after_delete is None
            
            print("✅ Azure Table Storage integration test passed!")
            
        except ImportError as e:
            print(f"⚠️ Skipping Azure Table Storage test - missing dependencies: {e}")
        finally:
            # Restore environment variables
            if original_store is not None:
                os.environ["FLOCK_WEBAPP_STORE"] = original_store
            elif "FLOCK_WEBAPP_STORE" in os.environ:
                del os.environ["FLOCK_WEBAPP_STORE"]
                
            if original_conn is not None:
                os.environ["FLOCK_WEBAPP_STORE_CONNECTION"] = original_conn
            elif "FLOCK_WEBAPP_STORE_CONNECTION" in os.environ:
                del os.environ["FLOCK_WEBAPP_STORE_CONNECTION"]

    def test_factory_with_invalid_store_type(self):
        """Test factory function with invalid store type."""
        original_store = os.environ.get("FLOCK_WEBAPP_STORE")
        
        try:
            os.environ["FLOCK_WEBAPP_STORE"] = "invalid-store-type"
            
            with pytest.raises(ValueError, match="Unsupported store type"):
                create_shared_link_store()
                
        finally:
            if original_store is not None:
                os.environ["FLOCK_WEBAPP_STORE"] = original_store
            elif "FLOCK_WEBAPP_STORE" in os.environ:
                del os.environ["FLOCK_WEBAPP_STORE"]

    def test_factory_defaults(self):
        """Test factory function with default values."""
        # Clear environment variables
        original_store = os.environ.get("FLOCK_WEBAPP_STORE")
        original_conn = os.environ.get("FLOCK_WEBAPP_STORE_CONNECTION")
        
        try:
            if "FLOCK_WEBAPP_STORE" in os.environ:
                del os.environ["FLOCK_WEBAPP_STORE"]
            if "FLOCK_WEBAPP_STORE_CONNECTION" in os.environ:
                del os.environ["FLOCK_WEBAPP_STORE_CONNECTION"]
            
            # Should default to SQLite with default path
            store = create_shared_link_store()
            assert store.__class__.__name__ == "SQLiteSharedLinkStore"
            
        finally:
            # Restore environment variables
            if original_store is not None:
                os.environ["FLOCK_WEBAPP_STORE"] = original_store
            if original_conn is not None:
                os.environ["FLOCK_WEBAPP_STORE_CONNECTION"] = original_conn


if __name__ == "__main__":
    # Run basic tests
    test_suite = TestSharedLinkStoreIntegration()
    
    print("🧪 Running Shared Link Store Integration Tests...")
    print()
    
    # Test SQLite
    print("📁 Testing SQLite store...")
    test_suite.test_sqlite_store_integration()
    print()
    
    # Test Azure (if connection string available)
    if os.environ.get("AZURE_STORAGE_CONNECTION_STRING"):
        print("☁️ Testing Azure Table Storage...")
        test_suite.test_azure_table_store_integration()
    else:
        print("⚠️ Skipping Azure Table Storage test - set AZURE_STORAGE_CONNECTION_STRING to test")
    print()
    
    # Test factory
    print("🏭 Testing factory function...")
    test_suite.test_factory_defaults()
    test_suite.test_factory_with_invalid_store_type()
    print("✅ Factory tests passed!")
    print()
    
    print("🎉 All integration tests completed!")
```

### tests\modules\test_enterprise_memory_store.py

- **Lines**: 48
- **Last modified**: 2025-05-21 19:51:15

```py
import pytest

from types import SimpleNamespace
from flock.modules.enterprise_memory.enterprise_memory_module import EnterpriseMemoryStore, EnterpriseMemoryModuleConfig
from flock.adapter.vector_base import VectorAdapter, VectorHit


class DummyAdapter(VectorAdapter):
    """In-memory adapter used solely for tests."""

    def __init__(self):
        super().__init__()
        self._store = {}

    def add(self, *, id: str, content: str, embedding: list[float], metadata=None):
        self._store[id] = (content, embedding, metadata)

    def query(self, *, embedding: list[float], k: int):
        # naive similarity = inverse L2
        import numpy as np
        hits = []
        for _id, (content, vec, meta) in self._store.items():
            dist = float(np.linalg.norm(np.array(vec) - np.array(embedding)))
            score = 1 - dist
            hits.append(VectorHit(id=_id, content=content, metadata=meta or {}, score=score))
        hits.sort(key=lambda h: h.score, reverse=True)
        return hits[:k]


@pytest.mark.asyncio
async def test_enterprise_memory_store_add_and_search(monkeypatch):
    # Prepare config with save_interval=0 to avoid graph writes
    cfg = EnterpriseMemoryModuleConfig(vector_backend="faiss", save_interval=0)
    store = EnterpriseMemoryStore(cfg)

    # Patch adapter and embedding model
    dummy_adapter = DummyAdapter()
    monkeypatch.setattr(store, "_ensure_adapter", lambda: dummy_adapter)
    monkeypatch.setattr(store, "_ensure_embedding_model", lambda: SimpleNamespace(encode=lambda txt: [0.1, 0.2, 0.3]))

    # Add entry
    entry_id = await store.add_entry("hello world", {"hello"})
    assert entry_id in dummy_adapter._store

    # Search
    results = await store.search("hello world", threshold=0.0, k=5)
    assert results
    assert results[0]["id"] == entry_id 
```

### tests\modules\test_vector_adapters.py

- **Lines**: 40
- **Last modified**: 2025-05-21 19:51:15

```py
import pytest
import random

from flock.adapter.vector_base import VectorHit


@pytest.mark.parametrize("adapter_name", ["faiss", "chroma"])  # add others conditionally
def test_adapter_add_and_query(adapter_name, tmp_path):
    """Smoke-test that each adapter can add vectors and retrieve them back.

    Skips adapters whose third-party packages are missing.
    """
    import importlib
    if adapter_name == "chroma":
        chroma = importlib.util.find_spec("chromadb")
        if chroma is None:
            pytest.skip("chromadb not installed")
        from flock.adapter.chroma_adapter import ChromaAdapter
        adapter = ChromaAdapter(path=str(tmp_path / "chroma"))
    elif adapter_name == "faiss":
        faiss_spec = importlib.util.find_spec("faiss")
        if faiss_spec is None:
            pytest.skip("faiss not installed")
        from flock.adapter.faiss_adapter import FAISSAdapter
        adapter = FAISSAdapter(index_path=str(tmp_path / "faiss.index"))
    else:
        pytest.skip("backend not in test matrix")

    # Generate deterministic embeddings
    vec1 = [random.random() for _ in range(5)]
    vec2 = [v + 0.01 for v in vec1]

    adapter.add(id="a", content="doc a", embedding=vec1, metadata={})
    adapter.add(id="b", content="doc b", embedding=vec2, metadata={})

    hits = adapter.query(embedding=vec1, k=2)
    assert hits, "No hits returned"
    assert isinstance(hits[0], VectorHit)
    ids = [hit.id for hit in hits]
    assert "a" in ids  # Original doc should be retrieved 
```

### tests\serialization\__init__.py

- **Lines**: 1
- **Last modified**: 2025-04-16 00:11:15

```py
"""Test package for Flock.""" 
```

### tests\serialization\test_enhanced_serialization.py

- **Lines**: 135
- **Last modified**: 2025-04-16 00:11:15

```py
"""Tests for enhanced serialization with type and component definitions."""
import os
import tempfile
from pathlib import Path
from typing import Literal

from pydantic import BaseModel

from flock.core import Flock, FlockFactory
from flock.core.flock_registry import flock_type


# Define a custom type for testing
@flock_type
class TestPerson(BaseModel):
    """Test person model for serialization tests."""
    name: str
    age: int
    role: Literal["admin", "user", "guest"]
    bio: str = ""

@flock_type
class TestCompany(BaseModel):
    """Test company model for serialization tests."""
    name: str
    industry: str
    employees: int


def test_serialization_with_custom_type():
    """Test serialization with custom type definitions."""
    # Create a test Flock with an agent using the custom type
    flock = Flock(name="test_flock")
    
    # Create an agent that uses TestPerson in its output
    agent = FlockFactory.create_default_agent(
        name="person_agent",
        input="query: str",
        output="result: TestPerson",
    )
    flock.add_agent(agent)
    
    # Create a temporary file for the YAML
    with tempfile.NamedTemporaryFile(suffix=".yaml", delete=False) as temp_file:
        yaml_path = temp_file.name
    
    try:
        # Serialize the Flock to YAML
        flock.to_yaml_file(yaml_path)
        
        # Read the YAML to verify it includes type definitions
        yaml_content = Path(yaml_path).read_text()
        
        # Check that type definitions are included
        assert "types:" in yaml_content
        assert "TestPerson:" in yaml_content
        
        # Check that component definitions are included
        assert "components:" in yaml_content
        assert "DeclarativeEvaluator:" in yaml_content
        
        # Load the Flock back from YAML
        loaded_flock = Flock.load_from_file(yaml_path)
        
        # Verify the loaded Flock structure
        assert loaded_flock.name == "test_flock"
        assert "person_agent" in loaded_flock.agents
        assert loaded_flock.agents["person_agent"].output == "result: TestPerson"
        
        # Try running the loaded Flock (this will validate the TestPerson type is available)
        # Just a basic smoke test - not testing actual output
        result = loaded_flock.run(
            start_agent="person_agent",
            input={"query": "Get a test person"},
        )
        
        # Verify result contains a TestPerson
        assert hasattr(result, "result")
        
    finally:
        # Clean up the temporary file
        if os.path.exists(yaml_path):
            os.unlink(yaml_path)


def test_serialization_with_multiple_types():
    """Test serialization with multiple custom types."""
    # Define another custom type for testing
    
        
    # Create a test Flock with agents using both custom types
    flock = Flock(name="multi_type_flock")
    
    # Create agents that use both custom types
    person_agent = FlockFactory.create_default_agent(
        name="person_agent",
        input="query: str",
        output="result: TestPerson",
    )
    flock.add_agent(person_agent)
    
    company_agent = FlockFactory.create_default_agent(
        name="company_agent",
        input="industry: str",
        output="result: TestCompany",
    )
    flock.add_agent(company_agent)
    
    # Create a temporary file for the YAML
    with tempfile.NamedTemporaryFile(suffix=".yaml", delete=False) as temp_file:
        yaml_path = temp_file.name
    
    try:
        # Serialize the Flock to YAML
        flock.to_yaml_file(yaml_path)
        
        # Read the YAML to verify it includes both type definitions
        yaml_content = Path(yaml_path).read_text()
        print(yaml_content)
        
        # Check that both type definitions are included
        assert "TestPerson:" in yaml_content
        assert "TestCompany:" in yaml_content
        
        # Load the Flock back from YAML
        loaded_flock = Flock.load_from_file(yaml_path)
        
        # Verify both agents are present
        assert "person_agent" in loaded_flock.agents
        assert "company_agent" in loaded_flock.agents
        
    finally:
        # Clean up the temporary file
        if os.path.exists(yaml_path):
            os.unlink(yaml_path) 
```

### tests\serialization\test_file_path_serialization.py

- **Lines**: 107
- **Last modified**: 2025-04-19 00:37:16

```py
"""Tests for file path serialization and component loading."""
import os
import tempfile
from pathlib import Path

from flock.core import Flock, FlockFactory
from flock.core.flock_registry import flock_component
from pydantic import BaseModel, Field


def test_component_file_path_serialization():
    """Test that component file paths are serialized correctly."""
    # Create a test Flock
    flock = Flock(name="file_path_test_flock")
    
    # Create an agent with default components
    agent = FlockFactory.create_default_agent(
        name="test_agent",
        input="query: str",
        output="result: str",
    )
    flock.add_agent(agent)
    
    # Create a temporary file for the YAML
    with tempfile.NamedTemporaryFile(suffix=".yaml", delete=False) as temp_file:
        yaml_path = temp_file.name
    
    try:
        # Serialize the Flock to YAML
        flock.to_yaml_file(yaml_path)
        
        # Read the YAML to verify it includes file paths
        yaml_content = Path(yaml_path).read_text()
        
        # Check that file paths are included for components
        assert "file_path:" in yaml_content
        
        # Load the Flock back from YAML
        loaded_flock = Flock.load_from_file(yaml_path)
        
        # Verify the loaded Flock structure
        assert loaded_flock.name == "file_path_test_flock"
        assert "test_agent" in loaded_flock.agents
        
        # Basic smoke test - can we run the agent?
        result = loaded_flock.run(
            start_agent="test_agent",
            input={"query": "Test query"},
        )
        
        # Verify we got a result
        assert hasattr(result, "result")
        
    finally:
        # Clean up the temporary file
        if os.path.exists(yaml_path):
            os.unlink(yaml_path)


def test_loading_component_from_file_path():
    """Test loading a component using its file path when module import fails."""
    # This test is more complex and would require creating a real file on disk
    # with a component definition that we can load by path.
    # For simplicity, we'll check if the serialized YAML contains the file path.
    
    # Create a test Flock
    flock = Flock(name="file_path_load_test")
    
    # Create an agent with default components
    agent = FlockFactory.create_default_agent(
        name="path_agent",
        input="query: str",
        output="result: str",
    )
    flock.add_agent(agent)
    
    # Create a temporary file for the YAML
    with tempfile.NamedTemporaryFile(suffix=".yaml", delete=False) as temp_file:
        yaml_path = temp_file.name
    
    try:
        # Serialize the Flock to YAML
        flock.to_yaml_file(yaml_path, path_type="absolute")
        
        # Read the YAML to extract a component's file path
        yaml_content = Path(yaml_path).read_text()
        
        # Check that file paths exist and seem reasonable
        assert "file_path:" in yaml_content
        
        # Verify the paths in the YAML are absolute and pointing to real files
        lines = yaml_content.split("\n")
        file_path_lines = [line.strip() for line in lines if "file_path:" in line]
        
        for line in file_path_lines:
            if "null" not in line:  # Skip null file paths
                # Extract the path using a simple split (this is a test, so it's fine)
                path = line.split("file_path:", 1)[1].strip()
                if path:
                    # Check that this is an absolute path that exists
                    assert path.startswith("/") or path.startswith("C:")
                    assert os.path.exists(path), f"Path does not exist: {path}"
        
    finally:
        # Clean up the temporary file
        if os.path.exists(yaml_path):
            os.unlink(yaml_path) 
```

### tests\serialization\test_flock_serializer.py

- **Lines**: 419
- **Last modified**: 2025-05-22 21:27:37

```py
# tests/serialization/test_flock_serializer.py
import os
import random
from unittest.mock import patch
import pytest
from typing import List, Literal
from dataclasses import dataclass
from datetime import timedelta # Ensure timedelta is imported

from pydantic import BaseModel, Field

from flock.core.flock import Flock
from flock.core.flock_agent import FlockAgent
from flock.core.flock_evaluator import FlockEvaluator, FlockEvaluatorConfig
from flock.core.flock_module import FlockModule, FlockModuleConfig
from flock.core.flock_router import FlockRouter, FlockRouterConfig, HandOffRequest
from flock.core.serialization.flock_serializer import FlockSerializer
from flock.core.flock_registry import FlockRegistry, get_registry, flock_component, flock_tool, flock_type
from flock.core.serialization.serializable import Serializable # Needed for mocks if they inherit

# Import Temporal config models
from flock.workflow.temporal_config import (
    TemporalWorkflowConfig,
    TemporalRetryPolicyConfig,
    TemporalActivityConfig
)

# --- Mock Components for Testing ---
# Ensure these are registered before tests that need them run

@pytest.fixture(autouse=True)
def setup_registry():
    """Fixture to ensure mocks are registered before each test."""
    registry = get_registry()
    registry._initialize() # Start fresh

    # Register Mocks
    registry.register_component(MockEvaluator)
    registry.register_component(MockModule)
    registry.register_component(MockRouter)
    registry.register_callable(sample_tool) # Register by function object
    registry.register_callable(print)       # Register built-in
    registry.register_type(MyCustomData)

    yield # Run test

    registry._initialize() # Clean up


class MockEvalConfig(FlockEvaluatorConfig):
    mock_eval_param: str = "eval_default"

@flock_component # Auto-register is fine too
class MockEvaluator(FlockEvaluator, Serializable):
    config: MockEvalConfig = MockEvalConfig()
    # Add evaluate if needed by agent.run, but not strictly required for serialization test
    async def evaluate(self, agent, inputs, tools): return {"mock_result": "mock"}
    # Need serialization methods if not relying solely on Pydantic model_dump in agent
    def to_dict(self): return {"name": self.name, "config": self.config.model_dump(), "type": "MockEvaluator"}
    @classmethod
    def from_dict(cls, data): return cls(name=data.get("name", "default_eval_name"), config=MockEvalConfig(**data.get("config",{})))


class MockModuleConfig(FlockModuleConfig):
    mock_module_param: bool = True

@flock_component
class MockModule(FlockModule, Serializable):
    config: MockModuleConfig = MockModuleConfig()
    def to_dict(self): return {"name": self.name, "config": self.config.model_dump(), "type": "MockModule"}
    @classmethod
    def from_dict(cls, data): return cls(name=data.get("name", "default_mod_name"), config=MockModuleConfig(**data.get("config",{})))


class MockRouterConfig(FlockRouterConfig):
    next_agent_name: str = "default_next"

@flock_component
class MockRouter(FlockRouter, Serializable):
    config: MockRouterConfig = MockRouterConfig()
    async def route(self, current_agent, result, context): return HandOffRequest(next_agent=self.config.next_agent_name)
    def to_dict(self): return {"name": self.name, "config": self.config.model_dump(), "type": "MockRouter"}
    @classmethod
    def from_dict(cls, data): return cls(name=data.get("name", "default_router_name"), config=MockRouterConfig(**data.get("config",{})))


@flock_tool
def sample_tool(text: str) -> str:
    """A sample registered tool."""
    return text.upper()

@flock_type
class MyCustomData(BaseModel):
    id: int = Field(default_factory=lambda: random.randint(1, 1000000))
    value: str = Field(default_factory=lambda: "random_value")

# --- Fixtures ---
@pytest.fixture
def basic_flock():
    return Flock(name="serial_flock", model="serial_model", description="Serialization test flock", show_flock_banner=False)

@pytest.fixture
def flock_with_agents(basic_flock):
    agent1 = FlockAgent(name="a1", input="in", output="out1")
    agent2 = FlockAgent(
        name="a2",
        input="out1", output="data: MyCustomData", # Use custom type
        evaluator=MockEvaluator(name="a2_eval", config=MockEvalConfig(mock_eval_param="a2_eval")),
        modules={"m1": MockModule(name="m1", config=MockModuleConfig(mock_module_param=False))},
        tools=[sample_tool, print],
        handoff_router=MockRouter(name="a2_router", config=MockRouterConfig(next_agent_name="a1"))
    )
    basic_flock.add_agent(agent1)
    basic_flock.add_agent(agent2)
    return basic_flock


# --- Serialization Tests ---

def test_serialize_basic_flock(basic_flock):
    """Test serializing a Flock with no agents or complex components."""
    serialized_data = FlockSerializer.serialize(basic_flock)

    assert serialized_data["name"] == "serial_flock"
    assert serialized_data["model"] == "serial_model"
    assert serialized_data["description"] == "Serialization test flock"
    assert "agents" in serialized_data and serialized_data["agents"] == {}
    assert "types" not in serialized_data # No custom types used
    assert "components" not in serialized_data # No components used
    assert "metadata" in serialized_data
    assert serialized_data["metadata"]["path_type"] == "relative" # Default for serialize

def test_serialize_flock_with_agents(flock_with_agents):
    """Test serializing a Flock with agents and their components."""
    serialized_data = FlockSerializer.serialize(flock_with_agents)

    assert "agents" in serialized_data
    assert "a1" in serialized_data["agents"]
    assert "a2" in serialized_data["agents"]

    # Check agent a2 serialization details
    agent2_data = serialized_data["agents"]["a2"]
    assert agent2_data["name"] == "a2"
    assert agent2_data["input"] == "out1"
    assert agent2_data["output"] == "data: MyCustomData"

    # Check components attached to agent a2
    assert "evaluator" in agent2_data
    assert agent2_data["evaluator"]["type"] == "MockEvaluator"
    assert agent2_data["evaluator"]["config"]["mock_eval_param"] == "a2_eval"

    assert "modules" in agent2_data
    assert "m1" in agent2_data["modules"]
    assert agent2_data["modules"]["m1"]["type"] == "MockModule"
    assert agent2_data["modules"]["m1"]["config"]["mock_module_param"] is False

    assert "tools" in agent2_data
    assert agent2_data["tools"] == ["sample_tool", "print"] # Just names

    assert "handoff_router" in agent2_data
    assert agent2_data["handoff_router"]["type"] == "MockRouter"
    assert agent2_data["handoff_router"]["config"]["next_agent_name"] == "a1"

def test_serialize_includes_types(flock_with_agents):
    """Test that custom types used in agent signatures are included."""
    serialized_data = FlockSerializer.serialize(flock_with_agents)

    assert "types" in serialized_data
    assert "MyCustomData" in serialized_data["types"]
    type_def = serialized_data["types"]["MyCustomData"]
    assert type_def["type"] == "pydantic.BaseModel" # Assuming @flock_type registers it as such
    assert "module_path" in type_def
    assert "schema" in type_def
    assert "id" in type_def["schema"]["properties"]
    assert type_def["schema"]["properties"]["id"]["type"] == "integer"

def test_serialize_includes_components(flock_with_agents):
    """Test that components (evaluators, modules, routers, tools) are included."""
    serialized_data = FlockSerializer.serialize(flock_with_agents)

    assert "components" in serialized_data
    components = serialized_data["components"]

    # Check components from agent a2
    assert "MockEvaluator" in components
    assert components["MockEvaluator"]["type"] == "flock_component"
    assert "file_path" in components["MockEvaluator"] # Path should be present

    assert "MockModule" in components
    assert components["MockModule"]["type"] == "flock_component"

    assert "MockRouter" in components
    assert components["MockRouter"]["type"] == "flock_component"

    # Check tools (callables)
    assert "sample_tool" in components
    assert components["sample_tool"]["type"] == "flock_callable"
    assert components["sample_tool"]["module_path"] == "tests.serialization.test_flock_serializer"

    assert "print" in components # Built-in
    assert components["print"]["type"] == "flock_callable"
    assert components["print"]["module_path"] == "builtins"


def test_serialize_path_type_absolute(flock_with_agents, mocker):
    """Test serialization with absolute path type."""
    # Mock os.path.abspath to ensure we can check its call
    mock_relpath = mocker.patch('os.path.relpath', side_effect=lambda x: f"relative/path/to/{os.path.basename(x)}")
    mocker.patch('inspect.getfile', side_effect=lambda x: f"/abs/path/{x.__name__}.py") # Mock getfile

    serialized_data = FlockSerializer.serialize(flock_with_agents, path_type="absolute")
    components = serialized_data["components"]

    assert "MockEvaluator" in components
    assert components["MockEvaluator"]["file_path"] == "/abs/path/MockEvaluator.py" # Check absolute

    assert "sample_tool" in components
    assert components["sample_tool"]["file_path"] == "/abs/path/sample_tool.py" # Check absolute

    assert mock_relpath.call_count == 0 # Ensure abspath was involved

def test_serialize_path_type_relative(flock_with_agents, mocker):
    """Test serialization with relative path type."""
    # Mock os.path.relpath
    mock_relpath = mocker.patch('os.path.relpath', side_effect=lambda x: f"relative/path/to/{os.path.basename(x)}")
    mocker.patch('inspect.getfile', side_effect=lambda x: f"/abs/path/{x.__name__}.py") # Mock getfile

    serialized_data = FlockSerializer.serialize(flock_with_agents, path_type="relative")
    components = serialized_data["components"]

    assert "MockEvaluator" in components
    assert components["MockEvaluator"]["file_path"] == "relative/path/to/MockEvaluator.py"

    assert "sample_tool" in components
    assert components["sample_tool"]["file_path"] == "relative/path/to/sample_tool.py"

    assert mock_relpath.call_count > 0 # Ensure relpath was involved

def test_serialize_flock_with_temporal_config(basic_flock):
    """Test serializing a Flock includes temporal_config if set."""
    retry_config = TemporalRetryPolicyConfig(maximum_attempts=5)
    wf_config = TemporalWorkflowConfig(
        task_queue="serialize-test-queue",
        workflow_execution_timeout=timedelta(minutes=10),
        default_activity_retry_policy=retry_config
    )
    basic_flock.temporal_config = wf_config

    # Assuming Flock.to_dict uses the serializer implicitly or explicitly
    serialized_data = basic_flock.to_dict()

    assert "temporal_config" in serialized_data
    assert serialized_data["temporal_config"]["task_queue"] == "serialize-test-queue"
    # Pydantic v2 model_dump with mode='json' serializes timedelta to seconds (float)
    # Update: Seems it serializes to ISO 8601 string in this setup.
    assert serialized_data["temporal_config"]["workflow_execution_timeout"] == "PT600S"  or  serialized_data["temporal_config"]["workflow_execution_timeout"] == "PT10M"
    assert "default_activity_retry_policy" in serialized_data["temporal_config"]
    assert serialized_data["temporal_config"]["default_activity_retry_policy"]["maximum_attempts"] == 5

def test_serialize_agent_with_temporal_config(basic_flock):
    """Test serializing an agent includes temporal_activity_config if set."""
    retry_config = TemporalRetryPolicyConfig(initial_interval=timedelta(seconds=5))
    act_config = TemporalActivityConfig(
        start_to_close_timeout=timedelta(seconds=90),
        retry_policy=retry_config,
        task_queue="agent-queue"
    )
    agent = FlockAgent(
        name="temporal_agent",
        input="in", output="out",
        temporal_activity_config=act_config
    )
    # No need to add to flock for this test, just test agent serialization
    # basic_flock.add_agent(agent) 

    agent_data = agent.to_dict() 

    assert "temporal_activity_config" in agent_data
    assert agent_data["temporal_activity_config"]["task_queue"] == "agent-queue"
    # Pydantic v2 model_dump with mode='json' serializes timedelta to seconds (float)
    # Update: Seems it serializes to ISO 8601 string in this setup.
    assert agent_data["temporal_activity_config"]["start_to_close_timeout"] == "PT90S" or agent_data["temporal_activity_config"]["start_to_close_timeout"] == "PT1M30S" # Expect ISO string (90 seconds)
    assert "retry_policy" in agent_data["temporal_activity_config"]
    # Update: Check initial_interval serialization
    assert agent_data["temporal_activity_config"]["retry_policy"]["initial_interval"] == "PT5S" # Expect ISO string (5 seconds)


# --- Deserialization Tests ---

def test_deserialize_basic_flock(basic_flock):
    """Test deserializing a basic Flock from its dictionary representation."""
    serialized_data = FlockSerializer.serialize(basic_flock)
    loaded_flock = FlockSerializer.deserialize(Flock, serialized_data)

    assert isinstance(loaded_flock, Flock)
    assert loaded_flock.name == basic_flock.name
    assert loaded_flock.model == basic_flock.model
    assert loaded_flock.agents == {}

def test_deserialize_flock_with_agents_and_components(flock_with_agents):
    """Test deserializing a complex Flock with agents and components."""
    serialized_data = FlockSerializer.serialize(flock_with_agents)
    loaded_flock = FlockSerializer.deserialize(Flock, serialized_data)

    assert isinstance(loaded_flock, Flock)
    assert len(loaded_flock.agents) == 2
    assert "a1" in loaded_flock.agents
    assert "a2" in loaded_flock.agents

    # Check agent a2 structure after loading
    agent2 = loaded_flock.agents["a2"]
    assert isinstance(agent2, FlockAgent)
    assert isinstance(agent2.evaluator, MockEvaluator)
    assert agent2.evaluator.config.mock_eval_param == "a2_eval"
    assert isinstance(agent2.modules["m1"], MockModule)
    assert agent2.modules["m1"].config.mock_module_param is False
    assert isinstance(agent2.handoff_router, MockRouter)
    assert agent2.handoff_router.config.next_agent_name == "a1"
    assert len(agent2.tools) == 2
    assert agent2.tools[0] is sample_tool
    assert agent2.tools[1] is print

def test_deserialize_registers_components_and_types(flock_with_agents):
    """Verify that deserialization registers the necessary components and types."""
    serialized_data = FlockSerializer.serialize(flock_with_agents)

    # Create a new clean registry to simulate loading in a fresh environment
    new_registry = FlockRegistry()
    new_registry._initialize()

    with patch('flock.core.flock_registry.get_registry', return_value=new_registry):
        # Ensure mocks are NOT initially registered in the new registry
        with pytest.raises(KeyError):
            new_registry.get_type("MockEvaluator")
        with pytest.raises(KeyError):
            new_registry.get_callable("sample_tool")

        # Deserialize using the new registry
        loaded_flock = FlockSerializer.deserialize(Flock, serialized_data)

        # Assert that components and types ARE NOW registered
        assert new_registry.get_component("MockEvaluator") is MockEvaluator
        assert new_registry.get_callable("sample_tool") is sample_tool
        assert new_registry.get_type("MyCustomData") is MyCustomData
        assert new_registry.get_callable("print") is print # Built-in should be handled

def test_deserialize_missing_component_definition(basic_flock, caplog):
    """Test deserialization when a component's definition is missing."""
    # Create agent data referencing a non-existent component type
    agent_data = {
        "name": "missing_comp_agent",
        "input": "in",
        "output": "out",
        "evaluator": {
            "type": "NonExistentEvaluator", # This type is not registered
            "name": "bad_eval",
            "config": {}
        }
    }
    flock_data = FlockSerializer.serialize(basic_flock)
    flock_data["agents"] = {"missing_comp_agent": agent_data}
    # Manually remove component definition if it snuck in somehow (it shouldn't)
    flock_data.pop("components", None)

    # Act
    loaded_flock = FlockSerializer.deserialize(Flock, flock_data)

    # Assert
    assert "missing_comp_agent" in loaded_flock.agents
    agent = loaded_flock.agents["missing_comp_agent"]
    # The agent should be created, but the component should be None or raise during use
    assert agent.evaluator is None

def test_deserialize_flock_with_temporal_config(basic_flock):
    """Test deserializing a Flock with temporal_config."""
    retry_config = TemporalRetryPolicyConfig(maximum_attempts=5)
    wf_config = TemporalWorkflowConfig(
        task_queue="serialize-test-queue",
        workflow_execution_timeout=timedelta(minutes=10),
        default_activity_retry_policy=retry_config
    )
    basic_flock.temporal_config = wf_config
    serialized_data = basic_flock.to_dict()

    loaded_flock = Flock.from_dict(serialized_data)

    assert isinstance(loaded_flock.temporal_config, TemporalWorkflowConfig)
    assert loaded_flock.temporal_config.task_queue == "serialize-test-queue"
    assert loaded_flock.temporal_config.workflow_execution_timeout == timedelta(minutes=10)
    assert isinstance(loaded_flock.temporal_config.default_activity_retry_policy, TemporalRetryPolicyConfig)
    assert loaded_flock.temporal_config.default_activity_retry_policy.maximum_attempts == 5

def test_deserialize_agent_with_temporal_config(flock_with_agents):
    """Test deserializing an agent with temporal_activity_config."""
    # Modify agent a2 in the fixture to have temporal config for the test
    retry_config = TemporalRetryPolicyConfig(initial_interval=timedelta(seconds=7))
    act_config = TemporalActivityConfig(
        start_to_close_timeout=timedelta(seconds=45),
        retry_policy=retry_config
    )
    # Directly modify the agent instance held by the fixture
    agent_a2 = flock_with_agents.agents["a2"]
    agent_a2.temporal_activity_config = act_config 

    # Serialize the whole flock which includes the modified agent
    # Use Flock's to_dict which should call agent's to_dict
    serialized_data = flock_with_agents.to_dict()

    # Deserialize the flock using Flock's classmethod
    loaded_flock = Flock.from_dict(serialized_data)

    # Check the deserialized agent
    loaded_agent = loaded_flock.agents.get("a2")
    assert loaded_agent is not None
    assert isinstance(loaded_agent.temporal_activity_config, TemporalActivityConfig)
    assert loaded_agent.temporal_activity_config.start_to_close_timeout == timedelta(seconds=45)
    assert isinstance(loaded_agent.temporal_activity_config.retry_policy, TemporalRetryPolicyConfig)
    assert loaded_agent.temporal_activity_config.retry_policy.initial_interval == timedelta(seconds=7)
    assert loaded_agent.temporal_activity_config.task_queue is None # Wasn't set
```

### tests\serialization\test_nested_serialization.py

- **Lines**: 95
- **Last modified**: 2025-04-16 00:11:15

```py
"""Tests for serialization with nested type structures."""
import os
import tempfile
from pathlib import Path
from typing import Dict, List

from pydantic import BaseModel

from flock.core import Flock, FlockFactory
from flock.core.flock_registry import flock_type


# Define nested custom types for testing
@flock_type
class Address(BaseModel):
    """Address model for nested serialization test."""
    street: str
    city: str
    zip_code: str


@flock_type
class Contact(BaseModel):
    """Contact information for nested serialization test."""
    email: str
    phone: str
    address: Address  # Nested model


@flock_type
class Company(BaseModel):
    """Company model with nested types for serialization test."""
    name: str
    industry: str
    headquarters: Address  # Nested model
    contacts: List[Contact]  # List of nested models
    departments: Dict[str, List[str]]  # Dictionary with nested list


def test_nested_type_serialization():
    """Test serialization with nested type structures."""
    # Create a test Flock with an agent using nested types
    flock = Flock(name="nested_types_flock")
    
    # Create an agent that uses Company in its output (which contains nested types)
    agent = FlockFactory.create_default_agent(
        name="company_agent",
        input="query: str",
        output="result: Company",
    )
    flock.add_agent(agent)
    
    # Create a temporary file for the YAML
    with tempfile.NamedTemporaryFile(suffix=".yaml", delete=False) as temp_file:
        yaml_path = temp_file.name
    
    try:
        # Serialize the Flock to YAML
        flock.to_yaml_file(yaml_path)
        
        # Read the YAML to verify it includes nested type definitions
        yaml_content = Path(yaml_path).read_text()
        
        # Check that all type definitions are included
        assert "Company:" in yaml_content
        assert "Address:" in yaml_content  
        assert "Contact:" in yaml_content
        
        # Load the Flock back from YAML
        loaded_flock = Flock.load_from_file(yaml_path)
        
        # Verify the loaded Flock can be used
        assert "company_agent" in loaded_flock.agents
        assert loaded_flock.agents["company_agent"].output == "result: Company"
        
        # Try running the loaded Flock (this will validate all nested types are available)
        result = loaded_flock.run(
            start_agent="company_agent",
            input={"query": "Get a test company"},
        )
        
        # Basic validation that the result has the expected structure
        company = result.result
        assert hasattr(company, "name")
        assert hasattr(company, "headquarters")
        assert hasattr(company.headquarters, "street")
        assert isinstance(company.contacts, list)
        if company.contacts:
            assert hasattr(company.contacts[0], "address")
            assert hasattr(company.contacts[0].address, "city")
        
    finally:
        # Clean up the temporary file
        if os.path.exists(yaml_path):
            os.unlink(yaml_path) 
```

### tests\serialization\test_yaml_serialization.py

- **Lines**: 286
- **Last modified**: 2025-05-06 12:21:20

```py
# tests/serialization/test_yaml_serialization.py

import os
import tempfile
from pathlib import Path
from typing import Any, Dict, List, Optional, Callable
from dataclasses import dataclass

import pytest
import yaml

# --- Core Flock Imports ---
# Assume these are correctly implemented and importable
from flock.core.flock import Flock
from flock.core.flock_agent import FlockAgent
from flock.core.flock_evaluator import FlockEvaluator, FlockEvaluatorConfig
from flock.core.flock_module import FlockModule, FlockModuleConfig
from flock.core.flock_router import FlockRouter, FlockRouterConfig, HandOffRequest
from flock.core.context.context import FlockContext
from flock.core.serialization.serializable import Serializable

# --- Registry and Decorators ---
from flock.core.flock_registry import (
    flock_component,
    flock_tool,
    flock_type,
    get_registry,
)

# Get registry instance
FlockRegistry = get_registry()

# --- Mock Components for Testing ---

class MockEvalConfig(FlockEvaluatorConfig):
    mock_eval_param: str = "eval_default"

@flock_component # Register this component class
class MockEvaluator(FlockEvaluator, Serializable): # Inherit Serializable
    name: str = "mock_evaluator"
    config: MockEvalConfig = MockEvalConfig()

    # Needed for serialization if not just using Pydantic dump
    def to_dict(self) -> Dict[str, Any]:
        return {"name": self.name, "config": self.config.model_dump(), "type": self.__class__.__name__}

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "MockEvaluator":
        config = MockEvalConfig(**data.get("config", {}))
        return cls(name=data.get("name", "mock_evaluator"), config=config)

    async def evaluate(self, agent: Any, inputs: Dict[str, Any], tools: List[Any]) -> Dict[str, Any]:
        return {"mock_result": f"evaluated {inputs.get('test_input', '')} with {self.config.mock_eval_param}"}

class MockModuleConfig(FlockModuleConfig):
    mock_module_param: bool = True

@flock_component # Register this component class
class MockModule(FlockModule, Serializable): # Inherit Serializable
    config: MockModuleConfig = MockModuleConfig()

    def to_dict(self) -> Dict[str, Any]:
        return {"name": self.name, "config": self.config.model_dump(), "type": self.__class__.__name__}

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "MockModule":
        config = MockModuleConfig(**data.get("config", {}))
        return cls(name=data.get("name", "mock_module"), config=config)

    # Mock lifecycle methods if needed for testing interactions
    async def on_post_evaluate(self, agent: FlockAgent, inputs: dict[str, Any], result: dict[str, Any], context: FlockContext | None = None) -> dict[str, Any]:
        result["mock_module_added"] = self.config.mock_module_param
        return result

class MockRouterConfig(FlockRouterConfig):
    next_agent_name: str = "default_next"

@flock_component # Register this component class
class MockRouter(FlockRouter, Serializable): # Inherit Serializable
    config: MockRouterConfig = MockRouterConfig()

    def to_dict(self) -> Dict[str, Any]:
        return {"name": self.name, "config": self.config.model_dump(), "type": self.__class__.__name__}

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "MockRouter":
        config = MockRouterConfig(**data.get("config", {}))
        return cls(name=data.get("name", "mock_router"), config=config)

    async def route(self, current_agent: Any, result: Dict[str, Any], context: FlockContext) -> HandOffRequest:
        return HandOffRequest(next_agent=self.config.next_agent_name)

# --- Sample Tool Function ---
@flock_tool # Register this tool function
def sample_tool(text: str, capitalize: bool = True) -> str:
    """A sample registered tool."""
    return text.upper() if capitalize else text.lower()

# --- Sample Custom Type ---
@flock_type # Register this custom type
@dataclass
class MyCustomData:
    id: int
    value: str
    tags: List[str] | None = None

# --- Test Class ---

class TestFlockYAMLSerialization:

    def test_agent_serialization_basic(self, tmp_path):
        """Test serializing and deserializing a basic FlockAgent."""
        agent = FlockAgent(
            name="basic_agent",
            model="test_model",
            description="A basic agent",
            input="query: str",
            output="answer: str"
        )
        file_path = tmp_path / "basic_agent.yaml"

        # Act
        agent.to_yaml_file(file_path)
        loaded_agent = FlockAgent.from_yaml_file(file_path)

        # Assert
        assert isinstance(loaded_agent, FlockAgent)
        assert loaded_agent.name == agent.name
        assert loaded_agent.model == agent.model
        assert loaded_agent.description == agent.description
        assert loaded_agent.input == agent.input
        assert loaded_agent.output == agent.output
        assert loaded_agent.evaluator is None # Default should be None before factory
        assert loaded_agent.modules == {}
        assert loaded_agent.handoff_router is None
        assert loaded_agent.tools == []

    def test_agent_serialization_with_components(self, tmp_path):
        """Test agent serialization with evaluator, module, and router."""
        evaluator = MockEvaluator(name="mock_evaluator", config=MockEvalConfig(mock_eval_param="test_eval"))
        router = MockRouter(name="mock_router", config=MockRouterConfig(next_agent_name="agent_two"))
        module = MockModule(name="extra_module", config=MockModuleConfig(mock_module_param=False))

        agent = FlockAgent(
            name="component_agent",
            model="test_model_comp",
            evaluator=evaluator,
            handoff_router=router,
            modules={"extra_module": module}
        )
        file_path = tmp_path / "component_agent.yaml"

        # Act
        agent.to_yaml_file(file_path)
        loaded_agent = FlockAgent.from_yaml_file(file_path)

        # Assert
        assert loaded_agent.name == "component_agent"
        assert isinstance(loaded_agent.evaluator, MockEvaluator)
        assert loaded_agent.evaluator.name == "mock_evaluator" # Default name from mock
        assert loaded_agent.evaluator.config.mock_eval_param == "test_eval"

        assert isinstance(loaded_agent.handoff_router, MockRouter)
        assert loaded_agent.handoff_router.name == "mock_router" # Default name from mock
        assert loaded_agent.handoff_router.config.next_agent_name == "agent_two"

        assert "extra_module" in loaded_agent.modules
        assert isinstance(loaded_agent.modules["extra_module"], MockModule)
        assert loaded_agent.modules["extra_module"].config.mock_module_param is False

    def test_agent_serialization_with_tools(self, tmp_path):
        """Test agent serialization with callable tools."""
        agent = FlockAgent(
            name="tool_agent",
            model="tool_model",
            tools=[sample_tool, print] # Include a built-in for testing path gen
        )
        file_path = tmp_path / "tool_agent.yaml"

        # Act
        agent.to_yaml_file(file_path)
        # Optional: Inspect YAML for callable refs
        yaml_content = file_path.read_text()
        assert "sample_tool" in yaml_content
        assert "print" in yaml_content # Check built-in

        loaded_agent = FlockAgent.from_yaml_file(file_path)

        # Assert
        assert loaded_agent.name == "tool_agent"
        assert loaded_agent.tools is not None
        assert len(loaded_agent.tools) == 2
        assert loaded_agent.tools[0] is sample_tool # Check identity after registry lookup
        assert loaded_agent.tools[1] is print       # Check identity for built-in
        # Test calling the loaded tool
        assert loaded_agent.tools[0]("hello") == "HELLO"

    def test_agent_serialization_with_custom_type(self, tmp_path):
        """Test agent serialization where signature uses a registered custom type."""
        agent = FlockAgent(
            name="custom_type_agent",
            model="custom_model",
            input="data: MyCustomData", # Use the registered custom type
            output="result_tags: list[str]"
        )
        file_path = tmp_path / "custom_type_agent.yaml"

        # Act
        agent.to_yaml_file(file_path)
        loaded_agent = FlockAgent.from_yaml_file(file_path)

        # Assert - Primarily check that loading worked and fields are correct
        assert loaded_agent.name == "custom_type_agent"
        assert loaded_agent.input == "data: MyCustomData"
        assert loaded_agent.output == "result_tags: list[str]"
        # We don't directly test type resolution here, but successful loading implies
        # the type string was stored correctly. Resolution is tested implicitly by DSPy mixin tests.

    def test_flock_serialization_basic(self, tmp_path):
        """Test serializing and deserializing a basic Flock instance."""
        flock = Flock(
            model="global_model",
            description="Test Flock Instance"
        )
        file_path = tmp_path / "basic_flock.yaml"

        # Act
        flock.to_yaml_file(file_path)
        loaded_flock = Flock.from_yaml_file(file_path)

        # Assert
        assert isinstance(loaded_flock, Flock)
        assert loaded_flock.model == flock.model
        assert loaded_flock.description == flock.description
        assert loaded_flock.agents == {} # No agents added

    def test_flock_serialization_with_agents(self, tmp_path):
        """Test Flock serialization with multiple agents."""
        flock = Flock(model="flock_model")
        agent1 = FlockAgent(name="agent_one", input="in1", output="out1")
        agent2 = FlockAgent(
            name="agent_two",
            input="out1", output="out2",
            evaluator=MockEvaluator(config=MockEvalConfig(mock_eval_param="agent2_eval"))
        )
        flock.add_agent(agent1)
        flock.add_agent(agent2)

        file_path = tmp_path / "flock_with_agents.yaml"

        # Act
        flock.to_yaml_file(file_path)
        loaded_flock = Flock.from_yaml_file(file_path)

        # Assert
        assert loaded_flock.model == "flock_model"
        assert len(loaded_flock.agents) == 2
        assert "agent_one" in loaded_flock.agents
        assert "agent_two" in loaded_flock.agents

        loaded_a1 = loaded_flock.agents["agent_one"]
        loaded_a2 = loaded_flock.agents["agent_two"]

        assert isinstance(loaded_a1, FlockAgent)
        assert loaded_a1.name == "agent_one"
        assert loaded_a1.input == "in1"

        assert isinstance(loaded_a2, FlockAgent)
        assert loaded_a2.name == "agent_two"
        assert loaded_a2.input == "out1"
        assert isinstance(loaded_a2.evaluator, MockEvaluator)
        assert loaded_a2.evaluator.config.mock_eval_param == "agent2_eval"


    def test_yaml_dump_options(self, tmp_path):
        """Verify that options can be passed to yaml.dump."""
        agent = FlockAgent(name="dump_options_test")
        file_path = tmp_path / "dump_options.yaml"

        # Act - Use sort_keys=True
        agent.to_yaml_file(file_path, sort_keys=True)
        content = file_path.read_text()

        # Assert - Check if keys are roughly sorted (basic check)
        # Note: Exact order depends on implementation details, this is a basic check
        assert content.startswith("description:") or content.startswith("evaluator:") # description might come first alphabetically
```

### tests\tools\test_zendesk_tools.py

- **Lines**: 29
- **Last modified**: 2025-05-21 19:51:15

```py


from flock.tools.zendesk_tools import zendesk_get_comments_by_ticket_id, zendesk_get_ticket_by_id, zendesk_get_tickets


def test_get_tickets():
    tickets = zendesk_get_tickets()
    assert len(tickets) > 0
    assert tickets[0]["id"] is not None
    assert tickets[0]["subject"] is not None
    
    
def test_get_ticket_by_id():
    ticket = zendesk_get_ticket_by_id("366354")
    assert ticket["id"] is not None
    assert ticket["subject"] is not None
    

def test_get_comments_by_ticket_id():
    comments = zendesk_get_comments_by_ticket_id("366354")
    assert len(comments) > 0
    assert comments[0]["id"] is not None
    assert comments[0]["body"] is not None
    
    




```

